{
  "Netflix": [
    {
      "url": "https://netflixtechblog.com/",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/?gi=dae142431503",
        "loadedTime": "2023-12-06T00:02:38.052Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 0,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/",
        "title": "Netflix TechBlog",
        "description": "Learn about Netflix’s world class engineering efforts, company culture, product developments and more.",
        "author": null,
        "keywords": "NETFLIX, ENGINEERING, CULTURE, TECHNOLOGY, PRODUCT",
        "languageCode": null
      },
      "screenshotUrl": null,
      "text": "Latest\nDetecting Speech and Music in Audio Content\nDetecting Speech and Music in Audio Content\nIroro Orife, Chih-Wei Wu and Yun-Ning (Amy) Hung\nNetflix Technology Blog\nNov 13\nThe Next Step in Personalization: Dynamic Sizzles\nThe Next Step in Personalization: Dynamic Sizzles\nAuthors:Bruce Wobbe, Leticia Kwok\nNetflix Technology Blog\nNov 8\nBuilding In-Video Search\nBuilding In-Video Search\nEmpowering video editors with multimodal machine learning to discover perfect moments across the entire Netflix catalog\nNetflix Technology Blog\nNov 6\nStreaming SQL in Data Mesh\nStreaming SQL in Data Mesh\nDemocratizing Stream Processing @ Netflix\nNetflix Technology Blog\nNov 3\nKubernetes And Kernel Panics\nKubernetes And Kernel Panics\nHow Netflix’s Container Platform Connects Linux Kernel Panics to Pods\nNetflix Technology Blog\nOct 27\nZero Configuration Service Mesh with On-Demand Cluster Discovery\nZero Configuration Service Mesh with On-Demand Cluster Discovery\nNetflix’s service mesh adoption: history, motivations, and how we worked with the Envoy community on a feature to streamline mesh adoption\nNetflix Technology Blog\nAug 29\nAVA Discovery View: Surfacing Authentic Moments\nAVA Discovery View: Surfacing Authentic Moments\nBy: Hamid Shahid, Laura Johnson, Tiffany Low\nNetflix Technology Blog\nAug 17\nCurbing Connection Churn in Zuul\nCurbing Connection Churn in Zuul\nNetflix’s Zuul Gateway eliminated tens of millions of connections and reduced almost all connection churn to backends\nNetflix Technology Blog\nAug 16\nDetecting Scene Changes in Audiovisual Content\nDetecting Scene Changes in Audiovisual Content\nAvneesh Saluja, Andy Yao, Hossein Taghavi\nNetflix Technology Blog\nJun 20\nMigrating Netflix to GraphQL Safely\nMigrating Netflix to GraphQL Safely\nBy Jennifer Shin, Tejas Shikhare, Will Emmanuel\nNetflix Technology Blog\nJun 14\nEscrow Buddy: An open-source tool from Netflix for remediation of missing FileVault keys in MDM\nEscrow Buddy: An open-source tool from Netflix for remediation of missing FileVault keys in MDM\nNetflix has open-sourced Escrow Buddy, which helps Security and IT teams ensure they have valid FileVault recovery keys for all their Macs…\nNetflix Technology Blog\nJun 12\nNative Frame Rate Playback\nNative Frame Rate Playback\nThis article talks about a novel HDMI technology and how it is used within the Netflix Application to improve a user’s experience.\nNetflix Technology Blog\nJun 5\nEnsuring the Successful Launch of Ads on Netflix\nEnsuring the Successful Launch of Ads on Netflix\nBy Jose Fernandez, Ed Barker, Hank Jacobs\nNetflix Technology Blog\nJun 1\nMigrating Critical Traffic At Scale with No Downtime — Part 2\nMigrating Critical Traffic At Scale with No Downtime — Part 2\nShyam Gala, Javier Fernandez-Ivern, Anup Rokkam Pratap, Devang Shah\nNetflix Technology Blog\nMay 23\nDebugging a FUSE deadlock in the Linux kernel\nTycho Andersen\nNetflix Technology Blog\nMay 19\nABAC on SpiceDB: Enabling Netflix’s Complex Identity Types\nABAC on SpiceDB: Enabling Netflix’s Complex Identity Types\nBy Chris Wolfe, Joey Schorr, and Victor Roldán Betancort\nNetflix Technology Blog\nMay 19\nMigrating Critical Traffic At Scale with No Downtime — Part 1\nMigrating Critical Traffic At Scale with No Downtime — Part 1\nShyam Gala, Javier Fernandez-Ivern, Anup Rokkam Pratap, Devang Shah\nNetflix Technology Blog\nMay 4\nImproved Alerting with Atlas Streaming Eval\nImproved Alerting with Atlas Streaming Eval\nRuchir Jha, Brian Harrington, Yingwu Zhao\nNetflix Technology Blog\nApr 27\nBuilding a Media Understanding Platform for ML Innovations\nBuilding a Media Understanding Platform for ML Innovations\nThe media understanding platform serves as an abstraction layer between Machine Learning (ML) algos and various applications.\nNetflix Technology Blog\nMar 14\nData ingestion pipeline with Operation Management\nData ingestion pipeline with Operation Management\nby Varun Sekhri, Meenakshi Jindal, Burak Bacioglu\nNetflix Technology Blog\nMar 2\nScaling Media Machine Learning at Netflix\nScaling Media Machine Learning at Netflix\nWe tackle some of the unique challenges of scaling multimodal machine learning models that operate on media assets (video, audio, and…\nNetflix Technology Blog\nFeb 13\nDiscovering Creative Insights in Promotional Artwork\nDiscovering Creative Insights in Promotional Artwork\nBy Grace Tang, Aneesh Vartakavi, Julija Bagdonaite, Cristina Segalin, and Vi Iyengar\nNetflix Technology Blog\nJan 30\nScalable Annotation Service — Marken\nScalable Annotation Service — Marken\nNetflix Technology Blog\nJan 25\nCausal Machine Learning for Creative Insights\nCausal Machine Learning for Creative Insights\nA framework to identify the causal impact of successful visual components.\nNetflix Technology Blog\nJan 11",
      "markdown": "Latest\n\n[Detecting Speech and Music in Audio Content](https://netflixtechblog.com/detecting-speech-and-music-in-audio-content-afd64e6a5bf8?source=collection_home---4------0-----------------------)\n\n[\n\n### \n\nDetecting Speech and Music in Audio Content\n\nIroro Orife, Chih-Wei Wu and Yun-Ning (Amy) Hung\n\n\n\n](https://netflixtechblog.com/detecting-speech-and-music-in-audio-content-afd64e6a5bf8?source=collection_home---4------0-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nNov 13\n\n[The Next Step in Personalization: Dynamic Sizzles](https://netflixtechblog.com/the-next-step-in-personalization-dynamic-sizzles-4dc4ce2011ef?source=collection_home---4------1-----------------------)\n\n[\n\n### \n\nThe Next Step in Personalization: Dynamic Sizzles\n\nAuthors:Bruce Wobbe, Leticia Kwok\n\n\n\n](https://netflixtechblog.com/the-next-step-in-personalization-dynamic-sizzles-4dc4ce2011ef?source=collection_home---4------1-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nNov 8\n\n[Building In-Video Search](https://netflixtechblog.com/building-in-video-search-936766f0017c?source=collection_home---4------2-----------------------)\n\n[\n\n### \n\nBuilding In-Video Search\n\nEmpowering video editors with multimodal machine learning to discover perfect moments across the entire Netflix catalog\n\n\n\n](https://netflixtechblog.com/building-in-video-search-936766f0017c?source=collection_home---4------2-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nNov 6\n\n[Streaming SQL in Data Mesh](https://netflixtechblog.com/streaming-sql-in-data-mesh-0d83f5a00d08?source=collection_home---4------3-----------------------)\n\n[\n\n### \n\nStreaming SQL in Data Mesh\n\nDemocratizing Stream Processing @ Netflix\n\n\n\n](https://netflixtechblog.com/streaming-sql-in-data-mesh-0d83f5a00d08?source=collection_home---4------3-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nNov 3\n\n[Kubernetes And Kernel Panics](https://netflixtechblog.com/kubernetes-and-kernel-panics-ed620b9c6225?source=collection_home---4------4-----------------------)\n\n[\n\n### \n\nKubernetes And Kernel Panics\n\nHow Netflix’s Container Platform Connects Linux Kernel Panics to Pods\n\n\n\n](https://netflixtechblog.com/kubernetes-and-kernel-panics-ed620b9c6225?source=collection_home---4------4-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nOct 27\n\n[Zero Configuration Service Mesh with On-Demand Cluster Discovery](https://netflixtechblog.com/zero-configuration-service-mesh-with-on-demand-cluster-discovery-ac6483b52a51?source=collection_home---4------5-----------------------)\n\n[\n\n### \n\nZero Configuration Service Mesh with On-Demand Cluster Discovery\n\nNetflix’s service mesh adoption: history, motivations, and how we worked with the Envoy community on a feature to streamline mesh adoption\n\n\n\n](https://netflixtechblog.com/zero-configuration-service-mesh-with-on-demand-cluster-discovery-ac6483b52a51?source=collection_home---4------5-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nAug 29\n\n[AVA Discovery View: Surfacing Authentic Moments](https://netflixtechblog.com/ava-discovery-view-surfacing-authentic-moments-b8cd145491cc?source=collection_home---4------6-----------------------)\n\n[\n\n### \n\nAVA Discovery View: Surfacing Authentic Moments\n\nBy: Hamid Shahid, Laura Johnson, Tiffany Low\n\n\n\n](https://netflixtechblog.com/ava-discovery-view-surfacing-authentic-moments-b8cd145491cc?source=collection_home---4------6-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nAug 17\n\n[Curbing Connection Churn in Zuul](https://netflixtechblog.com/curbing-connection-churn-in-zuul-2feb273a3598?source=collection_home---4------7-----------------------)\n\n[\n\n### \n\nCurbing Connection Churn in Zuul\n\nNetflix’s Zuul Gateway eliminated tens of millions of connections and reduced almost all connection churn to backends\n\n\n\n](https://netflixtechblog.com/curbing-connection-churn-in-zuul-2feb273a3598?source=collection_home---4------7-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nAug 16\n\n[Detecting Scene Changes in Audiovisual Content](https://netflixtechblog.com/detecting-scene-changes-in-audiovisual-content-77a61d3eaad6?source=collection_home---4------8-----------------------)\n\n[\n\n### \n\nDetecting Scene Changes in Audiovisual Content\n\nAvneesh Saluja, Andy Yao, Hossein Taghavi\n\n\n\n](https://netflixtechblog.com/detecting-scene-changes-in-audiovisual-content-77a61d3eaad6?source=collection_home---4------8-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nJun 20\n\n[Migrating Netflix to GraphQL Safely](https://netflixtechblog.com/migrating-netflix-to-graphql-safely-8e1e4d4f1e72?source=collection_home---4------9-----------------------)\n\n[\n\n### \n\nMigrating Netflix to GraphQL Safely\n\nBy Jennifer Shin, Tejas Shikhare, Will Emmanuel\n\n\n\n](https://netflixtechblog.com/migrating-netflix-to-graphql-safely-8e1e4d4f1e72?source=collection_home---4------9-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nJun 14\n\n[Escrow Buddy: An open-source tool from Netflix for remediation of missing FileVault keys in MDM](https://netflixtechblog.com/escrow-buddy-an-open-source-tool-from-netflix-for-remediation-of-missing-filevault-keys-in-mdm-815aef5107cd?source=collection_home---4------10-----------------------)\n\n[\n\n### \n\nEscrow Buddy: An open-source tool from Netflix for remediation of missing FileVault keys in MDM\n\nNetflix has open-sourced Escrow Buddy, which helps Security and IT teams ensure they have valid FileVault recovery keys for all their Macs…\n\n\n\n](https://netflixtechblog.com/escrow-buddy-an-open-source-tool-from-netflix-for-remediation-of-missing-filevault-keys-in-mdm-815aef5107cd?source=collection_home---4------10-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nJun 12\n\n[Native Frame Rate Playback](https://netflixtechblog.com/native-frame-rate-playback-6c87836a948?source=collection_home---4------11-----------------------)\n\n[\n\n### \n\nNative Frame Rate Playback\n\nThis article talks about a novel HDMI technology and how it is used within the Netflix Application to improve a user’s experience.\n\n\n\n](https://netflixtechblog.com/native-frame-rate-playback-6c87836a948?source=collection_home---4------11-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nJun 5\n\n[Ensuring the Successful Launch of Ads on Netflix](https://netflixtechblog.com/ensuring-the-successful-launch-of-ads-on-netflix-f99490fdf1ba?source=collection_home---4------12-----------------------)\n\n[\n\n### \n\nEnsuring the Successful Launch of Ads on Netflix\n\nBy Jose Fernandez, Ed Barker, Hank Jacobs\n\n\n\n](https://netflixtechblog.com/ensuring-the-successful-launch-of-ads-on-netflix-f99490fdf1ba?source=collection_home---4------12-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nJun 1\n\n[Migrating Critical Traffic At Scale with No Downtime — Part 2](https://netflixtechblog.com/migrating-critical-traffic-at-scale-with-no-downtime-part-2-4b1c8c7155c1?source=collection_home---4------13-----------------------)\n\n[\n\n### \n\nMigrating Critical Traffic At Scale with No Downtime — Part 2\n\nShyam Gala, Javier Fernandez-Ivern, Anup Rokkam Pratap, Devang Shah\n\n\n\n](https://netflixtechblog.com/migrating-critical-traffic-at-scale-with-no-downtime-part-2-4b1c8c7155c1?source=collection_home---4------13-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nMay 23\n\n[\n\n### \n\nDebugging a FUSE deadlock in the Linux kernel\n\nTycho Andersen\n\n\n\n](https://netflixtechblog.com/debugging-a-fuse-deadlock-in-the-linux-kernel-c75cd7989b6d?source=collection_home---4------14-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nMay 19\n\n[ABAC on SpiceDB: Enabling Netflix’s Complex Identity Types](https://netflixtechblog.com/abac-on-spicedb-enabling-netflixs-complex-identity-types-c118f374fa89?source=collection_home---4------15-----------------------)\n\n[\n\n### \n\nABAC on SpiceDB: Enabling Netflix’s Complex Identity Types\n\nBy Chris Wolfe, Joey Schorr, and Victor Roldán Betancort\n\n\n\n](https://netflixtechblog.com/abac-on-spicedb-enabling-netflixs-complex-identity-types-c118f374fa89?source=collection_home---4------15-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nMay 19\n\n[Migrating Critical Traffic At Scale with No Downtime — Part 1](https://netflixtechblog.com/migrating-critical-traffic-at-scale-with-no-downtime-part-1-ba1c7a1c7835?source=collection_home---4------16-----------------------)\n\n[\n\n### \n\nMigrating Critical Traffic At Scale with No Downtime — Part 1\n\nShyam Gala, Javier Fernandez-Ivern, Anup Rokkam Pratap, Devang Shah\n\n\n\n](https://netflixtechblog.com/migrating-critical-traffic-at-scale-with-no-downtime-part-1-ba1c7a1c7835?source=collection_home---4------16-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nMay 4\n\n[Improved Alerting with Atlas Streaming Eval](https://netflixtechblog.com/improved-alerting-with-atlas-streaming-eval-e691c60dc61e?source=collection_home---4------17-----------------------)\n\n[\n\n### \n\nImproved Alerting with Atlas Streaming Eval\n\nRuchir Jha, Brian Harrington, Yingwu Zhao\n\n\n\n](https://netflixtechblog.com/improved-alerting-with-atlas-streaming-eval-e691c60dc61e?source=collection_home---4------17-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nApr 27\n\n[Building a Media Understanding Platform for ML Innovations](https://netflixtechblog.com/building-a-media-understanding-platform-for-ml-innovations-9bef9962dcb7?source=collection_home---4------18-----------------------)\n\n[\n\n### \n\nBuilding a Media Understanding Platform for ML Innovations\n\nThe media understanding platform serves as an abstraction layer between Machine Learning (ML) algos and various applications.\n\n\n\n](https://netflixtechblog.com/building-a-media-understanding-platform-for-ml-innovations-9bef9962dcb7?source=collection_home---4------18-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nMar 14\n\n[Data ingestion pipeline with Operation Management](https://netflixtechblog.com/data-ingestion-pipeline-with-operation-management-3c5c638740a8?source=collection_home---4------19-----------------------)\n\n[\n\n### \n\nData ingestion pipeline with Operation Management\n\nby Varun Sekhri, Meenakshi Jindal, Burak Bacioglu\n\n\n\n](https://netflixtechblog.com/data-ingestion-pipeline-with-operation-management-3c5c638740a8?source=collection_home---4------19-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nMar 2\n\n[Scaling Media Machine Learning at Netflix](https://netflixtechblog.com/scaling-media-machine-learning-at-netflix-f19b400243?source=collection_home---4------20-----------------------)\n\n[\n\n### \n\nScaling Media Machine Learning at Netflix\n\nWe tackle some of the unique challenges of scaling multimodal machine learning models that operate on media assets (video, audio, and…\n\n\n\n](https://netflixtechblog.com/scaling-media-machine-learning-at-netflix-f19b400243?source=collection_home---4------20-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nFeb 13\n\n[Discovering Creative Insights in Promotional Artwork](https://netflixtechblog.com/discovering-creative-insights-in-promotional-artwork-295e4d788db5?source=collection_home---4------21-----------------------)\n\n[\n\n### \n\nDiscovering Creative Insights in Promotional Artwork\n\nBy Grace Tang, Aneesh Vartakavi, Julija Bagdonaite, Cristina Segalin, and Vi Iyengar\n\n\n\n](https://netflixtechblog.com/discovering-creative-insights-in-promotional-artwork-295e4d788db5?source=collection_home---4------21-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nJan 30\n\n[Scalable Annotation Service — Marken](https://netflixtechblog.com/scalable-annotation-service-marken-f5ba9266d428?source=collection_home---4------22-----------------------)\n\n[\n\n### \n\nScalable Annotation Service — Marken\n\n\n\n](https://netflixtechblog.com/scalable-annotation-service-marken-f5ba9266d428?source=collection_home---4------22-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nJan 25\n\n[Causal Machine Learning for Creative Insights](https://netflixtechblog.com/causal-machine-learning-for-creative-insights-4b0ce22a8a96?source=collection_home---4------23-----------------------)\n\n[\n\n### \n\nCausal Machine Learning for Creative Insights\n\nA framework to identify the causal impact of successful visual components.\n\n\n\n](https://netflixtechblog.com/causal-machine-learning-for-creative-insights-4b0ce22a8a96?source=collection_home---4------23-----------------------)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog)\n\nJan 11"
    },
    {
      "url": "https://netflixtechblog.com/osd.xml",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/osd.xml?gi=43f9eb78b7b6",
        "loadedTime": "2023-12-06T00:02:47.918Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/osd.xml?gi=43f9eb78b7b6",
        "title": "",
        "description": null,
        "author": null,
        "keywords": null,
        "languageCode": null
      },
      "screenshotUrl": null,
      "text": "Netflix TechBloghttps://miro.medium.com/1*m-R_BkNf1Qjr1YbyOIJY2w.pngUTF-8UTF-8https://netflixtechblog.com",
      "markdown": "Netflix TechBloghttps://miro.medium.com/1\\*m-R\\_BkNf1Qjr1YbyOIJY2w.pngUTF-8UTF-8https://netflixtechblog.com"
    },
    {
      "url": "https://netflixtechblog.com/@netflixtechblog",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.medium.com/",
        "loadedTime": "2023-12-06T00:02:56.575Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.medium.com/",
        "title": "Netflix Technology Blog – Medium",
        "description": "Read writing from Netflix Technology Blog on Medium. Learn more about how Netflix designs, builds, and operates our systems and engineering organizations.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Building In-Video Search\nBoris Chen, Ben Klein, Jason Ge, Avneesh Saluja, Guru Tahasildar, Abhishek Soni, Juan Vimberg, Gustavo Carmo, Meenakshi Jindal, Elliot Chow, Amir Ziai, Varun Sekhri, Santiago Castro, Keila Fong, Kelli Griggs, Mallia Sherzai, Robert Mayer, Andy Yao, Vi Iyengar, Jonathan Solorzano-Hamilton, Hossein Taghavi, Ritwik Kumar Introduction Today we’re going to take a…",
      "markdown": "[\n\n## Building In-Video Search\n\nBoris Chen, Ben Klein, Jason Ge, Avneesh Saluja, Guru Tahasildar, Abhishek Soni, Juan Vimberg, Gustavo Carmo, Meenakshi Jindal, Elliot Chow, Amir Ziai, Varun Sekhri, Santiago Castro, Keila Fong, Kelli Griggs, Mallia Sherzai, Robert Mayer, Andy Yao, Vi Iyengar, Jonathan Solorzano-Hamilton, Hossein Taghavi, Ritwik Kumar Introduction Today we’re going to take a…\n\n](https://netflixtechblog.com/building-in-video-search-936766f0017c?source=user_profile---------8----------------------------)"
    },
    {
      "url": "https://netflixtechblog.com/all-of-netflixs-hdr-video-streaming-is-now-dynamically-optimized-e9e0cb15f2ba?source=collection_home---4------0-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/all-of-netflixs-hdr-video-streaming-is-now-dynamically-optimized-e9e0cb15f2ba?gi=133d8fd0ddb8&source=collection_home---4------0-----------------------",
        "loadedTime": "2023-12-06T00:02:56.162Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/all-of-netflixs-hdr-video-streaming-is-now-dynamically-optimized-e9e0cb15f2ba",
        "title": "All of Netflix’s HDR video streaming is now dynamically optimized | by Netflix Technology Blog | Nov, 2023 | Netflix TechBlog",
        "description": "High dynamic range (HDR) video brings a wider range of luminance and a wider gamut of colors, paving the way for a stunning viewing experience. Separately, our invention of Dynamically Optimized (DO)…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "All of Netflix’s HDR video streaming is now dynamically optimized\nby Aditya Mavlankar, Zhi Li, Lukáš Krasula and Christos Bampis\nHigh dynamic range (HDR) video brings a wider range of luminance and a wider gamut of colors, paving the way for a stunning viewing experience. Separately, our invention of Dynamically Optimized (DO) encoding helps achieve optimized bitrate-quality tradeoffs depending on the complexity of the content.\nHDR was launched at Netflix in 2016 and the number of titles available in HDR has been growing ever since. We were, however, missing the systematic ability to measure perceptual quality (VMAF) of HDR streams since VMAF was limited to standard dynamic range (SDR) video signals.\nAs noted in an earlier blog post, we began developing an HDR variant of VMAF; let’s call it HDR-VMAF. A vital aspect of such development is subjective testing with HDR encodes in order to generate training data. The pandemic, however, posed unique challenges in conducting a conventional in-lab subjective test with HDR encodes. We improvised as part of a collaborative effort with Dolby Laboratories and conducted subjective tests with 4K-HDR content using high-end OLED panels in calibrated conditions created in participants’ homes [1],[2]. Details pertaining to HDR-VMAF exceed the scope of this article and will be covered in a future blog post; for now, suffice it to say that the first version of HDR-VMAF landed internally in 2021 and we have been improving the metric ever since.\nThe arrival of HDR-VMAF allowed us to create HDR streams with DO applied, i.e., HDR-DO encodes. Prior to that, we were using a fixed ladder with predetermined bitrates — regardless of content characteristics — for HDR video streaming. We A/B tested HDR-DO encodes in production in Q3-Q4 2021, followed by improving the ladder generation algorithm further in early 2022. We started backfilling HDR-DO encodes for existing titles from Q2 2022. By June 2023 the entire HDR catalog was optimized. The graphic below (Fig. 1) depicts the migration of traffic from fixed bitrates to DO encodes.\nFig. 1: Migration of traffic from fixed-ladder encodes to DO encodes.\nBitrate versus quality comparison\nHDR-VMAF is designed to be format-agnostic — it measures the perceptual quality of HDR video signal regardless of its container format, for example, Dolby Vision or HDR10. HDR-VMAF focuses on the signal characteristics (as a result of lossy encoding) instead of display characteristics, and thus it does not include display mapping in its pipeline. Display mapping is the specific tone mapping applied by the display based on its own characteristics — peak luminance, black level, color gamut, etc. — and based on content characteristics and/or metadata signaled in the bitstream.\nTwo ways that HDR10 and Dolby Vision differ are: 1) the preprocessing applied to the signal before encoding 2) the metadata informing the display mapping on different displays. So, HDR-VMAF will capture the effect of 1) but ignore the effect of 2). Display capabilities vary a lot among the heterogeneous population of devices that stream HDR content — this aspect is similar to other factors that vary session to session such as ambient lighting, viewing distance, upscaling algorithm on the device, etc. “VMAF not incorporating display mapping” implies the scores are computed for an “ideal display” that’s capable of representing the entire luminance range and the entire color gamut spanned by the video signal — thus not requiring display mapping. This background is useful to have before looking at rate vs quality curves pertaining to these two formats.\nShown below are rate versus quality examples for a couple of titles from our HDR catalog. We present two sets. Within each set we show curves for both Dolby Vision and HDR10. The first set (Fig. 2) corresponds to an episode from a gourmet cooking show incorporating fast-paced scenes from around the world. The second set (Fig. 3) corresponds to an episode from a relatively slower drama series; slower in terms of camera action. The optimized encodes are chosen from the convex hull formed by various rate-quality points corresponding to different bitrates, spatial resolutions and encoding recipes.\nFor brevity we skipped annotating ladder points with their spatial resolutions but the overall observations from our previous article on SDR-4K encode optimization apply here as well. The fixed ladder is slow in ramping up spatial resolution, so the quality stays almost flat among two successive 1080p points or two successive 4K points. On the other hand, the optimized ladder presents a sharper increase in quality with increasing bitrate.\nThe fixed ladder has predetermined 4K bitrates — 8, 10, 12 and 16 Mbps — it deterministically maxes out at 16 Mbps. On the other hand, the optimized ladder targets very high levels of quality on the top rung of the bitrate ladder, even at the cost of higher bitrates if the content is complex, thereby satisfying the most discerning viewers. In spite of reaching higher qualities than the fixed ladder, the HDR-DO ladder, on average, occupies only 58% of the storage space compared to fixed-bitrate ladder. This is achieved by more efficiently spacing the ladder points, especially in the high-bitrate region. After all, there is little to no benefit in packing multiple high-bitrate points so close to each other — for example, 3 QHD (2560x1440) points placed in the 6 to 7.5 Mbps range followed by the four 4K points at 8, 10, 12 and 16 Mbps, as was done on the fixed ladder.\nFig. 2: Rate-quality curves comparing fixed and optimized ladders corresponding to an episode from a gourmet cooking show incorporating fast-paced scenes from around the world.Fig. 3: Rate-quality curves comparing fixed and optimized ladders corresponding to an episode from a drama series, which is slower in terms of camera action.\nIt is important to note that the fixed-ladder encodes had constant duration group-of-pictures (GoPs) and suffered from some inefficiency due to shot boundaries not aligning with Instantaneous Decoder Refresh (IDR) frames. The DO encodes are shot-based and so the IDR frames align with shot boundaries. For a given rate-quality operating point, the DO process helps allocate bits among the various shots while maximizing an overall objective function. Also thanks to the DO framework, within a given rate-quality operating point, challenging shots can and do burst in bitrate up to the codec level limit associated with that point.\nMember benefits\nWe A/B tested the fixed and optimized ladders; first and foremost to make sure that devices in the field can handle the new streams and serving new streams doesn’t cause unintended playback issues. A/B testing also allows us to get a read on the improvement in quality of experience (QoE). Overall, the improvements can be summarized as:\n40% fewer rebuffers\nHigher video quality for both bandwidth-constrained as well as unconstrained sessions\nLower initial bitrate\nHigher initial quality\nLower play delay\nLess variation in delivered video quality\nLower Internet data usage, especially on mobiles and tablets\nWill HDR-VMAF be open-source?\nYes, we are committed to supporting the open-source community. The current implementation, however, is largely tailored to our internal pipelines. We are working to ensure it is versatile, stable, and easy-to-use for the community. Additionally, the current version has some algorithmic limitations that we are in the process of improving before the official release. When we do release it, HDR-VMAF will have higher accuracy in perceptual quality prediction, and be easier to use “out of the box”.\nSummary\nThanks to the arrival of HDR-VMAF, we were able to optimize our HDR encodes. Fixed-ladder HDR encodes have been fully replaced by optimized ones, reducing storage footprint and Internet data usage — and most importantly, improving the video quality for our members. Improvements have been seen across all device categories ranging from TVs to mobiles and tablets.\nAcknowledgments\nWe thank all the volunteers who participated in the subjective experiments. We also want to acknowledge the contributions of our colleagues from Dolby, namely Anustup Kumar Choudhury, Scott Daly, Robin Atkins, Ludovic Malfait, and Suzanne Farrell, who helped with preparations and conducting of the subjective tests.\nWe thank Matthew Donato, Adithya Prakash, Rich Gerber, Joe Drago, Benbuck Nason and Joseph McCormick for all the interesting discussions on HDR video.\nWe thank various internal teams at Netflix for the crucial roles they play:\nThe various client device and UI engineering teams at Netflix that manage the Netflix experience on various device platforms\nThe data science and engineering teams at Netflix that help us run and analyze A/B tests; we thank Chris Pham in particular for generating various data insights for the encoding team\nThe Playback Systems team that steers the Netflix experience for every client device including the experience served in various encoding A/B tests\nThe Open Connect team that manages Netflix’s own content delivery network\nThe Content Infrastructure and Solutions team that manages the compute platform that enables us to execute video encoding at scale\nThe Streaming Encoding Pipeline team that helps us orchestrate the generation of various streaming assets\nFind our work interesting? Join us and be a part of the amazing team that brought you this tech-blog; open positions:\nSoftware Engineer, Cloud Gaming\nSoftware Engineer, Live Streaming\nReferences\n[1] L. Krasula, A. Choudhury, S. Daly, Z. Li, R. Atkins, L. Malfait, A. Mavlankar, “Subjective video quality for 4K HDR-WCG content using a browser-based approach for “at-home” testing,” Electronic Imaging, vol. 35, pp. 263–1–8 (2023) [online]\n[2] A. Choudhury, L. Krasula, S. Daly, Z. Li, R. Atkins, L. Malfait, “Testing 4K HDR-WCG professional video content for subjective quality using a remote testing approach,” SMPTE Media Technology Summit 2023",
      "markdown": "## All of Netflix’s HDR video streaming is now dynamically optimized\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----e9e0cb15f2ba--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----e9e0cb15f2ba--------------------------------)\n\nby [Aditya Mavlankar](https://www.linkedin.com/in/aditya-mavlankar-7139791/), [Zhi Li](https://www.linkedin.com/in/henryzhili/), [Lukáš Krasula](https://www.linkedin.com/in/luk%C3%A1%C5%A1-krasula-a0171b6a/) and [Christos Bampis](https://www.linkedin.com/in/christosbampis/)\n\nHigh dynamic range ([HDR](https://developer.apple.com/videos/play/tech-talks/502/)) video brings a wider range of luminance and a wider gamut of colors, paving the way for a stunning viewing experience. Separately, our invention of Dynamically Optimized ([DO](https://netflixtechblog.com/dynamic-optimizer-a-perceptual-video-encoding-optimization-framework-e19f1e3a277f)) encoding helps achieve optimized bitrate-quality tradeoffs depending on the complexity of the content.\n\nHDR was launched at Netflix in 2016 and the number of titles available in HDR has been growing ever since. We were, however, missing the systematic ability to measure perceptual quality ([VMAF](https://netflixtechblog.com/vmaf-the-journey-continues-44b51ee9ed12)) of HDR streams since VMAF was limited to standard dynamic range (SDR) video signals.\n\nAs noted in [an earlier blog post](https://netflixtechblog.com/optimized-shot-based-encodes-for-4k-now-streaming-47b516b10bbb), we began developing an HDR variant of VMAF; let’s call it HDR-VMAF. A vital aspect of such development is subjective testing with HDR encodes in order to generate training data. The pandemic, however, posed unique challenges in conducting a conventional in-lab subjective test with HDR encodes. We improvised as part of a collaborative effort with Dolby Laboratories and conducted subjective tests with 4K-HDR content using high-end OLED panels in calibrated conditions created in participants’ homes \\[1\\],\\[2\\]. Details pertaining to HDR-VMAF exceed the scope of this article and will be covered in a future blog post; for now, suffice it to say that the first version of HDR-VMAF landed internally in 2021 and we have been improving the metric ever since.\n\nThe arrival of HDR-VMAF allowed us to create HDR streams with DO applied, i.e., HDR-DO encodes. Prior to that, we were using a fixed ladder with predetermined bitrates — regardless of content characteristics — for HDR video streaming. We A/B tested HDR-DO encodes in production in Q3-Q4 2021, followed by improving the ladder generation algorithm further in early 2022. We started backfilling HDR-DO encodes for existing titles from Q2 2022. By June 2023 the entire HDR catalog was optimized. The graphic below (Fig. 1) depicts the migration of traffic from fixed bitrates to DO encodes.\n\nFig. 1: Migration of traffic from fixed-ladder encodes to DO encodes.\n\n## Bitrate versus quality comparison\n\nHDR-VMAF is designed to be format-agnostic — it measures the perceptual quality of HDR video signal regardless of its container format, for example, Dolby Vision or HDR10. HDR-VMAF focuses on the signal characteristics (as a result of lossy encoding) instead of display characteristics, and thus it does not include display mapping in its pipeline. Display mapping is the specific tone mapping applied by the display based on its own characteristics — peak luminance, black level, color gamut, etc. — and based on content characteristics and/or metadata signaled in the bitstream.\n\nTwo ways that HDR10 and Dolby Vision differ are: **1)** the preprocessing applied to the signal before encoding **2)** the metadata informing the display mapping on different displays. So, HDR-VMAF will capture the effect of **1)** but ignore the effect of **2)**. Display capabilities vary a lot among the heterogeneous population of devices that stream HDR content — this aspect is similar to other factors that vary session to session such as ambient lighting, viewing distance, upscaling algorithm on the device, etc. “VMAF not incorporating display mapping” implies the scores are computed for an “ideal display” that’s capable of representing the entire luminance range and the entire color gamut spanned by the video signal — thus not requiring display mapping. This background is useful to have before looking at rate vs quality curves pertaining to these two formats.\n\nShown below are rate versus quality examples for a couple of titles from our HDR catalog. We present two sets. Within each set we show curves for both Dolby Vision and HDR10. The first set (Fig. 2) corresponds to an episode from a gourmet cooking show incorporating fast-paced scenes from around the world. The second set (Fig. 3) corresponds to an episode from a relatively slower drama series; slower in terms of camera action. The optimized encodes are chosen from the convex hull formed by various rate-quality points corresponding to different bitrates, spatial resolutions and encoding recipes.\n\nFor brevity we skipped annotating ladder points with their spatial resolutions but the overall observations from our [previous article on SDR-4K encode optimization](https://netflixtechblog.com/optimized-shot-based-encodes-for-4k-now-streaming-47b516b10bbb) apply here as well. The fixed ladder is slow in ramping up spatial resolution, so the quality stays almost flat among two successive 1080p points or two successive 4K points. On the other hand, the optimized ladder presents a sharper increase in quality with increasing bitrate.\n\nThe fixed ladder has predetermined 4K bitrates — 8, 10, 12 and 16 Mbps — it deterministically maxes out at 16 Mbps. On the other hand, the optimized ladder targets very high levels of quality on the top rung of the bitrate ladder, even at the cost of higher bitrates if the content is complex, thereby satisfying the most discerning viewers. In spite of reaching higher qualities than the fixed ladder, the HDR-DO ladder, on average, occupies only 58% of the storage space compared to fixed-bitrate ladder. This is achieved by more efficiently spacing the ladder points, especially in the high-bitrate region. After all, there is little to no benefit in packing multiple high-bitrate points so close to each other — for example, 3 QHD (2560x1440) points placed in the 6 to 7.5 Mbps range followed by the four 4K points at 8, 10, 12 and 16 Mbps, as was done on the fixed ladder.\n\nFig. 2: Rate-quality curves comparing fixed and optimized ladders corresponding to an episode from a gourmet cooking show incorporating fast-paced scenes from around the world.\n\nFig. 3: Rate-quality curves comparing fixed and optimized ladders corresponding to an episode from a drama series, which is slower in terms of camera action.\n\nIt is important to note that the fixed-ladder encodes had constant duration group-of-pictures (GoPs) and suffered from some inefficiency due to shot boundaries not aligning with Instantaneous Decoder Refresh (IDR) frames. The DO encodes are shot-based and so the IDR frames align with shot boundaries. For a given rate-quality operating point, the DO process helps allocate bits among the various shots while maximizing an overall objective function. Also thanks to the DO framework, within a given rate-quality operating point, _challenging shots_ can and do burst in bitrate up to the _codec level limit_ associated with that point.\n\n## Member benefits\n\nWe A/B tested the fixed and optimized ladders; first and foremost to make sure that devices in the field can handle the new streams and serving new streams doesn’t cause unintended playback issues. A/B testing also allows us to get a read on the improvement in quality of experience (QoE). Overall, the improvements can be summarized as:\n\n*   40% fewer rebuffers\n*   Higher video quality for both bandwidth-constrained as well as unconstrained sessions\n*   Lower initial bitrate\n*   Higher initial quality\n*   Lower play delay\n*   Less variation in delivered video quality\n*   Lower Internet data usage, especially on mobiles and tablets\n\n## Will HDR-VMAF be open-source?\n\nYes, we are committed to supporting the open-source community. The current implementation, however, is largely tailored to our internal pipelines. We are working to ensure it is versatile, stable, and easy-to-use for the community. Additionally, the current version has some algorithmic limitations that we are in the process of improving before the official release. When we do release it, HDR-VMAF will have higher accuracy in perceptual quality prediction, and be easier to use “out of the box”.\n\n## Summary\n\nThanks to the arrival of HDR-VMAF, we were able to optimize our HDR encodes. Fixed-ladder HDR encodes have been fully replaced by optimized ones, reducing storage footprint and Internet data usage — and most importantly, improving the video quality for our members. Improvements have been seen across all device categories ranging from TVs to mobiles and tablets.\n\n## Acknowledgments\n\nWe thank all the volunteers who participated in the subjective experiments. We also want to acknowledge the contributions of our colleagues from Dolby, namely Anustup Kumar Choudhury, Scott Daly, Robin Atkins, Ludovic Malfait, and Suzanne Farrell, who helped with preparations and conducting of the subjective tests.\n\nWe thank Matthew Donato, Adithya Prakash, Rich Gerber, Joe Drago, Benbuck Nason and Joseph McCormick for all the interesting discussions on HDR video.\n\nWe thank various internal teams at Netflix for the crucial roles they play:\n\n*   The various [client device and UI engineering](https://jobs.netflix.com/team?slug=client-and-ui-engineering) teams at Netflix that manage the Netflix experience on various device platforms\n*   The [data science and engineering](https://jobs.netflix.com/team?slug=data-science-and-engineering) teams at Netflix that help us run and analyze A/B tests; we thank Chris Pham in particular for generating various data insights for the encoding team\n*   The [Playback Systems](https://www.youtube.com/watch?v=5ju4W9KAzcY) team that steers the Netflix experience for every client device including the experience served in various encoding A/B tests\n*   The [Open Connect](https://openconnect.netflix.com/en/) team that manages Netflix’s own content delivery network\n*   The Content Infrastructure and Solutions team that manages the [compute platform](https://netflixtechblog.com/the-netflix-cosmos-platform-35c14d9351ad) that enables us to execute video encoding at scale\n*   The Streaming Encoding Pipeline team that helps us orchestrate the generation of various streaming assets\n\n_Find our work interesting? Join us and be a part of the amazing team that brought you this tech-blog; open positions:_\n\n*   [_Software Engineer, Cloud Gaming_](https://jobs.netflix.com/jobs/305482718)\n*   [_Software Engineer, Live Streaming_](https://jobs.netflix.com/jobs/296600425)\n\n## References\n\n**\\[1\\]** L. Krasula, A. Choudhury, S. Daly, Z. Li, R. Atkins, L. Malfait, A. Mavlankar, “Subjective video quality for 4K HDR-WCG content using a browser-based approach for “at-home” testing,” Electronic Imaging, vol. 35, pp. 263–1–8 (2023) \\[[online](https://library.imaging.org/admin/apis/public/api/ist/website/downloadArticle/ei/35/8/IQSP-263)\\]  \n**\\[2\\]** A. Choudhury, L. Krasula, S. Daly, Z. Li, R. Atkins, L. Malfait, “Testing 4K HDR-WCG professional video content for subjective quality using a remote testing approach,” SMPTE Media Technology Summit 2023"
    },
    {
      "url": "https://netflixtechblog.com/netflix-original-research-mit-code-2023-9340b879176a?source=collection_home---4------0-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/netflix-original-research-mit-code-2023-9340b879176a?gi=1685dd141134&source=collection_home---4------0-----------------------",
        "loadedTime": "2023-12-06T00:03:01.140Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/netflix-original-research-mit-code-2023-9340b879176a",
        "title": "Netflix Original Research: MIT CODE 2023 | by Netflix Technology Blog | Nov, 2023 | Netflix TechBlog",
        "description": "Netflix was thrilled to be the premier sponsor for the 2nd year in a row at the 2023 Conference on Digital Experimentation (CODE@MIT) in Cambridge, MA. The conference features a balanced blend of…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Netflix was thrilled to be the premier sponsor for the 2nd year in a row at the 2023 Conference on Digital Experimentation (CODE@MIT) in Cambridge, MA. The conference features a balanced blend of academic and industry research from some wicked smart folks, and we’re proud to have contributed a number of talks and posters along with a plenary session.\nOur contributions kicked off with a concept that is crucial to our understanding of A/B tests: surrogates!\nOur first talk was given by Aurelien Bibaut (with co-authors Nathan Kallus, Simon Ejdemyr and Michael Zhao) in which we discussed how to confidently measure long-term outcomes using short term surrogates in the presence of bias. For example, how do we estimate the effects of innovations on retention a year later without running all our experiments for a year? We proposed an estimation method using cross-fold procedures, and construct valid confidence intervals for long term effects before that effect is fully observed.\nLater on, Michael Zhao (with Vickie Zhang, Anh Le and Nathan Kallus) spoke about the evaluation of surrogate index models for product decision making. Using 200 real A/B tests performed at Netflix, we showed that surrogate-index models, constructed using only 2 weeks of data, lead to the same product ship decisions ~95% of the time when compared to making a call based on 2 months of data. This means we can reliably run shorter tests with confidence without needing to wait months for results!\nOur next topic focused on how to understand and balance competing engagement metrics; for example, should 1 hour of gaming equal 1 hour of streaming? Michael Zhao and Jordan Schafer shared a poster on how they built an Overall Evaluation Criterion (OEC) metric that provides holistic evaluation for A/B tests, appropriately weighting different engagement metrics to serve a single overall objective. This new framework has enabled fast and confident decision making in tests, and is being actively adapted as our business continues to expand into new areas.\nIn the second plenary session of the day, Martin Tingley took us on a compelling and fun journey of complexity, exploring key challenges in digital experimentation and how they differ from the challenges faced by agricultural researchers a century ago. He highlighted different areas of complexity and provided perspectives on how to tackle the right challenges based on business objectives.\nOur final talk was given by Apoorva Lal (with co-authors Samir Khan and Johan Ugander) in which we show how partial identification of the dose-response function (DRF) under non-parametric assumptions can be used to provide more insightful analyses of experimental data than the standard ATE analysis does. We revisited a study that reduced like-minded content algorithmically, and showed how we could extend the binary ATE learning to answer how the amount of like-minded content a user sees affects their political attitudes.\nWe had a blast connecting with the CODE@MIT community and bonding over our shared enthusiasm for not only rigorous measurement in experimentation, but also stats-themed stickers and swag!\nOne of our stickers this year, can you guess what this is showing?!\nWe look forward to next year’s iteration of the conference and hope to see you there!\nPsst! We’re hiring Data Scientists across a variety of domains at Netflix — check out our open roles.",
      "markdown": "[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----9340b879176a--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----9340b879176a--------------------------------)\n\nNetflix was thrilled to be the premier sponsor for the 2nd year in a row at the [2023 Conference on Digital Experimentation](https://ide.mit.edu/events/2023-conference-on-digital-experimentation-mit-codemit/) (CODE@MIT) in Cambridge, MA. The conference features a balanced blend of academic and industry research from some _wicked smart_ folks, and we’re proud to have contributed a number of talks and posters along with a plenary session.\n\nOur contributions kicked off with a concept that is crucial to our understanding of A/B tests: surrogates!\n\nOur first talk was given by [Aurelien Bibaut](https://www.linkedin.com/in/aurelien-bibaut/) (with co-authors [Nathan Kallus](https://www.linkedin.com/in/kallus/), [Simon Ejdemyr](https://www.linkedin.com/in/simon-ejdemyr-22b920123/) and [Michael Zhao](https://www.linkedin.com/in/mfzhao/)) in which we discussed how to confidently measure long-term outcomes using short term surrogates in the presence of bias. For example, how do we estimate the effects of innovations on retention a year later without running all our experiments for a year? We proposed an estimation method using cross-fold procedures, and construct valid confidence intervals for long term effects before that effect is fully observed.\n\nLater on, [Michael Zhao](https://www.linkedin.com/in/mfzhao/) (with [Vickie Zhang](https://www.linkedin.com/in/zhangvickie/), [Anh Le](https://www.linkedin.com/in/anhqle/) and [Nathan Kallus](https://www.linkedin.com/in/kallus/)) spoke about the evaluation of surrogate index models for product decision making. Using 200 real A/B tests performed at Netflix, we showed that surrogate-index models, constructed using only 2 weeks of data, lead to the same product ship decisions ~95% of the time when compared to making a call based on 2 months of data. This means we can reliably run shorter tests with confidence without needing to wait months for results!\n\nOur next topic focused on how to understand and balance competing engagement metrics; for example, should 1 hour of gaming equal 1 hour of streaming? [Michael Zhao](https://www.linkedin.com/in/mfzhao/) and [Jordan Schafer](https://www.linkedin.com/in/jjschafer/) shared a poster on how they built an Overall Evaluation Criterion (OEC) metric that provides holistic evaluation for A/B tests, appropriately weighting different engagement metrics to serve a single overall objective. This new framework has enabled fast and confident decision making in tests, and is being actively adapted as our business continues to expand into new areas.\n\nIn the second plenary session of the day, [Martin Tingley](https://www.linkedin.com/in/martintingley/) took us on a compelling and fun journey of complexity, exploring key challenges in digital experimentation and how they differ from the challenges faced by agricultural researchers a century ago. He highlighted different areas of complexity and provided perspectives on how to tackle the right challenges based on business objectives.\n\nOur final talk was given by [Apoorva Lal](https://www.linkedin.com/in/apoorvalal/) (with co-authors [Samir Khan](https://www.linkedin.com/in/samir-khan-9536a9175/) and [Johan Ugander](https://www.linkedin.com/in/jugander/)) in which we show how partial identification of the dose-response function (DRF) under non-parametric assumptions can be used to provide more insightful analyses of experimental data than the standard ATE analysis does. We revisited a study that reduced like-minded content algorithmically, and showed how we could extend the binary ATE learning to answer how the amount of like-minded content a user sees affects their political attitudes.\n\nWe had a blast connecting with the CODE@MIT community and bonding over our shared enthusiasm for not only rigorous measurement in experimentation, but also stats-themed stickers and swag!\n\n_One of our stickers this year, can you guess what this is showing?!_\n\nWe look forward to next year’s iteration of the conference and hope to see you there!\n\n_Psst! We’re hiring Data Scientists across a variety of domains at Netflix — check out our_ [_open roles._](https://jobs.netflix.com/search?q=data+scientist)"
    },
    {
      "url": "https://netflixtechblog.com/incremental-processing-using-netflix-maestro-and-apache-iceberg-b8ba072ddeeb?source=collection_home---4------1-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/incremental-processing-using-netflix-maestro-and-apache-iceberg-b8ba072ddeeb?gi=435f2f8f7077&source=collection_home---4------1-----------------------",
        "loadedTime": "2023-12-06T00:03:04.184Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/incremental-processing-using-netflix-maestro-and-apache-iceberg-b8ba072ddeeb",
        "title": "Incremental Processing using Netflix Maestro and Apache Iceberg | by Netflix Technology Blog | Nov, 2023 | Netflix TechBlog",
        "description": "Incremental processing is an approach to process new or changed data in workflows. The key advantage is that it only incrementally processes data that are newly added or updated to a dataset, instead…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Incremental Processing using Netflix Maestro and Apache Iceberg\nby Jun He, Yingyi Zhang, and Pawan Dixit\nIncremental processing is an approach to process new or changed data in workflows. The key advantage is that it only incrementally processes data that are newly added or updated to a dataset, instead of re-processing the complete dataset. This not only reduces the cost of compute resources but also reduces the execution time in a significant manner. When workflow execution has a shorter duration, chances of failure and manual intervention reduce. It also improves the engineering productivity by simplifying the existing pipelines and unlocking the new patterns.\nIn this blog post, we talk about the landscape and the challenges in workflows at Netflix. We will show how we are building a clean and efficient incremental processing solution (IPS) by using Netflix Maestro and Apache Iceberg. IPS provides the incremental processing support with data accuracy, data freshness, and backfill for users and addresses many of the challenges in workflows. IPS enables users to continue to use the data processing patterns with minimal changes.\nIntroduction\nNetflix relies on data to power its business in all phases. Whether in analyzing A/B tests, optimizing studio production, training algorithms, investing in content acquisition, detecting security breaches, or optimizing payments, well structured and accurate data is foundational. As our business scales globally, the demand for data is growing and the needs for scalable low latency incremental processing begin to emerge. There are three common issues that the dataset owners usually face.\nData Freshness: Large datasets from Iceberg tables needed to be processed quickly and accurately to generate insights to enable faster product decisions. The hourly processing semantics along with valid–through-timestamp watermark or data signals provided by the Data Platform toolset today satisfies many use cases, but is not the best for low-latency batch processing. Before IPS, the Data Platform did not have a solution for tracking the state and progression of data sets as a single easy to use offering. This has led to a few internal solutions such as Psyberg. These internal libraries process data by capturing the changed partitions, which works only on specific use cases. Additionally, the libraries have tight coupling to the user business logic, which often incurs higher migration costs, maintenance costs, and requires heavy coordination with the Data Platform team.\nData Accuracy: Late arriving data causes datasets processed in the past to become incomplete and as a result inaccurate. To compensate for that, ETL workflows often use a lookback window, based on which they reprocess the data in that certain time window. For example, a job would reprocess aggregates for the past 3 days because it assumes that there would be late arriving data, but data prior to 3 days isn’t worth the cost of reprocessing.\nBackfill: Backfilling datasets is a common operation in big data processing. This requires repopulating data for a historical time period which is before the scheduled processing. The need for backfilling could be due to a variety of factors, e.g. (1) upstream data sets got repopulated due to changes in business logic of its data pipeline, (2) business logic was changed in a data pipeline, (3) anew metric was created that needs to be populated for historical time ranges, (4) historical data was found missing, etc.\nThese challenges are currently addressed in suboptimal and less cost efficient ways by individual local teams to fulfill the needs, such as\nLookback: This is a generic and simple approach that data engineers use to solve the data accuracy problem. Users configure the workflow to read the data in a window (e.g. past 3 hours or 10 days). The window is set based on users’ domain knowledge so that users have a high confidence that the late arriving data will be included or will not matter (i.e. data arrives too late to be useful). It ensures the correctness with a high cost in terms of time and compute resources.\nForeach pattern: Users build backfill workflows using Maestro foreach support. It works well to backfill data produced by a single workflow. If the pipeline has multiple stages or many downstream workflows, users have to manually create backfill workflows for each of them and that requires significant manual work.\nThe incremental processing solution (IPS) described here has been designed to address the above problems. The design goal is to provide a clean and easy to adopt solution for the Incremental processing to ensure data freshness, data accuracy, and to provide easy backfill support.\nData Freshness: provide the support for scheduling workflows in a micro batch fashion (e.g. 15 min interval) with state tracking functionality\nData Accuracy: provide the support to process all late arriving data to achieve data accuracy needed by the business with significantly improved performance in terms of multifold time and cost efficiency\nBackfill: provide managed backfill support to build, monitor, and validate the backfill, including automatically propagating changes from upstream to downstream workflows, to greatly improve engineering productivity (i.e. a few days or weeks of engineering work to build backfill workflows vs one click for managed backfill)\nApproach Overview\nGeneral Concept\nIncremental processing is an approach to process data in batch — but only on new or changed data. To support incremental processing, we need an approach for not only capturing incremental data changes but also tracking their states (i.e. whether a change is processed by a workflow or not). It must be aware of the change and can capture the changes from the source table(s) and then keep tracking those changes. Here, changes mean more than just new data itself. For example, a row in an aggregation target table needs all the rows from the source table associated with the aggregation row. Also, if there are multiple source tables, usually the union of the changed data ranges from all input tables gives the full change data set. Thus, change information captured must include all related data including those unchanged rows in the source table as well. Due to previously mentioned complexities, change tracking cannot be simply achieved by using a single watermark. IPS has to track those captured changes in finer granularity.\nThe changes from the source tables might affect the transformed result in the target table in various ways.\nIf one row in the target table is derived from one row in the source table, newly captured data change will be the complete input dataset for the workflow pipeline.\nIf one row in the target table is derived from multiple rows in the source table, capturing new data will only tell us the rows have to be re-processed. But the dataset needed for ETL is beyond the change data itself. For example, an aggregation based on account id requires all rows from the source table about an account id. The change dataset will tell us which account ids are changed and then the user business logic needs to load all data associated with those account ids found in the change data.\nIf one row in the target table is derived based on the data beyond the changed data set, e.g. joining source table with other tables, newly captured data is still useful and can indicate a range of data to be affected. Then the workflow will re-process the data based on the range. For example, assuming we have a table that keeps the accumulated view time for a given account partitioned by the day. If the view time 3-days ago is updated right now due to late arriving data, then the view time for the following two days has to be re-calculated for this account. In this case, the captured late arriving data will tell us the start of the re-calculation, which is much more accurate than recomputing everything for the past X days by guesstimate, where X is a cutoff lookback window decided by business domain knowledge.\nOnce the change information (data or range) is captured, a workflow has to write the data to the target table in a slightly more complicated way because the simple INSERT OVERWRITE mechanism won’t work well. There are two alternatives:\nMerge pattern: In some compute frameworks, e.g. Spark 3, it supports MERGE INTO to allow new data to be merged into the existing data set. That solves the write problem for incremental processing. Note that the workflow/step can be safely restarted without worrying about duplicate data being inserted when using MERGE INTO.\nAppend pattern: Users can also use append only write (e.g. INSERT INTO) to add the new data to the existing data set. Once the processing is completed, the append data is committed to the table. If users want to re-run or re-build the data set, they will run a backfill workflow to completely overwrite the target data set (e.g. INSERT OVERWRITE).\nAdditionally, the IPS will naturally support the backfill in many cases. Downstream workflows (if there is no business logic change) will be triggered by the data change due to backfill. This enables auto propagation of backfill data in multi-stage pipelines. Note that the backfill support is skipped in this blog. We will talk about IPS backfill support in another following blog post.\nNetflix Maestro\nMaestro is the Netflix data workflow orchestration platform built to meet the current and future needs of Netflix. It is a general-purpose workflow orchestrator that provides a fully managed workflow-as-a-service (WAAS) to the data platform users at Netflix. It serves thousands of users, including data scientists, data engineers, machine learning engineers, software engineers, content producers, and business analysts, in various use cases. Maestro is highly scalable and extensible to support existing and new use cases and offers enhanced usability to end users.\nSince the last blog on Maestro, we have migrated all the workflows to it on behalf of users with minimal interruption. Maestro has been fully deployed in production with 100% workload running on it.\nIPS is built upon Maestro as an extension by adding two building blocks, i.e. a new trigger mechanism and step job type, to enable incremental processing for all workflows. It is seamlessly integrated into the whole Maestro ecosystem with minimal onboarding cost.\nApache Iceberg\nIceberg is a high-performance format for huge analytic tables. Iceberg brings the reliability and simplicity of SQL tables to big data, while making it possible for engines like Spark, Trino, Flink, Presto, Hive and Impala to safely work with the same tables, at the same time. It supports expressive SQL, full schema evolution, hidden partitioning, data compaction, and time travel & rollback. In the IPS, we leverage the rich features provided by Apache Iceberg to develop a lightweight approach to capture the table changes.\nIncremental Change Capture Design\nUsing Netflix Maestro and Apache Iceberg, we created a novel solution for incremental processing, which provides the incremental change (data and range) capture in a super lightweight way without copying any data. During our exploration, we see a huge opportunity to improve cost efficiency and engineering productivity using incremental processing.\nHere is our solution to achieve incremental change capture built upon Apache Iceberg features. As we know, an iceberg table contains a list of snapshots with a set of metadata data. Snapshots include references to the actual immutable data files. A snapshot can contain data files from different partitions.\nThe graph above shows that s0 contains data for Partition P0 and P1 at T1. Then at T2, a new snapshot s1 is committed to the table with a list of new data files, which includes late arriving data for partition P0 and P1 and data for P2.\nWe implemented a lightweight approach to create an iceberg table (called ICDC table), which has its own snapshot but only includes the new data file references from the original table without copying the data files. It is highly efficient with a low cost. Then workflow pipelines can just load the ICDC table to process only the change data from partition P0, P1, P2 without reprocessing the unchanged data in P0 and P1. Meanwhile, the change range is also captured for the specified data field as the Iceberg table metadata contains the upper and lower bound information of each data field for each data file. Moreover, IPS will track the changes in data file granularity for each workflow.\nThis lightweight approach is seamlessly integrated with Maestro to allow all (thousands) scheduler users to use this new building block (i.e. incremental processing) in their tens of thousands of workflows. Each workflow using IPS will be injected with a table parameter, which is the table name of the lightweight ICDC table. The ICDC table contains only the change data. Additionally, if the workflow needs the change range, a list of parameters will be injected to the user workflow to include the change range information. The incremental processing can be enabled by a new step job type (ICDC) and/or a new incremental trigger mechanism. Users can use them together with all existing Maestro features, e.g. foreach patterns, step dependencies based on valid–through-timestamp watermark, write-audit-publish templatized pattern, etc.\nMain Advantages\nWith this design, user workflows can adopt incremental processing with very low efforts. The user business logic is also decoupled from the IPS implementation. Multi-stage pipelines can also mix the incremental processing workflows with existing normal workflows. We also found that user workflows can be simplified after using IPS by removing additional steps to handle the complexity of the lookback window or calling some internal libraries.\nAdding incremental processing features into Netflix Maestro as new features/building blocks for users will enable users to build their workflows in a much more efficient way and bridge the gaps to solve many challenging problems (e.g. dealing with late arriving data) in a much simpler way.\nEmerging Incremental Processing Patterns\nWhile onboarding user pipelines to IPS, we have discovered a few incremental processing patterns:\nIncrementally process the captured incremental change data and directly append them to the target table\nThis is the straightforward incremental processing use case, where the change data carries all the information needed for the data processing. Upstream changes (usually from a single source table) are propagated to the downstream (usually another target table) and the workflow pipeline only needs to process the change data (might join with other dimension tables) and then merge into (usually append) to the target table. This pattern will replace lookback window patterns to take care of late arriving data. Instead of overwriting past X days of data completely by using a lookback window pattern, user workflows just need to MERGE the change data (including late arriving data) into the target table by processing the ICDC table.\nUse captured incremental change data as the row level filter list to remove unnecessary transformation\nETL jobs usually need to aggregate data based on certain group-by keys. Change data will disclose all the group-by keys that require a re-aggregation due to the new landing data from the source table(s). Then ETL jobs can join the original source table with the ICDC table on those group-by keys by using ICDC as a filter to speed up the processing to enable calculations of a much smaller set of data. There is no change to business transform logic and no re-design of ETL workflow. ETL pipelines keep all the benefits of batch workflows.\nUse the captured range parameters in the business logic\nThis pattern is usually used in complicated use cases, such as joining multiple tables and doing complex processings. In this case, the change data do not give the full picture of the input needed by the ETL workflow. Instead, the change data indicates a range of changed data sets for a specific set of fields (might be partition keys) in a given input table or usually multiple input tables. Then, the union of the change ranges from all input tables gives the full change data set needed by the workflow. Additionally, the whole range of data usually has to be overwritten because the transformation is not stateless and depends on the outcome result from the previous ranges. Another example is that the aggregated record in the target table or window function in the query has to be updated based on the whole data set in the partition (e.g. calculating a medium across the whole partition). Basically, the range derived from the change data indicates the dataset to be re-processed.\nUse cases\nData workflows at Netflix usually have to deal with late arriving data which is commonly solved by using lookback window pattern due to its simplicity and ease of implementation. In the lookback pattern, the ETL pipeline will always consume the past X number of partition data from the source table and then overwrite the target table in every run. Here, X is a number decided by the pipeline owners based on their domain expertise. The drawback is the cost of computation and execution time. It usually costs almost X times more than the pipeline without considering late arriving data. Given the fact that the late arriving data is sparse, the majority of the processing is done on the data that have been already processed, which is unnecessary. Also, note that this approach is based on domain knowledge and sometimes is subject to changes of the business environment or the domain expertise of data engineers. In certain cases, it is challenging to come up with a good constant number.\nBelow, we will use a two-stage data pipeline to illustrate how to rebuild it using IPS to improve the cost efficiency. We will observe a significant cost reduction (> 80%) with little changes in the business logic. In this use case, we will set the lookback window size X to be 14 days, which varies in different real pipelines.\nOriginal Data Pipeline with Lookback Window\nplayback_table: an iceberg table holding playback events from user devices ingested by streaming pipelines with late arriving data, which is sparse, only about few percents of the data is late arriving.\nplayback_daily_workflow: a daily scheduled workflow to process the past X days playback_table data and write the transformed data to the target table for the past X days\nplayback_daily_table: the target table of the playback_daily_workflow and get overwritten every day for the past X days\nplayback_daily_agg_workflow: a daily scheduled workflow to process the past X days’ playback_daily_table data and write the aggregated data to the target table for the past X days\nplayback_daily_agg_table: the target table of the playback_daily_agg_workflow and get overwritten every day for the past 14 days.\nWe ran this pipeline in a sample dataset using the real business logic and here is the average execution result of sample runs\nThe first stage workflow takes about 7 hours to process playback_table data\nThe second stage workflow takes about 3.5 hours to process playback_daily_table data\nNew Data Pipeline with Incremental Processing\nUsing IPS, we rewrite the pipeline to avoid re-processing data as much as possible. The new pipeline is shown below.\nStage 1:\nips_playback_daily_workflow: it is the updated version of playback_daily_workflow.\nThe workflow spark sql job then reads an incremental change data capture (ICDC) iceberg table (i.e. playback_icdc_table), which only includes the new data added into the playback_table. It includes the late arriving data but does not include any unchanged data from playback_table.\nThe business logic will replace INSERT OVERWRITE by MERGE INTO SQL query and then the new data will be merged into the playback_daily_table.\nStage 2:\nIPS captures the changed data of playback_daily_table and also keeps the change data in an ICDC source table (playback_daily_icdc_table). So we don’t need to hard code the lookback window in the business logic. If there are only Y days having changed data in playback_daily_table, then it only needs to load data for Y days.\nIn ips_playback_daily_agg_workflow, the business logic will be the same for the current day’s partition. We then need to update business logic to take care of late arriving data by\nJOIN the playback_daily table with playback_daily_icdc_table on the aggregation group-by keys for the past 2 to X days, excluding the current day (i.e. day 1)\nBecause late arriving data is sparse, JOIN will narrow down the playback_daily_table data set so as to only process a very small portion of it.\nThe business logic will use MERGE INTO SQL query then the change will be propagated to the downstream target table\nFor the current day, the business logic will be the same and consume the data from playback_daily_table and then write the outcome to the target table playback_daily_agg_table using INSERT OVERWRITE because there is no need to join with the ICDC table.\nWith these small changes, the data pipeline efficiency is greatly improved. In our sample run,\nThe first stage workflow takes just about 30 minutes to process X day change data from playback_table.\nThe second stage workflow takes about 15 minutes to process change data between day 2 to day X from playback_daily_table by joining with playback_daily_cdc_table data and takes another 15 minutes to process the current day (i.e. day 1) playback_daily_table change data.\nHere the spark job settings are the same in original and new pipelines. So in total, the new IPS based pipeline overall needs around 10% of resources (measured by the execution time) to finish.\nLooking Forward\nWe will improve IPS to support more complicated cases beyond append-only cases. IPS will be able to keep track of the progress of the table changes and support multiple Iceberg table change types (e.g. append, overwrite, etc.). We will also add managed backfill support into IPS to help users to build, monitor, and validate the backfill.\nWe are taking Big Data Orchestration to the next level and constantly solving new problems and challenges, please stay tuned. If you are motivated to solve large scale orchestration problems, please join us.\nAcknowledgements\nThanks to our Product Manager Ashim Pokharel for driving the strategy and requirements. We’d also like to thank Andy Chu, Kyoko Shimada, Abhinaya Shetty, Bharath Mummadisetty, John Zhuge, Rakesh Veeramacheneni, and other stunning colleagues at Netflix for their suggestions and feedback while developing IPS. We’d also like to thank Prashanth Ramdas, Eva Tse, Charles Smith, and other leaders of Netflix engineering organizations for their constructive feedback and suggestions on the IPS architecture and design.",
      "markdown": "## Incremental Processing using Netflix Maestro and Apache Iceberg\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----b8ba072ddeeb--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----b8ba072ddeeb--------------------------------)\n\nby [Jun He](https://www.linkedin.com/in/jheua/), [Yingyi Zhang](https://www.linkedin.com/in/yingyi-zhang-a0a164111/), and [Pawan Dixit](https://www.linkedin.com/in/pawan-dixit-b4307b2/)\n\nIncremental processing is an approach to process new or changed data in workflows. The key advantage is that it only incrementally processes data that are newly added or updated to a dataset, instead of re-processing the complete dataset. This not only reduces the cost of compute resources but also reduces the execution time in a significant manner. When workflow execution has a shorter duration, chances of failure and manual intervention reduce. It also improves the engineering productivity by simplifying the existing pipelines and unlocking the new patterns.\n\nIn this blog post, we talk about the landscape and the challenges in workflows at Netflix. We will show how we are building a clean and efficient incremental processing solution (IPS) by using Netflix Maestro and Apache Iceberg. IPS provides the incremental processing support with data accuracy, data freshness, and backfill for users and addresses many of the challenges in workflows. IPS enables users to continue to use the data processing patterns with minimal changes.\n\n## Introduction\n\nNetflix relies on data to power its business in all phases. Whether in analyzing A/B tests, optimizing studio production, training algorithms, investing in content acquisition, detecting security breaches, or optimizing payments, well structured and accurate data is foundational. As our business scales globally, the demand for data is growing and the needs for scalable low latency incremental processing begin to emerge. There are three common issues that the dataset owners usually face.\n\n*   **Data Freshness:** Large datasets from Iceberg tables needed to be processed quickly and accurately to generate insights to enable faster product decisions. The hourly processing semantics along with valid–through-timestamp watermark or data signals provided by the Data Platform toolset today satisfies many use cases, but is not the best for low-latency batch processing. Before IPS, the Data Platform did not have a solution for tracking the state and progression of data sets as a single easy to use offering. This has led to a few internal solutions such as [Psyberg](https://netflixtechblog.com/2-diving-deeper-into-psyberg-stateless-vs-stateful-data-processing-1d273b3aaefb). These internal libraries process data by capturing the changed partitions, which works only on specific use cases. Additionally, the libraries have tight coupling to the user business logic, which often incurs higher migration costs, maintenance costs, and requires heavy coordination with the Data Platform team.\n*   **Data Accuracy:** Late arriving data causes datasets processed in the past to become incomplete and as a result inaccurate. To compensate for that, ETL workflows often use a lookback window, based on which they reprocess the data in that certain time window. For example, a job would reprocess aggregates for the past 3 days because it assumes that there would be late arriving data, but data prior to 3 days isn’t worth the cost of reprocessing.\n*   **Backfill:** Backfilling datasets is a common operation in big data processing. This requires repopulating data for a historical time period which is before the scheduled processing. The need for backfilling could be due to a variety of factors, e.g. (1) upstream data sets got repopulated due to changes in business logic of its data pipeline, (2) business logic was changed in a data pipeline, (3) anew metric was created that needs to be populated for historical time ranges, (4) historical data was found missing, etc.\n\nThese challenges are currently addressed in suboptimal and less cost efficient ways by individual local teams to fulfill the needs, such as\n\n*   **Lookback:** This is a generic and simple approach that data engineers use to solve the data accuracy problem. Users configure the workflow to read the data in a window (e.g. past 3 hours or 10 days). The window is set based on users’ domain knowledge so that users have a high confidence that the late arriving data will be included or will not matter (i.e. data arrives too late to be useful). It ensures the correctness with a high cost in terms of time and compute resources.\n*   **Foreach pattern:** Users build backfill workflows using [Maestro foreach support](https://netflixtechblog.com/orchestrating-data-ml-workflows-at-scale-with-netflix-maestro-aaa2b41b800c#7d0f). It works well to backfill data produced by a single workflow. If the pipeline has multiple stages or many downstream workflows, users have to manually create backfill workflows for each of them and that requires significant manual work.\n\nThe incremental processing solution (IPS) described here has been designed to address the above problems. The design goal is to provide a clean and easy to adopt solution for the Incremental processing to ensure data freshness, data accuracy, and to provide easy backfill support.\n\n*   **Data Freshness:** provide the support for scheduling workflows in a **micro batch** fashion (e.g. 15 min interval) with state tracking functionality\n*   **Data Accuracy:** provide the support to process all late arriving data to achieve data accuracy needed by the business with significantly improved performance in terms of multifold **time and cost efficiency**\n*   **Backfill:** provide managed backfill support to build, monitor, and validate the backfill, including automatically propagating changes from upstream to downstream workflows, to greatly improve **engineering productivity** (i.e. a few days or weeks of engineering work to build backfill workflows vs one click for managed backfill)\n\n## Approach Overview\n\n## General Concept\n\n**Incremental processing** is an approach to process data in batch — but only on new or changed data. To support incremental processing, we need an approach for not only capturing incremental data changes but also tracking their states (i.e. whether a change is processed by a workflow or not). It must be aware of the change and can capture the changes from the source table(s) and then keep tracking those changes. Here, changes mean more than just new data itself. For example, a row in an aggregation target table needs all the rows from the source table associated with the aggregation row. Also, if there are multiple source tables, usually the union of the changed data ranges from all input tables gives the full change data set. Thus, change information captured must include all related data including those unchanged rows in the source table as well. Due to previously mentioned complexities, change tracking cannot be simply achieved by using a single watermark. IPS has to track those captured changes in finer granularity.\n\nThe changes from the source tables might affect the transformed result in the target table in various ways.\n\n*   If one row in the target table is derived from one row in the source table, newly captured data change will be the complete input dataset for the workflow pipeline.\n*   If one row in the target table is derived from multiple rows in the source table, capturing new data will only tell us the rows have to be re-processed. But the dataset needed for ETL is beyond the change data itself. For example, an aggregation based on account id requires all rows from the source table about an account id. The change dataset will tell us which account ids are changed and then the user business logic needs to load all data associated with those account ids found in the change data.\n*   If one row in the target table is derived based on the data beyond the changed data set, e.g. joining source table with other tables, newly captured data is still useful and can indicate a range of data to be affected. Then the workflow will re-process the data based on the range. For example, assuming we have a table that keeps the accumulated view time for a given account partitioned by the day. If the view time 3-days ago is updated right now due to late arriving data, then the view time for the following two days has to be re-calculated for this account. In this case, the captured late arriving data will tell us the start of the re-calculation, which is much more accurate than recomputing everything for the past X days by guesstimate, where X is a cutoff lookback window decided by business domain knowledge.\n\nOnce the change information (data or range) is captured, a workflow has to write the data to the target table in a slightly more complicated way because the simple **INSERT OVERWRITE** mechanism won’t work well. There are two alternatives:\n\n*   **Merge pattern:** In some compute frameworks, e.g. Spark 3, it supports MERGE INTO to allow new data to be merged into the existing data set. That solves the write problem for incremental processing. Note that the workflow/step can be safely restarted without worrying about duplicate data being inserted when using MERGE INTO.\n*   **Append pattern:** Users can also use append only write (e.g. INSERT INTO) to add the new data to the existing data set. Once the processing is completed, the append data is committed to the table. If users want to re-run or re-build the data set, they will run a backfill workflow to completely overwrite the target data set (e.g. INSERT OVERWRITE).\n\nAdditionally, the IPS will naturally support the backfill in many cases. Downstream workflows (if there is no business logic change) will be triggered by the data change due to backfill. This enables auto propagation of backfill data in multi-stage pipelines. Note that the backfill support is skipped in this blog. We will talk about IPS backfill support in another following blog post.\n\n## Netflix Maestro\n\n[Maestro](https://netflixtechblog.com/orchestrating-data-ml-workflows-at-scale-with-netflix-maestro-aaa2b41b800c) is the Netflix data workflow orchestration platform built to meet the current and future needs of Netflix. It is a general-purpose workflow orchestrator that provides a fully managed workflow-as-a-service (WAAS) to the data platform users at Netflix. It serves thousands of users, including data scientists, data engineers, machine learning engineers, software engineers, content producers, and business analysts, in various use cases. Maestro is highly scalable and extensible to support existing and new use cases and offers enhanced usability to end users.\n\nSince the last blog on [Maestro](https://netflixtechblog.com/orchestrating-data-ml-workflows-at-scale-with-netflix-maestro-aaa2b41b800c), we have migrated all the workflows to it on behalf of users with minimal interruption. Maestro has been fully deployed in production with 100% workload running on it.\n\nIPS is built upon Maestro as an extension by adding two building blocks, i.e. a new trigger mechanism and step job type, to enable incremental processing for all workflows. It is seamlessly integrated into the whole Maestro ecosystem with minimal onboarding cost.\n\n## Apache Iceberg\n\n[Iceberg](https://iceberg.apache.org/) is a high-performance format for huge analytic tables. Iceberg brings the reliability and simplicity of SQL tables to big data, while making it possible for engines like Spark, Trino, Flink, Presto, Hive and Impala to safely work with the same tables, at the same time. It supports expressive SQL, full schema evolution, hidden partitioning, data compaction, and time travel & rollback. In the IPS, we leverage the rich features provided by Apache Iceberg to develop a lightweight approach to capture the table changes.\n\n## Incremental Change Capture Design\n\nUsing Netflix Maestro and Apache Iceberg, we created a novel solution for incremental processing, which provides the incremental change (data and range) capture in a super lightweight way without copying any data. During our exploration, we see a huge opportunity to improve cost efficiency and engineering productivity using incremental processing.\n\nHere is our solution to achieve incremental change capture built upon Apache Iceberg features. As we know, an iceberg table contains a list of snapshots with a set of metadata data. Snapshots include references to the actual immutable data files. A snapshot can contain data files from different partitions.\n\nThe graph above shows that s0 contains data for Partition P0 and P1 at T1. Then at T2, a new snapshot s1 is committed to the table with a list of new data files, which includes late arriving data for partition P0 and P1 and data for P2.\n\nWe implemented a lightweight approach to create an iceberg table (called ICDC table), which has its own snapshot but only includes the new data file references from the original table without copying the data files. It is highly efficient with a low cost. Then workflow pipelines can just load the ICDC table to process only the change data from partition P0, P1, P2 without reprocessing the unchanged data in P0 and P1. Meanwhile, the change range is also captured for the specified data field as the Iceberg table metadata contains the upper and lower bound information of each data field for each data file. Moreover, IPS will track the changes in data file granularity for each workflow.\n\nThis lightweight approach is seamlessly integrated with Maestro to allow all (thousands) scheduler users to use this new building block (i.e. incremental processing) in their tens of thousands of workflows. Each workflow using IPS will be injected with a table parameter, which is the table name of the lightweight ICDC table. The ICDC table contains only the change data. Additionally, if the workflow needs the change range, a list of parameters will be injected to the user workflow to include the change range information. The incremental processing can be enabled by a new step job type (ICDC) and/or a new incremental trigger mechanism. Users can use them together with all existing Maestro features, e.g. foreach patterns, step dependencies based on valid–through-timestamp watermark, write-audit-publish templatized pattern, etc.\n\n## Main Advantages\n\nWith this design, user workflows can adopt incremental processing with very low efforts. The user business logic is also decoupled from the IPS implementation. Multi-stage pipelines can also mix the incremental processing workflows with existing normal workflows. We also found that user workflows can be simplified after using IPS by removing additional steps to handle the complexity of the lookback window or calling some internal libraries.\n\nAdding incremental processing features into Netflix Maestro as new features/building blocks for users will enable users to build their workflows in a much more efficient way and bridge the gaps to solve many challenging problems (e.g. dealing with late arriving data) in a much simpler way.\n\n## Emerging Incremental Processing Patterns\n\nWhile onboarding user pipelines to IPS, we have discovered a few incremental processing patterns:\n\n## Incrementally process the captured incremental change data and directly append them to the target table\n\nThis is the straightforward incremental processing use case, where the change data carries all the information needed for the data processing. Upstream changes (usually from a single source table) are propagated to the downstream (usually another target table) and the workflow pipeline only needs to process the change data (might join with other dimension tables) and then merge into (usually append) to the target table. This pattern will replace lookback window patterns to take care of late arriving data. Instead of overwriting past X days of data completely by using a lookback window pattern, user workflows just need to MERGE the change data (including late arriving data) into the target table by processing the ICDC table.\n\n## Use captured incremental change data as the row level filter list to remove unnecessary transformation\n\nETL jobs usually need to aggregate data based on certain group-by keys. Change data will disclose all the group-by keys that require a re-aggregation due to the new landing data from the source table(s). Then ETL jobs can join the original source table with the ICDC table on those group-by keys by using ICDC as a filter to speed up the processing to enable calculations of a much smaller set of data. There is no change to business transform logic and no re-design of ETL workflow. ETL pipelines keep all the benefits of batch workflows.\n\n## Use the captured range parameters in the business logic\n\nThis pattern is usually used in complicated use cases, such as joining multiple tables and doing complex processings. In this case, the change data do not give the full picture of the input needed by the ETL workflow. Instead, the change data indicates a range of changed data sets for a specific set of fields (might be partition keys) in a given input table or usually multiple input tables. Then, the union of the change ranges from all input tables gives the full change data set needed by the workflow. Additionally, the whole range of data usually has to be overwritten because the transformation is not stateless and depends on the outcome result from the previous ranges. Another example is that the aggregated record in the target table or window function in the query has to be updated based on the whole data set in the partition (e.g. calculating a medium across the whole partition). Basically, the range derived from the change data indicates the dataset to be re-processed.\n\n## Use cases\n\nData workflows at Netflix usually have to deal with late arriving data which is commonly solved by using lookback window pattern due to its simplicity and ease of implementation. In the lookback pattern, the ETL pipeline will always consume the past X number of partition data from the source table and then overwrite the target table in every run. Here, X is a number decided by the pipeline owners based on their domain expertise. The drawback is the cost of computation and execution time. It usually costs almost X times more than the pipeline without considering late arriving data. Given the fact that the late arriving data is sparse, the majority of the processing is done on the data that have been already processed, which is unnecessary. Also, note that this approach is based on domain knowledge and sometimes is subject to changes of the business environment or the domain expertise of data engineers. In certain cases, it is challenging to come up with a good constant number.\n\nBelow, we will use a two-stage data pipeline to illustrate how to rebuild it using IPS to improve the cost efficiency. We will observe a significant cost reduction (> 80%) with little changes in the business logic. In this use case, we will set the lookback window size X to be 14 days, which varies in different real pipelines.\n\n## Original Data Pipeline with Lookback Window\n\n*   **playback\\_table**: an iceberg table holding playback events from user devices ingested by streaming pipelines with late arriving data, which is sparse, only about few percents of the data is late arriving.\n*   **playback\\_daily\\_workflow**: a daily scheduled workflow to process the past X days playback\\_table data and write the transformed data to the target table for the past X days\n*   **playback\\_daily\\_table**: the target table of the playback\\_daily\\_workflow and get overwritten every day for the past X days\n*   **playback\\_daily\\_agg\\_workflow**: a daily scheduled workflow to process the past X days’ playback\\_daily\\_table data and write the aggregated data to the target table for the past X days\n*   **playback\\_daily\\_agg\\_table**: the target table of the playback\\_daily\\_agg\\_workflow and get overwritten every day for the past 14 days.\n\nWe ran this pipeline in a sample dataset using the real business logic and here is the average execution result of sample runs\n\n*   The first stage workflow takes about 7 hours to process playback\\_table data\n*   The second stage workflow takes about 3.5 hours to process playback\\_daily\\_table data\n\n## New Data Pipeline with Incremental Processing\n\nUsing IPS, we rewrite the pipeline to avoid re-processing data as much as possible. The new pipeline is shown below.\n\n**Stage 1:**\n\n*   **ips\\_playback\\_daily\\_workflow**: it is the updated version of playback\\_daily\\_workflow.\n*   The workflow spark sql job then reads an incremental change data capture (ICDC) iceberg table (i.e. **playback\\_icdc\\_table**), which only includes the new data added into the playback\\_table. It includes the late arriving data but does not include any unchanged data from playback\\_table.\n*   The business logic will replace **INSERT OVERWRITE** by **MERGE INTO** SQL query and then the new data will be merged into the playback\\_daily\\_table.\n\n**Stage 2:**\n\n*   IPS captures the changed data of playback\\_daily\\_table and also keeps the change data in an ICDC source table (**playback\\_daily\\_icdc\\_table**). So we don’t need to hard code the lookback window in the business logic. If there are only Y days having changed data in playback\\_daily\\_table, then it only needs to load data for Y days.\n*   In **ips\\_playback\\_daily\\_agg\\_workflow**, the business logic will be the same for the current day’s partition. We then need to update business logic to take care of late arriving data by\n*   JOIN the playback\\_daily table with playback\\_daily\\_icdc\\_table on the aggregation group-by keys for the past 2 to X days, excluding the current day (i.e. day 1)\n*   Because late arriving data is sparse, JOIN will narrow down the playback\\_daily\\_table data set so as to only process a very small portion of it.\n*   The business logic will use **MERGE INTO** SQL query then the change will be propagated to the downstream target table\n*   For the current day, the business logic will be the same and consume the data from playback\\_daily\\_table and then write the outcome to the target table playback\\_daily\\_agg\\_table using **INSERT OVERWRITE** because there is no need to join with the ICDC table.\n\nWith these small changes, the data pipeline efficiency is greatly improved. In our sample run,\n\n*   The first stage workflow takes just about 30 minutes to process X day change data from playback\\_table.\n*   The second stage workflow takes about 15 minutes to process change data between day 2 to day X from playback\\_daily\\_table by joining with playback\\_daily\\_cdc\\_table data and takes another 15 minutes to process the current day (i.e. day 1) playback\\_daily\\_table change data.\n\nHere the spark job settings are the same in original and new pipelines. So in total, the new IPS based pipeline overall needs around **10%** of resources (measured by the execution time) to finish.\n\n## Looking Forward\n\nWe will improve IPS to support more complicated cases beyond append-only cases. IPS will be able to keep track of the progress of the table changes and support multiple Iceberg table change types (e.g. append, overwrite, etc.). We will also add managed backfill support into IPS to help users to build, monitor, and validate the backfill.\n\nWe are taking Big Data Orchestration to the next level and constantly solving new problems and challenges, please stay tuned. If you are motivated to solve large scale orchestration problems, please [join us](https://jobs.netflix.com/search?team=Data+Platform).\n\n## Acknowledgements\n\nThanks to our Product Manager [Ashim Pokharel](https://www.linkedin.com/in/ashpokh/) for driving the strategy and requirements. We’d also like to thank Andy Chu, Kyoko Shimada, Abhinaya Shetty, Bharath Mummadisetty, John Zhuge, Rakesh Veeramacheneni, and other stunning colleagues at Netflix for their suggestions and feedback while developing IPS. We’d also like to thank Prashanth Ramdas, Eva Tse, Charles Smith, and other leaders of Netflix engineering organizations for their constructive feedback and suggestions on the IPS architecture and design."
    },
    {
      "url": "https://netflixtechblog.com/3-psyberg-automated-end-to-end-catch-up-260fbe366fe2?source=collection_home---4------0-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/3-psyberg-automated-end-to-end-catch-up-260fbe366fe2?gi=0cbf33931313&source=collection_home---4------0-----------------------",
        "loadedTime": "2023-12-06T00:03:07.049Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/3-psyberg-automated-end-to-end-catch-up-260fbe366fe2",
        "title": "Psyberg: Automated end to end catch up | by Netflix Technology Blog | Nov, 2023 | Netflix TechBlog",
        "description": "This blog post will cover how Psyberg helps automate the end-to-end catchup of different pipelines, including dimension tables. In the previous installments of this series, we introduced Psyberg and…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Psyberg: Automated end to end catch up\nBy Abhinaya Shetty, Bharath Mummadisetty\nThis blog post will cover how Psyberg helps automate the end-to-end catchup of different pipelines, including dimension tables.\nIn the previous installments of this series, we introduced Psyberg and delved into its core operational modes: Stateless and Stateful Data Processing. Now, let’s explore the state of our pipelines after incorporating Psyberg.\nPipelines After Psyberg\nLet’s explore how different modes of Psyberg could help with a multistep data pipeline. We’ll return to the sample customer lifecycle:\nProcessing Requirement: \nKeep track of the end-of-hour state of accounts, e.g., Active/Upgraded/Downgraded/Canceled.\nSolution: \nOne potential approach here would be as follows\nCreate two stateless fact tables :\na. Signups\nb. Account Plans\nCreate one stateful fact table:\na. Cancels\nCreate a stateful dimension that reads the above fact tables every hour and derives the latest account state.\nLet’s look at how this can be integrated with Psyberg to auto-handle late-arriving data and corresponding end-to-end data catchup.\nNavigating the Workflow: How Psyberg Handles Late-Arriving Data\nWe follow a generic workflow structure for both stateful and stateless processing with Psyberg; this helps maintain consistency and makes debugging and understanding these pipelines easier. The following is a concise overview of the various stages involved; for a more detailed exploration of the workflow specifics, please turn to the second installment of this series.\n1. Psyberg Initialization\nThe workflow starts with the Psyberg initialization (init) step.\nInput: List of source tables and required processing mode\nOutput: Psyberg identifies new events that have occurred since the last high watermark (HWM) and records them in the session metadata table.\nThe session metadata table can then be read to determine the pipeline input.\n2. Write-Audit-Publish (WAP) Process\nThis is the general pattern we use in our ETL pipelines.\na. Write\nApply the ETL business logic to the input data identified in Step 1 and write to an unpublished iceberg snapshot based on the Psyberg mode\nb. Audit\nRun various quality checks on the staged data. Psyberg’s metadata session table is used to identify the partitions included in a batch run. Several audits, such as verifying source and target counts, are performed on this batch of data.\nc. Publish\nIf the audits are successful, cherry-pick the staging snapshot to publish the data to production.\n3. Psyberg Commit\nNow that the data pipeline has been executed successfully, the new high watermark identified in the initialization step is committed to Psyberg’s high watermark metadata table. This ensures that the next instance of the workflow will pick up newer updates.\nCallouts\nHaving the Psyberg step isolated from the core data pipeline allows us to maintain a consistent pattern that can be applied across stateless and stateful processing pipelines with varying requirements.\nThis also enables us to update the Psyberg layer without touching the workflows.\nThis is compatible with both Python and Scala Spark.\nDebugging/figuring out what was loaded in every run is made easy with the help of workflow parameters and Psyberg Metadata.\nThe Setup: Automated end-to-end catchup\nLet’s go back to our customer lifecycle example. Once we integrate all four components with Psyberg, here’s how we would set it up for automated catchup.\nThe three fact tables, comprising the signup and plan facts encapsulated in Psyberg’s stateless mode, along with the cancel fact in stateful mode, serve as inputs for the stateful sequential load ETL pipeline. This data pipeline monitors the various stages in the customer lifecycle.\nIn the sequential load ETL, we have the following features:\nCatchup Threshold: This defines the lookback period for the data being read. For instance, only consider the last 12 hours of data.\nData Load Type: The ETL can either load the missed/new data specifically or reload the entire specified range.\nMetadata Recording: Metadata is persisted for traceability.\nHere is a walkthrough on how this system would automatically catch up in the event of late-arriving data:\nPremise: All the tables were last loaded up to hour 5, meaning that any data from hour 6 onwards is considered new, and anything before that is classified as late data (as indicated in red above)\nFact level catchup:\nDuring the Psyberg initialization phase, the signup and plan facts identify the late data from hours 2 and 3, as well as the most recent data from hour 6. The ETL then appends this data to the corresponding partitions within the fact tables.\nThe Psyberg initialization for the cancel fact identifies late data from hour 5 and additional data from hours 6 and 7. Since this ETL operates in stateful mode, the data in the target table from hours 5 to 7 will be overwritten with the new data.\nBy focusing solely on updates and avoiding reprocessing of data based on a fixed lookback window, both Stateless and Stateful Data Processing maintain a minimal change footprint. This approach ensures data processing is both efficient and accurate.\nDimension level catchup:\nThe Psyberg wrapper for this stateful ETL looks at the updates to the upstream Psyberg powered fact tables to determine the date-hour range to reprocess. Here’s how it would calculate the above range:\nMinHr = least(min processing hour from each source table)\nThis ensures that we don’t miss out on any data, including late-arriving data. In this case, the minimum hour to process the data is hour 2.\nMaxHr = least(max processing hour from each source table)\nThis ensures we do not process partial data, i.e., hours for which data has not been loaded into all source tables. In this case, the maximum hour to process the data is hour 6.\nThe ETL process uses this time range to compute the state in the changed partitions and overwrite them in the target table. This helps overwrite data only when required and minimizes unnecessary reprocessing.\nAs seen above, by chaining these Psyberg workflows, we could automate the catchup for late-arriving data from hours 2 and 6. The Data Engineer does not need to perform any manual intervention in this case and can thus focus on more important things!\nThe Impact: How Psyberg Transformed Our Workflows\nThe introduction of Psyberg into our workflows has served as a valuable tool in enhancing accuracy and performance. The following are key areas that have seen improvements from using Psyberg:\nComputational Resources Used: \nIn certain instances, we’ve noticed a significant reduction in resource utilization, with the number of Spark cores used dropping by 90% following the implementation of Psyberg, compared to using fixed lookback windows\nWorkflow and Table Onboarding: \nWe have onboarded 30 tables and 13 workflows into incremental processing since implementing Psyberg\nReliability and Accuracy: \nSince onboarding workflows to Psyberg, we have experienced zero manual catchups or missing data incidents\nBootstrap template: \nThe process of integrating new tables into incremental processing has been made more accessible and now requires minimal effort using Psyberg\nThese performance metrics suggest that adopting Psyberg has been beneficial to the efficiency of our data processing workflows.\nNext Steps and Conclusion\nIntegrating Psyberg into our operations has improved our data workflows and opened up exciting possibilities for the future. As we continue to innovate, Netflix’s data platform team is focused on creating a comprehensive solution for incremental processing use cases. This platform-level solution is intended to enhance our data processing capabilities across the organization. You can read more about this here!\nIn conclusion, Psyberg has proven to be a reliable and effective solution for our data processing needs. As we look to the future, we’re excited about the potential for further advancements in our data platform capabilities.",
      "markdown": "## Psyberg: Automated end to end catch up\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----260fbe366fe2--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----260fbe366fe2--------------------------------)\n\nBy [_Abhinaya Shetty_](https://www.linkedin.com/in/abhinaya-shetty-ab871418/), [_Bharath Mummadisetty_](https://www.linkedin.com/in/bharath-chandra-mummadisetty-27591a88/)\n\nThis blog post will cover how Psyberg helps automate the end-to-end catchup of different pipelines, including dimension tables.\n\nIn the previous installments of this series, we [introduced Psyberg](https://netflixtechblog.medium.com/f68830617dd1) and delved into its core operational modes: [Stateless and Stateful Data Processing](https://netflixtechblog.medium.com/1d273b3aaefb). Now, let’s explore the state of our pipelines after incorporating Psyberg.\n\n## Pipelines After Psyberg\n\nLet’s explore how different modes of Psyberg could help with a multistep data pipeline. We’ll return to the sample customer lifecycle:\n\n**Processing Requirement**:  \nKeep track of the end-of-hour state of accounts, e.g., **Active/Upgraded/Downgraded/Canceled.**\n\n**Solution**:  \nOne potential approach here would be as follows\n\n1.  Create **two stateless** **fact** tables :  \n    a. Signups  \n    b. Account Plans\n2.  Create **one stateful** **fact** table:  \n    a. Cancels\n3.  Create a **stateful dimension** that reads the above fact tables every hour and derives the latest account state.\n\nLet’s look at how this can be integrated with Psyberg to auto-handle late-arriving data and corresponding end-to-end data catchup.\n\n## **Navigating the Workflow: How Psyberg Handles Late-Arriving Data**\n\nWe follow a generic workflow structure for both stateful and stateless processing with Psyberg; this helps maintain consistency and makes debugging and understanding these pipelines easier. The following is a concise overview of the various stages involved; for a more detailed exploration of the workflow specifics, please turn to the [second installment](https://netflixtechblog.medium.com/1d273b3aaefb) of this series.\n\n## 1\\. Psyberg Initialization\n\nThe workflow starts with the Psyberg initialization (init) step.\n\n*   **Input**: List of source tables and required processing mode\n*   **Output**: Psyberg identifies new events that have occurred since the last high watermark (HWM) and records them in the session metadata table.\n\nThe session metadata table can then be read to determine the pipeline input.\n\n## 2\\. Write-Audit-Publish (WAP) Process\n\nThis is the general pattern we use in our ETL pipelines.\n\n**a. Write  \n**Apply the ETL business logic to the input data identified in Step 1 and write to an unpublished iceberg snapshot based on the Psyberg mode\n\n**b**. **Audit  \n**Run various quality checks on the staged data. Psyberg’s metadata session table is used to identify the partitions included in a batch run. Several audits, such as verifying source and target counts, are performed on this batch of data.\n\n**c. Publish  \n**If the audits are successful, cherry-pick the staging snapshot to publish the data to production.\n\n## 3\\. Psyberg Commit\n\nNow that the data pipeline has been executed successfully, the new high watermark identified in the initialization step is committed to Psyberg’s high watermark metadata table. This ensures that the next instance of the workflow will pick up newer updates.\n\n## Callouts\n\n*   Having the Psyberg step isolated from the core data pipeline allows us to maintain a consistent pattern that can be applied across stateless and stateful processing pipelines with varying requirements.\n*   This also enables us to update the Psyberg layer without touching the workflows.\n*   This is compatible with both Python and Scala Spark.\n*   Debugging/figuring out what was loaded in every run is made easy with the help of workflow parameters and Psyberg Metadata.\n\n## The Setup: Automated end-to-end catchup\n\nLet’s go back to our customer lifecycle example. Once we integrate all four components with Psyberg, here’s how we would set it up for automated catchup.\n\nThe three fact tables, comprising the signup and plan facts encapsulated in Psyberg’s stateless mode, along with the cancel fact in stateful mode, serve as inputs for the stateful sequential load ETL pipeline. This data pipeline monitors the various stages in the customer lifecycle.\n\nIn the sequential load ETL, we have the following features:\n\n*   **Catchup Threshold**: This defines the lookback period for the data being read. For instance, only consider the last 12 hours of data.\n*   **Data Load Type**: The ETL can either load the missed/new data specifically or reload the entire specified range.\n*   **Metadata Recording**: Metadata is persisted for traceability.\n\nHere is a **walkthrough** on how this system would automatically catch up in the event of late-arriving data:\n\n**Premise:** All the tables were last loaded up to hour 5, meaning that any data from hour 6 onwards is considered new, and anything before that is classified as late data (as indicated in red above)\n\n**Fact level catchup**:\n\n1.  During the Psyberg initialization phase, the signup and plan facts identify the late data from hours 2 and 3, as well as the most recent data from hour 6. The ETL then appends this data to the corresponding partitions within the fact tables.\n2.  The Psyberg initialization for the cancel fact identifies late data from hour 5 and additional data from hours 6 and 7. Since this ETL operates in stateful mode, the data in the target table from hours 5 to 7 will be overwritten with the new data.\n3.  By focusing solely on updates and avoiding reprocessing of data based on a fixed lookback window, both Stateless and Stateful Data Processing maintain a minimal change footprint. This approach ensures data processing is both efficient and accurate.\n\n**Dimension level catchup**:\n\n1.  The Psyberg wrapper for this stateful ETL looks at the updates to the upstream Psyberg powered fact tables to determine the date-hour range to reprocess. Here’s how it would calculate the above range:  \n    **MinHr = least(min processing hour from each source table)**  \n    This ensures that we don’t miss out on any data, including late-arriving data. In this case, the minimum hour to process the data is hour 2.  \n    **MaxHr = least(max processing hour from each source table)  \n    **This ensures we do not process partial data, i.e., hours for which data has not been loaded into all source tables. In this case, the maximum hour to process the data is hour 6.\n2.  The ETL process uses this time range to compute the state in the changed partitions and overwrite them in the target table. This helps overwrite data only when required and minimizes unnecessary reprocessing.\n\nAs seen above, by chaining these Psyberg workflows, we could automate the catchup for late-arriving data from hours 2 and 6. The Data Engineer does not need to perform any manual intervention in this case and can thus focus on more important things!\n\n## The Impact: How Psyberg Transformed Our Workflows\n\nThe introduction of Psyberg into our workflows has served as a valuable tool in enhancing accuracy and performance. The following are key areas that have seen improvements from using Psyberg:\n\n*   **Computational Resources Used:  \n    **In certain instances, we’ve noticed a significant reduction in resource utilization, with the number of Spark cores used dropping by 90% following the implementation of Psyberg, compared to using fixed lookback windows\n*   **Workflow and Table Onboarding:  \n    **We have onboarded 30 tables and 13 workflows into incremental processing since implementing Psyberg\n*   **Reliability and Accuracy:  \n    **Since onboarding workflows to Psyberg, we have experienced zero manual catchups or missing data incidents\n*   **Bootstrap template:  \n    **The process of integrating new tables into incremental processing has been made more accessible and now requires minimal effort using Psyberg\n\nThese performance metrics suggest that adopting Psyberg has been beneficial to the efficiency of our data processing workflows.\n\n## Next Steps and Conclusion\n\nIntegrating Psyberg into our operations has improved our data workflows and opened up exciting possibilities for the future. As we continue to innovate, Netflix’s data platform team is focused on creating a comprehensive solution for incremental processing use cases. This platform-level solution is intended to enhance our data processing capabilities across the organization. You can read more about this [here](https://netflixtechblog.com/incremental-processing-using-netflix-maestro-and-apache-iceberg-b8ba072ddeeb)!\n\nIn conclusion, Psyberg has proven to be a reliable and effective solution for our data processing needs. As we look to the future, we’re excited about the potential for further advancements in our data platform capabilities."
    },
    {
      "url": "https://netflixtechblog.com/2-diving-deeper-into-psyberg-stateless-vs-stateful-data-processing-1d273b3aaefb?source=collection_home---4------1-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/2-diving-deeper-into-psyberg-stateless-vs-stateful-data-processing-1d273b3aaefb?gi=0a217b4a3fe0&source=collection_home---4------1-----------------------",
        "loadedTime": "2023-12-06T00:03:07.545Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/2-diving-deeper-into-psyberg-stateless-vs-stateful-data-processing-1d273b3aaefb",
        "title": "Diving Deeper into Psyberg: Stateless vs Stateful Data Processing | by Netflix Technology Blog | Nov, 2023 | Netflix TechBlog",
        "description": "In the inaugural blog post of this series, we introduced you to the state of our pipelines before Psyberg and the challenges with incremental processing that led us to create the Psyberg framework…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Diving Deeper into Psyberg: Stateless vs Stateful Data Processing\nBy Abhinaya Shetty, Bharath Mummadisetty\nIn the inaugural blog post of this series, we introduced you to the state of our pipelines before Psyberg and the challenges with incremental processing that led us to create the Psyberg framework within Netflix’s Membership and Finance data engineering team. In this post, we will delve into a more detailed exploration of Psyberg’s two primary operational modes: stateless and stateful.\nModes of Operation of Psyberg\nPsyberg has two main modes of operation or patterns, as we call them. Understanding the nature of the late-arriving data and processing requirements will help decide which pattern is most appropriate for a use case.\nStateless Data Processing: As the name suggests, one should use this pattern in scenarios where the columns in the target table solely depend on the content of the incoming events, irrespective of their order of occurrence. For instance, consider a scenario where we need to keep track of all the customer signups over time. In this case, the order of signups wouldn’t matter, and individual signup records are independent of each other. This information has only one source, and we can append new/late records to the fact table as and when the events are received.\nStateful Data Processing: This pattern is useful when the output depends on a sequence of events across one or more input streams. For example, the customer account lifecycle in a business might involve multiple stages, such as account creation, plan upgrades, downgrades, and cancellation. To derive attributes like the lifetime of an account or the latest plan the account is on, we need to track the sequence of these events across different input streams. A missed event in such a scenario would result in incorrect analysis due to a wrong derived state. Late-arriving data in such cases requires overwriting data that was previously processed to ensure all events are accounted for.\nLet’s visualize how these two modes work within our data processing pipeline using a general workflow for loading a fact table. If you would like to learn more about how the workflows are orchestrated in Netflix Maestro scheduler, please check out this blog post from our data platform team.\nWith this illustration as our guide, let’s explore each mode in more detail.\nThe Psyberg Initialization Phase\nThis step invokes Psyberg with the required parameters. Based on these parameters, Psyberg then computes the correct data range for the pipeline processing needs.\nInput parameters in this step include the following:\nInitialization for Stateless Data Processing\nLet’s use the signup fact table as an example here. This table’s workflow runs hourly, with the main input source being an Iceberg table storing all raw signup events partitioned by landing date, hour, and batch id.\nHere’s a YAML snippet outlining the configuration for this during the Psyberg initialization step:\n- job:\nid: psyberg_session_init\ntype: Spark\nspark:\napp_args:\n- --process_name=signup_fact_load\n- --src_tables=raw_signups\n- --psyberg_session_id=20230914061001\n- --psyberg_hwm_table=high_water_mark_table\n- --psyberg_session_table=psyberg_session_metadata\n- --etl_pattern_id=1\nBehind the scenes, Psyberg identifies that this pipeline is configured for a stateless pattern since etl_pattern_id=1.\nPsyberg also uses the provided inputs to detect the Iceberg snapshots that persisted after the latest high watermark available in the watermark table. Using the summary column in snapshot metadata [see the Iceberg Metadata section in post 1 for more details], we parse out the partition information for each Iceberg snapshot of the source table.\nPsyberg then retains these processing URIs (an array of JSON strings containing combinations of landing date, hour, and batch IDs) as determined by the snapshot changes. This information and other calculated metadata are stored in the psyberg_session_f table. This stored data is then available for the subsequent LOAD.FACT_TABLE job in the workflow to utilize and for analysis and debugging purposes.\nInitialization for Stateful Data Processing\nStateful Data Processing is used when the output depends on a sequence of events across one or more input streams.\nLet’s consider the example of creating a cancel fact table, which takes the following as input:\nRaw cancellation events indicating when the customer account was canceled\nA fact table that stores incoming customer requests to cancel their subscription at the end of the billing period\nThese inputs help derive additional stateful analytical attributes like the type of churn i.e. voluntary or involuntary, etc.\nThe initialization step for Stateful Data Processing differs slightly from Stateless. Psyberg offers additional configurations according to the pipeline needs. Here’s a YAML snippet outlining the configuration for the cancel fact table during the Psyberg initialization step:\n- job:\nid: psyberg_session_init\ntype: Spark\nspark:\napp_args:\n- --process_name=cancel_fact_load\n- --src_tables=raw_cancels|processing_ts,cancel_request_fact\n- --psyberg_session_id=20230914061501\n- --psyberg_hwm_table=high_water_mark_table\n- --psyberg_session_table=psyberg_session_metadata\n- --etl_pattern_id=2\nBehind the scenes, Psyberg identifies that this pipeline is configured for a stateful pattern since etl_pattern_id is 2.\nNotice the additional detail in the src_tables list corresponding to raw_cancels above. The processing_ts here represents the event processing timestamp which is different from the regular Iceberg snapshot commit timestamp i.e. event_landing_ts as described in part 1 of this series.\nIt is important to capture the range of a consolidated batch of events from all the sources i.e. both raw_cancels and cancel_request_fact, while factoring in late-arriving events. Changes to the source table snapshots can be tracked using different timestamp fields. Knowing which timestamp field to use i.e. event_landing_ts or something like processing_ts helps avoid missing events.\nSimilar to the approach in stateless data processing, Psyberg uses the provided inputs to parse out the partition information for each Iceberg snapshot of the source table.\nThis is then used to query the partitions metadata table which has the min and max range for each column in the source table. In this case, we look at the min and max range of the processing_ts column to determine actual partitions for any late-arriving events. The minimum value here helps determine the lower limit of the data to be processed i.e. the derived minimum date and hour based on the input epoch timestamp.\nIt also tracks the VTTS (Valid To TimeStamp) of all the input streams and determines the minimum VTTS of all the streams together. This helps determine the upper limit of data to be processed, thus restricting the data load based on data completeness of all the streams combined.\nUsing this metadata from different streams, Psyberg calculates several parameters like minimum/maximum processing date and hour and event landing date hour. These parameters, along with other metadata, discussed in the previous post, are persisted in the psyberg_session_f table for analysis and debugging purposes.\nWrite Audit Publish (WAP) process\nThe Write Audit Publish (WAP) process is a general pattern we use in our ETLs to validate writes to the uncommitted Iceberg snapshot before publishing to the target table. The LOAD.FACT_TABLE step takes psyberg_session_id and process_name as input arguments.\nFor stateless pattern, the processing URIs to be processed as part of the load step are identified by reading the psyberg_session_f table. This information is then used to filter the source table and apply the business logic to create the signup fact table. Any late-arriving signup events data is appended to the target table partitions as part of this. All these writes go into the uncommitted Iceberg snapshot managed by the WAP pattern.\nSimilarly, in the stateful pattern, the ETL step reads the psyberg_session_f table to identify the derived minimum and maximum date hour range to be processed, which acts as a filter for different input tables involved in the ETL. After applying the corresponding business logic for cancellation events, we create the cancel fact table along with columns like cancellation type (i.e., voluntary vs involuntary churn) representing the state of the canceled account. If there are any late-arriving events, Psyberg handles them automatically by providing the correct range to the data process to derive the state changes correctly.\nAudits\nWe run different audits on the uncommitted Iceberg snapshot created as part of the job run. Leveraging Psyberg metadata, we can identify the cohort of data involved as part of the job run. This helps in pinpointing changes and applying blocking audits efficiently. Audits like source-to-target count comparison and checking for no missing events in the target Iceberg snapshot ensure data integrity and completeness. Once the audits pass successfully, the data is published to the target table.\nHWM Commit\nLeveraging Psyberg metadata tables, we determine the latest timestamp associated with the Iceberg snapshot seen as part of the job run. This timestamp is used to update the high watermark table with the new high watermark so that the subsequent pipeline instance can pick up the next set of changes.\nConclusion\nThis exploration shows how Psyberg brings efficiency, accuracy, and timeliness to Stateless and Stateful Data Processing within the Membership and Finance data engineering team. Join us in the next part of our blog series, where we’ll discuss how it also helps automate the end-to-end catchup of different pipelines.",
      "markdown": "## Diving Deeper into Psyberg: Stateless vs Stateful Data Processing\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----1d273b3aaefb--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----1d273b3aaefb--------------------------------)\n\nBy [_Abhinaya Shetty_](https://www.linkedin.com/in/abhinaya-shetty-ab871418/), [_Bharath Mummadisetty_](https://www.linkedin.com/in/bharath-chandra-mummadisetty-27591a88/)\n\nIn the [inaugural blog post](https://netflixtechblog.medium.com/f68830617dd1) of this series, we introduced you to the state of our pipelines before Psyberg and the challenges with incremental processing that led us to create the Psyberg framework within Netflix’s Membership and Finance data engineering team. In this post, we will delve into a more detailed exploration of Psyberg’s two primary operational modes: stateless and stateful.\n\n## Modes of Operation of Psyberg\n\nPsyberg has two main modes of operation or patterns, as we call them. Understanding the nature of the late-arriving data and processing requirements will help decide which pattern is most appropriate for a use case.\n\n1.  **Stateless Data Processing**: As the name suggests, one should use this pattern in scenarios where the columns in the target table solely depend on the content of the incoming events, irrespective of their order of occurrence. For instance, consider a scenario where we need to keep track of all the customer signups over time. In this case, the order of signups wouldn’t matter, and individual signup records are independent of each other. This information has only one source, and we can append new/late records to the fact table as and when the events are received.\n2.  **Stateful Data Processing**: This pattern is useful when the output depends on a sequence of events across one or more input streams. For example, the customer account lifecycle in a business might involve multiple stages, such as account creation, plan upgrades, downgrades, and cancellation. To derive attributes like the lifetime of an account or the latest plan the account is on, we need to track the sequence of these events across different input streams. A missed event in such a scenario would result in incorrect analysis due to a wrong derived state. Late-arriving data in such cases requires overwriting data that was previously processed to ensure all events are accounted for.\n\nLet’s visualize how these two modes work within our data processing pipeline using a general workflow for loading a fact table. If you would like to learn more about how the workflows are orchestrated in Netflix Maestro scheduler, please check out this [blog post](https://netflixtechblog.com/orchestrating-data-ml-workflows-at-scale-with-netflix-maestro-aaa2b41b800c) from our data platform team.\n\nWith this illustration as our guide, let’s explore each mode in more detail.\n\n## The Psyberg Initialization Phase\n\nThis step invokes Psyberg with the required parameters. Based on these parameters, Psyberg then computes the correct data range for the pipeline processing needs.\n\nInput parameters in this step include the following:\n\n## Initialization for Stateless Data Processing\n\nLet’s use the signup fact table as an example here. This table’s workflow runs hourly, with the main input source being an Iceberg table storing all raw signup events partitioned by landing date, hour, and batch id.\n\nHere’s a YAML snippet outlining the configuration for this during the Psyberg initialization step:\n\n\\- job:  \n   id: psyberg\\_session\\_init  \n   type: Spark  \n   spark:  \n     app\\_args:  \n       - --process\\_name=signup\\_fact\\_load  \n       - --src\\_tables=raw\\_signups  \n       - --psyberg\\_session\\_id=20230914061001  \n       - --psyberg\\_hwm\\_table=high\\_water\\_mark\\_table  \n       - --psyberg\\_session\\_table=psyberg\\_session\\_metadata  \n       - --etl\\_pattern\\_id=1\n\nBehind the scenes, Psyberg identifies that this pipeline is configured for a stateless pattern since **etl\\_pattern\\_id=1**.\n\nPsyberg also uses the provided inputs to detect the Iceberg snapshots that persisted after the latest high watermark available in the watermark table. Using the **summary column in snapshot metadata** \\[see the [Iceberg Metadata section in post 1](https://netflixtechblog.medium.com/f68830617dd1) for more details\\], we parse out the partition information for each Iceberg snapshot of the source table.\n\nPsyberg then retains these processing URIs (an array of JSON strings containing combinations of landing date, hour, and batch IDs) as determined by the snapshot changes. This information and other calculated metadata are stored in the **psyberg\\_session\\_f** table. This stored data is then available for the subsequent **LOAD.FACT\\_TABLE** job in the workflow to utilize and for analysis and debugging purposes.\n\n## Initialization for Stateful Data Processing\n\nStateful Data Processing is used when the output depends on a sequence of events across one or more input streams.\n\nLet’s consider the example of creating a cancel fact table, which takes the following as input:\n\n1.  **Raw cancellation events** indicating when the customer account was canceled\n2.  A fact table that stores incoming **customer requests** to cancel their subscription at the end of the billing period\n\nThese inputs help derive additional stateful analytical attributes like the type of churn i.e. voluntary or involuntary, etc.\n\nThe initialization step for Stateful Data Processing differs slightly from Stateless. Psyberg offers additional configurations according to the pipeline needs. Here’s a YAML snippet outlining the configuration for the cancel fact table during the Psyberg initialization step:\n\n\\- job:  \n   id: psyberg\\_session\\_init  \n   type: Spark  \n   spark:  \n     app\\_args:  \n       - --process\\_name=cancel\\_fact\\_load  \n       - --src\\_tables=raw\\_cancels|processing\\_ts,cancel\\_request\\_fact  \n       - --psyberg\\_session\\_id=20230914061501  \n       - --psyberg\\_hwm\\_table=high\\_water\\_mark\\_table  \n       - --psyberg\\_session\\_table=psyberg\\_session\\_metadata  \n       - --etl\\_pattern\\_id=2\n\nBehind the scenes, Psyberg identifies that this pipeline is configured for a stateful pattern since **etl\\_pattern\\_id** is 2.\n\nNotice the additional detail in the src\\_tables list corresponding to raw\\_cancels above. The **processing\\_ts** here represents the event processing timestamp which is different from the regular Iceberg snapshot commit timestamp i.e. **event\\_landing\\_ts** as described in [part 1](https://netflixtechblog.medium.com/f68830617dd1) of this series.\n\nIt is important to capture the range of a consolidated batch of events from all the sources i.e. both raw\\_cancels and cancel\\_request\\_fact, while factoring in late-arriving events. Changes to the source table snapshots can be tracked using different timestamp fields. Knowing which timestamp field to use i.e. **event\\_landing\\_ts** or something like **processing\\_ts** helps avoid missing events.\n\nSimilar to the approach in stateless data processing, Psyberg uses the provided inputs to parse out the partition information for each Iceberg snapshot of the source table.\n\nThis is then used to query the partitions metadata table which has the min and max range for each column in the source table. In this case, we look at the min and max range of the **processing\\_ts** column to determine actual partitions for any late-arriving events. The minimum value here helps determine the lower limit of the data to be processed i.e. the derived minimum date and hour based on the input epoch timestamp.\n\nIt also tracks the VTTS (Valid To TimeStamp) of all the input streams and determines the minimum VTTS of all the streams together. This helps determine the upper limit of data to be processed, thus restricting the data load based on data completeness of all the streams combined.\n\nUsing this metadata from different streams, Psyberg calculates several parameters like minimum/maximum processing date and hour and event landing date hour. These parameters, along with other metadata, discussed in the previous post, are persisted in the **psyberg\\_session\\_f** table for analysis and debugging purposes.\n\n## Write Audit Publish (WAP) process\n\nThe [Write Audit Publish (WAP) process](https://www.dremio.com/resources/webinars/the-write-audit-publish-pattern-via-apache-iceberg/) is a general pattern we use in our ETLs to validate writes to the uncommitted Iceberg snapshot before publishing to the target table. The **LOAD.FACT\\_TABLE** step takes **psyberg\\_session\\_id** and **process\\_name** as input arguments.\n\nFor stateless pattern, the processing URIs to be processed as part of the load step are identified by reading the **psyberg\\_session\\_f** table. This information is then used to filter the source table and apply the business logic to create the signup fact table. Any late-arriving signup events data is appended to the target table partitions as part of this. All these writes go into the uncommitted Iceberg snapshot managed by the WAP pattern.\n\nSimilarly, in the stateful pattern, the ETL step reads the **psyberg\\_session\\_f** table to identify the derived minimum and maximum date hour range to be processed, which acts as a filter for different input tables involved in the ETL. After applying the corresponding business logic for cancellation events, we create the cancel fact table along with columns like cancellation type (i.e., voluntary vs involuntary churn) representing the state of the canceled account. If there are any late-arriving events, Psyberg handles them automatically by providing the correct range to the data process to derive the state changes correctly.\n\n## Audits\n\nWe run different audits on the uncommitted Iceberg snapshot created as part of the job run. Leveraging Psyberg metadata, we can identify the cohort of data involved as part of the job run. This helps in pinpointing changes and applying blocking audits efficiently. Audits like source-to-target count comparison and checking for no missing events in the target Iceberg snapshot ensure data integrity and completeness. Once the audits pass successfully, the data is published to the target table.\n\n## HWM Commit\n\nLeveraging Psyberg metadata tables, we determine the latest timestamp associated with the Iceberg snapshot seen as part of the job run. This timestamp is used to update the high watermark table with the new high watermark so that the subsequent pipeline instance can pick up the next set of changes.\n\n## Conclusion\n\nThis exploration shows how Psyberg brings efficiency, accuracy, and timeliness to Stateless and Stateful Data Processing within the Membership and Finance data engineering team. Join us in the [next part](https://netflixtechblog.medium.com/260fbe366fe2) of our blog series, where we’ll discuss how it also helps automate the end-to-end catchup of different pipelines."
    },
    {
      "url": "https://netflixtechblog.com/1-streamlining-membership-data-engineering-at-netflix-with-psyberg-f68830617dd1?source=collection_home---4------2-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/1-streamlining-membership-data-engineering-at-netflix-with-psyberg-f68830617dd1?gi=8ce04c625dc8&source=collection_home---4------2-----------------------",
        "loadedTime": "2023-12-06T00:03:11.345Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/1-streamlining-membership-data-engineering-at-netflix-with-psyberg-f68830617dd1",
        "title": "Streamlining Membership Data Engineering at Netflix with Psyberg | by Netflix Technology Blog | Nov, 2023 | Netflix TechBlog | Netflix TechBlog",
        "description": "At Netflix, our Membership and Finance Data Engineering team harnesses diverse data related to plans, pricing, membership life cycle, and revenue to fuel analytics, power various dashboards, and make…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Streamlining Membership Data Engineering at Netflix with Psyberg\nBy Abhinaya Shetty, Bharath Mummadisetty\nAt Netflix, our Membership and Finance Data Engineering team harnesses diverse data related to plans, pricing, membership life cycle, and revenue to fuel analytics, power various dashboards, and make data-informed decisions. Many metrics in Netflix’s financial reports are powered and reconciled with efforts from our team! Given our role on this critical path, accuracy is paramount. In this context, managing the data, especially when it arrives late, can present a substantial challenge!\nIn this three-part blog post series, we introduce you to Psyberg, our incremental data processing framework designed to tackle such challenges! We’ll discuss batch data processing, the limitations we faced, and how Psyberg emerged as a solution. Furthermore, we’ll delve into the inner workings of Psyberg, its unique features, and how it integrates into our data pipelining workflows. By the end of this series, we hope you will gain an understanding of how Psyberg transformed our data processing, making our pipelines more efficient, accurate, and timely. Let’s dive in!\nThe Challenge: Incremental Data Processing with Late Arriving Data\nOur teams’ data processing model mainly comprises batch pipelines, which run at different intervals ranging from hourly to multiple times a day (also known as intraday) and even daily. We expect complete and accurate data at the end of each run. To meet such expectations, we generally run our pipelines with a lag of a few hours to leave room for late-arriving data.\nWhat is late-arriving data?\nLate-arriving data is essentially delayed data due to system retries, network delays, batch processing schedules, system outages, delayed upstream workflows, or reconciliation in source systems.\nHow does late-arriving data impact us?\nYou could think of our data as a puzzle. With each new piece of data, we must fit it into the larger picture and ensure it’s accurate and complete. Thus, we must reprocess the missed data to ensure data completeness and accuracy.\nTypes of late-arriving data\nBased on the structure of our upstream systems, we’ve classified late-arriving data into two categories, each named after the timestamps of the updated partition:\nWays to process such data\nOur team previously employed some strategies to manage these scenarios, which often led to unnecessarily reprocessing unchanged data. Some techniques we used were:\n1. Using fixed lookback windows to always reprocess data, assuming that most late-arriving events will occur within that window. However, this approach usually leads to redundant data reprocessing, thereby increasing ETL processing time and compute costs. It also becomes inefficient as the data scale increases. Imagine reprocessing the past 6 hours of data every hour!\n2. Add alerts to flag when late arriving data appears, block the pipelines, and perform a manual intervention where we triggered backfill pipelines to handle the missed events. This approach was a simple solution with minimal extra processing for the most part and, hence, was our preferred solution. However, when the late events occurred, the pain of reprocessing data and catching up on all the dependent pipelines was not worth it! We will talk about this shortly.\nAt a high level, both these approaches were inefficient for intraday pipelines and impacted cost, performance, accuracy, and time. We developed Psyberg, an incremental processing framework using Iceberg to handle these challenges more effectively.\nThe state of our pipelines before Psyberg\nBefore diving into the world of Psyberg, it’s crucial to take a step back and reflect on the state of the data pipelines in our team before its implementation. The complexities involved in these processes and the difficulties they posed led to the development of Psyberg.\nAt Netflix, our backend microservices continuously generate real-time event data that gets streamed into Kafka. These raw events are the source of various data processing workflows within our team. We ingest this diverse event data and transform it into standardized fact tables. The fact tables then feed downstream intraday pipelines that process the data hourly. The sequential load ETL shown in the diagram below depicts one such pipeline that calculates an account's state every hour.\nRaw data for hours 3 and 6 arrive. Hour 6 data flows through the various workflows, while hour 3 triggers a late data audit alert.\nLet’s walk through an example to understand the complexity of this pre-Psyberg world.\nConsider a simplified version of our pipelines where we process three events: signups, plan changes, and cancels. Now imagine that some signup events from hour 3 were delayed and sent in at hour 6 instead. Our audits would detect this and alert the on-call data engineer (DE). The on-call DE would then face the daunting task of making things right!\nStep 1: Dive into the audit logs to identify the late-arriving data and the impacted workflows. In this case, they would discover that the late-arriving data for hour 3 must be included in the signup facts.\nStep 2: Stop all impacted workflows and downstream jobs (such as the sequential load ETL) and patch the missed data in the fact tables. Now, the data in the signup fact is patched.\nStep 3: Identify the number of partitions to be rerun for the sequential stateful load jobs to account for the delayed data and rerun them from the impacted date-hour. The DE would note that the data for hours 3–6 needs to be reprocessed and will retrigger four instances to be run sequentially. This step is crucial because missing signup events from hour 3 would result in us missing subsequent events for those affected accounts (e.g., a cancel event for a missed signup would have had no effect). As we capture the state of an account based on the sequence of different types of events, rerunning the sequential load ETL from hours 3 to 6 ensures the accurate representation of account states.\nStep 4: Now that we’ve spent significant time triaging and resolving the alert, the sequential ETL workflow likely experienced a delay. As a result, we need to catch up to schedule. To compensate for the lost time, the DE must trigger a few additional instances until the latest hour that would have run if the data hadn’t arrived late.\nThis entire process was challenging and required significant manual intervention from the on-call DE perspective. Note that these are hourly jobs, so the alert could be triggered at any time of the day (or night!). Yes, they were infrequent, but a big pain point when they occurred! Also, the on-call DE was usually not the SME for these pipelines, as the late data could have arrived in any of our upstream pipelines. To solve these problems, we came up with Psyberg!\nPsyberg: The Game Changer!\nPsyberg automates our data loads, making it suitable for various data processing needs, including intraday pipeline use cases. It leverages Iceberg metadata to facilitate processing incremental and batch-based data pipelines.\nOne of the critical features of Psyberg is its ability to detect and manage late-arriving data, no matter the partition it lands in. This feature allows data pipelines to handle late-arriving data effectively without manual intervention, ensuring higher data accuracy in our systems. Iceberg metadata and Psyberg’s own metadata form the backbone of its efficient data processing capabilities.\nETL Process High Watermark\nThis is the last recorded update timestamp for any data pipeline process. This is mainly used to identify new changes since the last update.\nIceberg Metadata\nPsyberg primarily harnesses two key iceberg metadata tables — snapshots and partitions — to manage the workload. All Iceberg tables have associated metadata that provide insight into changes or updates within the data tables.\nThe snapshots metadata table records essential metadata such as:\nThe creation time of a snapshot\nThe type of operation performed (append, overwrite, etc.)\nA summary of partitions created/updated during the generation of the Iceberg snapshot\nThese details enable Psyberg to track different operations and identify changes made to a source table since the previous high watermark. For example:\nThe partitions metadata table is particularly interesting as it stores:\nInformation about partition keys used in the data table\nColumn names and the range of values for each column within a specific partition\nOne unique aspect of Netflix’s internal implementation is that it provides the range of values for each column within a partition in a deserialized format. This information helps Psyberg comprehend the timestamp ranges for both types of late-arriving data (event and processing time) without querying the actual data.\nPsyberg Metadata\nIn addition to Iceberg metadata, Psyberg maintains its own metadata tables — the session table and the high watermark table. Both these tables are partitioned by the pipeline process name to maintain information related to each data pipeline independently.\nThe session table captures metadata specific to each pipeline run, including:\nProcess name partition to track all the runs associated with the data pipeline process\nSession ID to track unique runs within the process\nProcessing URIs to identify the input partitions involved in the load\n“from date”, “from hour”, “to date” and “to hour” for both event and processing times\nThe high watermark table stores relevant values from the session table at the end of each pipeline run:\nLatest and previous high water mark timestamp\nMetadata related to the latest run\nThis information is vital for each pipeline run instance as it helps determine the data to be loaded, updates the high water mark after processing, and finally generates output signals to inform downstream workflows about the date-hour up to which data is complete and available. It also serves as an essential resource for debugging and creating audits on the pipeline jobs.\nConclusion\nIn this post, we described our data architecture at a high level, along with the pain points that led to the development of Psyberg. We also went into details related to the metadata that powers Psyberg. If you understand the challenges faced by the on-call DE and would like to learn more about our solution, please check out the next iteration of this three-part series, where we delve deeper into different modes of Psyberg.",
      "markdown": "## Streamlining Membership Data Engineering at Netflix with Psyberg\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----f68830617dd1--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----f68830617dd1--------------------------------)\n\nBy [_Abhinaya Shetty_](https://www.linkedin.com/in/abhinaya-shetty-ab871418/), [_Bharath Mummadisetty_](https://www.linkedin.com/in/bharath-chandra-mummadisetty-27591a88/)\n\nAt Netflix, our **Membership and Finance Data Engineering team** harnesses diverse data related to plans, pricing, membership life cycle, and revenue to fuel analytics, power various dashboards, and make data-informed decisions. Many metrics in [**Netflix’s financial reports**](https://s2.bl-1.com/h/i/dtZJ85P6/tWbBNBk) are powered and reconciled with efforts from our team! Given our role on this critical path, **accuracy** is paramount. In this context, managing the data, especially when it arrives late, can present a substantial challenge!\n\nIn this three-part blog post series, we introduce you to **_Psyberg_, our incremental data processing framework** designed to tackle such challenges! We’ll discuss batch data processing, the limitations we faced, and how Psyberg emerged as a solution. Furthermore, we’ll delve into the inner workings of Psyberg, its unique features, and how it integrates into our data pipelining workflows. By the end of this series, we hope you will gain an understanding of how Psyberg transformed our data processing, making our pipelines more efficient, accurate, and timely. Let’s dive in!\n\n## The Challenge: Incremental Data Processing with Late Arriving Data\n\nOur teams’ data processing model mainly comprises **batch pipelines**, which run at different intervals ranging from hourly to multiple times a day (also known as intraday) and even daily. We expect **complete and accurate data** at the end of each run. To meet such expectations, we generally run our pipelines with a lag of a few hours to leave room for late-arriving data.\n\n## What is late-arriving data?\n\nLate-arriving data is essentially delayed data due to system retries, network delays, batch processing schedules, system outages, delayed upstream workflows, or reconciliation in source systems.\n\n## How does late-arriving data impact us?\n\nYou could think of our data as a puzzle. With each new piece of data, we must fit it into the larger picture and ensure it’s accurate and complete. Thus, we must reprocess the missed data to ensure data completeness and accuracy.\n\n## Types of late-arriving data\n\nBased on the structure of our upstream systems, we’ve classified late-arriving data into two categories, each named after the timestamps of the updated partition:\n\n## Ways to process such data\n\nOur team previously employed some strategies to manage these scenarios, which often led to unnecessarily reprocessing unchanged data. Some techniques we used were:\n\n1\\. Using **fixed lookback** windows to always reprocess data, assuming that most late-arriving events will occur within that window. However, this approach usually leads to redundant data reprocessing, thereby increasing [ETL](https://en.wikipedia.org/wiki/Extract,_transform,_load) processing time and compute costs. It also becomes inefficient as the data scale increases. Imagine reprocessing the past 6 hours of data every hour!\n\n2\\. **Add alerts** to flag when late arriving data appears, block the pipelines, and perform a manual intervention where we triggered backfill pipelines to handle the missed events. This approach was a simple solution with minimal extra processing for the most part and, hence, was our preferred solution. However, when the late events occurred, the pain of reprocessing data and catching up on all the dependent pipelines was not worth it! We will talk about this shortly.\n\nAt a high level, both these approaches were inefficient for intraday pipelines and impacted cost, performance, accuracy, and time. We developed **Psyberg**, an incremental processing framework using [Iceberg](https://iceberg.apache.org/) to handle these challenges more effectively.\n\n## The state of our pipelines before Psyberg\n\nBefore diving into the world of Psyberg, it’s crucial to take a step back and reflect on the state of the data pipelines in our team before its implementation. The complexities involved in these processes and the difficulties they posed led to the development of Psyberg.\n\nAt Netflix, our backend microservices continuously generate real-time event data that gets streamed into Kafka. These raw events are the source of various data processing workflows within our team. We ingest this diverse event data and transform it into standardized fact tables. The fact tables then feed downstream intraday pipelines that process the data hourly. The sequential load ETL shown in the diagram below depicts one such pipeline that calculates an account's state every hour.\n\nRaw data for hours 3 and 6 arrive. Hour 6 data flows through the various workflows, while hour 3 triggers a late data audit alert.\n\nLet’s walk through an example to understand the complexity of this pre-Psyberg world.\n\nConsider a simplified version of our pipelines where we process three events: signups, plan changes, and cancels. Now imagine that some signup events from hour 3 were delayed and sent in at hour 6 instead. Our audits would detect this and alert the on-call data engineer (DE). The on-call DE would then face the daunting task of making things right!\n\n**Step 1**: Dive into the audit logs to identify the late-arriving data and the impacted workflows. In this case, they would discover that the late-arriving data for hour 3 must be included in the signup facts.\n\n**Step 2**: Stop all impacted workflows and downstream jobs (such as the sequential load ETL) and patch the missed data in the fact tables. Now, the data in the signup fact is patched.\n\n**Step 3**: Identify the number of partitions to be rerun for the sequential stateful load jobs to account for the delayed data and rerun them from the impacted date-hour. The DE would note that the data for hours 3–6 needs to be reprocessed and will retrigger four instances to be run sequentially. This step is crucial because missing signup events from hour 3 would result in us missing subsequent events for those affected accounts (e.g., a cancel event for a missed signup would have had no effect). As we capture the state of an account based on the sequence of different types of events, rerunning the sequential load ETL from hours 3 to 6 ensures the accurate representation of account states.\n\n**Step 4**: Now that we’ve spent significant time triaging and resolving the alert, the sequential ETL workflow likely experienced a delay. As a result, we need to catch up to schedule. To compensate for the lost time, the DE must trigger a few additional instances until the latest hour that would have run if the data hadn’t arrived late.\n\nThis entire process was challenging and required significant manual intervention from the on-call DE perspective. Note that these are hourly jobs, so the alert could be triggered at any time of the day (or night!). Yes, they were infrequent, but a big pain point when they occurred! Also, the on-call DE was usually not the SME for these pipelines, as the late data could have arrived in any of our upstream pipelines. To solve these problems, we came up with Psyberg!\n\n## Psyberg: The Game Changer!\n\nPsyberg automates our data loads, making it suitable for various data processing needs, including intraday pipeline use cases. It leverages Iceberg metadata to facilitate processing incremental and batch-based data pipelines.\n\nOne of the critical features of Psyberg is its ability to detect and manage late-arriving data, no matter the partition it lands in. This feature allows data pipelines to handle late-arriving data effectively without manual intervention, ensuring higher data accuracy in our systems. [Iceberg metadata](https://iceberg.apache.org/spec/) and Psyberg’s own metadata form the backbone of its efficient data processing capabilities.\n\n## ETL Process High Watermark\n\nThis is the last recorded update timestamp for any data pipeline process. This is mainly used to identify new changes since the last update.\n\n## Iceberg Metadata\n\nPsyberg primarily harnesses two key iceberg metadata tables — _snapshots and partitions_ — to manage the workload. All Iceberg tables have associated metadata that provide insight into changes or updates within the data tables.\n\nThe snapshots metadata table records essential metadata such as:\n\n*   The creation time of a snapshot\n*   The type of operation performed (append, overwrite, etc.)\n*   A summary of partitions created/updated during the generation of the Iceberg snapshot\n\nThese details enable Psyberg to track different operations and identify changes made to a source table since the previous high watermark. For example:\n\nThe partitions metadata table is particularly interesting as it stores:\n\n*   Information about partition keys used in the data table\n*   Column names and the range of values for each column within a specific partition\n\nOne unique aspect of Netflix’s internal implementation is that it provides the range of values for each column within a partition in a deserialized format. This information helps Psyberg comprehend the timestamp ranges for both types of late-arriving data (event and processing time) without querying the actual data.\n\n## Psyberg Metadata\n\nIn addition to Iceberg metadata, Psyberg maintains its own metadata tables — the session table and the high watermark table. Both these tables are partitioned by the pipeline process name to maintain information related to each data pipeline independently.\n\nThe session table captures metadata specific to each pipeline run, including:\n\n*   Process name partition to track all the runs associated with the data pipeline process\n*   Session ID to track unique runs within the process\n*   Processing URIs to identify the input partitions involved in the load\n*   “from date”, “from hour”, “to date” and “to hour” for both event and processing times\n\nThe high watermark table stores relevant values from the session table at the end of each pipeline run:\n\n*   Latest and previous high water mark timestamp\n*   Metadata related to the latest run\n\nThis information is vital for each pipeline run instance as it helps determine the data to be loaded, updates the high water mark after processing, and finally generates output signals to inform downstream workflows about the date-hour up to which data is complete and available. It also serves as an essential resource for debugging and creating audits on the pipeline jobs.\n\n## Conclusion\n\nIn this post, we described our data architecture at a high level, along with the pain points that led to the development of Psyberg. We also went into details related to the metadata that powers Psyberg. If you understand the challenges faced by the on-call DE and would like to learn more about our solution, please check out the [next iteration](https://netflixtechblog.medium.com/1d273b3aaefb) of this three-part series, where we delve deeper into different modes of Psyberg."
    },
    {
      "url": "https://netflixtechblog.com/detecting-speech-and-music-in-audio-content-afd64e6a5bf8?source=collection_home---4------0-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/detecting-speech-and-music-in-audio-content-afd64e6a5bf8?gi=57cbc858af59&source=collection_home---4------0-----------------------",
        "loadedTime": "2023-12-06T00:03:14.442Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/detecting-speech-and-music-in-audio-content-afd64e6a5bf8",
        "title": "Detecting Speech and Music in Audio Content | by Netflix Technology Blog | Nov, 2023 | Netflix TechBlog",
        "description": "When you enjoy the latest season of Stranger Things or Casa de Papel (Money Heist), have you ever wondered about the secrets to fantastic story-telling, besides the stunning visual presentation? From…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Detecting Speech and Music in Audio Content\nIroro Orife, Chih-Wei Wu and Yun-Ning (Amy) Hung\nIntroduction\nWhen you enjoy the latest season of Stranger Things or Casa de Papel (Money Heist), have you ever wondered about the secrets to fantastic story-telling, besides the stunning visual presentation? From the violin melody accompanying a pivotal scene to the soaring orchestral arrangement and thunderous sound-effects propelling an edge-of-your-seat action sequence, the various components of the audio soundtrack combine to evoke the very essence of story-telling. To uncover the magic of audio soundtracks and further improve the sonic experience, we need a way to systematically examine the interaction of these components, typically categorized as dialogue, music and effects.\nIn this blog post, we will introduce speech and music detection as an enabling technology for a variety of audio applications in Film & TV, as well as introduce our speech and music activity detection (SMAD) system which we recently published as a journal article in EURASIP Journal on Audio, Speech, and Music Processing.\nLike semantic segmentation for audio, SMAD separately tracks the amount of speech and music in each frame in an audio file and is useful in content understanding tasks during the audio production and delivery lifecycle. The detailed temporal metadata SMAD provides about speech and music regions in a polyphonic audio mixture are a first step for structural audio segmentation, indexing and pre-processing audio for the following downstream tasks. Let’s have a look at a few applications.\nPractical use cases for speech & music activity\nAudio dataset preparation\nSpeech & music activity is an important preprocessing step to prepare corpora for training. SMAD classifies & segments long-form audio for use in large corpora, such as\nmusical segments for music information retrieval tasks (MIR).\nutterances for speech tasks like speaker diarization, emotion classification, semantic and phonetic transcription and translation.\nFrom “Audio Signal Classification” by David Gerhard\nDialogue analysis & processing\nDuring encoding at Netflix, speech-gated loudness is computed for every audio master track and used for loudness normalization. Speech-activity metadata is thus a central part of accurate catalog-wide loudness management and improved audio volume experience for Netflix members.\nSimilarly, algorithms for dialogue intelligibility, spoken-language-identification and speech-transcription are only applied to audio regions where there is measured speech.\nMusic information retrieval\nThere are a few studio use cases where music activity metadata is important, including quality-control (QC) and at-scale multimedia content analysis and tagging.\nThere are also inter-domain tasks like singer-identification and song lyrics transcription, which do not fit neatly into either speech or classical MIR tasks, but are useful for annotating musical passages with lyrics in closed captions and subtitles.\nConversely, where neither speech nor music activity is present, such audio regions are estimated to have content classified as noisy, environmental or sound-effects.\nLocalization & Dubbing\nFinally, there are post-production tasks, which take advantage of accurate speech segmentation at the the spoken utterance or sentence level, ahead of translation and dub-script generation. Likewise, authoring accessibility-features like Audio Description (AD) involves music and speech segmentation. The AD narration is typically mixed-in to not overlap with the primary dialogue, while music lyrics strongly tied to the plot of the story, are sometimes referenced by AD creators, especially for translated AD.\nA voice actor in the studio\nOur Approach to Speech and Music Activity Detection\nAlthough the application of deep learning methods has improved audio classification systems in recent years, this data driven approach for SMAD requires large amounts of audio source material with audio-frame level speech and music activity labels. The collection of such fine-resolution labels is costly and labor intensive and audio content often cannot be publicly shared due to the copyright limitations. We address the challenge from a different angle.\nContent, genre and languages\nInstead of augmenting or synthesizing training data, we sample the large scale data available in the Netflix catalog with noisy labels. In contrast to clean labels, which indicate precise start and end times for each speech/music region, noisy labels only provide approximate timing, which may impact SMAD classification performance. Nevertheless, noisy labels allow us to increase the scale of the dataset with minimal manual efforts and potentially generalize better across different types of content.\nOur dataset, which we introduced as TVSM (TV Speech and Music) in our publication, has a total number of 1608 hours of professionally recorded and produced audio. TVSM is significantly larger than other SMAD datasets and contains both speech and music labels at the frame level. TVSM also contains overlapping music and speech labels, and both classes have a similar total duration.\nTraining examples were produced between 2016 and 2019, in 13 countries, with 60% of the titles originating in the USA. Content duration ranged from 10 minutes to over 1 hour, across the various genres listed below.\nThe dataset contains audio tracks in three different languages, namely English, Spanish, and Japanese. The language distribution is shown in the figure below. The name of the episode/TV show for each sample remains unpublished. However, each sample has both a show-ID and a season-ID to help identify the connection between the samples. For instance, two samples from different seasons of the same show would share the same show ID and have different season IDs.\nWhat constitutes music or speech?\nTo evaluate and benchmark our dataset, we manually labeled 20 audio tracks from various TV shows which do not overlap with our training data. One of the fundamental issues encountered during the annotation of our manually-labeled TVSM-test set, was the definition of music and speech. The heavy usage of ambient sounds and sound effects blurs the boundaries between active music regions and non-music. Similarly, switches between conversational speech and singing voices in certain TV genres obscure where speech starts and music stops. Furthermore, must these two classes be mutually exclusive? To ensure label quality, consistency, and to avoid ambiguity, we converged on the following guidelines for differentiating music and speech:\nAny music that is perceivable by the annotator at a comfortable playback volume should be annotated.\nSince sung lyrics are often included in closed-captions or subtitles, human singing voices should all be annotated as both speech and music.\nAmbient sound or sound effects without apparent melodic contours should not be annotated as music. Traditional phone bell, ringing, or buzzing without apparent melodic contours should not be annotated as music.\nFilled pauses (uh, um, ah, er), backchannels (mhm, uh-huh), sighing, and screaming should not be annotated as speech.\nAudio format and preprocessing\nAll audio files were originally delivered from the post-production studios in the standard 5.1 surround format at 48 kHz sampling rate. We first normalize all files to an average loudness of −27 LKFS ± 2 LU dialog-gated, then downsample to 16 kHz before creating an ITU downmix.\nModel Architecture\nOur modeling choices take advantage of both convolutional and recurrent architectures, which are known to work well on audio sequence classification tasks, and are well supported by previous investigations. We adapted the SOTA convolutional recurrent neural network (CRNN) architecture to accommodate our requirements for input/output dimensionality and model complexity. The best model was a CRNN with three convolutional layers, followed by two bi-directional recurrent layers and one fully connected layer. The model has 832k trainable parameters and emits frame-level predictions for both speech and music with a temporal resolution of 5 frames per second.\nFor training, we leveraged our large and diverse catalog dataset with noisy labels, introduced above. Applying a random sampling strategy, each training sample is a 20 second segment obtained by randomly selecting an audio file and corresponding starting timecode offset on the fly. All models in our experiments were trained by minimizing binary cross-entropy (BCE) loss.\nEvaluation\nIn order to understand the influence of different variables in our experimental setup, e.g. model architecture, training data or input representation variants like log-Mel Spectrogram versus per-channel energy normalization (PCEN), we setup a detailed ablation study, which we encourage the reader to explore fully in our EURASIP journal article.\nFor each experiment, we reported the class-wise F-score and error rate with a segment size of 10ms. The error rate is the summation of deletion rate (false negative) and insertion rate (false positive). Since a binary decision must be attained for music and speech to calculate the F-score, a threshold of 0.5 was used to quantize the continuous output of speech and music activity functions.\nResults\nWe evaluated our models on four open datasets comprising audio data from TV programs, YouTube clips and various content such as concert, radio broadcasts, and low-fidelity folk music. The excellent performance of our models demonstrates the importance of building a robust system that detects overlapping speech and music and supports our assumption that a large but noisy-labeled real-world dataset can serve as a viable solution for SMAD.\nConclusion\nAt Netflix, tasks throughout the content production and delivery lifecycle work are most often interested in one part of the soundtrack. Tasks that operate on just dialogue, music or effects are performed hundreds of times a day, by teams around the globe, in dozens of different audio languages. So investments in algorithmically-assisted tools for automatic audio content understanding like SMAD, can yield substantial productivity returns at scale while minimizing tedium.\nAdditional Resources\nWe have made audio features and labels available via Zenodo. There is also GitHub repository with the following audio tools:\nPython code for data pre-processing, including scripts for 5.1 downmixing, Mel spectrogram generation, MFCCs generation, VGGish features generation, and the PCEN implementation.\nPython code for reproducing all experiments, including scripts of data loaders, model implementations, training and evaluation pipelines.\nPre-trained models for each conducted experiment.\nPrediction outputs for all audio in the evaluation datasets.\nSpecial thanks to the entire Audio Algorithms team, as well as Amir Ziai, Anna Pulido, and Angie Pollema.",
      "markdown": "## Detecting Speech and Music in Audio Content\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----afd64e6a5bf8--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----afd64e6a5bf8--------------------------------)\n\n[Iroro Orife](https://www.linkedin.com/in/iroroorife/), [Chih-Wei Wu](https://www.linkedin.com/in/chih-wei-wu-73081689/) and [Yun-Ning (Amy) Hung](https://www.linkedin.com/in/yun-ning-hung/)\n\n## Introduction\n\nWhen you enjoy the latest season of _Stranger Things_ or _Casa de Papel (Money Heist)_, have you ever wondered about the secrets to fantastic story-telling, besides the stunning visual presentation? From the violin melody accompanying a pivotal scene to the soaring orchestral arrangement and thunderous sound-effects propelling an edge-of-your-seat action sequence, the various components of the audio soundtrack combine to evoke the very essence of story-telling. To uncover the magic of audio soundtracks and further improve the sonic experience, we need a way to systematically examine the interaction of these components, typically categorized as [dialogue, music and effects](https://www.jstor.org/stable/j.ctt16t8zf9).\n\nIn this blog post, we will introduce speech and music detection as an enabling technology for a variety of audio applications in Film & TV, as well as introduce our speech and music activity detection (SMAD) system which we recently published as a [journal article](https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-022-00253-8) in EURASIP Journal on Audio, Speech, and Music Processing.\n\nLike semantic segmentation for audio, SMAD separately tracks the amount of speech and music in each frame in an audio file and is useful in _content understanding_ tasks during the audio production and delivery lifecycle. The detailed temporal metadata SMAD provides about speech and music regions in a polyphonic audio mixture are a first step for structural audio segmentation, indexing and pre-processing audio for the following downstream tasks. Let’s have a look at a few applications.\n\n## Practical use cases for speech & music activity\n\n## Audio dataset preparation\n\nSpeech & music activity is an important preprocessing step to prepare corpora for training. SMAD classifies & segments long-form audio for use in large corpora, such as\n\n*   musical segments for [music information retrieval tasks](https://www.ismir.net/resources/datasets/) (MIR).\n*   utterances for [speech tasks](https://github.com/s3prl/s3prl#downstream) like speaker diarization, emotion classification, semantic and phonetic transcription and translation.\n\nFrom “Audio Signal Classification” by David Gerhard\n\n## Dialogue analysis & processing\n\n*   During encoding at Netflix, speech-gated loudness is computed for every audio master track and used for loudness normalization. Speech-activity metadata is thus a central part of accurate catalog-wide loudness management and improved audio volume experience for Netflix members.\n*   Similarly, algorithms for dialogue intelligibility, spoken-language-identification and speech-transcription are only applied to audio regions where there is measured speech.\n\n## Music information retrieval\n\n*   There are a few studio use cases where music activity metadata is important, including quality-control (QC) and at-scale multimedia content analysis and tagging.\n*   There are also inter-domain tasks like singer-identification and song lyrics transcription, which do not fit neatly into either speech or classical MIR tasks, but are useful for annotating musical passages with lyrics in closed captions and subtitles.\n*   Conversely, where neither speech nor music activity is present, such audio regions are estimated to have content classified as noisy, environmental or sound-effects.\n\n## Localization & Dubbing\n\nFinally, there are [post-production tasks](https://netflixtechblog.com/introducing-netflix-timed-text-authoring-lineage-6fb57b72ad41), which take advantage of accurate speech segmentation at the the spoken utterance or sentence level, ahead of translation and dub-script generation. Likewise, authoring accessibility-features like [Audio Description](https://en.wikipedia.org/wiki/Audio_description) (AD) involves music and speech segmentation. The AD narration is typically mixed-in to not overlap with the primary dialogue, while music lyrics strongly tied to the plot of the story, are sometimes referenced by AD creators, especially for translated AD.\n\nA voice actor in the studio\n\n## Our Approach to Speech and Music Activity Detection\n\nAlthough the application of deep learning methods has improved audio classification systems in recent years, this data driven approach for SMAD requires large amounts of audio source material with audio-frame level speech and music activity labels. The collection of such fine-resolution labels is costly and labor intensive and audio content often cannot be publicly shared due to the copyright limitations. We address the challenge from a different angle.\n\n## Content, genre and languages\n\nInstead of augmenting or synthesizing training data, we sample the large scale data available in the Netflix catalog with noisy labels. In contrast to clean labels, which indicate precise start and end times for each speech/music region, noisy labels only provide approximate timing, which may impact SMAD classification performance. Nevertheless, noisy labels allow us to increase the scale of the dataset with minimal manual efforts and potentially generalize better across different types of content.\n\nOur dataset, which we introduced as TVSM (TV Speech and Music) in [our publication](https://www.springeropen.com/epdf/10.1186/s13636-022-00253-8?sharing_token=qUE9lQ50qcQxbhy4q7WuAm_BpE1tBhCbnbw3BuzI2RPYHxmYyj04FfJD9WVAT3xVEfjU0YvWAKHjSrjS3Pk16I2vFtdRuQgSdmgaSKkf5JiXbOSb0AglyInIbQCpnL8z0kJbzIzN5s368ENFJJSbKW1C3I7fzTQEHjPKYPBd2xM%3D), has a total number of 1608 hours of professionally recorded and produced audio. TVSM is significantly larger than other SMAD datasets and contains both speech and music labels at the frame level. TVSM also contains overlapping music and speech labels, and both classes have a similar total duration.\n\nTraining examples were produced between 2016 and 2019, in 13 countries, with 60% of the titles originating in the USA. Content duration ranged from 10 minutes to over 1 hour, across the various genres listed below.\n\nThe dataset contains audio tracks in three different languages, namely English, Spanish, and Japanese. The **language distribution** is shown in the figure below. The name of the episode/TV show for each sample remains unpublished. However, each sample has both a show-ID and a season-ID to help identify the connection between the samples. For instance, two samples from different seasons of the same show would share the same show ID and have different season IDs.\n\n## What constitutes music or speech?\n\nTo evaluate and benchmark our dataset, we manually labeled 20 audio tracks from various TV shows which do not overlap with our training data. One of the fundamental issues encountered during the annotation of our manually-labeled TVSM-test set, was the definition of music and speech. The heavy usage of ambient sounds and sound effects blurs the boundaries between active music regions and non-music. Similarly, switches between conversational speech and singing voices in certain TV genres obscure where speech starts and music stops. Furthermore, must these two classes be mutually exclusive? To ensure label quality, consistency, and to avoid ambiguity, we converged on the following guidelines for differentiating music and speech:\n\n*   Any music that is perceivable by the annotator at a comfortable playback volume should be annotated.\n*   Since sung lyrics are often included in closed-captions or subtitles, human singing voices should all be annotated as both speech and music.\n*   Ambient sound or sound effects without **_apparent melodic contours_** should not be annotated as music. Traditional phone bell, ringing, or buzzing without apparent melodic contours should not be annotated as music.\n*   Filled pauses (uh, um, ah, er), backchannels (mhm, uh-huh), sighing, and screaming should not be annotated as speech.\n\n## Audio format and preprocessing\n\nAll audio files were originally delivered from the post-production studios in the standard 5.1 surround format at 48 kHz sampling rate. We first normalize all files to an average loudness of −27 LKFS ± 2 LU dialog-gated, then downsample to 16 kHz before creating an [ITU downmix](https://www.itu.int/dms_pubrec/itu-r/rec/bs/R-REC-BS.775-1-199407-S!!PDF-E.pdf).\n\n## Model Architecture\n\nOur modeling choices take advantage of both convolutional and recurrent architectures, which are known to work well on audio sequence classification tasks, and are well supported by previous investigations. We adapted the SOTA convolutional recurrent neural network (**CRNN**) architecture to accommodate our requirements for input/output dimensionality and model complexity. The best model was a CRNN with three convolutional layers, followed by two bi-directional recurrent layers and one fully connected layer. The model has 832k trainable parameters and emits frame-level predictions for both speech and music with a temporal resolution of 5 frames per second.\n\nFor training, we leveraged our large and diverse catalog dataset with noisy labels, introduced above. Applying a random sampling strategy, each training sample is a 20 second segment obtained by randomly selecting an audio file and corresponding starting timecode offset on the fly. All models in our experiments were trained by minimizing **binary cross-entropy (BCE) loss**.\n\n## Evaluation\n\nIn order to understand the influence of different variables in our experimental setup, e.g. model architecture, training data or input representation variants like log-Mel Spectrogram versus per-channel energy normalization (PCEN), we setup **a detailed ablation study**, which we encourage the reader to explore fully in our [EURASIP journal article](https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-022-00253-8).\n\nFor each experiment, we reported the class-wise F-score and error rate with a segment size of 10ms. The error rate is the summation of deletion rate (false negative) and insertion rate (false positive). Since a binary decision must be attained for music and speech to calculate the F-score, a threshold of 0.5 was used to quantize the continuous output of speech and music activity functions.\n\n## Results\n\nWe evaluated our models on **four open datasets** comprising audio data from TV programs, YouTube clips and various content such as concert, radio broadcasts, and low-fidelity folk music. The excellent performance of our models demonstrates the importance of building a robust system that detects **overlapping speech and music** and supports our assumption that a large but noisy-labeled real-world dataset can serve as a viable solution for SMAD.\n\n## Conclusion\n\nAt Netflix, tasks throughout the content production and delivery lifecycle work are most often interested in one part of the soundtrack. Tasks that operate on just dialogue, music or effects are performed hundreds of times a day, by teams around the globe, in dozens of different audio languages. So investments in algorithmically-assisted tools for automatic audio content understanding like SMAD, can yield substantial productivity returns at scale while minimizing tedium.\n\n## Additional Resources\n\nWe have made audio features and labels available via [Zenodo](https://zenodo.org/record/7025971). There is also [GitHub repository](https://github.com/biboamy/TVSM-dataset) with the following audio tools:\n\n*   Python code for data pre-processing, including scripts for 5.1 downmixing, Mel spectrogram generation, MFCCs generation, VGGish features generation, and the PCEN implementation.\n*   Python code for reproducing all experiments, including scripts of data loaders, model implementations, training and evaluation pipelines.\n*   Pre-trained models for each conducted experiment.\n*   Prediction outputs for all audio in the evaluation datasets.\n\n_Special thanks to the entire Audio Algorithms team, as well as_ [_Amir Ziai_](https://www.linkedin.com/in/amirziai/)_,_ [_Anna Pulido_](https://www.linkedin.com/in/anna-pulido-61025063/)_, and_ [_Angie Pollema_](https://www.linkedin.com/in/angiepollema1/)_._"
    },
    {
      "url": "https://netflixtechblog.com/the-next-step-in-personalization-dynamic-sizzles-4dc4ce2011ef?source=collection_home---4------1-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/the-next-step-in-personalization-dynamic-sizzles-4dc4ce2011ef?gi=82c1f004a2c3&source=collection_home---4------1-----------------------",
        "loadedTime": "2023-12-06T00:03:14.734Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/the-next-step-in-personalization-dynamic-sizzles-4dc4ce2011ef",
        "title": "The Next Step in Personalization: Dynamic Sizzles | by Netflix Technology Blog | Nov, 2023 | Netflix TechBlog",
        "description": "At Netflix, we strive to give our members an excellent personalized experience, helping them make the most successful and satisfying selections from our thousands of titles. We already personalize…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "The Next Step in Personalization: Dynamic Sizzles\nAuthors:Bruce Wobbe, Leticia Kwok\nAdditional Credits:Sanford Holsapple, Eugene Lok, Jeremy Kelly\nIntroduction\nAt Netflix, we strive to give our members an excellent personalized experience, helping them make the most successful and satisfying selections from our thousands of titles. We already personalize artwork and trailers, but we hadn’t yet personalized sizzle reels — until now.\nA sizzle reel is a montage of video clips from different titles strung together into a seamless A/V asset that gets members excited about upcoming launches (for example, our Emmys nominations or holiday collections). Now Netflix can create a personalized sizzle reel dynamically in real time and on demand. The order of the clips and included titles are personalized per member, giving each a unique and effective experience. These new personalized reels are called Dynamic Sizzles.\nIn this post, we will dive into the exciting details of how we create Dynamic Sizzles with minimal human intervention, including the challenges we faced and the solutions we developed.\nAn example of a Dynamic Sizzle created for Chuseok, the Korean mid-autumn harvest festival collection.\nOverview\nIn the past, each sizzle reel was created manually. The time and cost of doing this prevents scaling and misses the invaluable benefit of personalization, which is a bedrock principle at Netflix. We wanted to figure out how to efficiently scale sizzle reel production, while also incorporating personalization — all in an effort to yield greater engagement and enjoyment for our members.\nEnter the creation of Dynamic Sizzles. We developed a systems-based approach that uses our interactive and creative technology to programmatically stitch together multiple video clips alongside a synced audio track. The process involves compiling personalized multi-title/multi-talent promotional A/V assets on the fly into a Mega Asset. A Mega Asset is a large A/V asset made up of video clips from various titles, acting as a library from which the Dynamic Sizzle pulls media. These clips are then used to construct a personalized Dynamic Sizzle according to a predefined cadence.\nWith Dynamic Sizzles, we can utilize more focused creative work from editors and generate a multitude of personalized sizzle reels efficiently and effectively — up to 70% in terms of time and cost savings than a manually created one. This gives us the ability to create thousands, if not millions, of combinations of video clips and assets that result in optimized and personalized sizzle reel experiences for Netflix members.\nCreating the Mega Asset\nWhere To Begin\nOur first challenge was figuring out how to create the Mega Asset, as each video clip needs to be precise in its selection and positioning. A Mega Asset can contain any number of clips, and millions of unique Dynamic Sizzles can be produced from a single Mega Asset.\nWe accomplished this by using human editors to select the clips — ensuring that they are well-defined from both a creative and technical standpoint — then laying them out in a specific known order in a timeline. We also need each clip marked with an index to its location — an extremely tedious and time consuming process for an editor. To solve this, we created an Adobe Premiere plug-in to automate the process. Further verifications can also be done programmatically via ingestion of the timecode data, as we can validate the structure of the Mega Asset by looking at the timecodes.\nAn example of a title’s video clips layout.\nThe above layout shows how a single title’s clips are ordered in a Mega Asset and in 3 different lengths: 160, 80 and 40 frame rates. Each clip should be unique per title; however, when using multiple titles, they may share the same frame rate. This gives us more variety to choose from while maintaining a structured order in the layout.\nCadence\nThe cadence is a predetermined collection of clip lengths that indicates when, where, and for how long a title shows within a Dynamic Sizzle. The cadence ensures that when a Dynamic Sizzle is played, it will show a balanced view of any titles chosen, while still giving more time to a member’s higher ranked titles. Cadence is something we can personalize or randomize, and will continue to evolve as needed.\nSample Cadence\nIn the above sample cadence, Title A refers to the highest ranked title in a member’s personalized sort, Title B the second highest, and so on. The cadence is made up of 3 distinct segments with 5 chosen titles (A-E) played in sequence using various clip lengths. Each clip in the cadence refers to a different clip in the Mega Asset. For example, the 80 frame clip for title A in the first (red) segment is different from the 80 frame clip for title A in the third (purple) segment.\nComposing the Dynamic Sizzle\nPersonalization\nWhen a request comes in for a sizzle reel, our system determines what titles are in the Mega Asset and based on the request, a personalized list of titles is created and sorted. The top titles for a member are then used to construct the Dynamic Sizzle by leveraging the clips in the Mega Asset. Higher ranked titles get more weight in placement and allotted time.\nFinding Timecodes\nFor the Dynamic Sizzle process, we have to quickly and dynamically determine the timecodes for each clip in the Mega Asset and make sure they are easily accessed at runtime. We accomplish this by utilizing Netflix’s Hollow technology. Hollow allows us to store timecodes for quick searches and use timecodes as a map — a key can be used to find the timecodes needed as defined by the cadence. The key can be as simple as titleId-clip-1.\nBuilding The Reel\nThe ordering of the clips are set by the predefined cadence, which dictates the final layout and helps easily build the Dynamic Sizzle. For example, if the system knows to use title 17 within the Mega Asset, we can easily calculate the time offset for all the clips because of the known ordering of the titles and clips within the Mega Asset. This all comes together in the following way:\nThe result is a series of timecodes indicating the start and stop times for each clip. These codes appear in the order they should be played and the player uses them to construct a seamless video experience as seen in the examples below:\nThe Beautiful Game Dynamic Sizzle\nWith Dynamic Sizzles, each member experiences a personalized sizzle reel.\nExample of what 2 different profiles might see for the same sizzle\nPlaying the Dynamic Sizzle\nDelivering To The Player\nThe player leverages the Mega Asset by using timecodes to know where to start and stop each clip, and then seamlessly plays each one right after the other. This required a change in the API that devices normally use to get trailers. The API change was twofold. First, on the request we need the device to indicate that it can support Dynamic Sizzles. Second, on the response the timecode list needs to be sent. (Changing the API and rolling it out took time, so this all had to be implemented before Dynamic Sizzles could actually be used, tested, and productized.)\nChallenges With The Player\nThere were two main challenges with the player. First, in order to support features like background music across multiple unique video segments, we needed to support asymmetrical segment streaming from discontiguous locations in the Mega Asset. This involved modifying existing schemas and adding corresponding support to the player to allow for the stitching of the video and audio together separately while still keeping the timecodes in sync. Second, we needed to optimize our streaming algorithms to account for these much shorter segments, as some of our previous assumptions were incorrect when dealing with dozens of discontiguous tiny segments in the asset.\nBuilding Great Things Together\nWe are just getting started on this journey to build truly great experiences. While the challenges may seem endless, the work is incredibly fulfilling. The core to bringing these great engineering solutions to life is the direct collaboration we have with our colleagues and innovating together to solve these challenges.\nIf you are interested in working on great technology like Dynamic Sizzles, we’d love to talk to you! We are hiring: jobs.netflix.com",
      "markdown": "## **The Next Step in Personalization: Dynamic Sizzles**\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----4dc4ce2011ef--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----4dc4ce2011ef--------------------------------)\n\nAuthors:[Bruce Wobbe](https://www.linkedin.com/in/bruce-wobbe-197395/), [Leticia Kwok](https://www.linkedin.com/in/leticiak/)\n\nAdditional Credits:[Sanford Holsapple](https://www.linkedin.com/in/sanford-holsapple-782a3a158/), [Eugene Lok](https://www.linkedin.com/in/eugene-lok-6465045b/), [Jeremy Kelly](https://www.linkedin.com/in/jeremy-kelly-526a30180/)\n\n## Introduction\n\nAt Netflix, we strive to give our members an excellent personalized experience, helping them make the most successful and satisfying selections from our thousands of titles. We already personalize [artwork](https://netflixtechblog.com/artwork-personalization-c589f074ad76) and trailers, but we hadn’t yet personalized sizzle reels — until now.\n\nA sizzle reel is a montage of video clips from different titles strung together into a seamless A/V asset that gets members excited about upcoming launches (for example, our Emmys nominations or holiday collections). Now Netflix can create a personalized sizzle reel dynamically in real time and on demand. The order of the clips and included titles are personalized per member, giving each a unique and effective experience. These new personalized reels are called _Dynamic Sizzles_.\n\nIn this post, we will dive into the exciting details of how we create Dynamic Sizzles with minimal human intervention, including the challenges we faced and the solutions we developed.\n\nAn example of a Dynamic Sizzle created for Chuseok, the Korean mid-autumn harvest festival collection.\n\n## Overview\n\nIn the past, each sizzle reel was created manually. The time and cost of doing this prevents scaling and misses the invaluable benefit of personalization, which is a bedrock principle at Netflix. We wanted to figure out how to efficiently scale sizzle reel production, while also incorporating personalization — all in an effort to yield greater engagement and enjoyment for our members.\n\nEnter the creation of Dynamic Sizzles. We developed a systems-based approach that uses our interactive and creative technology to programmatically stitch together multiple video clips alongside a synced audio track. The process involves compiling personalized multi-title/multi-talent promotional A/V assets on the fly into a _Mega Asset_. A Mega Asset is a large A/V asset made up of video clips from various titles, acting as a library from which the Dynamic Sizzle pulls media. These clips are then used to construct a personalized Dynamic Sizzle according to a predefined cadence.\n\nWith Dynamic Sizzles, we can utilize more focused creative work from editors and generate a multitude of personalized sizzle reels efficiently and effectively — up to 70% in terms of time and cost savings than a manually created one. This gives us the ability to create thousands, if not millions, of combinations of video clips and assets that result in optimized and personalized sizzle reel experiences for Netflix members.\n\n## Creating the Mega Asset\n\n## Where To Begin\n\nOur first challenge was figuring out how to create the Mega Asset, as each video clip needs to be precise in its selection and positioning. A Mega Asset can contain any number of clips, and millions of unique Dynamic Sizzles can be produced from a single Mega Asset.\n\nWe accomplished this by using human editors to select the clips — ensuring that they are well-defined from both a creative and technical standpoint — then laying them out in a specific known order in a timeline. We also need each clip marked with an index to its location — an extremely tedious and time consuming process for an editor. To solve this, we created an Adobe Premiere plug-in to automate the process. Further verifications can also be done programmatically via ingestion of the timecode data, as we can validate the structure of the Mega Asset by looking at the timecodes.\n\nAn example of a title’s video clips layout.\n\nThe above layout shows how a single title’s clips are ordered in a Mega Asset and in 3 different lengths: 160, 80 and 40 frame rates. Each clip should be unique per title; however, when using multiple titles, they may share the same frame rate. This gives us more variety to choose from while maintaining a structured order in the layout.\n\n## Cadence\n\nThe cadence is a predetermined collection of clip lengths that indicates when, where, and for how long a title shows within a Dynamic Sizzle. The cadence ensures that when a Dynamic Sizzle is played, it will show a balanced view of any titles chosen, while still giving more time to a member’s higher ranked titles. Cadence is something we can personalize or randomize, and will continue to evolve as needed.\n\nSample Cadence\n\nIn the above sample cadence, Title A refers to the highest ranked title in a member’s personalized sort, Title B the second highest, and so on. The cadence is made up of 3 distinct segments with 5 chosen titles (A-E) played in sequence using various clip lengths. Each clip in the cadence refers to a different clip in the Mega Asset. For example, the 80 frame clip for title A in the first (red) segment is different from the 80 frame clip for title A in the third (purple) segment.\n\n## Composing the Dynamic Sizzle\n\n## Personalization\n\nWhen a request comes in for a sizzle reel, our system determines what titles are in the Mega Asset and based on the request, a personalized list of titles is created and sorted. The top titles for a member are then used to construct the Dynamic Sizzle by leveraging the clips in the Mega Asset. Higher ranked titles get more weight in placement and allotted time.\n\n## Finding Timecodes\n\nFor the Dynamic Sizzle process, we have to quickly and dynamically determine the timecodes for each clip in the Mega Asset and make sure they are easily accessed at runtime. We accomplish this by utilizing Netflix’s [Hollow technology](https://hollow.how/). Hollow allows us to store timecodes for quick searches and use timecodes as a map — a key can be used to find the timecodes needed as defined by the cadence. The key can be as simple as _titleId-clip-1._\n\n## Building The Reel\n\nThe ordering of the clips are set by the predefined cadence, which dictates the final layout and helps easily build the Dynamic Sizzle. For example, if the system knows to use title 17 within the Mega Asset, we can easily calculate the time offset for all the clips because of the known ordering of the titles and clips within the Mega Asset. This all comes together in the following way:\n\nThe result is a series of timecodes indicating the start and stop times for each clip. These codes appear in the order they should be played and the player uses them to construct a seamless video experience as seen in the examples below:\n\nThe Beautiful Game Dynamic Sizzle\n\nWith Dynamic Sizzles, each member experiences a personalized sizzle reel.\n\nExample of what 2 different profiles might see for the same sizzle\n\n## Playing the Dynamic Sizzle\n\n## Delivering To The Player\n\nThe player leverages the Mega Asset by using timecodes to know where to start and stop each clip, and then seamlessly plays each one right after the other. This required a change in the API that devices normally use to get trailers. The API change was twofold. First, on the request we need the device to indicate that it can support Dynamic Sizzles. Second, on the response the timecode list needs to be sent. (Changing the API and rolling it out took time, so this all had to be implemented before Dynamic Sizzles could actually be used, tested, and productized.)\n\n## Challenges With The Player\n\nThere were two main challenges with the player. First, in order to support features like background music across multiple unique video segments, we needed to support asymmetrical segment streaming from discontiguous locations in the Mega Asset. This involved modifying existing schemas and adding corresponding support to the player to allow for the stitching of the video and audio together separately while still keeping the timecodes in sync. Second, we needed to optimize our streaming algorithms to account for these much shorter segments, as some of our previous assumptions were incorrect when dealing with dozens of discontiguous tiny segments in the asset.\n\n## Building Great Things Together\n\nWe are just getting started on this journey to build truly great experiences. While the challenges may seem endless, the work is incredibly fulfilling. The core to bringing these great engineering solutions to life is the direct collaboration we have with our colleagues and innovating together to solve these challenges.\n\nIf you are interested in working on great technology like Dynamic Sizzles, we’d love to talk to you! We are hiring: [jobs.netflix.com](https://jobs.netflix.com/)"
    },
    {
      "url": "https://netflixtechblog.com/building-in-video-search-936766f0017c?source=collection_home---4------2-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/building-in-video-search-936766f0017c?gi=9d6f22fb6919&source=collection_home---4------2-----------------------",
        "loadedTime": "2023-12-06T00:03:16.673Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/building-in-video-search-936766f0017c",
        "title": "Building In-Video Search. Empowering video editors with… | by Netflix Technology Blog | Nov, 2023 | Netflix TechBlog",
        "description": "Boris Chen, Ben Klein, Jason Ge, Avneesh Saluja, Guru Tahasildar, Abhishek Soni, Juan Vimberg, Gustavo Carmo, Meenakshi Jindal, Elliot Chow, Amir Ziai, Varun Sekhri, Santiago Castro, Keila Fong…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Building In-Video Search\nBoris Chen, Ben Klein, Jason Ge, Avneesh Saluja, Guru Tahasildar, Abhishek Soni, Juan Vimberg, Gustavo Carmo, Meenakshi Jindal, Elliot Chow, Amir Ziai, Varun Sekhri, Santiago Castro, Keila Fong, Kelli Griggs, Mallia Sherzai, Robert Mayer, Andy Yao, Vi Iyengar, Jonathan Solorzano-Hamilton, Hossein Taghavi, Ritwik Kumar\nIntroduction\nToday we’re going to take a look at the behind the scenes technology behind how Netflix creates great trailers, Instagram reels, video shorts and other promotional videos.\nSuppose you’re trying to create the trailer for the action thriller The Gray Man, and you know you want to use a shot of a car exploding. You don’t know if that shot exists or where it is in the film, and you have to look for it it by scrubbing through the whole film.\nExploding cars — The Gray Man (2022)\nOr suppose it’s Christmas, and you want to create a great instagram piece out all the best scenes across Netflix films of people shouting “Merry Christmas”! Or suppose it’s Anya Taylor Joy’s birthday, and you want to create a highlight reel of all her most iconic and dramatic shots.\nCreating these involves sifting through hundreds of thousands of movies and TV shows to find the right line of dialogue or the appropriate visual elements (objects, scenes, emotions, actions, etc.). We have built an internal system that allows someone to perform in-video search across the entire Netflix video catalog, and we’d like to share our experience in building this system.\nBuilding in-video search\nTo build such a visual search engine, we needed a machine learning system that can understand visual elements. Our early attempts included object detection, but found that general labels were both too limiting and too specific, yet not specific enough. Every show has special objects that are important (e.g. Demogorgon in Stranger Things) that don’t translate to other shows. The same was true for action recognition, and other common image and video tasks.\nThe Approach\nWe learned that contrastive learning works well for our objectives when applied to image and text pairs, as these models can effectively learn joint embedding spaces between the two modalities. This approach is also able to learn about objects, scenes, emotions, actions, and more in a single model. We also found that extending contrastive learning to videos and text provided a substantial improvement over frame-level models.\nIn order to train the model on internal training data (video clips with aligned text descriptions), we implemented a scalable version on Ray Train and switched to a more performant video decoding library. Lastly, the embeddings from the video encoder exhibit strong zero or few-shot performance on multiple video and content understanding tasks at Netflix and are used as a starting point in those applications.\nThe recent success of large-scale models that jointly train image and text embeddings has enabled new use cases around multimodal retrieval. These models are trained on large amounts of image-caption pairs via in-batch contrastive learning. For a (large) batch of N examples, we wish to maximize the embedding (cosine) similarity of the N correct image-text pairs, while minimizing the similarity of the other N²-N paired embeddings. This is done by treating the similarities as logits and minimizing the symmetric cross-entropy loss, which gives equal weighting to the two settings (treating the captions as labels to the images and vice versa).\nConsider the following two images and captions:\nImages are from Glass Onion: A Knives Out Mystery (2022)\nOnce properly trained, the embeddings for the corresponding images and text (i.e. captions) will be close to each other and farther away from unrelated pairs.\nTypically embedding spaces are hundred/thousand dimensional.\nAt query time, the input text query can be mapped into this embedding space, and we can return the closest matching images.\nThe query may have not existed in the training set. Cosine similarity can be used as a similarity measure.\nWhile these models are trained on image-text pairs, we have found that they are an excellent starting point to learning representations of video units like shots and scenes. As videos are a sequence of images (frames), additional parameters may need to be introduced to compute embeddings for these video units, although we have found that for shorter units like shots, an unparameterized aggregation like averaging (mean-pooling) can be more effective. To train these parameters as well as fine-tune the pretrained image-text model weights, we leverage in-house datasets that pair shots of varying durations with rich textual descriptions of their content. This additional adaptation step improves performance by 15–25% on video retrieval tasks (given a text prompt), depending on the starting model used and metric evaluated.\nOn top of video retrieval, there are a wide variety of video clip classifiers within Netflix that are trained specifically to find a particular attribute (e.g. closeup shots, caution elements). Instead of training from scratch, we have found that using the shot-level embeddings can give us a significant head start, even beyond the baseline image-text models that they were built on top of.\nLastly, shot embeddings can also be used for video-to-video search, a particularly useful application in the context of trailer and promotional asset creation.\nEngineering and Infrastructure\nOur trained model gives us a text encoder and a video encoder. Video embeddings are precomputed on the shot level, stored in our media feature store, and replicated to an elastic search cluster for real-time nearest neighbor queries. Our media feature management system automatically triggers the video embedding computation whenever new video assets are added, ensuring that we can search through the latest video assets.\nThe embedding computation is based on a large neural network model and has to be run on GPUs for optimal throughput. However, shot segmentation from a full-length movie is CPU-intensive. To fully utilize the GPUs in the cloud environment, we first run shot segmentation in parallel on multi-core CPU machines, store the result shots in S3 object storage encoded in video formats such as mp4. During GPU computation, we stream mp4 video shots from S3 directly to the GPUs using a data loader that performs prefetching and preprocessing. This approach ensures that the GPUs are efficiently utilized during inference, thereby increasing the overall throughput and cost-efficiency of our system.\nAt query time, a user submits a text string representing what they want to search for. For visual search queries, we use the text encoder from the trained model to extract a text embedding, which is then used to perform appropriate nearest neighbor search. Users can also select a subset of shows to search over, or perform a catalog wide search, which we also support.\nIf you’re interested in more details, see our other post covering the Media Understanding Platform.\nConclusion\nFinding a needle in a haystack is hard. We learned from talking to video creatives who make trailers and social media videos that being able to find needles was key, and a big pain point. The solution we described has been fruitful, works well in practice, and is relatively simple to maintain. Our search system allows our creatives to iterate faster, try more ideas, and make more engaging videos for our viewers to enjoy.\nWe hope this post has been interesting to you. If you are interested in working on problems like this, Netflix is always hiring great researchers, engineers and creators.",
      "markdown": "## Building In-Video Search\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----936766f0017c--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----936766f0017c--------------------------------)\n\n[Boris Chen](https://www.linkedin.com/in/boris-chen-b921a214/), [Ben Klein](https://www.linkedin.com/in/benjamin-klein-usa/), [Jason Ge](https://www.linkedin.com/in/jasonge27/), [Avneesh Saluja](https://www.linkedin.com/in/avneesh/), [Guru Tahasildar](https://www.linkedin.com/in/gurutahasildar/), [Abhishek Soni](https://www.linkedin.com/in/abhisheks0ni/), [Juan Vimberg](https://www.linkedin.com/in/jivimberg/), [Gustavo Carmo](https://www.linkedin.com/in/gucarmo/), [Meenakshi Jindal](https://www.linkedin.com/in/meenakshijindal), [Elliot Chow](https://www.linkedin.com/in/ellchow/), [Amir Ziai](https://www.linkedin.com/in/amirziai/), [Varun Sekhri](https://www.linkedin.com/in/varun-sekhri-087a213/), [Santiago Castro](https://www.linkedin.com/in/santiagocastroserra/), [Keila Fong](https://www.linkedin.com/in/keilafong/), [Kelli Griggs](https://www.linkedin.com/in/kelli-griggs-32990125/), [Mallia Sherzai](https://www.linkedin.com/in/mallia-sherzai-8a92862/), [Robert Mayer](https://www.linkedin.com/in/mayerr/), [Andy Yao](https://www.linkedin.com/in/yaoandy/), [Vi Iyengar](https://www.linkedin.com/in/vi-pallavika-iyengar-144abb1b/), [Jonathan Solorzano-Hamilton](https://www.linkedin.com/in/peachpie/), [Hossein Taghavi](https://www.linkedin.com/in/mhtaghavi/), [Ritwik Kumar](https://www.linkedin.com/in/ritwik-kumar/)\n\n## Introduction\n\nToday we’re going to take a look at the behind the scenes technology behind how Netflix creates great trailers, Instagram reels, video shorts and other promotional videos.\n\nSuppose you’re trying to create the trailer for the action thriller _The Gray Man_, and you know you want to use a shot of a car exploding. You don’t know if that shot exists or where it is in the film, and you have to look for it it by scrubbing through the whole film.\n\nExploding cars — [The Gray Man](https://www.netflix.com/title/81160697) (2022)\n\nOr suppose it’s Christmas, and you want to create a great instagram piece out all the best scenes across Netflix films of people shouting “Merry Christmas”! Or suppose it’s Anya Taylor Joy’s birthday, and you want to create a highlight reel of all her most iconic and dramatic shots.\n\nCreating these involves sifting through hundreds of thousands of movies and TV shows to find the right line of dialogue or the appropriate visual elements (objects, scenes, emotions, actions, etc.). We have built an internal system that allows someone to perform in-video search across the entire Netflix video catalog, and we’d like to share our experience in building this system.\n\n## Building in-video search\n\nTo build such a visual search engine, we needed a machine learning system that can understand visual elements. Our early attempts included object detection, but found that general labels were both too limiting and too specific, yet not specific enough. Every show has special objects that are important (e.g. Demogorgon in Stranger Things) that don’t translate to other shows. The same was true for action recognition, and other common image and video tasks.\n\n## The Approach\n\nWe learned that contrastive learning works well for our objectives when applied to image and text pairs, as these models can effectively learn joint embedding spaces between the two modalities. This approach is also able to learn about objects, scenes, emotions, actions, and more in a single model. We also found that extending contrastive learning to videos and text provided a substantial improvement over frame-level models.\n\nIn order to train the model on internal training data (video clips with aligned text descriptions), we implemented a scalable version on [Ray Train](https://docs.ray.io/en/latest/train/train.html) and switched to a [more performant video decoding library](https://github.com/dmlc/decord). Lastly, the embeddings from the video encoder exhibit strong zero or few-shot performance on multiple video and content understanding tasks at Netflix and are used as a starting point in those applications.\n\nThe recent success of large-scale models that jointly train image and text embeddings has enabled new use cases around multimodal retrieval. These models are trained on large amounts of image-caption pairs via in-batch contrastive learning. For a (large) batch of `N` examples, we wish to maximize the embedding (cosine) similarity of the `N` correct image-text pairs, while minimizing the similarity of the other `N²-N` paired embeddings. This is done by treating the similarities as logits and minimizing the symmetric cross-entropy loss, which gives equal weighting to the two settings (treating the captions as labels to the images and vice versa).\n\nConsider the following two images and captions:\n\nImages are from [Glass Onion: A Knives Out Mystery](https://www.netflix.com/title/81458416) (2022)\n\nOnce properly trained, the embeddings for the corresponding images and text (i.e. captions) will be close to each other and farther away from unrelated pairs.\n\nTypically embedding spaces are hundred/thousand dimensional.\n\nAt query time, the input text query can be mapped into this embedding space, and we can return the closest matching images.\n\nThe query may have not existed in the training set. [Cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) can be used as a similarity measure.\n\nWhile these models are trained on image-text pairs, we have found that they are an excellent starting point to learning representations of video units like shots and scenes. As videos are a sequence of images (frames), additional parameters may need to be introduced to compute embeddings for these video units, although we have found that for shorter units like shots, an unparameterized aggregation like averaging (mean-pooling) can be more effective. To train these parameters as well as fine-tune the pretrained image-text model weights, we leverage in-house datasets that pair shots of varying durations with rich textual descriptions of their content. This additional adaptation step improves performance by 15–25% on video retrieval tasks (given a text prompt), depending on the starting model used and metric evaluated.\n\nOn top of video retrieval, there are a wide variety of video clip classifiers within Netflix that are trained specifically to find a particular attribute (e.g. closeup shots, caution elements). Instead of training from scratch, we have found that using the shot-level embeddings can give us a significant head start, even beyond the baseline image-text models that they were built on top of.\n\nLastly, shot embeddings can also be used for video-to-video search, a particularly useful application in the context of trailer and promotional asset creation.\n\n## Engineering and Infrastructure\n\nOur trained model gives us a text encoder and a video encoder. Video embeddings are precomputed on the shot level, stored in our [media feature store](https://netflixtechblog.com/scaling-media-machine-learning-at-netflix-f19b400243), and replicated to an elastic search cluster for real-time nearest neighbor queries. Our media feature management system automatically triggers the video embedding computation whenever new video assets are added, ensuring that we can search through the latest video assets.\n\nThe embedding computation is based on a large neural network model and has to be run on GPUs for optimal throughput. However, shot segmentation from a full-length movie is CPU-intensive. To fully utilize the GPUs in the cloud environment, we first run shot segmentation in parallel on multi-core CPU machines, store the result shots in S3 object storage encoded in video formats such as mp4. During GPU computation, we stream mp4 video shots from S3 directly to the GPUs using a data loader that performs prefetching and preprocessing. This approach ensures that the GPUs are efficiently utilized during inference, thereby increasing the overall throughput and cost-efficiency of our system.\n\nAt query time, a user submits a text string representing what they want to search for. For visual search queries, we use the text encoder from the trained model to extract a text embedding, which is then used to perform appropriate nearest neighbor search. Users can also select a subset of shows to search over, or perform a catalog wide search, which we also support.\n\nIf you’re interested in more details, see our other post covering the [Media Understanding Platform](https://netflixtechblog.com/building-a-media-understanding-platform-for-ml-innovations-9bef9962dcb7).\n\n## Conclusion\n\nFinding a needle in a haystack is hard. We learned from talking to video creatives who make trailers and social media videos that being able to find needles was key, and a big pain point. The solution we described has been fruitful, works well in practice, and is relatively simple to maintain. Our search system allows our creatives to iterate faster, try more ideas, and make more engaging videos for our viewers to enjoy.\n\nWe hope this post has been interesting to you. If you are interested in working on problems like this, Netflix is always [hiring](https://jobs.netflix.com/) great researchers, engineers and creators."
    },
    {
      "url": "https://netflixtechblog.com/streaming-sql-in-data-mesh-0d83f5a00d08?source=collection_home---4------3-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/streaming-sql-in-data-mesh-0d83f5a00d08?gi=3f3040266aa5&source=collection_home---4------3-----------------------",
        "loadedTime": "2023-12-06T00:03:17.532Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/streaming-sql-in-data-mesh-0d83f5a00d08",
        "title": "Streaming SQL in Data Mesh by Netflix Technology Blog | Netflix TechBlog",
        "description": "Learn how Netflix is leveraging Apache Flink SQL to build a managed Stream Processing Platform to help engineers move and transform data.",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Streaming SQL in Data Mesh\nDemocratizing Stream Processing @ Netflix\nBy Guil Pires, Mark Cho, Mingliang Liu, Sujay Jain\nData powers much of what we do at Netflix. On the Data Platform team, we build the infrastructure used across the company to process data at scale.\nIn our last blog post, we introduced “Data Mesh” — A Data Movement and Processing Platform. When a user wants to leverage Data Mesh to move and transform data, they start by creating a new Data Mesh pipeline. The pipeline is composed of individual “Processors” that are connected by Kafka topics. The Processors themselves are implemented as Flink jobs that use the DataStream API.\nSince then, we have seen many use cases (including Netflix Graph Search) adopt Data Mesh for stream processing. We were able to onboard many of these use cases by offering some commonly used Processors out of the box, such as Projection, Filtering, Unioning, and Field Renaming.\nAn example of a Data Mesh pipeline which moves and transforms data using Union, GraphQL Enrichment, and Column Rename Processor before writing to an Iceberg table.\nBy keeping the logic of individual Processors simple, it allowed them to be reusable so we could centrally manage and operate them at scale. It also allowed them to be composable, so users could combine the different Processors to express the logic they needed.\nHowever, this design decision led to a different set of challenges.\nSome teams found the provided building blocks were not expressive enough. For use cases which were not solvable using existing Processors, users had to express their business logic by building a custom Processor. To do this, they had to use the low-level DataStream API from Flink and the Data Mesh SDK, which came with a steep learning curve. After it was built, they also had to operate the custom Processors themselves.\nFurthermore, many pipelines needed to be composed of multiple Processors. Since each Processor was implemented as a Flink Job connected by Kafka topics, it meant there was a relatively high runtime overhead cost for many pipelines.\nWe explored various options to solve these challenges, and eventually landed on building the Data Mesh SQL Processor that would provide additional flexibility for expressing users’ business logic.\nThe existing Data Mesh Processors have a lot of overlap with SQL. For example, filtering and projection can be expressed in SQL through SELECT and WHERE clauses. Additionally, instead of implementing business logic by composing multiple individual Processors together, users could express their logic in a single SQL query, avoiding the additional resource and latency overhead that came from multiple Flink jobs and Kafka topics. Furthermore, SQL can support User Defined Functions (UDFs) and custom connectors for lookup joins, which can be used to extend expressiveness.\nData Mesh SQL Processor\nSince Data Mesh Processors are built on top of Flink, it made sense to consider using Flink SQL instead of continuing to build additional Processors for every transform operation we needed to support.\nThe Data Mesh SQL Processor is a platform-managed, parameterized Flink Job that takes schematized sources and a Flink SQL query that will be executed against those sources. By leveraging Flink SQL within a Data Mesh Processor, we were able to support the streaming SQL functionality without changing the architecture of Data Mesh.\nUnderneath the hood, the Data Mesh SQL Processor is implemented using Flink’s Table API, which provides a powerful abstraction to convert between DataStreams and Dynamic Tables. Based on the sources that the processor is connected to, the SQL Processor will automatically convert the upstream sources as tables within Flink’s SQL engine. User’s query is then registered with the SQL engine and translated into a Flink job graph consisting of physical operators that can be executed on a Flink cluster. Unlike the low-level DataStream API, users do not have to manually build a job graph using low-level operators, as this is all managed by Flink’s SQL engine.\nSQL Experience on Data Mesh\nThe SQL Processor enables users to fully leverage the capabilities of the Data Mesh platform. This includes features such as autoscaling, the ability to manage pipelines declaratively via Infrastructure as Code, and a rich connector ecosystem.\nIn order to ensure a seamless user experience, we’ve enhanced the Data Mesh platform with SQL-centric features. These enhancements include an Interactive Query Mode, real-time query validation, and automated schema inference.\nTo understand how these features help the users be more productive, let’s take a look at a typical user workflow when using the Data Mesh SQL Processor.\nUsers start their journey by live sampling their upstream data sources using the Interactive Query Mode.\nAs the user iterate on their SQL query, the query validation service provides real-time feedback about the query.\nWith a valid query, users can leverage the Interactive Query Mode again to execute the query and get the live results streamed back to the UI within seconds.\nFor more efficient schema management and evolution, the platform will automatically infer the output schema based on the fields selected by the SQL query.\nOnce the user is done editing their query, it is saved to the Data Mesh Pipeline, which will then be deployed as a long running, streaming SQL job.\nOverview of the SQL Processor workflow.\nUsers typically iterate on their SQL query multiple times before deploying it. Validating and analyzing queries at runtime after deployment will not only slow down their iteration, but also make it difficult to automate schema evolution in Data Mesh.\nTo address this challenge, we have implemented a query validation service that can verify a Flink SQL query and provide a meaningful error message for violations in real time. This enables users to have prompt validation feedback while they are editing the query. We leverage Apache Flink’s internal Planner classes to parse and transform SQL queries without creating a fully-fledged streaming table environment. This makes the query service lightweight, scalable, and execution agnostic.\nTo effectively operate thousands of use cases at the platform layer, we built opinionated guardrails to limit some functionalities of Flink SQL. We plan on gradually expanding the supported capabilities over time. We implemented the guardrails by recursively inspecting the Calcite tree constructed from user’s query. If the tree contains nodes that we currently don’t support, the query will be rejected from being deployed. Additionally, we translate Flink’s internal exceptions containing cryptic error messages into more meaningful error messages for our users. We plan on continuing our investments into improving the guardrails, as having proper guardrails help to improve the user experience. Some ideas for the future include rules to reject expensive and suboptimal queries.\nTo help Data Mesh users iterate quickly on their business logic, we have built the Interactive Query Mode as part of the platform. Users can start live sampling their streaming data by executing a simple `SELECT * FROM <table>` query. Using the Interactive Query Mode, Data Mesh platform will execute the Flink SQL query and display the results in the UI in seconds. Since this is a Flink SQL query on streaming data, new results will continue to be delivered to the user in real-time.\nUsers can continue to iterate and modify their Flink SQL query and once they’re satisfied with their query output, they can save the query as part of their stream processing pipeline.\nTo provide this interactive experience, we maintain an always-running Flink Session Cluster that can run concurrent parameterized queries. These queries will output their data to a Mantis sink in order to stream the results back to the user’s browser.\nInteractive Query mode in action\nLearnings from our journey\nIn hindsight, we wish we had invested in enabling Flink SQL on the DataMesh platform much earlier. If we had the Data Mesh SQL Processor earlier, we would’ve been able to avoid spending engineering resources to build smaller building blocks such as the Union Processor, Column Rename Processor, Projection and Filtering Processor.\nSince we’ve productionized Data Mesh SQL Processor, we’ve seen excitement and quick adoption from our Data Mesh users. Thanks to the flexibility of Flink SQL, users have a new way to express their streaming transformation logic other than writing a custom processor using the low-level DataStream API.\nWhile Flink SQL is a powerful tool, we view the Data Mesh SQL Processor as a complimentary addition to our platform. It is not meant to be a replacement for custom processors and Flink jobs using low-level DataStream API. Since SQL is a higher-level abstraction, users no longer have control over low-level Flink operators and state. This means that if state evolution is critical to the user’s business logic, then having complete control over the state can only be done through low-level abstractions like the DataStream API. Even with this limitation, we have seen that there are many new use cases that are unlocked through the Data Mesh SQL Processor.\nOur early investment in guardrails has helped set clear expectations with our users and keep the operational burden manageable. It has allowed us to productionize queries and patterns that we are confident about supporting, while providing a framework to introduce new capabilities gradually.\nFuture of SQL on Data Mesh\nWhile introducing the SQL Processor to the Data Mesh platform was a great step forward, we still have much more work to do in order to unlock the power of stream processing at Netflix. We’ve been working with our partner teams to prioritize and build the next set of features to extend the SQL Processor. These include stream enrichment using Slowly-Changing-Dimension (SCD) tables, temporal joins, and windowed aggregations.\nStay tuned for more updates!",
      "markdown": "## Streaming SQL in Data Mesh\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----0d83f5a00d08--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----0d83f5a00d08--------------------------------)\n\nDemocratizing Stream Processing @ Netflix\n\nBy [_Guil Pires_](https://www.linkedin.com/in/guilhermesmi/)_,_ [_Mark Cho_](https://www.linkedin.com/in/markcho/)_,_ [_Mingliang Liu_](https://www.linkedin.com/in/liuml07/)_,_ [_Sujay Jain_](https://www.linkedin.com/in/sujayjain/)\n\nData powers much of what we do at Netflix. On the Data Platform team, we build the infrastructure used across the company to process data at scale.\n\nIn our last blog post, we introduced [“Data Mesh” — A Data Movement and Processing Platform](https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873). When a user wants to leverage Data Mesh to move and transform data, they start by creating a new Data Mesh pipeline. The pipeline is composed of individual “Processors” that are connected by Kafka topics. The Processors themselves are implemented as Flink jobs that use the DataStream API.\n\nSince then, we have seen many use cases (including [Netflix Graph Search](https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-5c0c1c7d7eaf)) adopt Data Mesh for stream processing. We were able to onboard many of these use cases by offering some commonly used Processors out of the box, such as _Projection_, _Filtering_, _Unioning_, and _Field Renaming_.\n\nAn example of a Data Mesh pipeline which moves and transforms data using Union, GraphQL Enrichment, and Column Rename Processor before writing to an Iceberg table.\n\nBy keeping the logic of individual Processors simple, it allowed them to be reusable so we could centrally manage and operate them at scale. It also allowed them to be composable, so users could combine the different Processors to express the logic they needed.\n\nHowever, this design decision led to a different set of challenges.\n\nSome teams found the provided building blocks were not expressive enough. For use cases which were not solvable using existing Processors, users had to express their business logic by building a custom Processor. To do this, they had to use the low-level DataStream API from Flink and the Data Mesh SDK, which came with a steep learning curve. After it was built, they also had to operate the custom Processors themselves.\n\nFurthermore, many pipelines needed to be composed of multiple Processors. Since each Processor was implemented as a Flink Job connected by Kafka topics, it meant there was a relatively high runtime overhead cost for many pipelines.\n\nWe explored various options to solve these challenges, and eventually landed on building the Data Mesh SQL Processor that would provide additional flexibility for expressing users’ business logic.\n\nThe existing Data Mesh Processors have a lot of overlap with SQL. For example, filtering and projection can be expressed in SQL through **_SELECT_** and **_WHERE_** clauses. Additionally, instead of implementing business logic by composing multiple individual Processors together, users could express their logic in a single SQL query, avoiding the additional resource and latency overhead that came from multiple Flink jobs and Kafka topics. Furthermore, SQL can support User Defined Functions (UDFs) and custom connectors for _lookup_ _joins_, which can be used to extend expressiveness.\n\n## Data Mesh SQL Processor\n\nSince Data Mesh Processors are built on top of Flink, it made sense to consider using Flink SQL instead of continuing to build additional Processors for every transform operation we needed to support.\n\nThe Data Mesh SQL Processor is a platform-managed, parameterized Flink Job that takes schematized sources and a Flink SQL query that will be executed against those sources. By leveraging Flink SQL within a Data Mesh Processor, we were able to support the streaming SQL functionality without changing the architecture of Data Mesh.\n\nUnderneath the hood, the Data Mesh SQL Processor is implemented using Flink’s Table API, which provides a powerful abstraction to convert between DataStreams and Dynamic Tables. Based on the sources that the processor is connected to, the SQL Processor will automatically convert the upstream sources as tables within Flink’s SQL engine. User’s query is then registered with the SQL engine and translated into a Flink job graph consisting of physical operators that can be executed on a Flink cluster. Unlike the low-level DataStream API, users do not have to manually build a job graph using low-level operators, as this is all managed by Flink’s SQL engine.\n\n## SQL Experience on Data Mesh\n\nThe SQL Processor enables users to fully leverage the capabilities of the Data Mesh platform. This includes features such as autoscaling, the ability to manage pipelines declaratively via Infrastructure as Code, and a rich connector ecosystem.\n\nIn order to ensure a seamless user experience, we’ve enhanced the Data Mesh platform with SQL-centric features. These enhancements include an Interactive Query Mode, real-time query validation, and automated schema inference.\n\nTo understand how these features help the users be more productive, let’s take a look at a typical user workflow when using the Data Mesh SQL Processor.\n\n*   Users start their journey by live sampling their upstream data sources using the Interactive Query Mode.\n*   As the user iterate on their SQL query, the query validation service provides real-time feedback about the query.\n*   With a valid query, users can leverage the Interactive Query Mode again to execute the query and get the live results streamed back to the UI within seconds.\n*   For more efficient schema management and evolution, the platform will automatically infer the output schema based on the fields selected by the SQL query.\n*   Once the user is done editing their query, it is saved to the Data Mesh Pipeline, which will then be deployed as a long running, streaming SQL job.\n\n_Overview of the SQL Processor workflow._\n\nUsers typically iterate on their SQL query multiple times before deploying it. Validating and analyzing queries at runtime after deployment will not only slow down their iteration, but also make it difficult to automate schema evolution in Data Mesh.\n\nTo address this challenge, we have implemented a query validation service that can verify a Flink SQL query and provide a meaningful error message for violations in real time. This enables users to have prompt validation feedback while they are editing the query. We leverage Apache Flink’s internal Planner classes to parse and transform SQL queries without creating a fully-fledged streaming table environment. This makes the query service lightweight, scalable, and execution agnostic.\n\nTo effectively operate thousands of use cases at the platform layer, we built opinionated guardrails to limit some functionalities of Flink SQL. We plan on gradually expanding the supported capabilities over time. We implemented the guardrails by recursively inspecting the Calcite tree constructed from user’s query. If the tree contains nodes that we currently don’t support, the query will be rejected from being deployed. Additionally, we translate Flink’s internal exceptions containing cryptic error messages into more meaningful error messages for our users. We plan on continuing our investments into improving the guardrails, as having proper guardrails help to improve the user experience. Some ideas for the future include rules to reject expensive and suboptimal queries.\n\nTo help Data Mesh users iterate quickly on their business logic, we have built the Interactive Query Mode as part of the platform. Users can start live sampling their streaming data by executing a simple \\`**_SELECT_** **_\\*_** **_FROM_** **_<table>\\`_** query. Using the Interactive Query Mode, Data Mesh platform will execute the Flink SQL query and display the results in the UI in seconds. Since this is a Flink SQL query on streaming data, new results will continue to be delivered to the user in real-time.\n\nUsers can continue to iterate and modify their Flink SQL query and once they’re satisfied with their query output, they can save the query as part of their stream processing pipeline.\n\nTo provide this interactive experience, we maintain an always-running Flink Session Cluster that can run concurrent parameterized queries. These queries will output their data to a [Mantis](https://netflix.github.io/mantis/) sink in order to stream the results back to the user’s browser.\n\n_Interactive Query mode in action_\n\n## Learnings from our journey\n\nIn hindsight, we wish we had invested in enabling Flink SQL on the DataMesh platform much earlier. If we had the Data Mesh SQL Processor earlier, we would’ve been able to avoid spending engineering resources to build smaller building blocks such as the Union Processor, Column Rename Processor, Projection and Filtering Processor.\n\nSince we’ve productionized Data Mesh SQL Processor, we’ve seen excitement and quick adoption from our Data Mesh users. Thanks to the flexibility of Flink SQL, users have a new way to express their streaming transformation logic other than writing a custom processor using the low-level DataStream API.\n\nWhile Flink SQL is a powerful tool, we view the Data Mesh SQL Processor as a complimentary addition to our platform. It is not meant to be a replacement for custom processors and Flink jobs using low-level DataStream API. Since SQL is a higher-level abstraction, users no longer have control over low-level Flink operators and state. This means that if state evolution is critical to the user’s business logic, then having complete control over the state can only be done through low-level abstractions like the DataStream API. Even with this limitation, we have seen that there are many new use cases that are unlocked through the Data Mesh SQL Processor.\n\nOur early investment in guardrails has helped set clear expectations with our users and keep the operational burden manageable. It has allowed us to productionize queries and patterns that we are confident about supporting, while providing a framework to introduce new capabilities gradually.\n\n## Future of SQL on Data Mesh\n\nWhile introducing the SQL Processor to the Data Mesh platform was a great step forward, we still have much more work to do in order to unlock the power of stream processing at Netflix. We’ve been working with our partner teams to prioritize and build the next set of features to extend the SQL Processor. These include stream enrichment using Slowly-Changing-Dimension (SCD) tables, temporal joins, and windowed aggregations.\n\nStay tuned for more updates!"
    },
    {
      "url": "https://netflixtechblog.com/zero-configuration-service-mesh-with-on-demand-cluster-discovery-ac6483b52a51?source=collection_home---4------5-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/zero-configuration-service-mesh-with-on-demand-cluster-discovery-ac6483b52a51?gi=e945f2c72508&source=collection_home---4------5-----------------------",
        "loadedTime": "2023-12-06T00:03:21.996Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/zero-configuration-service-mesh-with-on-demand-cluster-discovery-ac6483b52a51",
        "title": "Zero Configuration Service Mesh with On-Demand Cluster Discovery | by Netflix Technology Blog | Netflix TechBlog",
        "description": "Netflix’s service mesh adoption: history, motivations, and how we worked with the Envoy community on a streamlining service mesh adoption in complex microservice environments",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Zero Configuration Service Mesh with On-Demand Cluster Discovery\nby David Vroom, James Mulcahy, Ling Yuan, Rob Gulewich\nIn this post we discuss Netflix’s adoption of service mesh: some history, motivations, and how we worked with Kinvolk and the Envoy community on a feature that streamlines service mesh adoption in complex microservice environments: on-demand cluster discovery.\nA brief history of IPC at Netflix\nNetflix was early to the cloud, particularly for large-scale companies: we began the migration in 2008, and by 2010, Netflix streaming was fully run on AWS. Today we have a wealth of tools, both OSS and commercial, all designed for cloud-native environments. In 2010, however, nearly none of it existed: the CNCF wasn’t formed until 2015! Since there were no existing solutions available, we needed to build them ourselves.\nFor Inter-Process Communication (IPC) between services, we needed the rich feature set that a mid-tier load balancer typically provides. We also needed a solution that addressed the reality of working in the cloud: a highly dynamic environment where nodes are coming up and down, and services need to quickly react to changes and route around failures. To improve availability, we designed systems where components could fail separately and avoid single points of failure. These design principles led us to client-side load-balancing, and the 2012 Christmas Eve outage solidified this decision even further. During these early years in the cloud, we built Eureka for Service Discovery and Ribbon (internally known as NIWS) for IPC. Eureka solved the problem of how services discover what instances to talk to, and Ribbon provided the client-side logic for load-balancing, as well as many other resiliency features. These two technologies, alongside a host of other resiliency and chaos tools, made a massive difference: our reliability improved measurably as a result.\nEureka and Ribbon presented a simple but powerful interface, which made adopting them easy. In order for a service to talk to another, it needs to know two things: the name of the destination service, and whether or not the traffic should be secure. The abstractions that Eureka provides for this are Virtual IPs (VIPs) for insecure communication, and Secure VIPs (SVIPs) for secure. A service advertises a VIP name and port to Eureka (eg: myservice, port 8080), or an SVIP name and port (eg: myservice-secure, port 8443), or both. IPC clients are instantiated targeting that VIP or SVIP, and the Eureka client code handles the translation of that VIP to a set of IP and port pairs by fetching them from the Eureka server. The client can also optionally enable IPC features like retries or circuit breaking, or stick with a set of reasonable defaults.\nIn this architecture, service to service communication no longer goes through the single point of failure of a load balancer. The downside is that Eureka is a new single point of failure as the source of truth for what hosts are registered for VIPs. However, if Eureka goes down, services can continue to communicate with each other, though their host information will become stale over time as instances for a VIP come up and down. The ability to run in a degraded but available state during an outage is still a marked improvement over completely stopping traffic flow.\nWhy mesh?\nThe above architecture has served us well over the last decade, though changing business needs and evolving industry standards have added more complexity to our IPC ecosystem in a number of ways. First, we’ve grown the number of different IPC clients. Our internal IPC traffic is now a mix of plain REST, GraphQL, and gRPC. Second, we’ve moved from a Java-only environment to a Polyglot one: we now also support node.js, Python, and a variety of OSS and off the shelf software. Third, we’ve continued to add more functionality to our IPC clients: features such as adaptive concurrency limiting, circuit breaking, hedging, and fault injection have become standard tools that our engineers reach for to make our system more reliable. Compared to a decade ago, we now support more features, in more languages, in more clients. Keeping feature parity between all of these implementations and ensuring that they all behave the same way is challenging: what we want is a single, well-tested implementation of all of this functionality, so we can make changes and fix bugs in one place.\nThis is where service mesh comes in: we can centralize IPC features in a single implementation, and keep per-language clients as simple as possible: they only need to know how to talk to the local proxy. Envoy is a great fit for us as the proxy: it’s a battle-tested OSS product at use in high scale in the industry, with many critical resiliency features, and good extension points for when we need to extend its functionality. The ability to configure proxies via a central control plane is a killer feature: this allows us to dynamically configure client-side load balancing as if it was a central load balancer, but still avoids a load balancer as a single point of failure in the service to service request path.\nMoving to mesh\nOnce we decided that moving to service mesh was the right bet to make, the next question became: how should we go about moving? We decided on a number of constraints for the migration. First: we wanted to keep the existing interface. The abstraction of specifying a VIP name plus secure serves us well, and we didn’t want to break backwards compatibility. Second: we wanted to automate the migration and to make it as seamless as possible. These two constraints meant that we needed to support the Discovery abstractions in Envoy, so that IPC clients could continue to use it under the hood. Fortunately, Envoy had ready to use abstractions for this. VIPs could be represented as Envoy Clusters, and proxies could fetch them from our control plane using the Cluster Discovery Service (CDS). The hosts in those clusters are represented as Envoy Endpoints, and could be fetched using the Endpoint Discovery Service (EDS).\nWe soon ran into a stumbling block to a seamless migration: Envoy requires that clusters be specified as part of the proxy’s config. If service A needs to talk to clusters B and C, then you need to define clusters B and C as part of A’s proxy config. This can be challenging at scale: any given service might communicate with dozens of clusters, and that set of clusters is different for every app. In addition, Netflix is always changing: we’re constantly adding new initiatives like live streaming, ads and games, and evolving our architecture. This means the clusters that a service communicates with will change over time. There are a number of different approaches to populating cluster config that we evaluated, given the Envoy primitives available to us:\nGet service owners to define the clusters their service needs to talk to. This option seems simple, but in practice, service owners don’t always know, or want to know, what services they talk to. Services often import libraries provided by other teams that talk to multiple other services under the hood, or communicate with other operational services like telemetry and logging. This means that service owners would need to know how these auxiliary services and libraries are implemented under the hood, and adjust config when they change.\nAuto-generate Envoy config based on a service’s call graph. This method is simple for pre-existing services, but is challenging when bringing up a new service or adding a new upstream cluster to communicate with.\nPush all clusters to every app: this option was appealing in its simplicity, but back of the napkin math quickly showed us that pushing millions of endpoints to each proxy wasn’t feasible.\nGiven our goal of a seamless adoption, each of these options had significant enough downsides that we explored another option: what if we could fetch cluster information on-demand at runtime, rather than predefining it? At the time, the service mesh effort was still being bootstrapped, with only a few engineers working on it. We approached Kinvolk to see if they could work with us and the Envoy community in implementing this feature. The result of this collaboration was On-Demand Cluster Discovery (ODCDS). With this feature, proxies could now look up cluster information the first time they attempt to connect to it, rather than predefining all of the clusters in config.\nWith this capability in place, we needed to give the proxies cluster information to look up. We had already developed a service mesh control plane that implements the Envoy XDS services. We then needed to fetch service information from Eureka in order to return to the proxies. We represent Eureka VIPs and SVIPs as separate Envoy Cluster Discovery Service (CDS) clusters (so service myservice may have clusters myservice.vip and myservice.svip). Individual hosts in a cluster are represented as separate Endpoint Discovery Service (EDS) endpoints. This allows us to reuse the same Eureka abstractions, and IPC clients like Ribbon can move to mesh with minimal changes. With both the control plane and data plane changes in place, the flow works as follows:\nClient request comes into Envoy\nExtract the target cluster based on the Host / :authority header (the header used here is configurable, but this is our approach). If that cluster is known already, jump to step 7\nThe cluster doesn’t exist, so we pause the in flight request\nMake a request to the Cluster Discovery Service (CDS) endpoint on the control plane. The control plane generates a customized CDS response based on the service’s configuration and Eureka registration information\nEnvoy gets back the cluster (CDS), which triggers a pull of the endpoints via Endpoint Discovery Service (EDS). Endpoints for the cluster are returned based on Eureka status information for that VIP or SVIP\nClient request unpauses\nEnvoy handles the request as normal: it picks an endpoint using a load-balancing algorithm and issues the request\nThis flow is completed in a few milliseconds, but only on the first request to the cluster. Afterward, Envoy behaves as if the cluster was defined in the config. Critically, this system allows us to seamlessly migrate services to service mesh with no configuration required, satisfying one of our main adoption constraints. The abstraction we present continues to be VIP name plus secure, and we can migrate to mesh by configuring individual IPC clients to connect to the local proxy instead of the upstream app directly. We continue to use Eureka as the source of truth for VIPs and instance status, which allows us to support a heterogeneous environment of some apps on mesh and some not while we migrate. There’s an additional benefit: we can keep Envoy memory usage low by only fetching data for clusters that we’re actually communicating with.\nThere is a downside to fetching this data on-demand: this adds latency to the first request to a cluster. We have run into use-cases where services need very low-latency access on the first request, and adding a few extra milliseconds adds too much overhead. For these use-cases, the services need to either predefine the clusters they communicate with, or prime connections before their first request. We’ve also considered pre-pushing clusters from the control plane as proxies start up, based on historical request patterns. Overall, we feel the reduced complexity in the system justifies the downside for a small set of services.\nWe’re still early in our service mesh journey. Now that we’re using it in earnest, there are many more Envoy improvements that we’d love to work with the community on. The porting of our adaptive concurrency limiting implementation to Envoy was a great start — we’re looking forward to collaborating with the community on many more. We’re particularly interested in the community’s work on incremental EDS. EDS endpoints account for the largest volume of updates, and this puts undue pressure on both the control plane and Envoy.\nWe’d like to give a big thank-you to the folks at Kinvolk for their Envoy contributions: Alban Crequy, Andrew Randall, Danielle Tal, and in particular Krzesimir Nowak for his excellent work. We’d also like to thank the Envoy community for their support and razor-sharp reviews: Adi Peleg, Dmitri Dolguikh, Harvey Tuch, Matt Klein, and Mark Roth. It’s been a great experience working with you all on this.\nThis is the first in a series of posts on our journey to service mesh, so stay tuned. If this sounds like fun, and you want to work on service mesh at scale, come work with us — we’re hiring!",
      "markdown": "## Zero Configuration Service Mesh with On-Demand Cluster Discovery\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----ac6483b52a51--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----ac6483b52a51--------------------------------)\n\n_by David Vroom, James Mulcahy, Ling Yuan, Rob Gulewich_\n\nIn this post we discuss Netflix’s adoption of service mesh: some history, motivations, and how we worked with Kinvolk and the Envoy community on a feature that streamlines service mesh adoption in complex microservice environments: on-demand cluster discovery.\n\n## A brief history of IPC at Netflix\n\nNetflix was early to the cloud, particularly for large-scale companies: we began the migration in 2008, and by 2010, [Netflix streaming was fully run on AWS](https://netflixtechblog.com/four-reasons-we-choose-amazons-cloud-as-our-computing-platform-4aceb692afec). Today we have a wealth of tools, both OSS and commercial, all designed for cloud-native environments. In 2010, however, nearly none of it existed: the [CNCF](https://www.cncf.io/) wasn’t formed until 2015! Since there were no existing solutions available, we needed to build them ourselves.\n\nFor Inter-Process Communication (IPC) between services, we needed the rich feature set that a mid-tier load balancer typically provides. We also needed a solution that addressed the reality of working in the cloud: a highly dynamic environment where nodes are coming up and down, and services need to quickly react to changes and route around failures. To improve availability, we designed systems where components could fail separately and avoid single points of failure. These design principles led us to client-side load-balancing, and the [2012 Christmas Eve outage](https://netflixtechblog.com/a-closer-look-at-the-christmas-eve-outage-d7b409a529ee) solidified this decision even further. During these early years in the cloud, [we built Eureka](https://netflixtechblog.com/netflix-shares-cloud-load-balancing-and-failover-tool-eureka-c10647ef95e5) for Service Discovery and [Ribbon (internally known as NIWS) for IPC](https://netflixtechblog.com/announcing-ribbon-tying-the-netflix-mid-tier-services-together-a89346910a62). Eureka solved the problem of how services discover what instances to talk to, and Ribbon provided the client-side logic for load-balancing, as well as many other resiliency features. These two technologies, alongside a host of other resiliency and chaos tools, made a massive difference: our reliability improved measurably as a result.\n\nEureka and Ribbon presented a simple but powerful interface, which made adopting them easy. In order for a service to talk to another, it needs to know two things: the name of the destination service, and whether or not the traffic should be secure. The abstractions that Eureka provides for this are Virtual IPs (VIPs) for insecure communication, and Secure VIPs (SVIPs) for secure. A service advertises a VIP name and port to Eureka (eg: _myservice_, port _8080_), or an SVIP name and port (eg: _myservice-secure_, port 8443), or both. IPC clients are instantiated targeting that VIP or SVIP, and the Eureka client code handles the translation of that VIP to a set of IP and port pairs by fetching them from the Eureka server. The client can also optionally enable IPC features like retries or circuit breaking, or stick with a set of reasonable defaults.\n\nIn this architecture, service to service communication no longer goes through the single point of failure of a load balancer. The downside is that Eureka is a new single point of failure as the source of truth for what hosts are registered for VIPs. However, if Eureka goes down, services can continue to communicate with each other, though their host information will become stale over time as instances for a VIP come up and down. The ability to run in a degraded but available state during an outage is still a marked improvement over completely stopping traffic flow.\n\n## Why mesh?\n\nThe above architecture has served us well over the last decade, though changing business needs and evolving industry standards have added more complexity to our IPC ecosystem in a number of ways. First, we’ve grown the number of different IPC clients. Our internal IPC traffic is now a mix of plain REST, [GraphQL](https://netflixtechblog.com/how-netflix-scales-its-api-with-graphql-federation-part-1-ae3557c187e2), and [gRPC](https://netflixtechblog.com/practical-api-design-at-netflix-part-1-using-protobuf-fieldmask-35cfdc606518). Second, we’ve moved from a Java-only environment to a Polyglot one: we now also support [node.js](https://netflixtechblog.com/debugging-node-js-in-production-75901bb10f2d), [Python](https://netflixtechblog.com/python-at-netflix-bba45dae649e), and a variety of OSS and off the shelf software. Third, we’ve continued to add more functionality to our IPC clients: features such as [adaptive concurrency limiting](https://netflixtechblog.medium.com/performance-under-load-3e6fa9a60581), [circuit breaking](https://netflixtechblog.com/making-the-netflix-api-more-resilient-a8ec62159c2d), hedging, and fault injection have become standard tools that our engineers reach for to make our system more reliable. Compared to a decade ago, we now support more features, in more languages, in more clients. Keeping feature parity between all of these implementations and ensuring that they all behave the same way is challenging: what we want is a single, well-tested implementation of all of this functionality, so we can make changes and fix bugs in one place.\n\nThis is where service mesh comes in: we can centralize IPC features in a single implementation, and keep per-language clients as simple as possible: they only need to know how to talk to the local proxy. [Envoy](https://www.envoyproxy.io/) is a great fit for us as the proxy: it’s a battle-tested OSS product at use in high scale in the industry, with [many critical resiliency features](https://github.com/envoyproxy/envoy/issues/7789), and [good extension points](https://www.envoyproxy.io/docs/envoy/latest/configuration/listeners/network_filters/wasm_filter.html) for when we need to extend its functionality. The ability to [configure proxies via a central control plane](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/operations/dynamic_configuration) is a killer feature: this allows us to dynamically configure client-side load balancing as if it was a central load balancer, but still avoids a load balancer as a single point of failure in the service to service request path.\n\n## Moving to mesh\n\nOnce we decided that moving to service mesh was the right bet to make, the next question became: how should we go about moving? We decided on a number of constraints for the migration. First: we wanted to keep the existing interface. The abstraction of specifying a VIP name plus secure serves us well, and we didn’t want to break backwards compatibility. Second: we wanted to automate the migration and to make it as seamless as possible. These two constraints meant that we needed to support the Discovery abstractions in Envoy, so that IPC clients could continue to use it under the hood. Fortunately, Envoy had [ready to use abstractions](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/intro/terminology) for this. VIPs could be represented as Envoy Clusters, and proxies could fetch them from our control plane using the Cluster Discovery Service (CDS). The hosts in those clusters are represented as Envoy Endpoints, and could be fetched using the Endpoint Discovery Service (EDS).\n\nWe soon ran into a stumbling block to a seamless migration: Envoy requires that clusters be specified as part of the proxy’s config. If service A needs to talk to clusters B and C, then you need to define clusters B and C as part of A’s proxy config. This can be challenging at scale: any given service might communicate with dozens of clusters, and that set of clusters is different for every app. In addition, Netflix is always changing: we’re constantly adding new initiatives like live streaming, [ads](https://netflixtechblog.com/ensuring-the-successful-launch-of-ads-on-netflix-f99490fdf1ba) and games, and evolving our architecture. This means the clusters that a service communicates with will change over time. There are a number of different approaches to populating cluster config that we evaluated, given the Envoy primitives available to us:\n\n1.  Get service owners to define the clusters their service needs to talk to. This option seems simple, but in practice, service owners don’t always know, or want to know, what services they talk to. Services often import libraries provided by other teams that talk to multiple other services under the hood, or communicate with other operational services like telemetry and logging. This means that service owners would need to know how these auxiliary services and libraries are implemented under the hood, and adjust config when they change.\n2.  Auto-generate Envoy config based on a service’s call graph. This method is simple for pre-existing services, but is challenging when bringing up a new service or adding a new upstream cluster to communicate with.\n3.  Push all clusters to every app: this option was appealing in its simplicity, but back of the napkin math quickly showed us that pushing millions of endpoints to each proxy wasn’t feasible.\n\nGiven our goal of a seamless adoption, each of these options had significant enough downsides that we explored another option: what if we could fetch cluster information on-demand at runtime, rather than predefining it? At the time, the service mesh effort was still being bootstrapped, with only a few engineers working on it. We approached [Kinvolk](https://kinvolk.io/) to see if they could work with us and the Envoy community in implementing this feature. The result of this collaboration was [On-Demand Cluster Discovery](https://github.com/envoyproxy/envoy/pull/18723) (ODCDS). With this feature, proxies could now look up cluster information the first time they attempt to connect to it, rather than predefining all of the clusters in config.\n\nWith this capability in place, we needed to give the proxies cluster information to look up. We had already developed a service mesh control plane that implements the Envoy XDS services. We then needed to fetch service information from Eureka in order to return to the proxies. We represent Eureka VIPs and SVIPs as separate Envoy Cluster Discovery Service (CDS) clusters (so service _myservice_ may have clusters _myservice.vip_ and _myservice.svip_). Individual hosts in a cluster are represented as separate Endpoint Discovery Service (EDS) endpoints. This allows us to reuse the same Eureka abstractions, and IPC clients like Ribbon can move to mesh with minimal changes. With both the control plane and data plane changes in place, the flow works as follows:\n\n1.  Client request comes into Envoy\n2.  Extract the target cluster based on the Host / :authority header (the header used here is configurable, but this is our approach). If that cluster is known already, jump to step 7\n3.  The cluster doesn’t exist, so we pause the in flight request\n4.  Make a request to the Cluster Discovery Service (CDS) endpoint on the control plane. The control plane generates a customized CDS response based on the service’s configuration and Eureka registration information\n5.  Envoy gets back the cluster (CDS), which triggers a pull of the endpoints via Endpoint Discovery Service (EDS). Endpoints for the cluster are returned based on Eureka status information for that VIP or SVIP\n6.  Client request unpauses\n7.  Envoy handles the request as normal: it picks an endpoint using a load-balancing algorithm and issues the request\n\nThis flow is completed in a few milliseconds, but only on the first request to the cluster. Afterward, Envoy behaves as if the cluster was defined in the config. Critically, this system allows us to seamlessly migrate services to service mesh with no configuration required, satisfying one of our main adoption constraints. The abstraction we present continues to be VIP name plus secure, and we can migrate to mesh by configuring individual IPC clients to connect to the local proxy instead of the upstream app directly. We continue to use Eureka as the source of truth for VIPs and instance status, which allows us to support a heterogeneous environment of some apps on mesh and some not while we migrate. There’s an additional benefit: we can keep Envoy memory usage low by only fetching data for clusters that we’re actually communicating with.\n\nThere is a downside to fetching this data on-demand: this adds latency to the first request to a cluster. We have run into use-cases where services need very low-latency access on the first request, and adding a few extra milliseconds adds too much overhead. For these use-cases, the services need to either predefine the clusters they communicate with, or prime connections before their first request. We’ve also considered pre-pushing clusters from the control plane as proxies start up, based on historical request patterns. Overall, we feel the reduced complexity in the system justifies the downside for a small set of services.\n\nWe’re still early in our service mesh journey. Now that we’re using it in earnest, there are many more Envoy improvements that we’d love to work with the community on. The porting of our [adaptive concurrency limiting](https://github.com/envoyproxy/envoy/issues/7789) implementation to Envoy was a great start — we’re looking forward to collaborating with the community on many more. We’re particularly interested in the community’s work on incremental EDS. EDS endpoints account for the largest volume of updates, and this puts undue pressure on both the control plane and Envoy.\n\nWe’d like to give a big thank-you to the folks at Kinvolk for their Envoy contributions: Alban Crequy, Andrew Randall, Danielle Tal, and in particular Krzesimir Nowak for his excellent work. We’d also like to thank the Envoy community for their support and razor-sharp reviews: Adi Peleg, Dmitri Dolguikh, Harvey Tuch, Matt Klein, and Mark Roth. It’s been a great experience working with you all on this.\n\nThis is the first in a series of posts on our journey to service mesh, so stay tuned. If this sounds like fun, and you want to work on service mesh at scale, come work with us — [we’re hiring](https://jobs.netflix.com/jobs/271057970)!"
    },
    {
      "url": "https://netflixtechblog.com/ava-discovery-view-surfacing-authentic-moments-b8cd145491cc?source=collection_home---4------6-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/ava-discovery-view-surfacing-authentic-moments-b8cd145491cc?gi=9ccd70d5b6b4&source=collection_home---4------6-----------------------",
        "loadedTime": "2023-12-06T00:03:25.393Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/ava-discovery-view-surfacing-authentic-moments-b8cd145491cc",
        "title": "AVA Discovery View: Surfacing Authentic Moments | by Netflix Technology Blog | Netflix TechBlog",
        "description": "AVA Discovery view is an assistant created for Creatives at Netflix helping them to choose the best and most relevant frames to form the basis of their promotional assets.  The assistant is powered by Netflix's machine learning and computer vision algorithms",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "AVA Discovery View: Surfacing Authentic Moments\nBy: Hamid Shahid, Laura Johnson, Tiffany Low\nSynopsis\nAt Netflix, we have created millions of artwork to represent our titles. Each artwork tells a story about the title it represents. From our testing on promotional assets, we know which of these assets have performed well and which ones haven’t. Through this, our teams have developed an intuition of what visual and thematic artwork characteristics work well for what genres of titles. A piece of promotional artwork may resonate more in certain regions, for certain genres, or for fans of particular talent. The complexity of these factors makes it difficult to determine the best creative strategy for upcoming titles.\nOur assets are often created by selecting static image frames directly from our source videos. To improve it, we decided to invest in creating a Media Understanding Platform, which enables us to extract meaningful insights from media that we can then surface in our creative tools. In this post, we will take a deeper look into one of these tools, AVA Discovery View.\nIntro to AVA Discovery View\nAVA is an internal tool that surfaces still frames from video content. The tool provides an efficient way for creatives (photo editors, artwork designers, etc.) to pull moments from video content that authentically represent the title’s narrative themes, main characters, and visual characteristics. These still moments are used by multiple teams across Netflix for artwork (on and off the Netflix platform), Publicity, Marketing, Social teams, and more.\nStills are used to merchandise & publicize titles authentically, providing a diverse set of entry points to members who may watch for different reasons. For example, for our hit title “Wednesday”, one member may watch it because they love mysteries, while another may watch because they love coming-of-age stories or goth aesthetics. Another member may be drawn by talent. It’s a creative’s job to select frames with all these entry points in mind. Stills may be enhanced and combined to create a more polished piece of artwork or be used as is. For many teams and titles, Stills are essential to Netflix’s promotional asset strategy.\nWatching every moment of content to find the best frames and select them manually takes a lot of time, and this approach is often not scalable. While frames can be saved manually from the video content, AVA goes beyond providing the functionality to surface authentic frames — it suggests the best moments for creatives to use: enter AVA Discovery View.\nExample of AVA Discovery View\nAVA’s imagery-harvesting algorithms pre-select and group relevant frames into categories like Storylines & Tones, Prominent Characters, and Environments.\nLet’s look deeper at how different facets of a title are shown in one of Netflix’s biggest hits — “Wednesday”.\nStoryline / Tone\nThe title “Wednesday” involves a character with supernatural abilities sleuthing to solve a mystery. The title has a dark, imaginative tone with shades of wit and dry humor. The setting is an extraordinary high school where teenagers of supernatural abilities are enrolled. The main character is a teenager and has relationship issues with her parents.\nThe paragraph above provides a short glimpse of the title and is similar to the briefs that our creatives have to work with. Finding authentic moments from this information to build the base of the artwork suite is not trivial and has been very time-consuming for our creatives.\nThis is where AVA Discovery View comes in and functions as a creative assistant. Using the information about the storyline and tones associated with a title, it surfaces key moments, which not only provide a nice visual summary but also provide a quick landscape view of the title’s main narrative themes and its visual language.\nStoryline & Tone suggestions\nCreatives can click on any storyline to see moments that best reflect that storyline and the title’s overall tone. For example, the following images illustrate how it displays moments for the “imaginative” tone.\nProminent Characters\nTalent is a major draw for our titles, and our members want to see who is featured in a title to choose whether or not they want to watch that title. Getting to know the prominent characters for a title and then finding the best possible moments featuring them used to be an arduous task.\nWith the AVA Discovery View, all the prominent characters of the title and their best possible shots are presented to the creatives. They can see how much a character is featured in the title and find shots containing multiple characters and the best possible stills for the characters themselves.\nSensitivities\nWe don’t want the Netflix home screen to shock or offend audiences, so we aim to avoid artwork with violence, nudity, gore or similar attributes.\nTo help our creatives understand content sensitivities, AVA Discovery View lists moments where content contains gore, violence, intimacy, nudity, smoking, etc.\nSensitive Moments\nEnvironments\nThe setting and the filming location often provide great genre cues and form the basis of great-looking artwork. Finding moments from a virtual setting in the title or the actual filming location required a visual scan of all episodes of a title. Now, AVA Discovery View shows such moments as suggestions to the creatives.\nFor example, for the title “Wednesday”, the creatives are presented with “Nevermore Academy” as a suggested environment\nSuggested Environment — Nevermore Academy\nChallenges\nAlgorithm Quality\nAVA Discovery View included several different algorithms at the start, and since its release, we have expanded support to additional algorithms. Each algorithm needed a process of evaluation and tuning to get great results in AVA Discovery View.\nFor Visual Search\nWe found that the model was influenced by the text present in the image. For example, stills of title credits would often get picked up and highly recommended to users. We added a step where such stills with text results would be filtered out and not present in the search.\nWe also found that users preferred results that had a confidence threshold cutoff applied to them.\nFor Prominent Characters\nWe found that our current algorithm model did not handle animated faces well. As a result, we often find that poor or no suggestions are returned for animated content.\nFor Sensitive Moments\nWe found that setting a high confidence threshold was helpful. The algorithm was originally developed to be sensitive to bloody scenes, and when applied to scenes of cooking and painting, often flagged as false positives.\nOne challenge we encountered was the repetition of suggestions. Multiple suggestions from the same scene could be returned and lead to many visually similar moments. Users preferred seeing only the best frames and a diverse set of frames.\nWe added a ranking step to some algorithms to mark frames too visually similar to higher-ranked frames. These duplicate frames would be filtered out from the suggestions list.\nHowever, not all algorithms can take this approach. We are exploring using scene boundary algorithms to group similar moments together as a single recommendation.\nSuggestion Ranking\nAVA Discovery View presents multiple levels of algorithmic suggestions, and a challenge was to help users navigate through the best-performing suggestions and avoid selecting bad suggestions.\nThe suggestion categories are presented based on our users’ workflow relevance. We show Storyline/Tone, Prominent Characters, Environments, then Sensitivities.\nWithin each suggestion category, we display suggestions ranked by the number of results and tie break along the confidence threshold.\nAlgorithm Feedback\nAs we launched the initial set of algorithms for AVA Discovery View, our team interviewed users about their experiences. We also built mechanisms within the tool to get explicit and implicit user feedback.\nExplicit Feedback\nFor each algorithmic suggestion presented to a user, users can click a thumbs up or thumbs down to give direct feedback.\nImplicit Feedback\nWe have tracking enabled to detect when an algorithmic suggestion has been utilized (downloaded or published for use on Netflix promotional purposes).\nThis implicit feedback is much easier to collect, although it may not work for all algorithms. For example, suggestions from Sensitivities are meant to be content watch-outs that should not be used for promotional purposes. As a result, this row does poorly on implicit feedback as we do not expect downloads or publish actions on these suggestions.\nThis feedback is easily accessible by our algorithm partners and used in training improved versions of the models.\nIntersection Queries across Multiple Algorithms\nSeveral media understanding algorithms return clip or short-duration video segment suggestions. We compute the timecode intersections against a set of known high-quality frames to surface the best frame within these clips.\nWe also rely on intersection queries to help users narrow a large set of frames to a specific moment. For example, returning stills with two or more prominent characters or filtering only indoor scenes from a search query.\nTechnical Architecture\nDiscovery View Plugin Architecture\nDiscovery View Plugin Architecture\nWe built Discovery View as a pluggable feature that could quickly be extended to support more algorithms and other types of suggestions. Discovery View is available via Studio Gateway for AVA UI and other front-end applications to leverage.\nUnified Interface for Discovery\nAll Discovery View rows implement the same interface, and it’s simple to extend it and plug it into the existing view.\nScalable Categories\nIn the Discovery View feature, we dynamically hide categories or recommendations based on the results of algorithms. Categories can be hidden if no suggestions are found. On the other hand, for a large number of suggestions, only top suggestions are retrieved, and users have the ability to request more.\nGraceful Failure Handling\nWe load Discovery View suggestions independently for a responsive user experience.\nAsset Feedback MicroService\nAsset Feedback MicroService\nWe identified that Asset Feedback is a functionality that is useful elsewhere in our ecosystem as well, so we decided to create a separate microservice for it. The service serves an important function of getting feedback about the quality of stills and ties them to the algorithms. This information is available both at individual and aggregated levels for our algorithm partners.\nMedia Understanding Platform\nAVA Discovery View relies on the Media Understanding Platform (MUP) as the main interface for algorithm suggestions. The key features of this platform are\nUniform Query Interface\nHosting all of the algorithms in AVA Discovery View on MUP made it easier for product integration as the suggestions could be queried from each algorithm similarly\nRich Query Feature Set\nWe could test different confidence thresholds per algorithm, intersect across algorithm suggestions, and order suggestions by various fields.\nFast Algo Onboarding\nEach algorithm took fewer than two weeks to onboard, and the platform ensured that new titles delivered to Netflix would automatically generate algorithm suggestions. Our team was able to spend more time evaluating algorithm performance and quickly iterate on AVA Discovery View.\nTo learn more about MUP, please see a previous blog post from our team: Building a Media Understanding Platform for ML Innovations.\nImpact\nDiscovering authentic moments in an efficient and scalable way has a huge impact on Netflix and its creative teams. AVA has become a place to gain title insights and discover assets. It provides a concise brief on the main narratives, the visual language, and the title’s prominent characters. An AVA user can find relevant and visually stunning frames quickly and easily and leverage them as a context-gathering tool.\nFuture Work\nTo improve AVA Discovery View, our team needs to balance the number of frames returned and the quality of the suggestions so that creatives can build more trust with the feature.\nEliminating Repetition\nAVA Discovery View will often put the same frame into multiple categories, which results in creatives viewing and evaluating the same frame multiple times. How can we solve for an engaging frame being a part of multiple groupings without bloating each grouping with repetition?\nImproving Frame Quality\nWe’d like to only show creatives the best frames from a certain moment and work to eliminate frames that have either poor technical quality (a poor character expression) or poor editorial quality (not relevant to grouping, not relevant to narrative). Sifting through frames that aren’t up to quality standards creates user fatigue.\nBuilding User Trust\nCreatives don’t want to wonder whether there’s something better outside an AVA Discovery View grouping or if anything is missing from these suggested frames.\nWhen looking at a particular grouping (like “Wednesday”’s Solving a Mystery or Gothic), creatives need to trust that it doesn’t contain any frames that don’t belong there, that these are the best quality frames, and that there are no better frames that exist in the content that isn’t included in the grouping. Suppose a creative is leveraging AVA Discovery View and doing separate manual work to improve frame quality or check for missing moments. In that case, AVA Discovery View hasn’t yet fully optimized the user experience.\nAcknowledgment\nSpecial thanks to Abhishek Soni, Amir Ziai, Andrew Johnson, Ankush Agrawal, Aneesh Vartakavi, Audra Reed, Brianda Suarez, Faraz Ahmad, Faris Mustafa, Fifi Maree, Guru Tahasildar, Gustavo Carmo, Haley Jones Phillips, Janan Barge, Karen Williams, Laura Johnson, Maria Perkovic, Meenakshi Jindal, Nagendra Kamath, Nicola Pharoah, Qiang Liu, Samuel Carvajal, Shervin Ardeshir, Supriya Vadlamani, Varun Sekhri, and Vitali Kauhanka for making it all possible.",
      "markdown": "## AVA Discovery View: Surfacing Authentic Moments\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----b8cd145491cc--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----b8cd145491cc--------------------------------)\n\nBy: [Hamid Shahid](https://www.linkedin.com/in/hamidshahid), [Laura Johnson](https://www.linkedin.com/in/ljworks34/), [Tiffany Low](https://www.linkedin.com/in/tiffany-low/)\n\n## Synopsis\n\nAt Netflix, we have created millions of artwork to represent our titles. Each artwork tells a story about the title it represents. From our [testing on promotional assets](https://netflixtechblog.com/selecting-the-best-artwork-for-videos-through-a-b-testing-f6155c4595f6), we know which of these assets have performed well and which ones haven’t. Through this, our teams have developed an intuition of what visual and thematic artwork characteristics work well for what genres of titles. A piece of promotional artwork may resonate more in certain regions, for certain genres, or for fans of particular talent. The complexity of these factors makes it difficult to determine the best creative strategy for upcoming titles.\n\nOur assets are often created by selecting static image frames directly from our source videos. To improve it, we decided to invest in creating a [Media Understanding Platform](https://netflixtechblog.com/building-a-media-understanding-platform-for-ml-innovations-9bef9962dcb7), which enables us to extract meaningful insights from media that we can then surface in our creative tools. In this post, we will take a deeper look into one of these tools, AVA Discovery View.\n\n## Intro to AVA Discovery View\n\nAVA is an internal tool that surfaces still frames from video content. The tool provides an efficient way for creatives (photo editors, artwork designers, etc.) to pull moments from video content that authentically represent the title’s narrative themes, main characters, and visual characteristics. These still moments are used by multiple teams across Netflix for artwork (on and off the Netflix platform), Publicity, Marketing, Social teams, and more.\n\nStills are used to merchandise & publicize titles authentically, providing a diverse set of entry points to members who may watch for different reasons. For example, for our hit title “_Wednesday”_, one member may watch it because they love mysteries, while another may watch because they love coming-of-age stories or goth aesthetics. Another member may be drawn by talent. It’s a creative’s job to select frames with all these entry points in mind. Stills may be enhanced and combined to create a more polished piece of artwork or be used as is. For many teams and titles, Stills are essential to Netflix’s promotional asset strategy.\n\nWatching every moment of content to find the best frames and select them manually takes a lot of time, and this approach is often not scalable. While frames can be saved manually from the video content, AVA goes beyond providing the functionality to surface authentic frames — it suggests the best moments for creatives to use: enter AVA Discovery View.\n\n## Example of AVA Discovery View\n\nAVA’s imagery-harvesting algorithms pre-select and group relevant frames into categories like _Storylines & Tones_, _Prominent Characters,_ and _Environments_.\n\nLet’s look deeper at how different facets of a title are shown in one of Netflix’s biggest hits — “_Wednesday”_.\n\n## Storyline / Tone\n\nThe title _“Wednesday”_ involves a character with supernatural abilities sleuthing to solve a mystery. The title has a dark, imaginative tone with shades of wit and dry humor. The setting is an extraordinary high school where teenagers of supernatural abilities are enrolled. The main character is a teenager and has relationship issues with her parents.\n\nThe paragraph above provides a short glimpse of the title and is similar to the briefs that our creatives have to work with. Finding authentic moments from this information to build the base of the artwork suite is not trivial and has been very time-consuming for our creatives.\n\nThis is where AVA Discovery View comes in and functions as a creative assistant. Using the information about the storyline and tones associated with a title, it surfaces key moments, which not only provide a nice visual summary but also provide a quick landscape view of the title’s main narrative themes and its visual language.\n\nStoryline & Tone suggestions\n\nCreatives can click on any storyline to see moments that best reflect that storyline and the title’s overall tone. For example, the following images illustrate how it displays moments for the “imaginative” tone.\n\n## Prominent Characters\n\nTalent is a major draw for our titles, and our members want to see who is featured in a title to choose whether or not they want to watch that title. Getting to know the prominent characters for a title and then finding the best possible moments featuring them used to be an arduous task.\n\nWith the AVA Discovery View, all the prominent characters of the title and their best possible shots are presented to the creatives. They can see how much a character is featured in the title and find shots containing multiple characters and the best possible stills for the characters themselves.\n\n## Sensitivities\n\nWe don’t want the Netflix home screen to shock or offend audiences, so we aim to avoid artwork with violence, nudity, gore or similar attributes.\n\nTo help our creatives understand content sensitivities, AVA Discovery View lists moments where content contains gore, violence, intimacy, nudity, smoking, etc.\n\nSensitive Moments\n\n## Environments\n\nThe setting and the filming location often provide great genre cues and form the basis of great-looking artwork. Finding moments from a virtual setting in the title or the actual filming location required a visual scan of all episodes of a title. Now, AVA Discovery View shows such moments as suggestions to the creatives.\n\nFor example, for the title “_Wednesday”_, the creatives are presented with “Nevermore Academy” as a suggested environment\n\nSuggested Environment — Nevermore Academy\n\n## Challenges\n\n## Algorithm Quality\n\nAVA Discovery View included several different algorithms at the start, and since its release, we have expanded support to additional algorithms. Each algorithm needed a process of evaluation and tuning to get great results in AVA Discovery View.\n\n**For Visual Search**\n\n*   We found that the model was influenced by the text present in the image. For example, stills of title credits would often get picked up and highly recommended to users. We added a step where such stills with text results would be filtered out and not present in the search.\n*   We also found that users preferred results that had a confidence threshold cutoff applied to them.\n\n**For Prominent Characters**\n\n*   We found that our current algorithm model did not handle animated faces well. As a result, we often find that poor or no suggestions are returned for animated content.\n\n**For Sensitive Moments**\n\n*   We found that setting a high confidence threshold was helpful. The algorithm was originally developed to be sensitive to bloody scenes, and when applied to scenes of cooking and painting, often flagged as false positives.\n\nOne challenge we encountered was the repetition of suggestions. Multiple suggestions from the same scene could be returned and lead to many visually similar moments. Users preferred seeing only the best frames and a diverse set of frames.\n\n*   We added a ranking step to some algorithms to mark frames too visually similar to higher-ranked frames. These duplicate frames would be filtered out from the suggestions list.\n*   However, not all algorithms can take this approach. We are exploring using scene boundary algorithms to group similar moments together as a single recommendation.\n\n## Suggestion Ranking\n\nAVA Discovery View presents multiple levels of algorithmic suggestions, and a challenge was to help users navigate through the best-performing suggestions and avoid selecting bad suggestions.\n\n*   The suggestion categories are presented based on our users’ workflow relevance. We show Storyline/Tone, Prominent Characters, Environments, then Sensitivities.\n*   Within each suggestion category, we display suggestions ranked by the number of results and tie break along the confidence threshold.\n\n## Algorithm Feedback\n\nAs we launched the initial set of algorithms for AVA Discovery View, our team interviewed users about their experiences. We also built mechanisms within the tool to get explicit and implicit user feedback.\n\n**Explicit Feedback**\n\n*   For each algorithmic suggestion presented to a user, users can click a thumbs up or thumbs down to give direct feedback.\n\n**Implicit Feedback**\n\n*   We have tracking enabled to detect when an algorithmic suggestion has been utilized (downloaded or published for use on Netflix promotional purposes).\n*   This implicit feedback is much easier to collect, although it may not work for all algorithms. For example, suggestions from Sensitivities are meant to be content watch-outs that should not be used for promotional purposes. As a result, this row does poorly on implicit feedback as we do not expect downloads or publish actions on these suggestions.\n\nThis feedback is easily accessible by our algorithm partners and used in training improved versions of the models.\n\n## Intersection Queries across Multiple Algorithms\n\nSeveral media understanding algorithms return clip or short-duration video segment suggestions. We compute the timecode intersections against a set of known high-quality frames to surface the best frame within these clips.\n\nWe also rely on intersection queries to help users narrow a large set of frames to a specific moment. For example, returning stills with two or more prominent characters or filtering only indoor scenes from a search query.\n\n## Technical Architecture\n\n## Discovery View Plugin Architecture\n\nDiscovery View Plugin Architecture\n\nWe built Discovery View as a pluggable feature that could quickly be extended to support more algorithms and other types of suggestions. Discovery View is available via Studio Gateway for AVA UI and other front-end applications to leverage.\n\n## Unified Interface for Discovery\n\nAll Discovery View rows implement the same interface, and it’s simple to extend it and plug it into the existing view.\n\n**Scalable Categories  \n**In the Discovery View feature, we dynamically hide categories or recommendations based on the results of algorithms. Categories can be hidden if no suggestions are found. On the other hand, for a large number of suggestions, only top suggestions are retrieved, and users have the ability to request more.\n\n**Graceful Failure Handling  \n**We load Discovery View suggestions independently for a responsive user experience.\n\n## Asset Feedback MicroService\n\nAsset Feedback MicroService\n\nWe identified that Asset Feedback is a functionality that is useful elsewhere in our ecosystem as well, so we decided to create a separate microservice for it. The service serves an important function of getting feedback about the quality of stills and ties them to the algorithms. This information is available both at individual and aggregated levels for our algorithm partners.\n\n## Media Understanding Platform\n\nAVA Discovery View relies on the Media Understanding Platform (MUP) as the main interface for algorithm suggestions. The key features of this platform are\n\n## Uniform Query Interface\n\nHosting all of the algorithms in AVA Discovery View on MUP made it easier for product integration as the suggestions could be queried from each algorithm similarly\n\n## Rich Query Feature Set\n\nWe could test different confidence thresholds per algorithm, intersect across algorithm suggestions, and order suggestions by various fields.\n\n## Fast Algo Onboarding\n\nEach algorithm took fewer than two weeks to onboard, and the platform ensured that new titles delivered to Netflix would automatically generate algorithm suggestions. Our team was able to spend more time evaluating algorithm performance and quickly iterate on AVA Discovery View.\n\nTo learn more about MUP, please see a previous blog post from our team: [Building a Media Understanding Platform for ML Innovations](https://netflixtechblog.com/building-a-media-understanding-platform-for-ml-innovations-9bef9962dcb7).\n\n## Impact\n\nDiscovering authentic moments in an efficient and scalable way has a huge impact on Netflix and its creative teams. AVA has become a place to gain title insights and discover assets. It provides a concise brief on the main narratives, the visual language, and the title’s prominent characters. An AVA user can find relevant and visually stunning frames quickly and easily and leverage them as a context-gathering tool.\n\n## Future Work\n\nTo improve AVA Discovery View, our team needs to balance the number of frames returned and the quality of the suggestions so that creatives can build more trust with the feature.\n\n## Eliminating Repetition\n\nAVA Discovery View will often put the same frame into multiple categories, which results in creatives viewing and evaluating the same frame multiple times. How can we solve for an engaging frame being a part of multiple groupings without bloating each grouping with repetition?\n\n## Improving Frame Quality\n\nWe’d like to only show creatives the best frames from a certain moment and work to eliminate frames that have either poor technical quality (a poor character expression) or poor editorial quality (not relevant to grouping, not relevant to narrative). Sifting through frames that aren’t up to quality standards creates user fatigue.\n\n## Building User Trust\n\nCreatives don’t want to wonder whether there’s something better outside an AVA Discovery View grouping or if anything is missing from these suggested frames.\n\nWhen looking at a particular grouping (like “_Wednesday”’s_ _Solving a Mystery_ or _Gothic_), creatives need to trust that it doesn’t contain any frames that don’t belong there, that these are the best quality frames, and that there are no better frames that exist in the content that isn’t included in the grouping. Suppose a creative is leveraging AVA Discovery View and doing separate manual work to improve frame quality or check for missing moments. In that case, AVA Discovery View hasn’t yet fully optimized the user experience.\n\n## Acknowledgment\n\nSpecial thanks to [Abhishek Soni](https://www.linkedin.com/in/abhisheks0ni/), [Amir Ziai](https://www.linkedin.com/in/amirziai/), [Andrew Johnson](https://www.linkedin.com/in/andrea-johnson-01535946/), [Ankush Agrawal](https://www.linkedin.com/in/ankushagrawal94/), [Aneesh Vartakavi](https://www.linkedin.com/in/aneeshvartakavi/), [Audra Reed](https://www.linkedin.com/in/audra-reed-83a0007/), [Brianda Suarez](https://www.linkedin.com/in/briandasg/), [Faraz Ahmad](https://www.linkedin.com/in/farazamiruddin/), [Faris Mustafa](https://www.linkedin.com/in/farisito/), [Fifi Maree](https://www.linkedin.com/in/fifi-mar%C3%A9e/), [Guru Tahasildar](https://www.linkedin.com/in/gurutahasildar/), [Gustavo Carmo](https://www.linkedin.com/in/gucarmo/), [Haley Jones Phillips](https://www.linkedin.com/in/haleyjonesphillips/), [Janan Barge](https://www.linkedin.com/in/jananbarge/), [Karen Williams](https://www.linkedin.com/in/karenannwilliams/), [Laura Johnson](https://www.linkedin.com/in/ljworks34/), [Maria Perkovic](https://www.linkedin.com/in/maria-perkovic/), [Meenakshi Jindal](https://www.linkedin.com/in/meenakshijindal/), [Nagendra Kamath](https://www.linkedin.com/in/nagendrak/), [Nicola Pharoah](https://www.linkedin.com/in/nicolapharoah/), [Qiang Liu](https://www.linkedin.com/in/qiang-liu-7a18b32a/), [Samuel Carvajal](https://www.linkedin.com/in/samuel-carvajal/), [Shervin Ardeshir](https://www.linkedin.com/in/shervin-ardeshir/), [Supriya Vadlamani](https://www.linkedin.com/in/supriya-vadlamani/), [Varun Sekhri](https://www.linkedin.com/in/varun-sekhri-087a213/), and [Vitali Kauhanka](https://www.linkedin.com/in/vitalikauhanka/) for making it all possible."
    },
    {
      "url": "https://netflixtechblog.com/kubernetes-and-kernel-panics-ed620b9c6225?source=collection_home---4------4-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/kubernetes-and-kernel-panics-ed620b9c6225?gi=72118dc3d726&source=collection_home---4------4-----------------------",
        "loadedTime": "2023-12-06T00:03:25.045Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/kubernetes-and-kernel-panics-ed620b9c6225",
        "title": "How Netflix's Container Platform Connects Linux Kernel Panics to Kubernetes Pods | Netflix TechBlog",
        "description": "With a recent effort to reduce customer (engineers, not end users) pain on our container platform Titus, I started investigating “orphaned” pods. There are pods that never got to finish and had to be…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Kubernetes And Kernel Panics\nHow Netflix’s Container Platform Connects Linux Kernel Panics to Kubernetes Pods\nBy Kyle Anderson\nWith a recent effort to reduce customer (engineers, not end users) pain on our container platform Titus, I started investigating “orphaned” pods. There are pods that never got to finish and had to be garbage collected with no real satisfactory final status. Our Service job (think ReplicatSet) owners don’t care too much, but our Batch users care a lot. Without a real return code, how can they know if it is safe to retry or not?\nThese orphaned pods represent real pain for our users, even if they are a small percentage of the total pods in the system. Where are they going, exactly? Why did they go away?\nThis blog post shows how to connect the dots from the worst case scenario (a kernel panic) through to Kubernetes (k8s) and eventually up to us operators so that we can track how and why our k8s nodes are going away.\nWhere Do Orphaned Pods Come From?\nOrphaned pods get lost because the underlying k8s node object goes away. Once that happens a GC process deletes the pod. On Titus we run a custom controller to store the history of Pod and Node objects, so that we can save some explanation and show it to our users. This failure mode looks like this in our UI:\nWhat it looks like to our users when a k8s node and its pods disappear\nThis is an explanation, but it wasn’t very satisfying to me or to our users. Why was the agent lost?\nWhere Do Lost Nodes Come From?\nNodes can go away for any reason, especially in “the cloud”. When this happens, usually a k8s cloud-controller provided by the cloud vendor will detect that the actual server, in our case an EC2 Instance, has actually gone away, and will in turn delete the k8s node object. That still doesn’t really answer the question of why.\nHow can we make sure that every instance that goes away has a reason, account for that reason, and bubble it up all the way to the pod? It all starts with an annotation:\n{\n\"apiVersion\": \"v1\",\n\"kind\": \"Pod\",\n\"metadata\": {\n\"annotations\": {\n\"pod.titus.netflix.com/pod-termination-reason\": \"Something really bad happened!\",\n...\nJust making a place to put this data is a great start. Now all we have to do is make our GC controllers aware of this annotation, and then sprinkle it into any process that could potentially make a pod or node go away unexpectedly. Adding an annotation (as opposed to patching the status) preserves the rest of the pod as-is for historical purposes. (We also add annotations for what did the terminating, and a short reason-code for tagging)\nThe pod-termination-reason annotation is useful to populate human readable messages like:\n“This pod was preempted by a higher priority job ($id)”\n“This pod had to be terminated because the underlying hardware failed ($failuretype)”\n“This pod had to be terminated because $user ran sudo halt on the node”\n“This pod died unexpectedly because the underlying node kernel panicked!”\nBut wait, how are we going to annotate a pod for a node that kernel panicked?\nCapturing Kernel Panics\nWhen the Linux kernel panics, there is just not much you can do. But what if you could send out some sort of “with my final breath, I curse Kubernetes!” UDP packet?\nInspired by this Google Spanner paper, where Spanner nodes send out a “last gasp” UDP packet to release leases & locks, you too can configure your servers to do the same upon kernel panic using a stock Linux module: netconsole.\nConfiguring Netconsole\nThe fact that the Linux kernel can even send out UDP packets with the string ‘kernel panic’, while it is panicking, is kind of amazing. This works because netconsole needs to be configured with almost the entire IP header filled out already beforehand. That is right, you have to tell Linux exactly what your source MAC, IP, and UDP Port are, as well as the destination MAC, IP, and UDP ports. You are practically constructing the UDP packet for the kernel. But, with that prework, when the time comes, the kernel can easily construct the packet and get it out the (preconfigured) network interface as things come crashing down. Luckily the netconsole-setup command makes the setup pretty easy. All the configuration options can be set dynamically as well, so that when the endpoint changes one can point to the new IP.\nOnce this is setup, kernel messages will start flowing right after modprobe. Imagine the whole thing operating like a dmesg | netcat -u $destination 6666, but in kernel space.\nNetconsole “Last Gasp” Packets\nWith netconsole setup, the last gasp from a crashing kernel looks like a set of UDP packets exactly like one might expect, where the data of the UDP packet is simply the text of the kernel message. In the case of a kernel panic, it will look something like this (one UDP packet per line):\nKernel panic - not syncing: buffer overrun at 0x4ba4c73e73acce54\n[ 8374.456345] CPU: 1 PID: 139616 Comm: insmod Kdump: loaded Tainted: G OE\n[ 8374.458506] Hardware name: Amazon EC2 r5.2xlarge/, BIOS 1.0 10/16/2017\n[ 8374.555629] Call Trace:\n[ 8374.556147] <TASK>\n[ 8374.556601] dump_stack_lvl+0x45/0x5b\n[ 8374.557361] panic+0x103/0x2db\n[ 8374.558166] ? __cond_resched+0x15/0x20\n[ 8374.559019] ? do_init_module+0x22/0x20a\n[ 8374.655123] ? 0xffffffffc0f56000\n[ 8374.655810] init_module+0x11/0x1000 [kpanic]\n[ 8374.656939] do_one_initcall+0x41/0x1e0\n[ 8374.657724] ? __cond_resched+0x15/0x20\n[ 8374.658505] ? kmem_cache_alloc_trace+0x3d/0x3c0\n[ 8374.754906] do_init_module+0x4b/0x20a\n[ 8374.755703] load_module+0x2a7a/0x3030\n[ 8374.756557] ? __do_sys_finit_module+0xaa/0x110\n[ 8374.757480] __do_sys_finit_module+0xaa/0x110\n[ 8374.758537] do_syscall_64+0x3a/0xc0\n[ 8374.759331] entry_SYSCALL_64_after_hwframe+0x62/0xcc\n[ 8374.855671] RIP: 0033:0x7f2869e8ee69\n...\nConnecting to Kubernetes\nThe last piece is to connect is Kubernetes (k8s). We need a k8s controller to do the following:\nListen for netconsole UDP packets on port 6666, watching for things that look like kernel panics from nodes.\nUpon kernel panic, lookup the k8s node object associated with the IP address of the incoming netconsole packet.\nFor that k8s node, find all the pods bound to it, annotate, then delete those pods (they are toast!).\nFor that k8s node, annotate the node and then delete it too (it is also toast!).\nParts 1&2 might look like this:\nfor {\nn, addr, err := serverConn.ReadFromUDP(buf)\nif err != nil {\nklog.Errorf(\"Error ReadFromUDP: %s\", err)\n} else {\nline := santizeNetConsoleBuffer(buf[0:n])\nif isKernelPanic(line) {\npanicCounter = 20\ngo handleKernelPanicOnNode(ctx, addr, nodeInformer, podInformer, kubeClient, line)\n}\n}\nif panicCounter > 0 {\nklog.Infof(\"KernelPanic context from %s: %s\", addr.IP, line)\npanicCounter++\n}\n}\nAnd then parts 3&4 might look like this:\nfunc handleKernelPanicOnNode(ctx context.Context, addr *net.UDPAddr, nodeInformer cache.SharedIndexInformer, podInformer cache.SharedIndexInformer, kubeClient kubernetes.Interface, line string) {\nnode := getNodeFromAddr(addr.IP.String(), nodeInformer)\nif node == nil {\nklog.Errorf(\"Got a kernel panic from %s, but couldn't find a k8s node object for it?\", addr.IP.String())\n} else {\npods := getPodsFromNode(node, podInformer)\nklog.Infof(\"Got a kernel panic from node %s, annotating and deleting all %d pods and that node.\", node.Name, len(pods))\nannotateAndDeletePodsWithReason(ctx, kubeClient, pods, line)\nerr := deleteNode(ctx, kubeClient, node.Name)\nif err != nil {\nklog.Errorf(\"Error deleting node %s: %s\", node.Name, err)\n} else {\nklog.Infof(\"Deleted panicked node %s\", node.Name)\n}\n}\n}\nWith that code in place, as soon as a kernel panic is detected, the pods and nodes immediately go away. No need to wait for any GC process. The annotations help document what happened to the node & pod:\nA real pod lost on a real k8s node that had a real kernel panic!\nConclusion\nMarking that a job failed because of a kernel panic may not be that satisfactory to our customers. But they can take satisfaction in knowing that we now have the required observability tools to start fixing those kernel panics!\nDo you also enjoy really getting to the bottom of why things fail in your systems or think kernel panics are cool? Join us on the Compute Team where we are building a world-class container platform for our engineers.",
      "markdown": "## Kubernetes And Kernel Panics\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----ed620b9c6225--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----ed620b9c6225--------------------------------)\n\nHow Netflix’s Container Platform Connects Linux Kernel Panics to Kubernetes Pods\n\n_By Kyle Anderson_\n\nWith a recent effort to reduce customer (engineers, not end users) pain on our container platform [Titus](https://netflixtechblog.com/tagged/titus), I started investigating “orphaned” pods. There are pods that never got to finish and had to be garbage collected with no real satisfactory final status. Our Service job (think [ReplicatSet](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/)) owners don’t care too much, but our Batch users care a lot. Without a real return code, how can they know if it is safe to retry or not?\n\nThese orphaned pods represent real pain for our users, even if they are a small percentage of the total pods in the system. Where are they going, exactly? Why did they go away?\n\nThis blog post shows how to connect the dots from the worst case scenario (a kernel panic) through to Kubernetes (k8s) and eventually up to us operators so that we can track how and why our k8s nodes are going away.\n\n## Where Do Orphaned Pods Come From?\n\nOrphaned pods get lost because the underlying k8s node object goes away. Once that happens a [GC](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection) process deletes the pod. On Titus we run a custom controller to store the history of Pod and Node objects, so that we can save some explanation and show it to our users. This failure mode looks like this in our UI:\n\nWhat it looks like to our users when a k8s node and its pods disappear\n\nThis is _an_ explanation, but it wasn’t very satisfying to me or to our users. _Why_ was the agent lost?\n\n## Where Do Lost Nodes Come From?\n\nNodes can go away for any reason, especially in “the cloud”. When this happens, usually a k8s cloud-controller provided by the cloud vendor will detect that the actual server, in our case an EC2 Instance, has actually gone away, and will in turn delete the k8s node object. That still doesn’t really answer the question of _why_.\n\nHow can we make sure that every instance that goes away has a reason, account for that reason, and bubble it up all the way to the pod? It all starts with an annotation:\n\n{  \n     \"apiVersion\": \"v1\",  \n     \"kind\": \"Pod\",  \n     \"metadata\": {  \n          \"annotations\": {  \n               \"pod.titus.netflix.com/pod-termination-reason\": \"Something really bad happened!\",  \n...\n\nJust making a place to put this data is a great start. Now all we have to do is make our GC controllers aware of this annotation, and then sprinkle it into any process that could potentially make a pod or node go away unexpectedly. Adding an annotation (as opposed to patching the status) preserves the rest of the pod as-is for historical purposes. (We also add annotations for what did the terminating, and a short `reason-code` for tagging)\n\nThe `pod-termination-reason` annotation is useful to populate human readable messages like:\n\n*   “This pod was preempted by a higher priority job ($id)”\n*   “This pod had to be terminated because the underlying hardware failed ($failuretype)”\n*   “This pod had to be terminated because $user ran sudo halt on the node”\n*   **“This pod died unexpectedly because the underlying node kernel panicked!”**\n\nBut wait, how are we going to annotate a pod for a node that kernel panicked?\n\n## Capturing Kernel Panics\n\nWhen the Linux kernel panics, there is just not much you can do. But what if you could send out some sort of “with my final breath, I curse Kubernetes!” UDP packet?\n\nInspired by this [Google Spanner paper](https://research.google/pubs/pub45855/), where Spanner nodes send out a “last gasp” UDP packet to release leases & locks, you too can configure your servers to do the same upon kernel panic using a stock Linux module: `[netconsole](https://www.kernel.org/doc/Documentation/networking/netconsole.txt)`.\n\n## Configuring Netconsole\n\nThe fact that the Linux kernel can even send out UDP packets with the string ‘kernel panic’, _while it is panicking_, is kind of amazing. This works because netconsole needs to be configured with almost the entire IP header filled out already beforehand. That is right, you have to tell Linux exactly what your source MAC, IP, and UDP Port are, as well as the destination MAC, IP, and UDP ports. You are practically constructing the UDP packet for the kernel. But, with that prework, when the time comes, the kernel can easily [construct](https://github.com/torvalds/linux/blob/94f6f0550c625fab1f373bb86a6669b45e9748b3/drivers/net/netconsole.c#L932) the packet and get it out the (preconfigured) network interface as things come crashing down. Luckily the `[netconsole-setup](https://manpages.ubuntu.com/manpages/jammy/en/man8/netconsole-setup.8.html)` command makes the setup pretty easy. All the configuration options can be set [dynamically](https://wiki.ubuntu.com/Kernel/Netconsole#Step_3:_Initialize_netconsole_at_boot_time) as well, so that when the endpoint changes one can point to the new IP.\n\nOnce this is setup, kernel messages will start flowing right after `modprobe`. Imagine the whole thing operating like a `dmesg | netcat -u $destination 6666`, but in kernel space.\n\n## Netconsole “Last Gasp” Packets\n\nWith `netconsole` setup, the last gasp from a crashing kernel looks like a set of UDP packets exactly like one might expect, where the data of the UDP packet is simply the text of the kernel message. In the case of a kernel panic, it will look something like this (one UDP packet per line):\n\nKernel panic - not syncing: buffer overrun at 0x4ba4c73e73acce54  \n\\[ 8374.456345\\] CPU: 1 PID: 139616 Comm: insmod Kdump: loaded Tainted: G OE  \n\\[ 8374.458506\\] Hardware name: Amazon EC2 r5.2xlarge/, BIOS 1.0 10/16/2017  \n\\[ 8374.555629\\] Call Trace:  \n\\[ 8374.556147\\] <TASK>  \n\\[ 8374.556601\\] dump\\_stack\\_lvl+0x45/0x5b  \n\\[ 8374.557361\\] panic+0x103/0x2db  \n\\[ 8374.558166\\] ? \\_\\_cond\\_resched+0x15/0x20  \n\\[ 8374.559019\\] ? do\\_init\\_module+0x22/0x20a  \n\\[ 8374.655123\\] ? 0xffffffffc0f56000  \n\\[ 8374.655810\\] init\\_module+0x11/0x1000 \\[kpanic\\]  \n\\[ 8374.656939\\] do\\_one\\_initcall+0x41/0x1e0  \n\\[ 8374.657724\\] ? \\_\\_cond\\_resched+0x15/0x20  \n\\[ 8374.658505\\] ? kmem\\_cache\\_alloc\\_trace+0x3d/0x3c0  \n\\[ 8374.754906\\] do\\_init\\_module+0x4b/0x20a  \n\\[ 8374.755703\\] load\\_module+0x2a7a/0x3030  \n\\[ 8374.756557\\] ? \\_\\_do\\_sys\\_finit\\_module+0xaa/0x110  \n\\[ 8374.757480\\] \\_\\_do\\_sys\\_finit\\_module+0xaa/0x110  \n\\[ 8374.758537\\] do\\_syscall\\_64+0x3a/0xc0  \n\\[ 8374.759331\\] entry\\_SYSCALL\\_64\\_after\\_hwframe+0x62/0xcc  \n\\[ 8374.855671\\] RIP: 0033:0x7f2869e8ee69  \n...\n\n## Connecting to Kubernetes\n\nThe last piece is to connect is Kubernetes (k8s). We need a k8s controller to do the following:\n\n1.  Listen for netconsole UDP packets on port 6666, watching for things that look like kernel panics from nodes.\n2.  Upon kernel panic, lookup the k8s node object associated with the IP address of the incoming netconsole packet.\n3.  For that k8s node, find all the pods bound to it, annotate, then delete those pods (they are toast!).\n4.  For that k8s node, annotate the node and then delete it too (it is also toast!).\n\nParts 1&2 might look like this:\n\nfor {  \n    n, addr, err := serverConn.ReadFromUDP(buf)  \n    if err != nil {  \n        klog.Errorf(\"Error ReadFromUDP: %s\", err)  \n    } else {  \n        line := santizeNetConsoleBuffer(buf\\[0:n\\])  \n        if isKernelPanic(line) {  \n            panicCounter = 20  \n            go handleKernelPanicOnNode(ctx, addr, nodeInformer, podInformer, kubeClient, line)  \n        }  \n    }  \n    if panicCounter > 0 {  \n        klog.Infof(\"KernelPanic context from %s: %s\", addr.IP, line)  \n        panicCounter++  \n    }  \n}\n\nAnd then parts 3&4 might look like this:\n\nfunc handleKernelPanicOnNode(ctx context.Context, addr \\*net.UDPAddr, nodeInformer cache.SharedIndexInformer, podInformer cache.SharedIndexInformer, kubeClient kubernetes.Interface, line string) {  \n    node := getNodeFromAddr(addr.IP.String(), nodeInformer)  \n    if node == nil {  \n        klog.Errorf(\"Got a kernel panic from %s, but couldn't find a k8s node object for it?\", addr.IP.String())  \n    } else {  \n        pods := getPodsFromNode(node, podInformer)  \n        klog.Infof(\"Got a kernel panic from node %s, annotating and deleting all %d pods and that node.\", node.Name, len(pods))  \n        annotateAndDeletePodsWithReason(ctx, kubeClient, pods, line)  \n        err := deleteNode(ctx, kubeClient, node.Name)  \n        if err != nil {  \n            klog.Errorf(\"Error deleting node %s: %s\", node.Name, err)  \n        } else {  \n            klog.Infof(\"Deleted panicked node %s\", node.Name)  \n        }  \n    }  \n}\n\nWith that code in place, as soon as a kernel panic is detected, the pods and nodes immediately go away. No need to wait for any GC process. The annotations help document what happened to the node & pod:\n\nA real pod lost on a real k8s node that had a real kernel panic!\n\n## Conclusion\n\nMarking that a job failed because of a kernel panic may not be _that_ satisfactory to our customers. But they can take satisfaction in knowing that we now have the required observability tools to start fixing those kernel panics!\n\nDo you also enjoy really getting to the bottom of why things fail in your systems or think kernel panics are cool? Join us on the [Compute Team](https://jobs.netflix.com/jobs/198642264) where we are building a world-class container platform for our engineers."
    },
    {
      "url": "https://netflixtechblog.com/curbing-connection-churn-in-zuul-2feb273a3598?source=collection_home---4------7-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/curbing-connection-churn-in-zuul-2feb273a3598?gi=e251b0ea122b&source=collection_home---4------7-----------------------",
        "loadedTime": "2023-12-06T00:03:25.749Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/curbing-connection-churn-in-zuul-2feb273a3598",
        "title": "Curbing Connection Churn in Zuul. Netflix’s Zuul Gateway eliminated tens… | by Netflix Technology Blog | Netflix TechBlog",
        "description": "When Zuul was designed and developed, there was an inherent assumption that connections were effectively free, given we weren’t using mutual TLS (mTLS). It’s built on top of Netty, using event loops…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Curbing Connection Churn in Zuul\nBy Arthur Gonigberg, Argha C\nPlaintext Past\nWhen Zuul was designed and developed, there was an inherent assumption that connections were effectively free, given we weren’t using mutual TLS (mTLS). It’s built on top of Netty, using event loops for non-blocking execution of requests, one loop per core. To reduce contention among event loops, we created connection pools for each, keeping them completely independent. The result is that the entire request-response cycle happens on the same thread, significantly reducing context switching.\nThere is also a significant downside. It means that if each event loop has a connection pool that connects to every origin (our name for backend) server, there would be a multiplication of event loops by servers by Zuul instances. For example, a 16-core box connecting to an 800-server origin would have 12,800 connections. If the Zuul cluster has 100 instances, that’s 1,280,000 connections. That’s a significant amount and certainly more than is necessary relative to the traffic on most clusters.\nAs streaming has grown over the years, these numbers multiplied with bigger Zuul and origin clusters. More acutely, if a traffic spike occurs and Zuul instances scale up, it exponentially increases connections open to origins. Although this has been a known issue for a long time, it has never been a critical pain point until we moved large streaming applications to mTLS and our Envoy-based service mesh.\nFixing the Flows\nThe first step in improving connection overhead was implementing HTTP/2 (H2) multiplexing to the origins. Multiplexing allows the reuse of existing connections by creating multiple streams per connection, each able to send a request. Rather than requiring a connection for every request, we could reuse the same connection for many simultaneous requests. The more we reuse connections, the less overhead we have in establishing mTLS sessions with roundtrips, handshaking, and so on.\nAlthough Zuul has had H2 proxying for some time, it never supported multiplexing. It effectively treated H2 connections as HTTP/1 (H1). For backward compatibility with existing H1 functionality, we modified the H2 connection bootstrap to create a stream and immediately release the connection back into the pool. Future requests will then be able to reuse the existing connection without creating a new one. Ideally, the connections to each origin server should converge towards 1 per event loop. It seems like a minor change, but it had to be seamlessly integrated into our existing metrics and connection bookkeeping.\nThe standard way to initiate H2 connections is, over TLS, via an upgrade with ALPN (Application-Layer Protocol Negotiation). ALPN allows us to gracefully downgrade back to H1 if the origin doesn’t support H2, so we can broadly enable it without impacting customers. Service mesh being available on many services made testing and rolling out this feature very easy because it enables ALPN by default. It meant that no work was required by service owners who were already on service mesh and mTLS.\nSadly, our plan hit a snag when we rolled out multiplexing. Although the feature was stable and functionally there was no impact, we didn’t get a reduction in overall connections. Because some origin clusters were so large, and we were connecting to them from all event loops, there wasn’t enough re-use of existing connections to trigger multiplexing. Even though we were now capable of multiplexing, we weren’t utilizing it.\nDivide and Conquer\nH2 multiplexing will improve connection spikes under load when there is a large demand for all the existing connections, but it didn’t help in steady-state. Partitioning the whole origin into subsets would allow us to reduce total connection counts while leveraging multiplexing to maintain existing throughput and headroom.\nWe had discussed subsetting many times over the years, but there was concern about disrupting load balancing with the algorithms available. An even distribution of traffic to origins is critical for accurate canary analysis and preventing hot-spotting of traffic on origin instances.\nSubsetting was also top of mind after reading a recent ACM paper published by Google. It describes an improvement on their long-standing Deterministic Subsetting algorithm that they’ve used for many years. The Ringsteady algorithm (figure below) creates an evenly distributed ring of servers (yellow nodes) and then walks the ring to allocate them to each front-end task (blue nodes).\nThe figure above is from Google’s ACM paper\nThe algorithm relies on the idea of low-discrepancy numeric sequences to create a naturally balanced distribution ring that is more consistent than one built on a randomness-based consistent hash. The particular sequence used is a binary variant of the Van der Corput sequence. As long as the sequence of added servers is monotonically incrementing, for each additional server, the distribution will be evenly balanced between 0–1. Below is an example of what the binary Van der Corput sequence looks like.\nAnother big benefit of this distribution is that it provides a consistent expansion of the ring as servers are removed and added over time, evenly spreading new nodes among the subsets. This results in the stability of subsets and no cascading churn based on origin changes over time. Each node added or removed will only affect one subset, and new nodes will be added to a different subset every time.\nHere’s a more concrete demonstration of the sequence above, in decimal form, with each number between 0–1 assigned to 4 subsets. In this example, each subset has 0.25 of that range depicted with its own color.\nYou can see that each new node added is balanced across subsets extremely well. If 50 nodes are added quickly, they will get distributed just as evenly. Similarly, if a large number of nodes are removed, it will affect all subsets equally.\nThe real killer feature, though, is that if a node is removed or added, it doesn’t require all the subsets to be shuffled and recomputed. Every single change will generally only create or remove one connection. This will hold for bigger changes, too, reducing almost all churn in the subsets.\nZuul’s Take\nOur approach to implement this in Zuul was to integrate with Eureka service discovery changes and feed them into a distribution ring, based on the ideas discussed above. When new origins register in Zuul, we load their instances and create a new ring, and from then on, manage it with incremental deltas. We also take the additional step of shuffling the order of nodes before adding them to the ring. This helps prevent accidental hot spotting or overlap among Zuul instances.\nThe quirk in any load balancing algorithm from Google is that they do their load balancing centrally. Their centralized service creates subsets and load balances across their entire fleet, with a global view of the world. To use this algorithm, the key insight was to apply it to the event loops rather than the instances themselves. This allows us to continue having decentralized, client-side load balancing while also having the benefits of accurate subsetting. Although Zuul continues connecting to all origin servers, each event loop’s connection pool only gets a small subset of the whole. We end up with a singular, global view of the distribution that we can control on each instance — and a single sequence number that we can increment for each origin’s ring.\nWhen a request comes in, Netty assigns it to an event loop, and it remains there for the duration of the request-response lifecycle. After running the inbound filters, we determine the destination and load the connection pool for this event loop. This will pull from a mapping of loop-to-subset, giving us the limited set of nodes we’re looking for. We then load balance using a modified choice-of-2, as discussed before. If this sounds familiar, it’s because there are no fundamental changes to how Zuul works. The only difference is that we provide a loop-bound subset of nodes to the load balancer as a starting point for its decision.\nAnother insight we had was that we needed to replicate the number of subsets among the event loops. This allows us to maintain low connection counts for large and small origins. At the same time, having a reasonable subset size ensures we can continue providing good balance and resiliency features for the origin. Most origins require this because they are not big enough to create enough instances in each subset.\nHowever, we also don’t want to change this replication factor too often because it would cause a reshuffling of the entire ring and introduce a lot of churn. After a lot of iteration, we ended up implementing this by starting with an “ideal” subset size. We achieve this by computing the subset size that would achieve the ideal replication factor for a given cardinality of origin nodes. We can scale the replication factor across origins by growing our subsets until the desired subset size is achieved, especially as they scale up or down based on traffic patterns. Finally, we work backward to divide the ring into even slices based on the computed subset size.\nOur ideal subset side is roughly 25–50 nodes, so an origin with 400 nodes will have 8 subsets of 50 nodes. On a 32-core instance, we’ll have a replication factor of 4. However, that also means that between 200 and 400 nodes, we’re not shuffling the subsets at all. An example of this subset recomputation is in the rollout graphs below.\nAn interesting challenge here was to satisfy the dual constraints of origin nodes with a range of cardinality, and the number of event loops that hold the subsets. Our goal is to scale the subsets as we run on instances with higher event loops, with a sub-linear increase in overall connections, and sufficient replication for availability guarantees. Scaling the replication factor elastically described above helped us achieve this successfully.\nSubsetting Success\nThe results were outstanding. We saw improvements across all key metrics on Zuul, but most importantly, there was a significant reduction in total connection counts and churn.\nTotal Connections\nThis graph (as well as the ones below) shows a week’s worth of data, with the typical diurnal cycle of Netflix usage. Each of the 3 colors represents our deployment regions in AWS, and the blue vertical line shows when we turned on the feature.\nTotal connections at peak were significantly reduced in all 3 regions by a factor of 10x. This is a huge improvement, and it makes sense if you dig into how subsetting works. For example, a machine running 16 event loops could have 8 subsets — each subset is on 2 event loops. That means we’re dividing an origin by 8, hence an 8x improvement. As to why peak improvement goes up to 10x, it’s probably related to reduced churn (below).\nChurn\nThis graph is a good proxy for churn. It shows how many TCP connections Zuul is opening per second. You can see the before and after very clearly. Looking at the peak-to-peak improvement, there is roughly an 8x improvement.\nThe decrease in churn is a testament to the stability of the subsets, even as origins scale up, down, and redeploy over time.\nLooking specifically at connections created in the pool, the reduction is even more impressive:\nThe peak-to-peak reduction is massive and clearly shows how stable this distribution is. Although hard to see on the graph, the reduction went from thousands per second at peak down to about 60. There is effectively no churn of connections, even at peak traffic.\nLoad Balancing\nThe key constraint to subsetting is ensuring that the load balance on the backends is still consistent and evenly distributed. You’ll notice all the RPS on origin nodes grouped tightly, as expected. The thicker lines represent the subset size and the total origin size.\nIn the second graph, you’ll note that we recompute the subset size (blue line) because the origin (purple line) became large enough that we could get away with less replication in the subsets. In this case, we went from a subset size of 100 for 400 servers (a division of 4) to 50 (a division of 8).\nSystem Metrics\nGiven the significant reduction in connections, we saw reduced CPU utilization (~4%), heap usage (~15%), and latency (~3%) on Zuul, as well.\nZuul canary metrics\nRolling it Out\nAs we rolled this feature out to our largest origins — streaming playback APIs — we saw the pattern above continue, but with scale, it became more impressive. On some Zuul shards, we saw a reduction of as much as 13 million connections at peak, with almost no churn.\nToday the feature is rolled out widely. We’re serving the same amount of traffic but with tens of millions fewer connections. Despite the reduction of connections, there is no decrease in resiliency or load balancing. H2 multiplexing allows us to scale up requests separately from connections, and our subsetting algorithm ensures an even traffic balance.\nAlthough challenging to get right, subsetting is a worthwhile investment.\nAcknowledgments\nWe would also like to thank Peter Ward, Paul Wankadia, and Kavita Guliani at Google for developing this algorithm and publishing their work for the benefit of the industry.",
      "markdown": "## Curbing Connection Churn in Zuul\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----2feb273a3598--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----2feb273a3598--------------------------------)\n\n_By_ [_Arthur Gonigberg_](https://twitter.com/agonigberg), [_Argha C_](https://www.linkedin.com/in/argha-c)\n\n## Plaintext Past\n\nWhen [Zuul](https://github.com/Netflix/zuul) was [designed and developed](https://netflixtechblog.com/zuul-2-the-netflix-journey-to-asynchronous-non-blocking-systems-45947377fb5c), there was an inherent assumption that connections were effectively free, given we weren’t using mutual TLS (mTLS). It’s built on top of [Netty](https://netty.io/), using event loops for non-blocking execution of requests, one loop per core. To reduce contention among event loops, we created connection pools for each, keeping them completely independent. The result is that the entire request-response cycle happens on the same thread, significantly reducing context switching.\n\nThere is also a significant downside. It means that if each event loop has a connection pool that connects to every origin (our name for backend) server, there would be a multiplication of event loops by servers by Zuul instances. For example, a 16-core box connecting to an 800-server origin would have 12,800 connections. If the Zuul cluster has 100 instances, that’s 1,280,000 connections. That’s a significant amount and certainly more than is necessary relative to the traffic on most clusters.\n\nAs streaming has grown over the years, these numbers multiplied with bigger Zuul and origin clusters. More acutely, if a traffic spike occurs and Zuul instances scale up, it exponentially increases connections open to origins. Although this has been a known issue for a long time, it has never been a critical pain point until we moved large streaming applications to mTLS and our Envoy-based service mesh.\n\n## Fixing the Flows\n\nThe first step in improving connection overhead was implementing HTTP/2 (H2) multiplexing to the origins. Multiplexing allows the reuse of existing connections by creating multiple streams per connection, each able to send a request. Rather than requiring a connection for every request, we could reuse the same connection for many simultaneous requests. The more we reuse connections, the less overhead we have in establishing mTLS sessions with roundtrips, handshaking, and so on.\n\nAlthough Zuul has had H2 proxying for some time, it never supported multiplexing. It effectively treated H2 connections as HTTP/1 (H1). For backward compatibility with existing H1 functionality, we modified the H2 connection bootstrap to create a stream and immediately release the connection back into the pool. Future requests will then be able to reuse the existing connection without creating a new one. Ideally, the connections to each origin server should converge towards 1 per event loop. It seems like a minor change, but it had to be seamlessly integrated into our existing metrics and connection bookkeeping.\n\nThe standard way to initiate H2 connections is, over TLS, via an upgrade with [ALPN (Application-Layer Protocol Negotiation](https://en.wikipedia.org/wiki/Application-Layer_Protocol_Negotiation)). ALPN allows us to gracefully downgrade back to H1 if the origin doesn’t support H2, so we can broadly enable it without impacting customers. Service mesh being available on many services made testing and rolling out this feature very easy because it enables ALPN by default. It meant that no work was required by service owners who were already on service mesh and mTLS.\n\nSadly, our plan hit a snag when we rolled out multiplexing. Although the feature was stable and functionally there was no impact, we didn’t get a reduction in overall connections. Because some origin clusters were so large, and we were connecting to them from all event loops, there wasn’t enough re-use of existing connections to trigger multiplexing. Even though we were now capable of multiplexing, we weren’t utilizing it.\n\n## Divide and Conquer\n\nH2 multiplexing will improve connection spikes under load when there is a large demand for all the existing connections, but it didn’t help in steady-state. Partitioning the whole origin into subsets would allow us to reduce total connection counts while leveraging multiplexing to maintain existing throughput and headroom.\n\nWe had discussed subsetting many times over the years, but there was concern about disrupting load balancing with the algorithms available. An even distribution of traffic to origins is critical for accurate [canary analysis](https://netflixtechblog.com/chap-chaos-automation-platform-53e6d528371f) and preventing hot-spotting of traffic on origin instances.\n\nSubsetting was also top of mind after reading a [recent ACM paper](https://queue.acm.org/detail.cfm?id=3570937) published by Google. It describes an improvement on their long-standing [Deterministic Subsetting](https://sre.google/sre-book/load-balancing-datacenter/) algorithm that they’ve used for many years. The Ringsteady algorithm (figure below) creates an evenly distributed ring of servers (yellow nodes) and then walks the ring to allocate them to each front-end task (blue nodes).\n\n_The figure above is from Google’s_ [_ACM paper_](https://queue.acm.org/detail.cfm?id=3570937)\n\nThe algorithm relies on the idea of [low-discrepancy numeric sequences](https://en.wikipedia.org/wiki/Low-discrepancy_sequence) to create a naturally balanced distribution ring that is more consistent than one built on a randomness-based consistent hash. The particular sequence used is a binary variant of the [Van der Corput sequence](https://en.wikipedia.org/wiki/Van_der_Corput_sequence). As long as the sequence of added servers is monotonically incrementing, for each additional server, the distribution will be evenly balanced between 0–1. Below is an example of what the binary Van der Corput sequence looks like.\n\nAnother big benefit of this distribution is that it provides a consistent expansion of the ring as servers are removed and added over time, evenly spreading new nodes among the subsets. This results in the stability of subsets and no cascading churn based on origin changes over time. Each node added or removed will only affect one subset, and new nodes will be added to a different subset every time.\n\nHere’s a more concrete demonstration of the sequence above, in decimal form, with each number between 0–1 assigned to 4 subsets. In this example, each subset has 0.25 of that range depicted with its own color.\n\nYou can see that each new node added is balanced across subsets extremely well. If 50 nodes are added quickly, they will get distributed just as evenly. Similarly, if a large number of nodes are removed, it will affect all subsets equally.\n\nThe real killer feature, though, is that if a node is removed or added, it doesn’t require all the subsets to be shuffled and recomputed. Every single change will generally only create or remove one connection. This will hold for bigger changes, too, reducing almost all churn in the subsets.\n\n## Zuul’s Take\n\nOur approach to implement this in Zuul was to integrate with [Eureka](https://github.com/Netflix/eureka) service discovery changes and feed them into a distribution ring, based on the ideas discussed above. When new origins register in Zuul, we load their instances and create a new ring, and from then on, manage it with incremental deltas. We also take the additional step of shuffling the order of nodes before adding them to the ring. This helps prevent accidental hot spotting or overlap among Zuul instances.\n\nThe quirk in any load balancing algorithm from Google is that they do their [load balancing centrally](https://sre.google/workbook/managing-load/#gslb). Their centralized service creates subsets and load balances across their entire fleet, with a global view of the world. To use this algorithm, **the key insight was to apply it to the event loops rather than the instances themselves**. This allows us to continue having decentralized, client-side load balancing while also having the benefits of accurate subsetting. Although Zuul continues connecting to all origin servers, each event loop’s connection pool only gets a small subset of the whole. We end up with a singular, global view of the distribution that we can control on each instance — and a single sequence number that we can increment for each origin’s ring.\n\nWhen a request comes in, Netty assigns it to an event loop, and it remains there for the duration of the request-response lifecycle. After running the inbound filters, we determine the destination and load the connection pool for this event loop. This will pull from a mapping of loop-to-subset, giving us the limited set of nodes we’re looking for. We then load balance using a modified choice-of-2, as [discussed before](https://netflixtechblog.com/netflix-edge-load-balancing-695308b5548c). If this sounds familiar, it’s because there are no fundamental changes to how Zuul works. The only difference is that we provide a loop-bound subset of nodes to the load balancer as a starting point for its decision.\n\nAnother insight we had was that we needed to replicate the number of subsets among the event loops. This allows us to maintain low connection counts for large and small origins. At the same time, having a reasonable subset size ensures we can continue providing good balance and resiliency features for the origin. Most origins require this because they are not big enough to create enough instances in each subset.\n\nHowever, we also don’t want to change this replication factor too often because it would cause a reshuffling of the entire ring and introduce a lot of churn. After a lot of iteration, we ended up implementing this by starting with an “ideal” subset size. We achieve this by computing the subset size that would achieve the ideal replication factor for a given cardinality of origin nodes. We can scale the replication factor across origins by growing our subsets until the desired subset size is achieved, especially as they scale up or down based on traffic patterns. Finally, we work backward to divide the ring into even slices based on the computed subset size.\n\nOur ideal subset side is roughly 25–50 nodes, so an origin with 400 nodes will have 8 subsets of 50 nodes. On a 32-core instance, we’ll have a replication factor of 4. However, that also means that between 200 and 400 nodes, we’re not shuffling the subsets at all. An example of this subset recomputation is in the rollout graphs [below](https://medium.com/p/2feb273a3598#5e4d).\n\nAn interesting challenge here was to satisfy the dual constraints of origin nodes with a range of cardinality, and the number of event loops that hold the subsets. Our goal is to scale the subsets as we run on instances with higher event loops, with a sub-linear increase in overall connections, and sufficient replication for availability guarantees. Scaling the replication factor elastically described above helped us achieve this successfully.\n\n## Subsetting Success\n\nThe results were outstanding. We saw improvements across all key metrics on Zuul, but most importantly, there was a significant reduction in total connection counts and churn.\n\n## **Total Connections**\n\nThis graph (as well as the ones below) shows a week’s worth of data, with the typical diurnal cycle of Netflix usage. Each of the 3 colors represents our deployment regions in AWS, and the blue vertical line shows when we turned on the feature.\n\n**Total connections at peak were significantly reduced in all 3 regions by a factor of 10x**. This is a huge improvement, and it makes sense if you dig into how subsetting works. For example, a machine running 16 event loops could have 8 subsets — each subset is on 2 event loops. That means we’re dividing an origin by 8, hence an 8x improvement. As to why peak improvement goes up to 10x, it’s probably related to reduced churn (below).\n\n## **Churn**\n\nThis graph is a good proxy for churn. It shows how many TCP connections Zuul is opening per second. You can see the before and after very clearly. Looking at the peak-to-peak improvement, there is roughly an 8x improvement.\n\nThe decrease in churn is a testament to the stability of the subsets, even as origins scale up, down, and redeploy over time.\n\nLooking specifically at connections created in the pool, the reduction is even more impressive:\n\nThe peak-to-peak reduction is massive and clearly shows how stable this distribution is. Although hard to see on the graph, the reduction went from thousands per second at peak down to about 60. There is **effectively no churn of connections, even at peak traffic**.\n\n## **Load Balancing**\n\nThe key constraint to subsetting is ensuring that the load balance on the backends is still consistent and evenly distributed. You’ll notice all the RPS on origin nodes grouped tightly, as expected. The thicker lines represent the subset size and the total origin size.\n\nIn the second graph, you’ll note that we recompute the subset size (blue line) because the origin (purple line) became large enough that we could get away with less replication in the subsets. In this case, we went from a subset size of 100 for 400 servers (a division of 4) to 50 (a division of 8).\n\n## System Metrics\n\nGiven the significant reduction in connections, we saw reduced CPU utilization (~4%), heap usage (~15%), and latency (~3%) on Zuul, as well.\n\nZuul canary metrics\n\n## Rolling it Out\n\nAs we rolled this feature out to our largest origins — streaming playback APIs — we saw the pattern above continue, but with scale, it became more impressive. On some Zuul shards, we saw a reduction of as much as 13 million connections at peak, with almost no churn.\n\nToday the feature is rolled out widely. We’re serving the same amount of traffic but with tens of millions fewer connections. Despite the reduction of connections, there is no decrease in resiliency or load balancing. H2 multiplexing allows us to scale up requests separately from connections, and our subsetting algorithm ensures an even traffic balance.\n\nAlthough challenging to get right, subsetting is a worthwhile investment.\n\n## Acknowledgments\n\nWe would also like to thank [Peter Ward](https://twitter.com/flowblok), [Paul Wankadia](https://twitter.com/junyer), and [Kavita Guliani](https://www.linkedin.com/in/kavita-guliani/) at Google for developing this algorithm and publishing their work for the benefit of the industry."
    },
    {
      "url": "https://netflixtechblog.com/migrating-netflix-to-graphql-safely-8e1e4d4f1e72?source=collection_home---4------9-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/migrating-netflix-to-graphql-safely-8e1e4d4f1e72?gi=eb8eaba61d74&source=collection_home---4------9-----------------------",
        "loadedTime": "2023-12-06T00:03:29.175Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/migrating-netflix-to-graphql-safely-8e1e4d4f1e72",
        "title": "Migrating Netflix to GraphQL Safely | by Netflix Technology Blog | Netflix TechBlog",
        "description": "In 2022, a major change was made to Netflix’s iOS and Android applications. We migrated Netflix’s mobile apps to GraphQL with zero downtime, which involved a total overhaul from the client to the API…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Migrating Netflix to GraphQL Safely\nBy Jennifer Shin, Tejas Shikhare, Will Emmanuel\nIn 2022, a major change was made to Netflix’s iOS and Android applications. We migrated Netflix’s mobile apps to GraphQL with zero downtime, which involved a total overhaul from the client to the API layer.\nUntil recently, an internal API framework, Falcor, powered our mobile apps. They are now backed by Federated GraphQL, a distributed approach to APIs where domain teams can independently manage and own specific sections of the API.\nDoing this safely for 100s of millions of customers without disruption is exceptionally challenging, especially considering the many dimensions of change involved. This blog post will share broadly-applicable techniques (beyond GraphQL) we used to perform this migration. The three strategies we will discuss today are AB Testing, Replay Testing, and Sticky Canaries.\nMigration Details\nBefore diving into these techniques, let’s briefly examine the migration plan.\nBefore GraphQL: Monolithic Falcor API implemented and maintained by the API Team\nBefore moving to GraphQL, our API layer consisted of a monolithic server built with Falcor. A single API team maintained both the Java implementation of the Falcor framework and the API Server.\nPhase 1\nCreated a GraphQL Shim Service on top of our existing Monolith Falcor API.\nBy the summer of 2020, many UI engineers were ready to move to GraphQL. Instead of embarking on a full-fledged migration top to bottom, we created a GraphQL shim on top of our existing Falcor API. The GraphQL shim enabled client engineers to move quickly onto GraphQL, figure out client-side concerns like cache normalization, experiment with different GraphQL clients, and investigate client performance without being blocked by server-side migrations. To launch Phase 1 safely, we used AB Testing.\nPhase 2\nDeprecate the GraphQL Shim Service and Legacy API Monolith in favor of GraphQL services owned by the domain teams.\nWe didn’t want the legacy Falcor API to linger forever, so we leaned into Federated GraphQL to power a single GraphQL API with multiple GraphQL servers.\nWe could also swap out the implementation of a field from GraphQL Shim to Video API with federation directives. To launch Phase 2 safely, we used Replay Testing and Sticky Canaries.\nTesting Strategies: A Summary\nTwo key factors determined our testing strategies:\nFunctional vs. non-functional requirements\nIdempotency\nIf we were testing functional requirements like data accuracy, and if the request was idempotent, we relied on Replay Testing. We knew we could test the same query with the same inputs and consistently expect the same results.\nWe couldn’t replay test GraphQL queries or mutations that requested non-idempotent fields.\nAnd we definitely couldn’t replay test non-functional requirements like caching and logging user interaction. In such cases, we were not testing for response data but overall behavior. So, we relied on higher-level metrics-based testing: AB Testing and Sticky Canaries.\nLet’s discuss the three testing strategies in further detail.\nTool: AB Testing\nNetflix traditionally uses AB Testing to evaluate whether new product features resonate with customers. In Phase 1, we leveraged the AB testing framework to isolate a user segment into two groups totaling 1 million users. The control group’s traffic utilized the legacy Falcor stack, while the experiment population leveraged the new GraphQL client and was directed to the GraphQL Shim. To determine customer impact, we could compare various metrics such as error rates, latencies, and time to render.\nWe set up a client-side AB experiment that tested Falcor versus GraphQL and reported coarse-grained quality of experience metrics (QoE). The AB experiment results hinted that GraphQL’s correctness was not up to par with the legacy system. We spent the next few months diving into these high-level metrics and fixing issues such as cache TTLs, flawed client assumptions, etc.\nWins\nHigh-Level Health Metrics: AB Testing provided the assurance we needed in our overall client-side GraphQL implementation. This helped us successfully migrate 100% of the traffic on the mobile homepage canvas to GraphQL in 6 months.\nGotchas\nError Diagnosis: With an AB test, we could see coarse-grained metrics which pointed to potential issues, but it was challenging to diagnose the exact issues.\nTool: Replay Testing — Validation at Scale!\nThe next phase in the migration was to reimplement our existing Falcor API in a GraphQL-first server (Video API Service). The Falcor API had become a logic-heavy monolith with over a decade of tech debt. So we had to ensure that the reimplemented Video API server was bug-free and identical to the already productized Shim service.\nWe developed a Replay Testing tool to verify that idempotent APIs were migrated correctly from the GraphQL Shim to the Video API service.\nHow does it work?\nThe Replay Testing framework leverages the @override directive available in GraphQL Federation. This directive tells the GraphQL Gateway to route to one GraphQL server over another. Take, for instance, the following two GraphQL schemas defined by the Shim Service and the Video Service:\nThe GraphQL Shim first defined the certificationRating field (things like Rated R or PG-13) in Phase 1. In Phase 2, we stood up the VideoService and defined the same certificationRating field marked with the @override directive. The presence of the identical field with the @override directive informed the GraphQL Gateway to route the resolution of this field to the new Video Service rather than the old Shim Service.\nThe Replay Tester tool samples raw traffic streams from Mantis. With these sampled events, the tool can capture a live request from production and run an identical GraphQL query against both the GraphQL Shim and the new Video API service. The tool then compares the results and outputs any differences in response payloads.\nNote: We do not replay test Personally Identifiable Information. It’s used only for non-sensitive product features on the Netflix UI.\nOnce the test is completed, the engineer can view the diffs displayed as a flattened JSON node. You can see the control value on the left side of the comma in parentheses and the experiment value on the right.\n/data/videos/0/tags/3/id: (81496962, null)\n/data/videos/0/tags/5/displayName: (Série, value: “S\\303\\251rie”)\nWe captured two diffs above, the first had missing data for an ID field in the experiment, and the second had an encoding difference. We also saw differences in localization, date precisions, and floating point accuracy. It gave us confidence in replicated business logic, where subscriber plans and user geographic location determined the customer’s catalog availability.\nWins\nConfidence in parity between the two GraphQL Implementations\nEnabled tuning configs in cases where data was missing due to over-eager timeouts\nTested business logic that required many (unknown) inputs and where correctness can be hard to eyeball\nGotchas\nPII and non-idempotent APIs should not be tested using Replay Tests, and it would be valuable to have a mechanism to prevent that.\nManually constructed queries are only as good as the features the developer remembers to test. We ended up with untested fields simply because we forgot about them.\nCorrectness: The idea of correctness can be confusing too. For example, is it more correct for an array to be empty or null, or is it just noise? Ultimately, we matched the existing behavior as much as possible because verifying the robustness of the client’s error handling was difficult.\nDespite these shortcomings, Replay Testing was a key indicator that we had achieved functional correctness of most idempotent queries.\nTool: Sticky Canary\nWhile Replay Testing validates the functional correctness of the new GraphQL APIs, it does not provide any performance or business metric insight, such as the overall perceived health of user interaction. Are users clicking play at the same rates? Are things loading in time before the user loses interest? Replay Testing also cannot be used for non-idempotent API validation. We reached for a Netflix tool called the Sticky Canary to build confidence.\nA Sticky Canary is an infrastructure experiment where customers are assigned either to a canary or baseline host for the entire duration of an experiment. All incoming traffic is allocated to an experimental or baseline host based on their device and profile, similar to a bucket hash. The experimental host deployment serves all the customers assigned to the experiment. Watch our Chaos Engineering talk from AWS Reinvent to learn more about Sticky Canaries.\nIn the case of our GraphQL APIs, we used a Sticky Canary experiment to run two instances of our GraphQL gateway. The baseline gateway used the existing schema, which routes all traffic to the GraphQL Shim. The experimental gateway used the new proposed schema, which routes traffic to the latest Video API service. Zuul, our primary edge gateway, assigns traffic to either cluster based on the experiment parameters.\nWe then collect and analyze the performance of the two clusters. Some KPIs we monitor closely include:\nMedian and tail latencies\nError rates\nLogs\nResource utilization–CPU, network traffic, memory, disk\nDevice QoE (Quality of Experience) metrics\nStreaming health metrics\nWe started small, with tiny customer allocations for hour-long experiments. After validating performance, we slowly built up scope. We increased the percentage of customer allocations, introduced multi-region tests, and eventually 12-hour or day-long experiments. Validating along the way is essential since Sticky Canaries impact live production traffic and are assigned persistently to a customer.\nAfter several sticky canary experiments, we had assurance that phase 2 of the migration improved all core metrics, and we could dial up GraphQL globally with confidence.\nWins\nSticky Canaries was essential to build confidence in our new GraphQL services.\nNon-Idempotent APIs: these tests are compatible with mutating or non-idempotent APIs\nBusiness metrics: Sticky Canaries validated our core Netflix business metrics had improved after the migration\nSystem performance: Insights into latency and resource usage help us understand how scaling profiles change after migration\nGotchas\nNegative Customer Impact: Sticky Canaries can impact real users. We needed confidence in our new services before persistently routing some customers to them. This is partially mitigated by real-time impact detection, which will automatically cancel experiments.\nShort-lived: Sticky Canaries are meant for short-lived experiments. For longer-lived tests, a full-blown AB test should be used.\nIn Summary\nTechnology is constantly changing, and we, as engineers, spend a large part of our careers performing migrations. The question is not whether we are migrating but whether we are migrating safely, with zero downtime, in a timely manner.\nAt Netflix, we have developed tools that ensure confidence in these migrations, targeted toward each specific use case being tested. We covered three tools, AB testing, Replay Testing, and Sticky Canaries that we used for the GraphQL Migration.\nThis blog post is part of our Migrating Critical Traffic series. Also, check out: Migrating Critical Traffic at Scale (part 1, part 2) and Ensuring the Successful Launch of Ads.",
      "markdown": "## Migrating Netflix to GraphQL Safely\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----8e1e4d4f1e72--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----8e1e4d4f1e72--------------------------------)\n\nBy [Jennifer Shin](https://www.linkedin.com/in/jennifer-shin-0019a516/), [Tejas Shikhare](https://www.linkedin.com/in/tejas-shikhare-81027b19/), [Will Emmanuel](https://www.linkedin.com/in/willemmanuel/)\n\nIn 2022, a major change was made to Netflix’s iOS and Android applications. We migrated Netflix’s mobile apps to GraphQL with zero downtime, which involved a total overhaul from the client to the API layer.\n\nUntil recently, an internal API framework, [Falcor](https://netflix.github.io/falcor/), powered our mobile apps. They are now backed by [Federated GraphQL](https://netflixtechblog.com/how-netflix-scales-its-api-with-graphql-federation-part-1-ae3557c187e2), a distributed approach to APIs where domain teams can independently manage and own specific sections of the API.\n\nDoing this **safely** for 100s of millions of customers without disruption is exceptionally challenging, especially considering the many dimensions of change involved. This blog post will share broadly-applicable techniques (beyond GraphQL) we used to perform this migration. The three strategies we will discuss today are **AB Testing**, **Replay Testing,** and **Sticky Canaries**.\n\n## Migration Details\n\nBefore diving into these techniques, let’s briefly examine the migration plan.\n\n**Before GraphQL: Monolithic Falcor API implemented and maintained by the API Team**\n\nBefore moving to GraphQL, our API layer consisted of a monolithic server built with [Falcor](https://netflix.github.io/falcor/). A single API team maintained both the Java implementation of the Falcor framework _and_ the API Server.\n\n## Phase 1\n\n**Created a GraphQL Shim Service on top of our existing Monolith Falcor API.**\n\nBy the summer of 2020, many UI engineers were ready to move to GraphQL. Instead of embarking on a full-fledged migration top to bottom, we created a GraphQL shim on top of our existing Falcor API. The GraphQL shim enabled client engineers to move quickly onto GraphQL, figure out client-side concerns like cache normalization, experiment with different GraphQL clients, and investigate client performance without being blocked by server-side migrations. To launch Phase 1 safely, we used **AB Testing**.\n\n## Phase 2\n\n**Deprecate the GraphQL Shim Service and Legacy API Monolith in favor of GraphQL services owned by the domain teams.**\n\nWe didn’t want the legacy Falcor API to linger forever, so we leaned into Federated GraphQL to power a single GraphQL API with multiple GraphQL servers.\n\nWe could also swap out the implementation of a field from GraphQL Shim to Video API with federation directives. To launch Phase 2 safely, we used **Replay Testing** and **Sticky Canaries**.\n\n## Testing Strategies: A Summary\n\nTwo key factors determined our testing strategies:\n\n*   Functional vs. non-functional requirements\n*   Idempotency\n\nIf we were testing **functional requirements** like data accuracy, and if the request was **idempotent**, we relied on **Replay Testing**. We knew we could test the same query with the same inputs and consistently expect the same results.\n\nWe couldn’t replay test GraphQL queries or mutations that requested non-idempotent fields.\n\nAnd we definitely couldn’t replay test **non-functional requirements** like caching and logging user interaction. In such cases, we were not testing for response data but overall behavior. So, we relied on higher-level metrics-based testing: **AB Testing** and **Sticky Canaries**.\n\nLet’s discuss the three testing strategies in further detail.\n\n## Tool: AB Testing\n\nNetflix traditionally uses AB Testing to evaluate whether new product features resonate with customers. **In Phase 1,** we leveraged the AB testing framework to isolate a user segment into two groups totaling 1 million users. The control group’s traffic utilized the legacy Falcor stack, while the experiment population leveraged the new GraphQL client and was directed to the GraphQL Shim. To determine customer impact, we could compare various metrics such as error rates, latencies, and time to render.\n\nWe set up a client-side AB experiment that tested Falcor versus GraphQL and reported coarse-grained quality of experience metrics (**QoE**). The AB experiment results hinted that GraphQL’s correctness was not up to par with the legacy system. We spent the next few months diving into these high-level metrics and fixing issues such as cache TTLs, flawed client assumptions, etc.\n\n## Wins\n\n**High-Level Health Metrics:** AB Testing provided the assurance we needed in our overall client-side GraphQL implementation. This helped us successfully migrate 100% of the traffic on the mobile homepage canvas to GraphQL in 6 months.\n\n## Gotchas\n\n**Error Diagnosis:** With an AB test, we could see coarse-grained metrics which pointed to potential issues, but it was challenging **to diagnose** the exact issues.\n\n## Tool: Replay Testing — Validation at Scale!\n\nThe next phase in the migration was to reimplement our existing Falcor API in a GraphQL-first server (Video API Service). The Falcor API had become a logic-heavy monolith with over a decade of tech debt. So we had to ensure that the reimplemented Video API server was bug-free and identical to the already productized Shim service.\n\nWe developed a Replay Testing tool to verify that **idempotent** APIs were migrated correctly from the GraphQL Shim to the Video API service.\n\n## How does it work?\n\nThe Replay Testing framework leverages the _@override_ directive available in GraphQL Federation. This directive tells the GraphQL Gateway to route to one GraphQL server over another. Take, for instance, the following two GraphQL schemas defined by the Shim Service and the Video Service:\n\nThe GraphQL Shim first defined the _certificationRating_ field (things like Rated R or PG-13) in Phase 1. In Phase 2, we stood up the VideoService and defined the same _certificationRating_ field marked with the _@override_ directive. The presence of the identical field with the _@override_ directive informed the GraphQL Gateway to route the resolution of this field to the new Video Service rather than the old Shim Service.\n\nThe Replay Tester tool samples raw traffic streams from [Mantis](https://netflix.github.io/mantis/). With these sampled events, the tool can capture a live request from production and run an **identical** GraphQL query against both the GraphQL Shim and the new Video API service. The tool then compares the results and outputs any differences in response payloads.\n\n**Note: We do not replay test Personally Identifiable Information. It’s used only for non-sensitive product features on the Netflix UI.**\n\nOnce the test is completed, the engineer can view the diffs displayed as a _flattened JSON node_. You can see the control value on the left side of the comma in parentheses and the experiment value on the right.\n\n/data/videos/0/tags/3/id: (81496962, null)\n\n/data/videos/0/tags/5/displayName: (Série, value: “S\\\\303\\\\251rie”)\n\nWe captured two diffs above, the first had missing data for an ID field in the experiment, and the second had an encoding difference. We also saw differences in localization, date precisions, and floating point accuracy. It gave us confidence in replicated business logic, where subscriber plans and user geographic location determined the customer’s catalog availability.\n\n## Wins\n\n*   **Confidence** in parity between the two GraphQL Implementations\n*   **Enabled tuning** **configs** in cases where data was missing due to over-eager timeouts\n*   **Tested** **business logic** that required many (unknown) inputs and where correctness can be hard to eyeball\n\n## Gotchas\n\n*   **PII** and non-idempotent APIs should **not** be tested using Replay Tests, and it would be valuable to have a mechanism to **_prevent_** that.\n*   **Manually constructed queries** are only as good as the features the developer remembers to test. We ended up with untested fields simply because we forgot about them.\n*   **Correctness:** The idea of correctness can be confusing too. For example, is it more correct for an array to be empty or null, or is it just noise? Ultimately, we matched the existing behavior as much as possible because verifying the robustness of the client’s error handling was difficult.\n\nDespite these shortcomings, Replay Testing was a key indicator that we had achieved functional correctness of _most_ idempotent queries.\n\n## Tool: Sticky Canary\n\nWhile Replay Testing validates the functional correctness of the new GraphQL APIs, it does not provide any performance or business metric insight, such as the **overall perceived health of user interaction**. Are users clicking play at the same rates? Are things loading in time before the user loses interest? Replay Testing also cannot be used for non-idempotent API validation. We reached for a Netflix tool called the Sticky Canary to build confidence.\n\nA Sticky Canary is an infrastructure experiment where customers are assigned either to a canary or baseline host for the entire duration of an experiment. All incoming traffic is allocated to an experimental or baseline host based on their device and profile, similar to a bucket hash. The experimental host deployment serves all the customers assigned to the experiment. Watch our [Chaos Engineering](https://www.youtube.com/watch?v=Xbn65E-BQhA) talk from AWS Reinvent to learn more about Sticky Canaries.\n\nIn the case of our GraphQL APIs, we used a Sticky Canary experiment to **run two instances of our GraphQL gateway**. The **baseline** gateway used the existing schema, which routes all traffic to the GraphQL Shim. The **experimental** gateway used the new proposed schema, which routes traffic to the latest Video API service. [Zuul](https://github.com/Netflix/zuul), our primary edge gateway, assigns traffic to either cluster based on the experiment parameters.\n\nWe then collect and analyze the performance of the two clusters. Some KPIs we monitor closely include:\n\n*   Median and tail latencies\n*   Error rates\n*   Logs\n*   Resource utilization–CPU, network traffic, memory, disk\n*   Device QoE (Quality of Experience) metrics\n*   Streaming health metrics\n\nWe started small, with tiny customer allocations for hour-long experiments. After validating performance, we slowly built up scope. We increased the percentage of customer allocations, introduced multi-region tests, and eventually 12-hour or day-long experiments. Validating along the way is essential since Sticky Canaries impact live production traffic and are assigned persistently to a customer.\n\nAfter several sticky canary experiments, we had assurance that phase 2 of the migration improved all core metrics, and we could dial up GraphQL globally with confidence.\n\n## Wins\n\nSticky Canaries was essential to build confidence in our new GraphQL services.\n\n*   **Non-Idempotent APIs:** these tests are compatible with mutating or non-idempotent APIs\n*   **Business metrics:** Sticky Canaries validated our core Netflix business metrics had improved after the migration\n*   **System performance:** Insights into latency and resource usage help us understand how scaling profiles change after migration\n\n## Gotchas\n\n*   **Negative Customer Impact:** Sticky Canaries can impact real users. We needed confidence in our new services before persistently routing some customers to them. This is partially mitigated by _real-time impact detection_, which will automatically cancel experiments.\n*   **Short-lived:** Sticky Canaries are meant for short-lived experiments. For longer-lived tests, a full-blown AB test should be used.\n\n## In Summary\n\nTechnology is constantly changing, and we, as engineers, spend a large part of our careers performing migrations. The question is not whether we are migrating but whether we are migrating **_safely_**, with zero downtime, in a timely manner.\n\nAt Netflix, we have developed tools that ensure confidence in these migrations, targeted toward each specific use case being tested. We covered three tools, **AB testing**, **Replay Testing**, and **Sticky Canaries** that we used for the GraphQL Migration.\n\nThis blog post is part of our Migrating Critical Traffic series. Also, check out: Migrating Critical Traffic at Scale ([part 1](https://netflixtechblog.com/migrating-critical-traffic-at-scale-with-no-downtime-part-1-ba1c7a1c7835), [part 2](https://netflixtechblog.medium.com/migrating-critical-traffic-at-scale-with-no-downtime-part-2-4b1c8c7155c1)) and [Ensuring the Successful Launch of Ads](https://netflixtechblog.com/ensuring-the-successful-launch-of-ads-on-netflix-f99490fdf1ba)."
    },
    {
      "url": "https://netflixtechblog.com/detecting-scene-changes-in-audiovisual-content-77a61d3eaad6?source=collection_home---4------8-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/detecting-scene-changes-in-audiovisual-content-77a61d3eaad6?gi=007bd0637aed&source=collection_home---4------8-----------------------",
        "loadedTime": "2023-12-06T00:03:29.489Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/detecting-scene-changes-in-audiovisual-content-77a61d3eaad6",
        "title": "Detecting Scene Changes in Audiovisual Content | by Netflix Technology Blog | Netflix TechBlog",
        "description": "When watching a movie or an episode of a TV show, we experience a cohesive narrative that unfolds before us, often without giving much thought to the underlying structure that makes it all possible…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Detecting Scene Changes in Audiovisual Content\nAvneesh Saluja, Andy Yao, Hossein Taghavi\nIntroduction\nWhen watching a movie or an episode of a TV show, we experience a cohesive narrative that unfolds before us, often without giving much thought to the underlying structure that makes it all possible. However, movies and episodes are not atomic units, but rather composed of smaller elements such as frames, shots, scenes, sequences, and acts. Understanding these elements and how they relate to each other is crucial for tasks such as video summarization and highlights detection, content-based video retrieval, dubbing quality assessment, and video editing. At Netflix, such workflows are performed hundreds of times a day by many teams around the world, so investing in algorithmically-assisted tooling around content understanding can reap outsized rewards.\nWhile segmentation of more granular units like frames and shot boundaries is either trivial or can primarily rely on pixel-based information, higher order segmentation¹ requires a more nuanced understanding of the content, such as the narrative or emotional arcs. Furthermore, some cues can be better inferred from modalities other than the video, e.g. the screenplay or the audio and dialogue track. Scene boundary detection, in particular, is the task of identifying the transitions between scenes, where a scene is defined as a continuous sequence of shots that take place in the same time and location (often with a relatively static set of characters) and share a common action or theme.\nIn this blog post, we present two complementary approaches to scene boundary detection in audiovisual content. The first method, which can be seen as a form of weak supervision, leverages auxiliary data in the form of a screenplay by aligning screenplay text with timed text (closed captions, audio descriptions) and assigning timestamps to the screenplay’s scene headers (a.k.a. sluglines). In the second approach, we show that a relatively simple, supervised sequential model (bidirectional LSTM or GRU) that uses rich, pretrained shot-level embeddings can outperform the current state-of-the-art baselines on our internal benchmarks.\nFigure 1: a scene consists of a sequence of shots.\nLeveraging Aligned Screenplay Information\nScreenplays are the blueprints of a movie or show. They are formatted in a specific way, with each scene beginning with a scene header, indicating attributes such as the location and time of day. This consistent formatting makes it possible to parse screenplays into a structured format. At the same time, a) changes made on the fly (directorial or actor discretion) or b) in post production and editing are rarely reflected in the screenplay, i.e. it isn’t rewritten to reflect the changes.\nFigure 2: screenplay elements, from The Witcher S1E1.\nIn order to leverage this noisily aligned data source, we need to align time-stamped text (e.g. closed captions and audio descriptions) with screenplay text (dialogue and action² lines), bearing in mind a) the on-the-fly changes that might result in semantically similar but not identical line pairs and b) the possible post-shoot changes that are more significant (reordering, removing, or inserting entire scenes). To address the first challenge, we use pre trained sentence-level embeddings, e.g. from an embedding model optimized for paraphrase identification, to represent text in both sources. For the second challenge, we use dynamic time warping (DTW), a method for measuring the similarity between two sequences that may vary in time or speed. While DTW assumes a monotonicity condition on the alignments³ which is frequently violated in practice, it is robust enough to recover from local misalignments and the vast majority of salient events (like scene boundaries) are well-aligned.\nAs a result of DTW, the scene headers have timestamps that can indicate possible scene boundaries in the video. The alignments can also be used to e.g., augment audiovisual ML models with screenplay information like scene-level embeddings, or transfer labels assigned to audiovisual content to train screenplay prediction models.\nFigure 3: alignments between screenplay and video via time stamped text for The Witcher S1E1.\nA Multimodal Sequential Model\nThe alignment method above is a great way to get up and running with the scene change task since it combines easy-to-use pretrained embeddings with a well-known dynamic programming technique. However, it presupposes the availability of high-quality screenplays. A complementary approach (which in fact, can use the above alignments as a feature) that we present next is to train a sequence model on annotated scene change data. Certain workflows in Netflix capture this information, and that is our primary data source; publicly-released datasets are also available.\nFrom an architectural perspective, the model is relatively simple — a bidirectional GRU (biGRU) that ingests shot representations at each step and predicts if a shot is at the end of a scene.⁴ The richness in the model comes from these pretrained, multimodal shot embeddings, a preferable design choice in our setting given the difficulty in obtaining labeled scene change data and the relatively larger scale at which we can pretrain various embedding models for shots.\nFor video embeddings, we leverage an in-house model pretrained on aligned video clips paired with text (the aforementioned “timestamped text”). For audio embeddings, we first perform source separation to try and separate foreground (speech) from background (music, sound effects, noise), embed each separated waveform separately using wav2vec2, and then concatenate the results. Both early and late-stage fusion approaches are explored; in the former (Figure 4a), the audio and video embeddings are concatenated and fed into a single biGRU, and in the latter (Figure 4b) each input modality is encoded with its own biGRU, after which the hidden states are concatenated prior to the output layer.\nFigure 4a: Early Fusion (concatenate embeddings at the input).Figure 4b: Late Fusion (concatenate prior to prediction output).\nWe find:\nOur results match and sometimes even outperform the state-of-the-art (benchmarked using the video modality only and on our evaluation data). We evaluate the outputs using F-1 score for the positive label, and also relax this evaluation to consider “off-by-n” F-1 i.e., if the model predicts scene changes within n shots of the ground truth. This is a more realistic measure for our use cases due to the human-in-the-loop setting that these models are deployed in.\nAs with previous work, adding audio features improves results by 10–15%. A primary driver of variation in performance is late vs. early fusion.\nLate fusion is consistently 3–7% better than early fusion. Intuitively, this result makes sense — the temporal dependencies between shots is likely modality-specific and should be encoded separately.\nConclusion\nWe have presented two complementary approaches to scene boundary detection that leverage a variety of available modalities — screenplay, audio, and video. Logically, the next steps are to a) combine these approaches and use screenplay features in a unified model and b) generalize the outputs across multiple shot-level inference tasks, e.g. shot type classification and memorable moments identification, as we hypothesize that this path would be useful for training general purpose video understanding models of longer-form content. Longer-form content also contains more complex narrative structure, and we envision this work as the first in a series of projects that aim to better integrate narrative understanding in our multimodal machine learning models.\nSpecial thanks to Amir Ziai, Anna Pulido, and Angie Pollema.",
      "markdown": "## Detecting Scene Changes in Audiovisual Content\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----77a61d3eaad6--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----77a61d3eaad6--------------------------------)\n\n[Avneesh Saluja](https://www.linkedin.com/in/avneesh/), [Andy Yao](https://www.linkedin.com/in/yaoandy/), [Hossein Taghavi](https://www.linkedin.com/in/mhtaghavi/)\n\n## Introduction\n\nWhen watching a movie or an episode of a TV show, we experience a cohesive narrative that unfolds before us, often without giving much thought to the underlying structure that makes it all possible. However, movies and episodes are not atomic units, but rather composed of smaller elements such as frames, shots, scenes, sequences, and acts. Understanding these elements and how they relate to each other is crucial for tasks such as video summarization and highlights detection, content-based video retrieval, dubbing quality assessment, and video editing. At Netflix, such workflows are performed hundreds of times a day by many teams around the world, so investing in algorithmically-assisted tooling around content understanding can reap outsized rewards.\n\nWhile segmentation of more granular units like frames and shot boundaries is either trivial or can primarily rely on [pixel-based information](https://arxiv.org/abs/2008.04838), higher order segmentation¹ requires a more nuanced understanding of the content, such as the narrative or emotional arcs. Furthermore, some cues can be better inferred from modalities other than the video, e.g. the screenplay or the audio and dialogue track. Scene boundary detection, in particular, is the task of identifying the transitions between scenes, where a scene is defined as a continuous sequence of shots that take place in the same time and location (often with a relatively static set of characters) and share a common action or theme.\n\nIn this blog post, we present two complementary approaches to scene boundary detection in audiovisual content. The first method, which can be seen as a form of [weak supervision](http://ai.stanford.edu/blog/weak-supervision/), leverages auxiliary data in the form of a screenplay by aligning screenplay text with timed text (closed captions, audio descriptions) and assigning timestamps to the screenplay’s scene headers (a.k.a. sluglines). In the second approach, we show that a relatively simple, supervised sequential model (bidirectional LSTM or GRU) that uses rich, pretrained shot-level embeddings can outperform the current state-of-the-art baselines on our internal benchmarks.\n\nFigure 1: a scene consists of a sequence of shots.\n\n## Leveraging Aligned Screenplay Information\n\nScreenplays are the blueprints of a movie or show. They are formatted in a specific way, with each scene beginning with a scene header, indicating attributes such as the location and time of day. This consistent formatting makes it possible to parse screenplays into a structured format. At the same time, a) changes made on the fly (directorial or actor discretion) or b) in post production and editing are rarely reflected in the screenplay, i.e. it isn’t rewritten to reflect the changes.\n\nFigure 2: screenplay elements, from _The Witcher S1E1_.\n\nIn order to leverage this noisily aligned data source, we need to align time-stamped text (e.g. closed captions and audio descriptions) with screenplay text (dialogue and action² lines), bearing in mind a) the on-the-fly changes that might result in semantically similar but not identical line pairs and b) the possible post-shoot changes that are more significant (reordering, removing, or inserting entire scenes). To address the first challenge, we use pre trained sentence-level embeddings, e.g. from an embedding model optimized for [paraphrase identification](https://www.sbert.net/examples/applications/paraphrase-mining/README.html), to represent text in both sources. For the second challenge, we use [dynamic time warping](https://en.wikipedia.org/wiki/Dynamic_time_warping) (DTW), a method for measuring the similarity between two sequences that may vary in time or speed. While DTW assumes a monotonicity condition on the alignments³ which is frequently violated in practice, it is robust enough to recover from local misalignments and the vast majority of salient events (like scene boundaries) are well-aligned.\n\nAs a result of DTW, the scene headers have timestamps that can indicate possible scene boundaries in the video. The alignments can also be used to e.g., augment audiovisual ML models with screenplay information like scene-level embeddings, or transfer labels assigned to audiovisual content to train screenplay prediction models.\n\nFigure 3: alignments between screenplay and video via time stamped text for _The Witcher S1E1_.\n\n## A Multimodal Sequential Model\n\nThe alignment method above is a great way to get up and running with the scene change task since it combines easy-to-use pretrained embeddings with a well-known dynamic programming technique. However, it presupposes the availability of high-quality screenplays. A complementary approach (which in fact, can use the above alignments as a feature) that we present next is to train a sequence model on annotated scene change data. Certain workflows in Netflix capture this information, and that is our primary data source; publicly-released datasets are also available.\n\nFrom an architectural perspective, the model is relatively simple — a bidirectional [GRU](https://arxiv.org/abs/1412.3555) (biGRU) that ingests shot representations at each step and predicts if a shot is at the end of a scene.⁴ The richness in the model comes from these pretrained, multimodal shot embeddings, a preferable design choice in our setting given the difficulty in obtaining labeled scene change data and the relatively larger scale at which we can pretrain various embedding models for shots.\n\nFor video embeddings, we leverage an in-house model pretrained on aligned video clips paired with text (the aforementioned “timestamped text”). For audio embeddings, we first perform [source separation](https://research.deezer.com/projects/spleeter.html) to try and separate foreground (speech) from background (music, sound effects, noise), embed each separated waveform separately using [wav2vec2](https://arxiv.org/abs/2006.11477), and then concatenate the results. Both early and late-stage fusion approaches are explored; in the former (Figure 4a), the audio and video embeddings are concatenated and fed into a single biGRU, and in the latter (Figure 4b) each input modality is encoded with its own biGRU, after which the hidden states are concatenated prior to the output layer.\n\nFigure 4a: Early Fusion (concatenate embeddings at the input).\n\nFigure 4b: Late Fusion (concatenate prior to prediction output).\n\nWe find:\n\n*   Our results match and sometimes even outperform the [state-of-the-art](https://openaccess.thecvf.com/content_CVPR_2020/papers/Rao_A_Local-to-Global_Approach_to_Multi-Modal_Movie_Scene_Segmentation_CVPR_2020_paper.pdf) (benchmarked using the video modality only and on our evaluation data). We evaluate the outputs using F-1 score for the positive label, and also relax this evaluation to consider “off-by-_n_” F-1 i.e., if the model predicts scene changes within _n_ shots of the ground truth. This is a more realistic measure for our use cases due to the human-in-the-loop setting that these models are deployed in.\n*   As with previous work, adding audio features improves results by 10–15%. A primary driver of variation in performance is late vs. early fusion.\n*   Late fusion is consistently 3–7% better than early fusion. Intuitively, this result makes sense — the temporal dependencies between shots is likely modality-specific and should be encoded separately.\n\n## Conclusion\n\nWe have presented two complementary approaches to scene boundary detection that leverage a variety of available modalities — screenplay, audio, and video. Logically, the next steps are to a) combine these approaches and use screenplay features in a unified model and b) generalize the outputs across multiple shot-level inference tasks, e.g. shot type classification and memorable moments identification, as we hypothesize that this path would be useful for training general purpose video understanding models of longer-form content. Longer-form content also contains more complex narrative structure, and we envision this work as the first in a series of projects that aim to better integrate narrative understanding in our multimodal machine learning models.\n\n_Special thanks to_ [_Amir Ziai_](https://www.linkedin.com/in/amirziai/)_,_ [_Anna Pulido_](https://www.linkedin.com/in/anna-pulido-61025063/)_, and_ [_Angie Pollema_](https://www.linkedin.com/in/angiepollema1/)_._"
    },
    {
      "url": "https://netflixtechblog.com/migrating-critical-traffic-at-scale-with-no-downtime-part-2-4b1c8c7155c1?source=collection_home---4------13-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/migrating-critical-traffic-at-scale-with-no-downtime-part-2-4b1c8c7155c1?gi=aeabb44330f5&source=collection_home---4------13-----------------------",
        "loadedTime": "2023-12-06T00:03:35.557Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/migrating-critical-traffic-at-scale-with-no-downtime-part-2-4b1c8c7155c1",
        "title": "Migrating Critical Traffic At Scale with No Downtime — Part 2 | by Netflix Technology Blog | Netflix TechBlog",
        "description": "Picture yourself enthralled by the latest episode of your beloved Netflix series, delighting in an uninterrupted, high-definition streaming experience. Behind these perfect moments of entertainment…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Migrating Critical Traffic At Scale with No Downtime — Part 2\nShyam Gala, Javier Fernandez-Ivern, Anup Rokkam Pratap, Devang Shah\nPicture yourself enthralled by the latest episode of your beloved Netflix series, delighting in an uninterrupted, high-definition streaming experience. Behind these perfect moments of entertainment is a complex mechanism, with numerous gears and cogs working in harmony. But what happens when this machinery needs a transformation? This is where large-scale system migrations come into play. Our previous blog post presented replay traffic testing — a crucial instrument in our toolkit that allows us to implement these transformations with precision and reliability.\nReplay traffic testing gives us the initial foundation of validation, but as our migration process unfolds, we are met with the need for a carefully controlled migration process. A process that doesn’t just minimize risk, but also facilitates a continuous evaluation of the rollout’s impact. This blog post will delve into the techniques leveraged at Netflix to introduce these changes to production.\nSticky Canaries\nCanary deployments are an effective mechanism for validating changes to a production backend service in a controlled and limited manner, thus mitigating the risk of unforeseen consequences that may arise due to the change. This process involves creating two new clusters for the updated service; a baseline cluster containing the current version running in production and a canary cluster containing the new version of the service. A small percentage of production traffic is redirected to the two new clusters, allowing us to monitor the new version’s performance and compare it against the current version. By collecting and analyzing key performance metrics of the service over time, we can assess the impact of the new changes and determine if they meet the availability, latency, and performance requirements.\nSome product features require a lifecycle of requests between the customer device and a set of backend services to drive the feature. For instance, video playback functionality on Netflix involves requesting URLs for the streams from a service, calling the CDN to download the bits from the streams, requesting a license to decrypt the streams from a separate service, and sending telemetry indicating the successful start of playback to yet another service. By tracking metrics only at the level of service being updated, we might miss capturing deviations in broader end-to-end system functionality.\nSticky Canary is an improvement to the traditional canary process that addresses this limitation. In this variation, the canary framework creates a pool of unique customer devices and then routes traffic for this pool consistently to the canary and baseline clusters for the duration of the experiment. Apart from measuring service-level metrics, the canary framework is able to keep track of broader system operational and customer metrics across the canary pool and thereby detect regressions on the entire request lifecycle flow.\nSticky Canary\nIt is important to note that with sticky canaries, devices in the canary pool continue to be routed to the canary throughout the experiment, potentially resulting in undesirable behavior persisting through retries on customer devices. Therefore, the canary framework is designed to monitor operational and customer KPI metrics to detect persistent deviations and terminate the canary experiment if necessary.\nCanaries and sticky canaries are valuable tools in the system migration process. Compared to replay testing, canaries allow us to extend the validation scope beyond the service level. They enable verification of the broader end-to-end system functionality across the request lifecycle for that functionality, giving us confidence that the migration will not cause any disruptions to the customer experience. Canaries also provide an opportunity to measure system performance under different load conditions, allowing us to identify and resolve any performance bottlenecks. They enable us to further fine-tune and configure the system, ensuring the new changes are integrated smoothly and seamlessly.\nA/B Testing\nA/B testing is a widely recognized method for verifying hypotheses through a controlled experiment. It involves dividing a portion of the population into two or more groups, each receiving a different treatment. The results are then evaluated using specific metrics to determine whether the hypothesis is valid. The industry frequently employs the technique to assess hypotheses related to product evolution and user interaction. It is also widely utilized at Netflix to test changes to product behavior and customer experience.\nA/B testing is also a valuable tool for assessing significant changes to backend systems. We can determine A/B test membership in either device application or backend code and selectively invoke new code paths and services. Within the context of migrations, A/B testing enables us to limit exposure to the migrated system by enabling the new path for a smaller percentage of the member base. Thereby controlling the risk of unexpected behavior resulting from the new changes. A/B testing is also a key technique in migrations where the updates to the architecture involve changing device contracts as well.\nCanary experiments are typically conducted over periods ranging from hours to days. However, in certain instances, migration-related experiments may be required to span weeks or months to obtain a more accurate understanding of the impact on specific Quality of Experience (QoE) metrics. Additionally, in-depth analyses of particular business Key Performance Indicators (KPIs) may require longer experiments. For instance, envision a migration scenario where we enhance the playback quality, anticipating that this improvement will lead to more customers engaging with the play button. Assessing relevant metrics across a considerable sample size is crucial for obtaining a reliable and confident evaluation of the hypothesis. A/B frameworks work as effective tools to accommodate this next step in the confidence-building process.\nIn addition to supporting extended durations, A/B testing frameworks offer other supplementary capabilities. This approach enables test allocation restrictions based on factors such as geography, device platforms, and device versions, while also allowing for analysis of migration metrics across similar dimensions. This ensures that the changes do not disproportionately impact specific customer segments. A/B testing also provides adaptability, permitting adjustments to allocation size throughout the experiment.\nWe might not use A/B testing for every backend migration. Instead, we use it for migrations in which changes are expected to impact device QoE or business KPIs significantly. For example, as discussed earlier, if the planned changes are expected to improve client QoE metrics, we would test the hypothesis via A/B testing.\nDialing Traffic\nAfter completing the various stages of validation, such as replay testing, sticky canaries, and A/B tests, we can confidently assert that the planned changes will not significantly impact SLAs (service-level-agreement), device level QoE, or business KPIs. However, it is imperative that the final rollout is regulated to ensure that any unnoticed and unexpected problems do not disrupt the customer experience. To this end, we have implemented traffic dialing as the last step in mitigating the risk associated with enabling the changes in production.\nA dial is a software construct that enables the controlled flow of traffic within a system. This construct samples inbound requests using a distribution function and determines whether they should be routed to the new path or kept on the existing path. The decision-making process involves assessing whether the distribution function’s output aligns within the range of the predefined target percentage. The sampling is done consistently using a fixed parameter associated with the request. The target percentage is controlled via a globally scoped dynamic property that can be updated in real-time. By increasing or decreasing the target percentage, traffic flow to the new path can be regulated instantaneously.\nDial\nThe selection of the actual sampling parameter depends on the specific migration requirements. A dial can be used to randomly sample all requests, which is achieved by selecting a variable parameter like a timestamp or a random number. Alternatively, in scenarios where the system path must remain constant with respect to customer devices, a constant device attribute such as deviceId is selected as the sampling parameter. Dials can be applied in several places, such as device application code, the relevant server component, or even at the API gateway for edge API systems, making them a versatile tool for managing migrations in complex systems.\nTraffic is dialed over to the new system in measured discrete steps. At every step, relevant stakeholders are informed, and key metrics are monitored, including service, device, operational, and business metrics. If we discover an unexpected issue or notice metrics trending in an undesired direction during the migration, the dial gives us the capability to quickly roll back the traffic to the old path and address the issue.\nThe dialing steps can also be scoped at the data center level if traffic is served from multiple data centers. We can start by dialing traffic in a single data center to allow for an easier side-by-side comparison of key metrics across data centers, thereby making it easier to observe any deviations in the metrics. The duration of how long we run the actual discrete dialing steps can also be adjusted. Running the dialing steps for longer periods increases the probability of surfacing issues that may only affect a small group of members or devices and might have been too low to capture and perform shadow traffic analysis. We can complete the final step of migrating all the production traffic to the new system using the combination of gradual step-wise dialing and monitoring.\nMigrating Persistent Stores\nStateful APIs pose unique challenges that require different strategies. While the replay testing technique discussed in the previous part of this blog series can be employed, additional measures outlined earlier are necessary.\nThis alternate migration strategy has proven effective for our systems that meet certain criteria. Specifically, our data model is simple, self-contained, and immutable, with no relational aspects. Our system doesn’t require strict consistency guarantees and does not use database transactions. We adopt an ETL-based dual-write strategy that roughly follows this sequence of steps:\nInitial Load through an ETL process: Data is extracted from the source data store, transformed into the new model, and written to the newer data store through an offline job. We use custom queries to verify the completeness of the migrated records.\nContinuous migration via Dual-writes: We utilize an active-active/dual-writes strategy to migrate the bulk of the data. As a safety mechanism, we use dials (discussed previously) to control the proportion of writes that go to the new data store. To maintain state parity across both stores, we write all state-altering requests of an entity to both stores. This is achieved by selecting a sampling parameter that makes the dial sticky to the entity’s lifecycle. We incrementally turn the dial up as we gain confidence in the system while carefully monitoring its overall health. The dial also acts as a switch to turn off all writes to the new data store if necessary.\nContinuous verification of records: When a record is read, the service reads from both data stores and verifies the functional correctness of the new record if found in both stores. One can perform this comparison live on the request path or offline based on the latency requirements of the particular use case. In the case of a live comparison, we can return records from the new datastore when the records match. This process gives us an idea of the functional correctness of the migration.\nEvaluation of migration completeness: To verify the completeness of the records, cold storage services are used to take periodic data dumps from the two data stores and compared for completeness. Gaps in the data are filled back with an ETL process.\nCut-over and clean-up: Once the data is verified for correctness and completeness, dual writes and reads are disabled, any client code is cleaned up, and read/writes only occur to the new data store.\nMigrating Stateful Systems\nClean-up\nClean-up of any migration-related code and configuration after the migration is crucial to ensure the system runs smoothly and efficiently and we don’t build up tech debt and complexity. Once the migration is complete and validated, all migration-related code, such as traffic dials, A/B tests, and replay traffic integrations, can be safely removed from the system. This includes cleaning up configuration changes, reverting to the original settings, and disabling any temporary components added during the migration. In addition, it is important to document the entire migration process and keep records of any issues encountered and their resolution. By performing a thorough clean-up and documentation process, future migrations can be executed more efficiently and effectively, building on the lessons learned from the previous migrations.\nParting Thoughts\nWe have utilized a range of techniques outlined in our blog posts to conduct numerous large, medium, and small-scale migrations on the Netflix platform. Our efforts have been largely successful, with minimal to no downtime or significant issues encountered. Throughout the process, we have gained valuable insights and refined our techniques. It should be noted that not all of the techniques presented are universally applicable, as each migration presents its own unique set of circumstances. Determining the appropriate level of validation, testing, and risk mitigation requires careful consideration of several factors, including the nature of the changes, potential impacts on customer experience, engineering effort, and product priorities. Ultimately, we aim to achieve seamless migrations without disruptions or downtime.\nIn a series of forthcoming blog posts, we will explore a selection of specific use cases where the techniques highlighted in this blog series were utilized effectively. They will focus on a comprehensive analysis of the Ads Tier Launch and an extensive GraphQL migration for various product APIs. These posts will offer readers invaluable insights into the practical application of these methodologies in real-world situations.",
      "markdown": "## Migrating Critical Traffic At Scale with No Downtime — Part 2\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----4b1c8c7155c1--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----4b1c8c7155c1--------------------------------)\n\n[Shyam Gala](https://www.linkedin.com/in/shyam-gala-5891224/), [Javier Fernandez-Ivern](https://www.linkedin.com/in/ivern/), [Anup Rokkam Pratap](https://www.linkedin.com/in/rokkampratap/), [Devang Shah](https://www.linkedin.com/in/shahdewang/)\n\nPicture yourself enthralled by the latest episode of your beloved Netflix series, delighting in an uninterrupted, high-definition streaming experience. Behind these perfect moments of entertainment is a complex mechanism, with numerous gears and cogs working in harmony. But what happens when this machinery needs a transformation? This is where large-scale system migrations come into play. Our [previous blog post](https://netflixtechblog.com/migrating-critical-traffic-at-scale-with-no-downtime-part-1-ba1c7a1c7835) presented replay traffic testing — a crucial instrument in our toolkit that allows us to implement these transformations with precision and reliability.\n\n**Replay traffic testing gives us the initial foundation of validation, but as our migration process unfolds, we are met with the need for a carefully controlled migration process. A process that doesn’t just minimize risk, but also facilitates a continuous evaluation of the rollout’s impact. This blog post will delve into the techniques leveraged at Netflix to introduce these changes to production.**\n\n## Sticky Canaries\n\n[Canary](https://netflixtechblog.com/automated-canary-analysis-at-netflix-with-kayenta-3260bc7acc69) deployments are an effective mechanism for validating changes to a production backend service in a controlled and limited manner, thus mitigating the risk of unforeseen consequences that may arise due to the change. This process involves creating two new clusters for the updated service; a baseline cluster containing the current version running in production and a canary cluster containing the new version of the service. A small percentage of production traffic is redirected to the two new clusters, allowing us to monitor the new version’s performance and compare it against the current version. By collecting and analyzing key performance metrics of the service over time, we can assess the impact of the new changes and determine if they meet the availability, latency, and performance requirements.\n\nSome product features require a lifecycle of requests between the customer device and a set of backend services to drive the feature. For instance, video playback functionality on Netflix involves requesting URLs for the streams from a service, calling the CDN to download the bits from the streams, requesting a license to decrypt the streams from a separate service, and sending telemetry indicating the successful start of playback to yet another service. By tracking metrics only at the level of service being updated, we might miss capturing deviations in broader end-to-end system functionality.\n\n[Sticky Canary](https://www.infoq.com/presentations/sticky-canaries/) is an improvement to the traditional canary process that addresses this limitation. In this variation, the canary framework creates a pool of unique customer devices and then routes traffic for this pool consistently to the canary and baseline clusters for the duration of the experiment. Apart from measuring service-level metrics, the canary framework is able to keep track of broader system operational and customer metrics across the canary pool and thereby detect regressions on the entire request lifecycle flow.\n\nSticky Canary\n\nIt is important to note that with sticky canaries, devices in the canary pool continue to be routed to the canary throughout the experiment, potentially resulting in undesirable behavior persisting through retries on customer devices. Therefore, the canary framework is designed to monitor operational and customer KPI metrics to detect persistent deviations and terminate the canary experiment if necessary.\n\n**Canaries and sticky canaries are valuable tools in the system migration process. Compared to replay testing, canaries allow us to extend the validation scope beyond the service level. They enable verification of the broader end-to-end system functionality across the request lifecycle for that functionality, giving us confidence that the migration will not cause any disruptions to the customer experience. Canaries also provide an opportunity to measure system performance under different load conditions, allowing us to identify and resolve any performance bottlenecks. They enable us to further fine-tune and configure the system, ensuring the new changes are integrated smoothly and seamlessly.**\n\n## A/B Testing\n\nA/B testing is a widely recognized method for verifying hypotheses through a controlled experiment. It involves dividing a portion of the population into two or more groups, each receiving a different treatment. The results are then evaluated using specific metrics to determine whether the hypothesis is valid. The industry frequently employs the technique to assess hypotheses related to product evolution and user interaction. It is also [widely utilized at Netflix](https://netflixtechblog.com/a-b-testing-and-beyond-improving-the-netflix-streaming-experience-with-experimentation-and-data-5b0ae9295bdf) to test changes to product behavior and customer experience.\n\nA/B testing is also a valuable tool for assessing significant changes to backend systems. We can determine A/B test membership in either device application or backend code and selectively invoke new code paths and services. Within the context of migrations, A/B testing enables us to limit exposure to the migrated system by enabling the new path for a smaller percentage of the member base. Thereby controlling the risk of unexpected behavior resulting from the new changes. A/B testing is also a key technique in migrations where the updates to the architecture involve changing device contracts as well.\n\n**Canary experiments are typically conducted over periods ranging from hours to days. However, in certain instances, migration-related experiments may be required to span weeks or months to obtain a more accurate understanding of the impact on specific Quality of Experience (QoE) metrics. Additionally, in-depth analyses of particular business Key Performance Indicators (KPIs) may require longer experiments. For instance, envision a migration scenario where we enhance the playback quality, anticipating that this improvement will lead to more customers engaging with the play button. Assessing relevant metrics across a considerable sample size is crucial for obtaining a reliable and confident evaluation of the hypothesis. A/B frameworks work as effective tools to accommodate this next step in the confidence-building process.**\n\nIn addition to supporting extended durations, A/B testing frameworks offer other supplementary capabilities. This approach enables test allocation restrictions based on factors such as geography, device platforms, and device versions, while also allowing for analysis of migration metrics across similar dimensions. This ensures that the changes do not disproportionately impact specific customer segments. A/B testing also provides adaptability, permitting adjustments to allocation size throughout the experiment.\n\nWe might not use A/B testing for every backend migration. Instead, we use it for migrations in which changes are expected to impact device QoE or business KPIs significantly. For example, as discussed earlier, if the planned changes are expected to improve client QoE metrics, we would test the hypothesis via A/B testing.\n\n## Dialing Traffic\n\nAfter completing the various stages of validation, such as replay testing, sticky canaries, and A/B tests, we can confidently assert that the planned changes will not significantly impact SLAs (service-level-agreement), device level QoE, or business KPIs. However, it is imperative that the final rollout is regulated to ensure that any unnoticed and unexpected problems do not disrupt the customer experience. To this end, we have implemented traffic dialing as the last step in mitigating the risk associated with enabling the changes in production.\n\nA dial is a software construct that enables the controlled flow of traffic within a system. This construct samples inbound requests using a distribution function and determines whether they should be routed to the new path or kept on the existing path. The decision-making process involves assessing whether the distribution function’s output aligns within the range of the predefined target percentage. The sampling is done consistently using a fixed parameter associated with the request. The target percentage is controlled via a globally scoped dynamic property that can be updated in real-time. By increasing or decreasing the target percentage, traffic flow to the new path can be regulated instantaneously.\n\nDial\n\nThe selection of the actual sampling parameter depends on the specific migration requirements. A dial can be used to randomly sample all requests, which is achieved by selecting a variable parameter like a timestamp or a random number. Alternatively, in scenarios where the system path must remain constant with respect to customer devices, a constant device attribute such as deviceId is selected as the sampling parameter. Dials can be applied in several places, such as device application code, the relevant server component, or even at the API gateway for edge API systems, making them a versatile tool for managing migrations in complex systems.\n\n**Traffic is dialed over to the new system in measured discrete steps. At every step, relevant stakeholders are informed, and key metrics are monitored, including service, device, operational, and business metrics. If we discover an unexpected issue or notice metrics trending in an undesired direction during the migration, the dial gives us the capability to quickly roll back the traffic to the old path and address the issue.**\n\nThe dialing steps can also be scoped at the data center level if traffic is served from multiple data centers. We can start by dialing traffic in a single data center to allow for an easier side-by-side comparison of key metrics across data centers, thereby making it easier to observe any deviations in the metrics. The duration of how long we run the actual discrete dialing steps can also be adjusted. Running the dialing steps for longer periods increases the probability of surfacing issues that may only affect a small group of members or devices and might have been too low to capture and perform shadow traffic analysis. We can complete the final step of migrating all the production traffic to the new system using the combination of gradual step-wise dialing and monitoring.\n\n## Migrating Persistent Stores\n\nStateful APIs pose unique challenges that require different strategies. While the replay testing technique discussed in the previous part of this blog series can be employed, additional measures [outlined earlier](https://netflixtechblog.com/migrating-critical-traffic-at-scale-with-no-downtime-part-1-ba1c7a1c7835) are necessary.\n\nThis alternate migration strategy has proven effective for our systems that meet certain criteria. Specifically, our data model is simple, self-contained, and immutable, with no relational aspects. Our system doesn’t require strict consistency guarantees and does not use database transactions. We adopt an ETL-based dual-write strategy that roughly follows this sequence of steps:\n\n*   **Initial Load through an ETL process:** Data is extracted from the source data store, transformed into the new model, and written to the newer data store through an offline job. We use custom queries to verify the completeness of the migrated records.\n*   **Continuous migration via Dual-writes:** We utilize an active-active/dual-writes strategy to migrate the bulk of the data. As a safety mechanism, we use dials (discussed previously) to control the proportion of writes that go to the new data store. To maintain state parity across both stores, we write all state-altering requests of an entity to both stores. This is achieved by selecting a sampling parameter that makes the dial sticky to the entity’s lifecycle. We incrementally turn the dial up as we gain confidence in the system while carefully monitoring its overall health. The dial also acts as a switch to turn off all writes to the new data store if necessary.\n*   **Continuous verification of records:** When a record is read, the service reads from both data stores and verifies the functional correctness of the new record if found in both stores. One can perform this comparison live on the request path or offline based on the latency requirements of the particular use case. In the case of a live comparison, we can return records from the new datastore when the records match. This process gives us an idea of the functional correctness of the migration.\n*   **Evaluation of migration completeness:** To verify the completeness of the records, cold storage services are used to take periodic data dumps from the two data stores and compared for completeness. Gaps in the data are filled back with an ETL process.\n*   **Cut-over and clean-up:** Once the data is verified for correctness and completeness, dual writes and reads are disabled, any client code is cleaned up, and read/writes only occur to the new data store.\n\nMigrating Stateful Systems\n\n## Clean-up\n\nClean-up of any migration-related code and configuration after the migration is crucial to ensure the system runs smoothly and efficiently and we don’t build up tech debt and complexity. Once the migration is complete and validated, all migration-related code, such as traffic dials, A/B tests, and replay traffic integrations, can be safely removed from the system. This includes cleaning up configuration changes, reverting to the original settings, and disabling any temporary components added during the migration. In addition, it is important to document the entire migration process and keep records of any issues encountered and their resolution. By performing a thorough clean-up and documentation process, future migrations can be executed more efficiently and effectively, building on the lessons learned from the previous migrations.\n\n## Parting Thoughts\n\nWe have utilized a range of techniques outlined in our blog posts to conduct numerous large, medium, and small-scale migrations on the Netflix platform. Our efforts have been largely successful, with minimal to no downtime or significant issues encountered. Throughout the process, we have gained valuable insights and refined our techniques. It should be noted that not all of the techniques presented are universally applicable, as each migration presents its own unique set of circumstances. Determining the appropriate level of validation, testing, and risk mitigation requires careful consideration of several factors, including the nature of the changes, potential impacts on customer experience, engineering effort, and product priorities. Ultimately, we aim to achieve seamless migrations without disruptions or downtime.\n\nIn a series of forthcoming blog posts, we will explore a selection of specific use cases where the techniques highlighted in this blog series were utilized effectively. They will focus on a [comprehensive analysis of the Ads Tier Launch](https://netflixtechblog.com/ensuring-the-successful-launch-of-ads-on-netflix-f99490fdf1ba) and [an extensive GraphQL migration for various product APIs](https://netflixtechblog.com/migrating-netflix-to-graphql-safely-8e1e4d4f1e72). These posts will offer readers invaluable insights into the practical application of these methodologies in real-world situations."
    },
    {
      "url": "https://netflixtechblog.com/debugging-a-fuse-deadlock-in-the-linux-kernel-c75cd7989b6d?source=collection_home---4------14-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/debugging-a-fuse-deadlock-in-the-linux-kernel-c75cd7989b6d?gi=828c737cc5c4&source=collection_home---4------14-----------------------",
        "loadedTime": "2023-12-06T00:03:36.051Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/debugging-a-fuse-deadlock-in-the-linux-kernel-c75cd7989b6d",
        "title": "Debugging a FUSE deadlock in the Linux kernel | by Netflix Technology Blog | Netflix TechBlog",
        "description": "The Compute team at Netflix is charged with managing all AWS and containerized workloads at Netflix, including autoscaling, deployment of containers, issue remediation, etc. As part of this team, I…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Debugging a FUSE deadlock in the Linux kernel\nTycho Andersen\nThe Compute team at Netflix is charged with managing all AWS and containerized workloads at Netflix, including autoscaling, deployment of containers, issue remediation, etc. As part of this team, I work on fixing strange things that users report.\nThis particular issue involved a custom internal FUSE filesystem: ndrive. It had been festering for some time, but needed someone to sit down and look at it in anger. This blog post describes how I poked at /procto get a sense of what was going on, before posting the issue to the kernel mailing list and getting schooled on how the kernel’s wait code actually works!\nSymptom: Stuck Docker Kill & A Zombie Process\nWe had a stuck docker API call:\ngoroutine 146 [select, 8817 minutes]:\nnet/http.(*persistConn).roundTrip(0xc000658fc0, 0xc0003fc080, 0x0, 0x0, 0x0)\n/usr/local/go/src/net/http/transport.go:2610 +0x765\nnet/http.(*Transport).roundTrip(0xc000420140, 0xc000966200, 0x30, 0x1366f20, 0x162)\n/usr/local/go/src/net/http/transport.go:592 +0xacb\nnet/http.(*Transport).RoundTrip(0xc000420140, 0xc000966200, 0xc000420140, 0x0, 0x0)\n/usr/local/go/src/net/http/roundtrip.go:17 +0x35\nnet/http.send(0xc000966200, 0x161eba0, 0xc000420140, 0x0, 0x0, 0x0, 0xc00000e050, 0x3, 0x1, 0x0)\n/usr/local/go/src/net/http/client.go:251 +0x454\nnet/http.(*Client).send(0xc000438480, 0xc000966200, 0x0, 0x0, 0x0, 0xc00000e050, 0x0, 0x1, 0x10000168e)\n/usr/local/go/src/net/http/client.go:175 +0xff\nnet/http.(*Client).do(0xc000438480, 0xc000966200, 0x0, 0x0, 0x0)\n/usr/local/go/src/net/http/client.go:717 +0x45f\nnet/http.(*Client).Do(...)\n/usr/local/go/src/net/http/client.go:585\ngolang.org/x/net/context/ctxhttp.Do(0x163bd48, 0xc000044090, 0xc000438480, 0xc000966100, 0x0, 0x0, 0x0)\n/go/pkg/mod/golang.org/x/net@v0.0.0-20211209124913-491a49abca63/context/ctxhttp/ctxhttp.go:27 +0x10f\ngithub.com/docker/docker/client.(*Client).doRequest(0xc0001a8200, 0x163bd48, 0xc000044090, 0xc000966100, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\n/go/pkg/mod/github.com/moby/moby@v0.0.0-20190408150954-50ebe4562dfc/client/request.go:132 +0xbe\ngithub.com/docker/docker/client.(*Client).sendRequest(0xc0001a8200, 0x163bd48, 0xc000044090, 0x13d8643, 0x3, 0xc00079a720, 0x51, 0x0, 0x0, 0x0, ...)\n/go/pkg/mod/github.com/moby/moby@v0.0.0-20190408150954-50ebe4562dfc/client/request.go:122 +0x156\ngithub.com/docker/docker/client.(*Client).get(...)\n/go/pkg/mod/github.com/moby/moby@v0.0.0-20190408150954-50ebe4562dfc/client/request.go:37\ngithub.com/docker/docker/client.(*Client).ContainerInspect(0xc0001a8200, 0x163bd48, 0xc000044090, 0xc0006a01c0, 0x40, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\n/go/pkg/mod/github.com/moby/moby@v0.0.0-20190408150954-50ebe4562dfc/client/container_inspect.go:18 +0x128\ngithub.com/Netflix/titus-executor/executor/runtime/docker.(*DockerRuntime).Kill(0xc000215180, 0x163bdb8, 0xc000938600, 0x1, 0x0, 0x0)\n/var/lib/buildkite-agent/builds/ip-192-168-1-90-1/netflix/titus-executor/executor/runtime/docker/docker.go:2835 +0x310\ngithub.com/Netflix/titus-executor/executor/runner.(*Runner).doShutdown(0xc000432dc0, 0x163bd10, 0xc000938390, 0x1, 0xc000b821e0, 0x1d, 0xc0005e4710)\n/var/lib/buildkite-agent/builds/ip-192-168-1-90-1/netflix/titus-executor/executor/runner/runner.go:326 +0x4f4\ngithub.com/Netflix/titus-executor/executor/runner.(*Runner).startRunner(0xc000432dc0, 0x163bdb8, 0xc00071e0c0, 0xc0a502e28c08b488, 0x24572b8, 0x1df5980)\n/var/lib/buildkite-agent/builds/ip-192-168-1-90-1/netflix/titus-executor/executor/runner/runner.go:122 +0x391\ncreated by github.com/Netflix/titus-executor/executor/runner.StartTaskWithRuntime\n/var/lib/buildkite-agent/builds/ip-192-168-1-90-1/netflix/titus-executor/executor/runner/runner.go:81 +0x411\nHere, our management engine has made an HTTP call to the Docker API’s unix socket asking it to kill a container. Our containers are configured to be killed via SIGKILL. But this is strange. kill(SIGKILL) should be relatively fatal, so what is the container doing?\n$ docker exec -it 6643cd073492 bash\nOCI runtime exec failed: exec failed: container_linux.go:380: starting container process caused: process_linux.go:130: executing setns process caused: exit status 1: unknown\nHmm. Seems like it’s alive, but setns(2) fails. Why would that be? If we look at the process tree via ps awwfux, we see:\n\\_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/6643cd073492ba9166100ed30dbe389ff1caef0dc3d35\n| \\_ [docker-init]\n| \\_ [ndrive] <defunct>\nOk, so the container’s init process is still alive, but it has one zombie child. What could the container’s init process possibly be doing?\n# cat /proc/1528591/stack\n[<0>] do_wait+0x156/0x2f0\n[<0>] kernel_wait4+0x8d/0x140\n[<0>] zap_pid_ns_processes+0x104/0x180\n[<0>] do_exit+0xa41/0xb80\n[<0>] do_group_exit+0x3a/0xa0\n[<0>] __x64_sys_exit_group+0x14/0x20\n[<0>] do_syscall_64+0x37/0xb0\n[<0>] entry_SYSCALL_64_after_hwframe+0x44/0xae\nIt is in the process of exiting, but it seems stuck. The only child is the ndrive process in Z (i.e. “zombie”) state, though. Zombies are processes that have successfully exited, and are waiting to be reaped by a corresponding wait() syscall from their parents. So how could the kernel be stuck waiting on a zombie?\n# ls /proc/1544450/task\n1544450 1544574\nAh ha, there are two threads in the thread group. One of them is a zombie, maybe the other one isn’t:\n# cat /proc/1544574/stack\n[<0>] request_wait_answer+0x12f/0x210\n[<0>] fuse_simple_request+0x109/0x2c0\n[<0>] fuse_flush+0x16f/0x1b0\n[<0>] filp_close+0x27/0x70\n[<0>] put_files_struct+0x6b/0xc0\n[<0>] do_exit+0x360/0xb80\n[<0>] do_group_exit+0x3a/0xa0\n[<0>] get_signal+0x140/0x870\n[<0>] arch_do_signal_or_restart+0xae/0x7c0\n[<0>] exit_to_user_mode_prepare+0x10f/0x1c0\n[<0>] syscall_exit_to_user_mode+0x26/0x40\n[<0>] do_syscall_64+0x46/0xb0\n[<0>] entry_SYSCALL_64_after_hwframe+0x44/0xae\nIndeed it is not a zombie. It is trying to become one as hard as it can, but it’s blocking inside FUSE for some reason. To find out why, let’s look at some kernel code. If we look at zap_pid_ns_processes(), it does:\n/*\n* Reap the EXIT_ZOMBIE children we had before we ignored SIGCHLD.\n* kernel_wait4() will also block until our children traced from the\n* parent namespace are detached and become EXIT_DEAD.\n*/\ndo {\nclear_thread_flag(TIF_SIGPENDING);\nrc = kernel_wait4(-1, NULL, __WALL, NULL);\n} while (rc != -ECHILD);\nwhich is where we are stuck, but before that, it has done:\n/* Don't allow any more processes into the pid namespace */\ndisable_pid_allocation(pid_ns);\nwhich is why docker can’t setns() — the namespace is a zombie. Ok, so we can’t setns(2), but why are we stuck in kernel_wait4()? To understand why, let’s look at what the other thread was doing in FUSE’s request_wait_answer():\n/*\n* Either request is already in userspace, or it was forced.\n* Wait it out.\n*/\nwait_event(req->waitq, test_bit(FR_FINISHED, &req->flags));\nOk, so we’re waiting for an event (in this case, that userspace has replied to the FUSE flush request). But zap_pid_ns_processes()sent a SIGKILL! SIGKILL should be very fatal to a process. If we look at the process, we can indeed see that there’s a pending SIGKILL:\n# grep Pnd /proc/1544574/status\nSigPnd: 0000000000000000\nShdPnd: 0000000000000100\nViewing process status this way, you can see 0x100 (i.e. the 9th bit is set) under ShdPnd, which is the signal number corresponding to SIGKILL. Pending signals are signals that have been generated by the kernel, but have not yet been delivered to userspace. Signals are only delivered at certain times, for example when entering or leaving a syscall, or when waiting on events. If the kernel is currently doing something on behalf of the task, the signal may be pending. Signals can also be blocked by a task, so that they are never delivered. Blocked signals will show up in their respective pending sets as well. However, man 7 signal says: “The signals SIGKILL and SIGSTOP cannot be caught, blocked, or ignored.” But here the kernel is telling us that we have a pending SIGKILL, aka that it is being ignored even while the task is waiting!\nRed Herring: How do Signals Work?\nWell that is weird. The wait code (i.e. include/linux/wait.h) is used everywhere in the kernel: semaphores, wait queues, completions, etc. Surely it knows to look for SIGKILLs. So what does wait_event() actually do? Digging through the macro expansions and wrappers, the meat of it is:\n#define ___wait_event(wq_head, condition, state, exclusive, ret, cmd) \\\n({ \\\n__label__ __out; \\\nstruct wait_queue_entry __wq_entry; \\\nlong __ret = ret; /* explicit shadow */ \\\n\\\ninit_wait_entry(&__wq_entry, exclusive ? WQ_FLAG_EXCLUSIVE : 0); \\\nfor (;;) { \\\nlong __int = prepare_to_wait_event(&wq_head, &__wq_entry, state);\\\n\\\nif (condition) \\\nbreak; \\\n\\\nif (___wait_is_interruptible(state) && __int) { \\\n__ret = __int; \\\ngoto __out; \\\n} \\\n\\\ncmd; \\\n} \\\nfinish_wait(&wq_head, &__wq_entry); \\\n__out: __ret; \\\n})\nSo it loops forever, doing prepare_to_wait_event(), checking the condition, then checking to see if we need to interrupt. Then it does cmd, which in this case is schedule(), i.e. “do something else for a while”. prepare_to_wait_event() looks like:\nlong prepare_to_wait_event(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry, int state)\n{\nunsigned long flags;\nlong ret = 0;\nspin_lock_irqsave(&wq_head->lock, flags);\nif (signal_pending_state(state, current)) {\n/*\n* Exclusive waiter must not fail if it was selected by wakeup,\n* it should \"consume\" the condition we were waiting for.\n*\n* The caller will recheck the condition and return success if\n* we were already woken up, we can not miss the event because\n* wakeup locks/unlocks the same wq_head->lock.\n*\n* But we need to ensure that set-condition + wakeup after that\n* can't see us, it should wake up another exclusive waiter if\n* we fail.\n*/\nlist_del_init(&wq_entry->entry);\nret = -ERESTARTSYS;\n} else {\nif (list_empty(&wq_entry->entry)) {\nif (wq_entry->flags & WQ_FLAG_EXCLUSIVE)\n__add_wait_queue_entry_tail(wq_head, wq_entry);\nelse\n__add_wait_queue(wq_head, wq_entry);\n}\nset_current_state(state);\n}\nspin_unlock_irqrestore(&wq_head->lock, flags);\nreturn ret;\n}\nEXPORT_SYMBOL(prepare_to_wait_event);\nIt looks like the only way we can break out of this with a non-zero exit code is if signal_pending_state() is true. Since our call site was just wait_event(), we know that state here is TASK_UNINTERRUPTIBLE; the definition of signal_pending_state() looks like:\nstatic inline int signal_pending_state(unsigned int state, struct task_struct *p)\n{\nif (!(state & (TASK_INTERRUPTIBLE | TASK_WAKEKILL)))\nreturn 0;\nif (!signal_pending(p))\nreturn 0;\nreturn (state & TASK_INTERRUPTIBLE) || __fatal_signal_pending(p);\n}\nOur task is not interruptible, so the first if fails. Our task should have a signal pending, though, right?\nstatic inline int signal_pending(struct task_struct *p)\n{\n/*\n* TIF_NOTIFY_SIGNAL isn't really a signal, but it requires the same\n* behavior in terms of ensuring that we break out of wait loops\n* so that notify signal callbacks can be processed.\n*/\nif (unlikely(test_tsk_thread_flag(p, TIF_NOTIFY_SIGNAL)))\nreturn 1;\nreturn task_sigpending(p);\n}\nAs the comment notes, TIF_NOTIFY_SIGNAL isn’t relevant here, in spite of its name, but let’s look at task_sigpending():\nstatic inline int task_sigpending(struct task_struct *p)\n{\nreturn unlikely(test_tsk_thread_flag(p,TIF_SIGPENDING));\n}\nHmm. Seems like we should have that flag set, right? To figure that out, let’s look at how signal delivery works. When we’re shutting down the pid namespace in zap_pid_ns_processes(), it does:\ngroup_send_sig_info(SIGKILL, SEND_SIG_PRIV, task, PIDTYPE_MAX);\nwhich eventually gets to __send_signal_locked(), which has:\npending = (type != PIDTYPE_PID) ? &t->signal->shared_pending : &t->pending;\n...\nsigaddset(&pending->signal, sig);\n...\ncomplete_signal(sig, t, type);\nUsing PIDTYPE_MAX here as the type is a little weird, but it roughly indicates “this is very privileged kernel stuff sending this signal, you should definitely deliver it”. There is a bit of unintended consequence here, though, in that __send_signal_locked() ends up sending the SIGKILL to the shared set, instead of the individual task’s set. If we look at the __fatal_signal_pending() code, we see:\nstatic inline int __fatal_signal_pending(struct task_struct *p)\n{\nreturn unlikely(sigismember(&p->pending.signal, SIGKILL));\n}\nBut it turns out this is a bit of a red herring (although it took a while for me to understand that).\nHow Signals Actually Get Delivered To a Process\nTo understand what’s really going on here, we need to look at complete_signal(), since it unconditionally adds a SIGKILL to the task’s pending set:\nsigaddset(&t->pending.signal, SIGKILL);\nbut why doesn’t it work? At the top of the function we have:\n/*\n* Now find a thread we can wake up to take the signal off the queue.\n*\n* If the main thread wants the signal, it gets first crack.\n* Probably the least surprising to the average bear.\n*/\nif (wants_signal(sig, p))\nt = p;\nelse if ((type == PIDTYPE_PID) || thread_group_empty(p))\n/*\n* There is just one thread and it does not need to be woken.\n* It will dequeue unblocked signals before it runs again.\n*/\nreturn;\nbut as Eric Biederman described, basically every thread can handle a SIGKILL at any time. Here’s wants_signal():\nstatic inline bool wants_signal(int sig, struct task_struct *p)\n{\nif (sigismember(&p->blocked, sig))\nreturn false;\nif (p->flags & PF_EXITING)\nreturn false;\nif (sig == SIGKILL)\nreturn true;\nif (task_is_stopped_or_traced(p))\nreturn false;\nreturn task_curr(p) || !task_sigpending(p);\n}\nSo… if a thread is already exiting (i.e. it has PF_EXITING), it doesn’t want a signal. Consider the following sequence of events:\n1. a task opens a FUSE file, and doesn’t close it, then exits. During that exit, the kernel dutifully calls do_exit(), which does the following:\nexit_signals(tsk); /* sets PF_EXITING */\n2. do_exit() continues on to exit_files(tsk);, which flushes all files that are still open, resulting in the stack trace above.\n3. the pid namespace exits, and enters zap_pid_ns_processes(), sends a SIGKILL to everyone (that it expects to be fatal), and then waits for everyone to exit.\n4. this kills the FUSE daemon in the pid ns so it can never respond.\n5. complete_signal() for the FUSE task that was already exiting ignores the signal, since it has PF_EXITING.\n6. Deadlock. Without manually aborting the FUSE connection, things will hang forever.\nSolution: don’t wait!\nIt doesn’t really make sense to wait for flushes in this case: the task is dying, so there’s nobody to tell the return code of flush() to. It also turns out that this bug can happen with several filesystems (anything that calls the kernel’s wait code in flush(), i.e. basically anything that talks to something outside the local kernel).\nIndividual filesystems will need to be patched in the meantime, for example the fix for FUSE is here, which was released on April 23 in Linux 6.3.\nWhile this blog post addresses FUSE deadlocks, there are definitely issues in the nfs code and elsewhere, which we have not hit in production yet, but almost certainly will. You can also see it as a symptom of other filesystem bugs. Something to look out for if you have a pid namespace that won’t exit.\nThis is just a small taste of the variety of strange issues we encounter running containers at scale at Netflix. Our team is hiring, so please reach out if you also love red herrings and kernel deadlocks!",
      "markdown": "## Debugging a FUSE deadlock in the Linux kernel\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----c75cd7989b6d--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----c75cd7989b6d--------------------------------)\n\n[Tycho Andersen](https://tycho.pizza/)\n\nThe Compute team at Netflix is charged with managing all AWS and containerized workloads at Netflix, including autoscaling, deployment of containers, issue remediation, etc. As part of this team, I work on fixing strange things that users report.\n\nThis particular issue involved a custom internal [FUSE filesystem](https://www.kernel.org/doc/html/latest/filesystems/fuse.html): [ndrive](https://netflixtechblog.com/netflix-drive-a607538c3055). It had been festering for some time, but needed someone to sit down and look at it in anger. This blog post describes how I poked at `/proc`to get a sense of what was going on, before posting the issue to the kernel mailing list and getting schooled on how the kernel’s wait code actually works!\n\n## Symptom: Stuck Docker Kill & A Zombie Process\n\nWe had a stuck docker API call:\n\ngoroutine 146 \\[select, 8817 minutes\\]:  \nnet/http.(\\*persistConn).roundTrip(0xc000658fc0, 0xc0003fc080, 0x0, 0x0, 0x0)  \n        /usr/local/go/src/net/http/transport.go:2610 +0x765  \nnet/http.(\\*Transport).roundTrip(0xc000420140, 0xc000966200, 0x30, 0x1366f20, 0x162)  \n        /usr/local/go/src/net/http/transport.go:592 +0xacb  \nnet/http.(\\*Transport).RoundTrip(0xc000420140, 0xc000966200, 0xc000420140, 0x0, 0x0)  \n        /usr/local/go/src/net/http/roundtrip.go:17 +0x35  \nnet/http.send(0xc000966200, 0x161eba0, 0xc000420140, 0x0, 0x0, 0x0, 0xc00000e050, 0x3, 0x1, 0x0)  \n        /usr/local/go/src/net/http/client.go:251 +0x454  \nnet/http.(\\*Client).send(0xc000438480, 0xc000966200, 0x0, 0x0, 0x0, 0xc00000e050, 0x0, 0x1, 0x10000168e)  \n        /usr/local/go/src/net/http/client.go:175 +0xff  \nnet/http.(\\*Client).do(0xc000438480, 0xc000966200, 0x0, 0x0, 0x0)  \n        /usr/local/go/src/net/http/client.go:717 +0x45f  \nnet/http.(\\*Client).Do(...)  \n        /usr/local/go/src/net/http/client.go:585  \ngolang.org/x/net/context/ctxhttp.Do(0x163bd48, 0xc000044090, 0xc000438480, 0xc000966100, 0x0, 0x0, 0x0)  \n        /go/pkg/mod/golang.org/x/net@v0.0.0-20211209124913-491a49abca63/context/ctxhttp/ctxhttp.go:27 +0x10f  \ngithub.com/docker/docker/client.(\\*Client).doRequest(0xc0001a8200, 0x163bd48, 0xc000044090, 0xc000966100, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)  \n        /go/pkg/mod/github.com/moby/moby@v0.0.0-20190408150954-50ebe4562dfc/client/request.go:132 +0xbe  \ngithub.com/docker/docker/client.(\\*Client).sendRequest(0xc0001a8200, 0x163bd48, 0xc000044090, 0x13d8643, 0x3, 0xc00079a720, 0x51, 0x0, 0x0, 0x0, ...)  \n        /go/pkg/mod/github.com/moby/moby@v0.0.0-20190408150954-50ebe4562dfc/client/request.go:122 +0x156  \ngithub.com/docker/docker/client.(\\*Client).get(...)  \n        /go/pkg/mod/github.com/moby/moby@v0.0.0-20190408150954-50ebe4562dfc/client/request.go:37  \ngithub.com/docker/docker/client.(\\*Client).ContainerInspect(0xc0001a8200, 0x163bd48, 0xc000044090, 0xc0006a01c0, 0x40, 0x0, 0x0, 0x0, 0x0, 0x0, ...)  \n        /go/pkg/mod/github.com/moby/moby@v0.0.0-20190408150954-50ebe4562dfc/client/container\\_inspect.go:18 +0x128  \ngithub.com/Netflix/titus-executor/executor/runtime/docker.(\\*DockerRuntime).Kill(0xc000215180, 0x163bdb8, 0xc000938600, 0x1, 0x0, 0x0)  \n        /var/lib/buildkite-agent/builds/ip-192-168-1-90-1/netflix/titus-executor/executor/runtime/docker/docker.go:2835 +0x310  \ngithub.com/Netflix/titus-executor/executor/runner.(\\*Runner).doShutdown(0xc000432dc0, 0x163bd10, 0xc000938390, 0x1, 0xc000b821e0, 0x1d, 0xc0005e4710)  \n        /var/lib/buildkite-agent/builds/ip-192-168-1-90-1/netflix/titus-executor/executor/runner/runner.go:326 +0x4f4  \ngithub.com/Netflix/titus-executor/executor/runner.(\\*Runner).startRunner(0xc000432dc0, 0x163bdb8, 0xc00071e0c0, 0xc0a502e28c08b488, 0x24572b8, 0x1df5980)  \n        /var/lib/buildkite-agent/builds/ip-192-168-1-90-1/netflix/titus-executor/executor/runner/runner.go:122 +0x391  \ncreated by github.com/Netflix/titus-executor/executor/runner.StartTaskWithRuntime  \n        /var/lib/buildkite-agent/builds/ip-192-168-1-90-1/netflix/titus-executor/executor/runner/runner.go:81 +0x411\n\nHere, our management engine has made an HTTP call to the Docker API’s unix socket asking it to kill a container. Our containers are configured to be killed via `SIGKILL`. But this is strange. `kill(SIGKILL)` should be relatively fatal, so what is the container doing?\n\n$ docker exec -it 6643cd073492 bash  \nOCI runtime exec failed: exec failed: container\\_linux.go:380: starting container process caused: process\\_linux.go:130: executing setns process caused: exit status 1: unknown\n\nHmm. Seems like it’s alive, but `setns(2)` fails. Why would that be? If we look at the process tree via `ps awwfux`, we see:\n\n\\\\\\_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/6643cd073492ba9166100ed30dbe389ff1caef0dc3d35  \n|  \\\\\\_ \\[docker-init\\]  \n|      \\\\\\_ \\[ndrive\\] <defunct>\n\nOk, so the container’s init process is still alive, but it has one zombie child. What could the container’s init process possibly be doing?\n\n\\# cat /proc/1528591/stack  \n\\[<0>\\] do\\_wait+0x156/0x2f0  \n\\[<0>\\] kernel\\_wait4+0x8d/0x140  \n\\[<0>\\] zap\\_pid\\_ns\\_processes+0x104/0x180  \n\\[<0>\\] do\\_exit+0xa41/0xb80  \n\\[<0>\\] do\\_group\\_exit+0x3a/0xa0  \n\\[<0>\\] \\_\\_x64\\_sys\\_exit\\_group+0x14/0x20  \n\\[<0>\\] do\\_syscall\\_64+0x37/0xb0  \n\\[<0>\\] entry\\_SYSCALL\\_64\\_after\\_hwframe+0x44/0xae\n\nIt is in the process of exiting, but it seems stuck. The only child is the ndrive process in Z (i.e. “zombie”) state, though. Zombies are processes that have successfully exited, and are waiting to be reaped by a corresponding `wait()` syscall from their parents. So how could the kernel be stuck waiting on a zombie?\n\n\\# ls /proc/1544450/task  \n1544450  1544574\n\nAh ha, there are two threads in the thread group. One of them is a zombie, maybe the other one isn’t:\n\n\\# cat /proc/1544574/stack  \n\\[<0>\\] request\\_wait\\_answer+0x12f/0x210  \n\\[<0>\\] fuse\\_simple\\_request+0x109/0x2c0  \n\\[<0>\\] fuse\\_flush+0x16f/0x1b0  \n\\[<0>\\] filp\\_close+0x27/0x70  \n\\[<0>\\] put\\_files\\_struct+0x6b/0xc0  \n\\[<0>\\] do\\_exit+0x360/0xb80  \n\\[<0>\\] do\\_group\\_exit+0x3a/0xa0  \n\\[<0>\\] get\\_signal+0x140/0x870  \n\\[<0>\\] arch\\_do\\_signal\\_or\\_restart+0xae/0x7c0  \n\\[<0>\\] exit\\_to\\_user\\_mode\\_prepare+0x10f/0x1c0  \n\\[<0>\\] syscall\\_exit\\_to\\_user\\_mode+0x26/0x40  \n\\[<0>\\] do\\_syscall\\_64+0x46/0xb0  \n\\[<0>\\] entry\\_SYSCALL\\_64\\_after\\_hwframe+0x44/0xae\n\nIndeed it is not a zombie. It is trying to become one as hard as it can, but it’s blocking inside FUSE for some reason. To find out why, let’s look at some kernel code. If we look at `[zap_pid_ns_processes()](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/kernel/pid_namespace.c?h=v5.19#n166)`, it does:\n\n/\\*  \n \\* Reap the EXIT\\_ZOMBIE children we had before we ignored SIGCHLD.  \n \\* kernel\\_wait4() will also block until our children traced from the  \n \\* parent namespace are detached and become EXIT\\_DEAD.  \n \\*/  \ndo {  \n        clear\\_thread\\_flag(TIF\\_SIGPENDING);  \n        rc = kernel\\_wait4(-1, NULL, \\_\\_WALL, NULL);  \n} while (rc != -ECHILD);\n\nwhich is where we are stuck, but before that, it has done:\n\n/\\* Don't allow any more processes into the pid namespace \\*/  \ndisable\\_pid\\_allocation(pid\\_ns);\n\nwhich is why docker can’t `setns()` — the _namespace_ is a zombie. Ok, so we can’t `setns(2)`, but why are we stuck in `kernel_wait4()`? To understand why, let’s look at what the other thread was doing in FUSE’s `[request_wait_answer()](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/fs/fuse/dev.c?h=v5.19#n407)`:\n\n/\\*  \n \\* Either request is already in userspace, or it was forced.  \n \\* Wait it out.  \n \\*/  \nwait\\_event(req->waitq, test\\_bit(FR\\_FINISHED, &req->flags));\n\nOk, so we’re waiting for an event (in this case, that userspace has replied to the FUSE flush request). But `zap_pid_ns_processes()`sent a `SIGKILL`! `SIGKILL` should be very fatal to a process. If we look at the process, we can indeed see that there’s a pending `SIGKILL`:\n\n\\# grep Pnd /proc/1544574/status  \nSigPnd: 0000000000000000  \nShdPnd: 0000000000000100\n\nViewing process status this way, you can see `0x100` (i.e. the 9th bit is set) under `ShdPnd`, which is the signal number corresponding to `SIGKILL`. Pending signals are signals that have been generated by the kernel, but have not yet been delivered to userspace. Signals are only delivered at certain times, for example when entering or leaving a syscall, or when waiting on events. If the kernel is currently doing something on behalf of the task, the signal may be pending. Signals can also be blocked by a task, so that they are never delivered. Blocked signals will show up in their respective pending sets as well. However, `man 7 signal` says: “The signals `SIGKILL` and `SIGSTOP` cannot be caught, blocked, or ignored.” But here the kernel is telling us that we have a pending `SIGKILL`, aka that it is being ignored even while the task is waiting!\n\n## Red Herring: How do Signals Work?\n\nWell that is weird. The wait code (i.e. `include/linux/wait.h`) is used everywhere in the kernel: semaphores, wait queues, completions, etc. Surely it knows to look for `SIGKILL`s. So what does `wait_event()` actually do? Digging through the macro expansions and wrappers, the meat of it is:\n\n#define \\_\\_\\_wait\\_event(wq\\_head, condition, state, exclusive, ret, cmd)           \\\\  \n({                                                                              \\\\  \n        \\_\\_label\\_\\_ \\_\\_out;                                                        \\\\  \n        struct wait\\_queue\\_entry \\_\\_wq\\_entry;                                     \\\\  \n        long \\_\\_ret = ret;       /\\* explicit shadow \\*/                           \\\\  \n                                                                                \\\\  \n        init\\_wait\\_entry(&\\_\\_wq\\_entry, exclusive ? WQ\\_FLAG\\_EXCLUSIVE : 0);        \\\\  \n        for (;;) {                                                              \\\\  \n                long \\_\\_int = prepare\\_to\\_wait\\_event(&wq\\_head, &\\_\\_wq\\_entry, state);\\\\  \n                                                                                \\\\  \n                if (condition)                                                  \\\\  \n                        break;                                                  \\\\  \n                                                                                \\\\  \n                if (\\_\\_\\_wait\\_is\\_interruptible(state) && \\_\\_int) {                 \\\\  \n                        \\_\\_ret = \\_\\_int;                                          \\\\  \n                        goto \\_\\_out;                                             \\\\  \n                }                                                               \\\\  \n                                                                                \\\\  \n                cmd;                                                            \\\\  \n        }                                                                       \\\\  \n        finish\\_wait(&wq\\_head, &\\_\\_wq\\_entry);                                     \\\\  \n\\_\\_out:  \\_\\_ret;                                                                  \\\\  \n})\n\nSo it loops forever, doing `prepare_to_wait_event()`, checking the condition, then checking to see if we need to interrupt. Then it does `cmd`, which in this case is `schedule()`, i.e. “do something else for a while”. `prepare_to_wait_event()` looks like:\n\nlong prepare\\_to\\_wait\\_event(struct wait\\_queue\\_head \\*wq\\_head, struct wait\\_queue\\_entry \\*wq\\_entry, int state)  \n{  \n        unsigned long flags;  \n        long ret = 0;\n\n        spin\\_lock\\_irqsave(&wq\\_head->lock, flags);  \n        if (signal\\_pending\\_state(state, current)) {  \n                /\\*  \n                 \\* Exclusive waiter must not fail if it was selected by wakeup,  \n                 \\* it should \"consume\" the condition we were waiting for.  \n                 \\*  \n                 \\* The caller will recheck the condition and return success if  \n                 \\* we were already woken up, we can not miss the event because  \n                 \\* wakeup locks/unlocks the same wq\\_head->lock.  \n                 \\*  \n                 \\* But we need to ensure that set-condition + wakeup after that  \n                 \\* can't see us, it should wake up another exclusive waiter if  \n                 \\* we fail.  \n                 \\*/  \n                list\\_del\\_init(&wq\\_entry->entry);  \n                ret = -ERESTARTSYS;  \n        } else {  \n                if (list\\_empty(&wq\\_entry->entry)) {  \n                        if (wq\\_entry->flags & WQ\\_FLAG\\_EXCLUSIVE)  \n                                \\_\\_add\\_wait\\_queue\\_entry\\_tail(wq\\_head, wq\\_entry);  \n                        else  \n                                \\_\\_add\\_wait\\_queue(wq\\_head, wq\\_entry);  \n                }  \n                set\\_current\\_state(state);  \n        }  \n        spin\\_unlock\\_irqrestore(&wq\\_head->lock, flags);\n\n        return ret;  \n}  \nEXPORT\\_SYMBOL(prepare\\_to\\_wait\\_event);\n\nIt looks like the only way we can break out of this with a non-zero exit code is if `signal_pending_state()` is true. Since our call site was just `wait_event()`, we know that state here is `TASK_UNINTERRUPTIBLE`; the definition of `signal_pending_state()` looks like:\n\nstatic inline int signal\\_pending\\_state(unsigned int state, struct task\\_struct \\*p)  \n{  \n        if (!(state & (TASK\\_INTERRUPTIBLE | TASK\\_WAKEKILL)))  \n                return 0;  \n        if (!signal\\_pending(p))  \n                return 0;\n\n        return (state & TASK\\_INTERRUPTIBLE) || \\_\\_fatal\\_signal\\_pending(p);  \n}\n\nOur task is not interruptible, so the first if fails. Our task should have a signal pending, though, right?\n\nstatic inline int signal\\_pending(struct task\\_struct \\*p)  \n{  \n        /\\*  \n         \\* TIF\\_NOTIFY\\_SIGNAL isn't really a signal, but it requires the same  \n         \\* behavior in terms of ensuring that we break out of wait loops  \n         \\* so that notify signal callbacks can be processed.  \n         \\*/  \n        if (unlikely(test\\_tsk\\_thread\\_flag(p, TIF\\_NOTIFY\\_SIGNAL)))  \n                return 1;  \n        return task\\_sigpending(p);  \n}\n\nAs the comment notes, `TIF_NOTIFY_SIGNAL` isn’t relevant here, in spite of its name, but let’s look at `task_sigpending()`:\n\nstatic inline int task\\_sigpending(struct task\\_struct \\*p)  \n{  \n        return unlikely(test\\_tsk\\_thread\\_flag(p,TIF\\_SIGPENDING));  \n}\n\nHmm. Seems like we should have that flag set, right? To figure that out, let’s look at how signal delivery works. When we’re shutting down the pid namespace in `zap_pid_ns_processes()`, it does:\n\ngroup\\_send\\_sig\\_info(SIGKILL, SEND\\_SIG\\_PRIV, task, PIDTYPE\\_MAX);\n\nwhich eventually gets to `__send_signal_locked()`, which has:\n\npending = (type != PIDTYPE\\_PID) ? &t->signal->shared\\_pending : &t->pending;  \n...  \nsigaddset(&pending->signal, sig);  \n...  \ncomplete\\_signal(sig, t, type);\n\nUsing `PIDTYPE_MAX` here as the type is a little weird, but it roughly indicates “this is very privileged kernel stuff sending this signal, you should definitely deliver it”. There is a bit of unintended consequence here, though, in that `__send_signal_locked()` ends up sending the `SIGKILL` to the shared set, instead of the individual task’s set. If we look at the `__fatal_signal_pending()` code, we see:\n\nstatic inline int \\_\\_fatal\\_signal\\_pending(struct task\\_struct \\*p)  \n{  \n        return unlikely(sigismember(&p->pending.signal, SIGKILL));  \n}\n\nBut it turns out this is a bit of a red herring ([although](https://lore.kernel.org/all/YuGUyayVWDB7R89i@tycho.pizza/) [it](https://lore.kernel.org/all/20220728091220.GA11207@redhat.com/) [took](https://lore.kernel.org/all/871qu6bjp3.fsf@email.froward.int.ebiederm.org/) [a](https://lore.kernel.org/all/8735elhy4u.fsf@email.froward.int.ebiederm.org/) [while](https://lore.kernel.org/all/87pmhofr1q.fsf@email.froward.int.ebiederm.org/) for me to understand that).\n\n## How Signals Actually Get Delivered To a Process\n\nTo understand what’s really going on here, we need to look at `complete_signal()`, since it unconditionally adds a `SIGKILL` to the task’s pending set:\n\nsigaddset(&t->pending.signal, SIGKILL);\n\nbut why doesn’t it work? At the top of the function we have:\n\n/\\*  \n \\* Now find a thread we can wake up to take the signal off the queue.  \n \\*  \n \\* If the main thread wants the signal, it gets first crack.  \n \\* Probably the least surprising to the average bear.  \n \\*/  \nif (wants\\_signal(sig, p))  \n        t = p;  \nelse if ((type == PIDTYPE\\_PID) || thread\\_group\\_empty(p))  \n        /\\*  \n         \\* There is just one thread and it does not need to be woken.  \n         \\* It will dequeue unblocked signals before it runs again.  \n         \\*/  \n        return;\n\nbut as [Eric Biederman described](https://lore.kernel.org/all/877d4jbabb.fsf@email.froward.int.ebiederm.org/), basically every thread can handle a `SIGKILL` at any time. Here’s `wants_signal()`:\n\nstatic inline bool wants\\_signal(int sig, struct task\\_struct \\*p)  \n{  \n        if (sigismember(&p->blocked, sig))  \n                return false;\n\n        if (p->flags & PF\\_EXITING)  \n                return false;\n\n        if (sig == SIGKILL)  \n                return true;\n\n        if (task\\_is\\_stopped\\_or\\_traced(p))  \n                return false;\n\n        return task\\_curr(p) || !task\\_sigpending(p);  \n}\n\nSo… if a thread is already exiting (i.e. it has `PF_EXITING`), it doesn’t want a signal. Consider the following sequence of events:\n\n1\\. a task opens a FUSE file, and doesn’t close it, then exits. During that exit, the kernel dutifully calls `do_exit()`, which does the following:\n\nexit\\_signals(tsk); /\\* sets PF\\_EXITING \\*/\n\n2\\. `do_exit()` continues on to `exit_files(tsk);`, which flushes all files that are still open, resulting in the stack trace above.\n\n3\\. the pid namespace exits, and enters `zap_pid_ns_processes()`, sends a `SIGKILL` to everyone (that it expects to be fatal), and then waits for everyone to exit.\n\n4\\. this kills the FUSE daemon in the pid ns so it can never respond.\n\n5\\. `complete_signal()` for the FUSE task that was already exiting ignores the signal, since it has `PF_EXITING`.\n\n6\\. Deadlock. Without manually aborting the FUSE connection, things will hang forever.\n\n## Solution: don’t wait!\n\nIt doesn’t really make sense to wait for flushes in this case: the task is dying, so there’s nobody to tell the return code of `flush()` to. It also turns out that this bug can happen with several filesystems (anything that calls the kernel’s wait code in `flush()`, i.e. basically anything that talks to something outside the local kernel).\n\nIndividual filesystems will need to be patched in the meantime, for example the fix for FUSE is [here](https://github.com/torvalds/linux/commit/14feceeeb012faf9def7d313d37f5d4f85e6572b), which was released on April 23 in Linux 6.3.\n\nWhile this blog post addresses FUSE deadlocks, there are definitely issues in the nfs code and elsewhere, which we have not hit in production yet, but almost certainly will. You can also see it as a [symptom of other filesystem bugs](https://lore.kernel.org/all/20230512225414.GE3223426@dread.disaster.area/). Something to look out for if you have a pid namespace that won’t exit.\n\nThis is just a small taste of the variety of strange issues we encounter running containers at scale at Netflix. Our team is hiring, so please reach out if you also love red herrings and kernel deadlocks!"
    },
    {
      "url": "https://netflixtechblog.com/escrow-buddy-an-open-source-tool-from-netflix-for-remediation-of-missing-filevault-keys-in-mdm-815aef5107cd?source=collection_home---4------10-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/escrow-buddy-an-open-source-tool-from-netflix-for-remediation-of-missing-filevault-keys-in-mdm-815aef5107cd?gi=6f100d692fe5&source=collection_home---4------10-----------------------",
        "loadedTime": "2023-12-06T00:03:36.376Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/escrow-buddy-an-open-source-tool-from-netflix-for-remediation-of-missing-filevault-keys-in-mdm-815aef5107cd",
        "title": "Escrow Buddy: An open-source tool from Netflix for remediation of missing FileVault keys in MDM | by Netflix Technology Blog | Netflix TechBlog",
        "description": "To be a client systems engineer is to take joy in small endpoint automations that make your fellow employees’ day a little better. When somebody is unable to log into their FileVault-encrypted Mac…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Escrow Buddy: An open-source tool from Netflix for remediation of missing FileVault keys in MDM\nNetflix has open-sourced Escrow Buddy, which helps Security and IT teams ensure they have valid FileVault recovery keys for all their Macs in MDM.\nTo be a client systems engineer is to take joy in small endpoint automations that make your fellow employees’ day a little better. When somebody is unable to log into their FileVault-encrypted Mac, few words are more joyful to hear than a support technician saying, “I’ve got your back. Let’s look up the recovery key.”\nSecurely and centrally escrowing FileVault personal recovery keys is one of many capabilities offered by Mobile Device Management (MDM). A configuration profile that contains the FDERecoveryKeyEscrow payload will cause any new recovery key generated on the device, either by initially enabling FileVault or by manually changing the recovery key, to be automatically escrowed to your MDM for later retrieval if needed.\nThe problem of missing FileVault keys\nHowever, just because you’re deploying the MDM escrow payload to your managed Macs doesn’t necessarily mean you have valid recovery keys for all of them. Recovery keys can be missing from MDM for numerous reasons:\nFileVault may have been enabled prior to enrollment in MDM\nThe MDM escrow payload may not have been present on the Mac due to scoping issues or misconfiguration on your MDM\nThe Macs may be migrating from a different MDM in which the keys are stored\nMDM database corruption or data loss events may have claimed some or all of your escrowed keys\nRegardless of the cause, the effect is people who get locked out of their Macs must resort to wiping their computer and starting fresh — a productivity killer if your data is backed up, and a massive data loss event if it’s not backed up.\nLess than ideal solutions\nIT and security teams have approached this problem from multiple angles in the past. On a per-computer basis, a new key can be generated by disabling and re-enabling FileVault, but this leaves the computer in an unencrypted state briefly and requires multiple steps. The built-in fdesetup command line tool can also be used to generate a new key, but not all users are comfortable entering Terminal commands. Plus, neither of these ideas scale to meet the needs of a fleet of Macs hundreds or thousands strong.\nAnother approach has been to use a tool capable of displaying an onscreen text input field to the user in order to display a password prompt, and then pass the provided password as input to the fdesetup tool for generating a new key. However, this requires IT and security teams to communicate in advance of the remediation campaign to affected users, in order to give them the context they need to respond to the additional password prompt. Even more concerning, this password prompt approach has a detrimental effect on security culture because it contributes to “consent fatigue.” Users will be more likely to approve other types of password prompt, which may inadvertently prime them to be targeted by malware or ransomware.\nThe ideal solution would be one which can be automated across your entire fleet while not requiring any additional user interaction.\nCrypt and its authorization plugin\nmacOS authorization plugins provide a way to connect with Apple’s authorization services API and participate in decisions around user login. They can also facilitate automations that require information available only in the “login window” context, such as the provided username and password.\nRelatively few authorization plugins are broadly used within the Mac admin community, but one popular example is the Crypt agent. In its typical configuration the Crypt agent enforces FileVault upon login and escrows the resulting recovery key to a corresponding Crypt server. The agent also enables rotation of recovery keys after use, local storage and validation of recovery keys, and other features.\nWhile the Crypt agent can be deployed standalone and configured to simply regenerate a key upon next login, escrowing keys to MDM isn’t Crypt’s primary use case. Additionally, not all organizations have the time, expertise, or interest to commit to hosting a Crypt server and its accompanying database, or auditing the parts of Crypt’s codebase relating to its server capabilities.\nIntroducing Escrow Buddy\nInspired by Crypt’s example, our Client Systems Engineering team created a minimal authorization plugin focused on serving the needs of organizations who escrow FileVault keys to MDM only. We call this new tool Escrow Buddy.\nEscrow Buddy’s authorization plugin includes a mechanism that, when added to the macOS login authorization database, will use the logging in user’s credentials as input to the fdesetup tool to automatically and seamlessly generate a new key during login. By integrating with the familiar and trusted macOS login experience, Escrow Buddy eliminates the need to display additional prompts or on-screen messages.\nSecurity and IT teams can take advantage of Escrow Buddy in three steps:\nEnsure your MDM is deploying the FDERecoveryKeyEscrow payload to your managed Macs. This will ensure any newly generated FileVault key, no matter the method of generation, will be automatically escrowed to MDM.\nDeploy Escrow Buddy. The latest installer is available here, and you can choose to deploy to all your managed Macs or just the subset for which you need to escrow new keys.\nOn Macs that lack a valid escrowed key, configure your MDM to run this command in root context:\ndefaults write /Library/Preferences/com.netflix.Escrow-Buddy.plist GenerateNewKey -bool true\nThat’s it! At next startup or login, the specified Macs should generate a new key, which will be automatically escrowed to your MDM when the Mac next responds to a SecurityInfo command. (Timing varies by MDM vendor but this is often during an inventory update.)\nCommunity contribution\nNetflix is making Escrow Buddy’s source available via the Mac Admins Open Source organization on GitHub, the home of many other important projects in the Mac IT and security community, including Nudge, InstallApplications, Outset, and the Munki signed builds. Thousands of organizations worldwide benefit from the tools and ideas shared by the Mac admin community, and Netflix is excited that Escrow Buddy will be among them.\nThe Escrow Buddy repository leverages GitHub Actions to streamline the process of building new codesigned and notarized releases when new changes are merged into the main branch. Our hope is that this will make it easy for contributors to collaborate and improve upon Escrow Buddy.\nA rising tide…\nEscrow Buddy represents our desire to elevate the industry standard around FileVault key regeneration. If your organization currently employs a password prompt workflow for this scenario, please consider trying Escrow Buddy instead. We hope you’ll find it more automatic, more supportive of security culture, and enables you to more often say “I’ve got your back” to your fellow employees who need a recovery key.\n— Elliot Jordan",
      "markdown": "## Escrow Buddy: An open-source tool from Netflix for remediation of missing FileVault keys in MDM\n\n## Netflix has open-sourced Escrow Buddy, which helps Security and IT teams ensure they have valid FileVault recovery keys for all their Macs in MDM.\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----815aef5107cd--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----815aef5107cd--------------------------------)\n\nTo be a client systems engineer is to take joy in small endpoint automations that make your fellow employees’ day a little better. When somebody is unable to log into their FileVault-encrypted Mac, few words are more joyful to hear than a support technician saying, “I’ve got your back. Let’s look up the recovery key.”\n\nSecurely and centrally escrowing FileVault personal recovery keys is one of many capabilities offered by Mobile Device Management (MDM). A configuration profile that contains the [FDERecoveryKeyEscrow](https://developer.apple.com/documentation/devicemanagement/fderecoverykeyescrow) payload will cause any new recovery key generated on the device, either by initially enabling FileVault or by manually changing the recovery key, to be automatically escrowed to your MDM for later retrieval if needed.\n\n## The problem of missing FileVault keys\n\nHowever, just because you’re deploying the MDM escrow payload to your managed Macs doesn’t necessarily mean you have valid recovery keys for all of them. Recovery keys can be missing from MDM for numerous reasons:\n\n*   FileVault may have been enabled prior to enrollment in MDM\n*   The MDM escrow payload may not have been present on the Mac due to scoping issues or misconfiguration on your MDM\n*   The Macs may be migrating from a different MDM in which the keys are stored\n*   MDM database corruption or data loss events may have claimed some or all of your escrowed keys\n\nRegardless of the cause, the effect is people who get locked out of their Macs must resort to wiping their computer and starting fresh — a productivity killer if your data is backed up, and a massive data loss event if it’s not backed up.\n\n## Less than ideal solutions\n\nIT and security teams have approached this problem from multiple angles in the past. On a per-computer basis, a new key can be generated by disabling and re-enabling FileVault, but this leaves the computer in an unencrypted state briefly and requires multiple steps. The built-in `fdesetup` command line tool can also be used to generate a new key, but not all users are comfortable entering Terminal commands. Plus, neither of these ideas scale to meet the needs of a fleet of Macs hundreds or thousands strong.\n\nAnother approach has been to use a tool capable of displaying an onscreen text input field to the user in order to display a password prompt, and then pass the provided password as input to the `fdesetup` tool for generating a new key. However, this requires IT and security teams to communicate in advance of the remediation campaign to affected users, in order to give them the context they need to respond to the additional password prompt. Even more concerning, this password prompt approach has a detrimental effect on security culture because it contributes to “consent fatigue.” Users will be more likely to approve other types of password prompt, which may inadvertently prime them to be targeted by malware or ransomware.\n\nThe ideal solution would be one which can be automated across your entire fleet while not requiring any additional user interaction.\n\n## Crypt and its authorization plugin\n\n[macOS authorization plugins](https://developer.apple.com/documentation/security/authorization_plug-ins) provide a way to connect with Apple’s authorization services API and participate in decisions around user login. They can also facilitate automations that require information available only in the “login window” context, such as the provided username and password.\n\nRelatively few authorization plugins are broadly used within the Mac admin community, but one popular example is the [Crypt](https://github.com/grahamgilbert/crypt) agent. In its typical configuration the Crypt agent enforces FileVault upon login and escrows the resulting recovery key to a corresponding [Crypt server](https://github.com/grahamgilbert/crypt-server). The agent also enables rotation of recovery keys after use, local storage and validation of recovery keys, and other features.\n\nWhile the Crypt agent can be deployed standalone and configured to simply regenerate a key upon next login, escrowing keys to MDM isn’t Crypt’s primary use case. Additionally, not all organizations have the time, expertise, or interest to commit to hosting a Crypt server and its accompanying database, or auditing the parts of Crypt’s codebase relating to its server capabilities.\n\n## Introducing Escrow Buddy\n\nInspired by Crypt’s example, our Client Systems Engineering team created a minimal authorization plugin focused on serving the needs of organizations who escrow FileVault keys to MDM only. We call this new tool **Escrow Buddy**.\n\n[](https://github.com/macadmins/escrow-buddy)\n\nEscrow Buddy’s authorization plugin includes a mechanism that, when added to the macOS login authorization database, will use the logging in user’s credentials as input to the **fdesetup** tool to automatically and seamlessly generate a new key during login. By integrating with the familiar and trusted macOS login experience, Escrow Buddy eliminates the need to display additional prompts or on-screen messages.\n\nSecurity and IT teams can take advantage of Escrow Buddy in three steps:\n\n1.  **Ensure your MDM is deploying the** [**FDERecoveryKeyEscrow**](https://developer.apple.com/documentation/devicemanagement/fderecoverykeyescrow) **payload** to your managed Macs**.** This will ensure any newly generated FileVault key, no matter the method of generation, will be automatically escrowed to MDM.\n2.  **Deploy Escrow Buddy.** The latest installer is available [here](https://github.com/macadmins/escrow-buddy/releases/latest), and you can choose to deploy to all your managed Macs or just the subset for which you need to escrow new keys.\n3.  On Macs that lack a valid escrowed key, **configure your MDM to run this command in root context**:\n\ndefaults write /Library/Preferences/com.netflix.Escrow-Buddy.plist GenerateNewKey -bool true\n\nThat’s it! At next startup or login, the specified Macs should generate a new key, which will be automatically escrowed to your MDM when the Mac next [responds to a SecurityInfo command](https://developer.apple.com/documentation/devicemanagement/securityinforesponse/securityinfo). (Timing varies by MDM vendor but this is often during an inventory update.)\n\n## Community contribution\n\nNetflix is making Escrow Buddy’s source available via the Mac Admins Open Source organization on GitHub, the home of many other important projects in the Mac IT and security community, including [Nudge](https://github.com/macadmins/nudge), [InstallApplications](https://github.com/macadmins/installapplications), [Outset](https://github.com/macadmins/outset), and the [Munki signed builds](https://github.com/macadmins/munki-builds). Thousands of organizations worldwide benefit from the tools and ideas shared by the Mac admin community, and Netflix is excited that Escrow Buddy will be among them.\n\nThe [Escrow Buddy repository](https://github.com/macadmins/escrow-buddy) leverages GitHub Actions to streamline the process of building new codesigned and notarized releases when new changes are merged into the `main` branch. Our hope is that this will make it easy for contributors to collaborate and improve upon Escrow Buddy.\n\n## A rising tide…\n\nEscrow Buddy represents our desire to elevate the industry standard around FileVault key regeneration. If your organization currently employs a password prompt workflow for this scenario, please consider trying Escrow Buddy instead. We hope you’ll find it more automatic, more supportive of security culture, and enables you to more often say “I’ve got your back” to your fellow employees who need a recovery key.\n\n— [Elliot Jordan](https://www.linkedin.com/in/reallyelliot/)"
    },
    {
      "url": "https://netflixtechblog.com/native-frame-rate-playback-6c87836a948?source=collection_home---4------11-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/native-frame-rate-playback-6c87836a948?gi=b339b1cd2028&source=collection_home---4------11-----------------------",
        "loadedTime": "2023-12-06T00:03:37.778Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/native-frame-rate-playback-6c87836a948",
        "title": "Native Frame Rate Playback. This article talks about a novel HDMI… | by Netflix Technology Blog | Netflix TechBlog",
        "description": "Maximizing immersion for our members is an important goal for the Netflix product and engineering teams to keep our members entertained and fully engaged in our content. Leveraging a good mix of…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Native Frame Rate Playback\nby Akshay Garg, Roger Quero\nIntroduction\nMaximizing immersion for our members is an important goal for the Netflix product and engineering teams to keep our members entertained and fully engaged in our content. Leveraging a good mix of mature and cutting-edge client device technologies to deliver a smooth playback experience with glitch-free in-app transitions is an important step towards achieving this goal. In this article we explain our journey towards productizing a better viewing experience for our members by utilizing features and capabilities in consumer streaming devices.\nIf you have a streaming device connected to your TV, such as a Roku Set Top Box (STB) or an Amazon FireTV Stick, you may have come across an option in the device display setting pertaining to content frame rate. Device manufacturers often call this feature “Match Content Frame Rate”, “Auto adjust display refresh rate” or something similar. If you’ve ever wondered what these features are and how they can improve your viewing experience, keep reading — the following sections cover the basics of this feature and explain the details of how the Netflix application uses it.\nProblem\nNetflix’s content catalog is composed of video captured and encoded in one of various frame rates ranging from 23.97 to 60 frames per second (fps). When a member chooses to watch a movie or a TV show on a source device (ex. Set-top box, Streaming stick, Game Console, etc…) the content is delivered and then decoded at its native frame rate, which is the frame rate it was captured and encoded in. After the decode step, the source device converts it to the HDMI output frame rate which was configured based on the capabilities of the HDMI input port of the connected sink device (TV, AVR, Monitor etc). In general, the output frame rate over HDMI is automatically set to 50fps for PAL regions and 60fps for NTSC regions.\nNetflix offers limited high frame rate content (50fps or 60fps), but the majority of our catalog and viewing hours can be attributed to members watching 23.97 to 30fps content. This essentially means that most of the time, our content goes through a process called frame rate conversion (aka FRC) on the source device which converts the content from its native frame rate to match the HDMI output frame rate by replicating frames. Figure 1 illustrates a simple FRC algorithm that converts 24fps content to 60fps.\nFigure 1 : 3:2 pulldown technique to convert 24FPS content to 60FPS\nConverting the content and transmitting it over HDMI at the output frame rate sounds logical and straightforward. In fact, FRC works well when the output frame rate is an integer multiple of the native frame rate ( ex. 24→48, 25→50, 30→60, 24→120, etc…). On the other hand, FRC introduces a visual artifact called Judder when non-integer multiple conversion is required (ex. 24→60, 25→60, etc…), which manifests as choppy video playback as illustrated below:\nWith JudderWithout Judder\nIt is important to note that the severity of the judder depends on the replication pattern. For this reason, judder is more prominent in PAL regions because of the process of converting 24fps content to 50fps over HDMI (see Figure 2):\nTotal of 50 frames must be transmitted over HDMI per second\nSource device must replicate the original 24 frames to fill in the missing 26 frames\n50 output frames from 24 original frames are derived as follows:\n22 frames are duplicated ( total of 44 frames )\n2 frames are repeated three times ( total of 6 frames )\nFigure 2: Example of a 24 to 50fps frame rate conversion algorithm\nAs a review, judder is more pronounced when the frequency of the number of repeated frames is inconsistent and spread out e.g. in the scenario mentioned above, the frame replication factor varies between 2 and 3 resulting in a more prominent judder.\nJudder Mitigation Solutions\nNow that we have a better understanding of the issue, let’s review the solutions that Netflix has invested in. Due to the fragmented nature of device capabilities in the ecosystem, we explored multiple solutions to address this issue for as many devices as possible. Each unique solution leverages existing or new source device capabilities and comes with various tradeoffs.\nSolution #1: Match HDMI frame rate to content Native Frame Rate\nThe first solution we explored and recently enabled leverages the capability of existing source & sink devices to change the outgoing frame rate on the HDMI link. Once this feature is enabled in the system settings, devices will match the HDMI output frame rate with the content frame rate, either exactly or an integer multiple, without user intervention.\nWhile this sounds like the perfect solution, devices that support older HDMI technologies e.g. HDMI v<2.1, can’t change the frame rate without also changing the HDMI data rate. This results in what is often referred as an “HDMI bonk” which causes the TV to display a blank screen momentarily. Not only is this a disruptive experience for members, but the duration of the blank screen varies depending on how fast the source and sink devices can resynchronize. Figure 3 below is an example of how this transition looks:\nFigure 3: Native frame rate experience with screen blanking\nSolution #2 : Match HDMI frame rate to content Native Frame Rate w/o screen blanking\nImprovements in the recent HDMI standards (HDMI 2.1+) now allow a source device to send the video content at its native frame rate without needing an HDMI resynchronization. This is possible through an innovative technology called Quick Media Switching (QMS) which is an extension of Variable Refresh Rate (VRR) targeted for content playback scenarios. QMS allows a source device to maintain a constant data rate on the HDMI link even during transmission of content with different frame rates. It does so by adjusting the amount of non-visible padding data while keeping the amount of visible video data constant. Due to the constant HDMI data rate, the HDMI transmitter and receiver don’t need to resynchronize, leading to a seamless/glitch-free transition as illustrated in Figure 4.\nHDMI QMS is positioned to be the ideal solution to address the problem we are presenting. Unfortunately, at present, this technology is relatively new and adoption into source and sink devices will take time.\nFigure 4: Native frame rate experience without screen blanking using HDMI QMS\nSolution #3: Frame Rate Conversion within Netflix Application\nApart from the above HDMI specification dependent solutions, it is possible for an application like Netflix to manipulate the presentation time stamp value of each video frame to minimize the effect of judder i.e. the application can present video frames to the underlying source device platform at a cadence that can help the source device to minimize the judder associated with FRC on the HDMI output link.\nLet us understand this idea with the help of an example. Let’s go back to the same 24 to 50 fps FRC scenario that was covered earlier. But, instead of thinking about the FRC rate per second (24 ⇒ 50 fps), let’s expand the FRC calculation time period to 3 seconds (24*3 = 72 ⇒50*3 = 150 fps). For content with a native frame rate of 24 fps, the source device needs to get 72 frames from the streaming application in a period of 3 seconds. Now instead of sending 24 frames per second at a regular per second cadence, for each 3 second period the Netflix application can decide to send 25 frames in the first 2 seconds (25 x 2 = 50) and 22 frames in the 3rd second thereby still sending a total of 72 (50+22) frames in 3 seconds. This approach creates an even FRC in the first 2 seconds (25 frames replicated twice evenly) and in the 3rd second the source device can do a 22 to 50 fps FRC which will create less visual judder compared to the 24->50 fps FRC given a more even frame replication pattern. This concept is illustrated in Figure 5 below.\nFigure 5: FRC Algorithm from Solution#3 for 24 to 50 fps conversion\nNOTE: This solution was developed by David Zheng in the Partner Experience Technology team at Netflix. Watch out for an upcoming article going into further details of this solution.\nHow the Netflix Application Uses these Solutions\nGiven the possible solutions available to use and the associated benefits and limitations, the Netflix application running on a source device adapts to use one of these approaches based on factors such as source and sink device capabilities, user preferences and the specific use case within the Netflix application. Let’s walk through each of these aspects briefly.\nDevice Capability\nEvery source device that integrates the Netflix application is required to let the application know if it and the connected sink device have the ability to send and receive video content at its native frame rate. In addition, a source device is required to inform whether it can support QMS and perform a seamless playback start of any content at its native frame rate on the connected HDMI link.\nAs discussed in the introduction section, the presence of a system setting like “Match Content Frame Rate” typically indicates that a source device is capable of this feature.\nUser Preference\nEven if a source device and the connected sink can support Native content frame rate streaming (seamless or non-seamless), a user might have selected not to do this via the source device system settings e.g. “Match Content Frame Rate” set to “Never”. Or they might have indicated a preference of doing this only when the native content frame rate play start can happen in a seamless manner e.g. “Match Content Frame Rate” set to “Seamless”.\nThe Netflix application needs to know this user selection in order to honor their preference. Hence, source devices are expected to relay this user preference to the Netflix application to help with this run-time decision making.\nNetflix Use Case\nIn spite of source device capability and the user preferences collectively indicating that the Native Content Frame Rate streaming should be enabled, the Netflix application can decide to disable this feature for specific member experiences. As an example, when the user is browsing Netflix content in the home UI, we cannot play Netflix trailers in their Native frame rate due to the following reasons:\nIf using Solution # 1, when the Netflix trailers are encoded in varying content frame rates, switching between trailers will result in screen blanking, thereby making the UI browsing unusable.\nIf using Solution # 2, sending Netflix trailers in their Native frame rate would mean that the associated UI components (movement of cursor, asset selection etc) would also be displayed at the reduced frame rate and this will result in a sluggish UI browsing experience. This is because on HDMI output from the source device, both graphics (Netflix application UI) and video components will go out at the same frame rate (native content frame rate of the trailer) after being blended together on the source device.\nTo handle these issues we follow an approach as shown in Figure 6 below where we enable the Native Frame Rate playback experience only when the user selects a title and watches it in full screen with minimal graphical UI elements.\nFigure 6: Native Frame Rate usage within Netflix application\nConclusion\nThis article presented features that aim to improve the content playback experience on HDMI source devices. The breadth of available technical solutions, user selectable preferences, device capabilities and the application of each of these permutations in the context of various in-app member journeys represent a typical engineering and product decision framework at Netflix. Here at Netflix, our goal is to maximize immersion for our members through introduction of new features that will improve their viewing experience and keep them fully engaged in our content.\nAcknowledgements\nWe would like to acknowledge the hard work of a number of teams that came together to deliver the features being discussed in this document. These include Core UI and JS Player development, Netflix Application Software development, AV Test and Tooling (earlier article from this team), Partner Engineering and Product teams in the Consumer Engineering organization and our data science friends in the Data Science and Engineering organization at Netflix. Diagrams in this article are courtesy of our Partner Enterprise Platform XD team.",
      "markdown": "## **Native Frame Rate Playback**\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----6c87836a948--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----6c87836a948--------------------------------)\n\n_by_ [_Akshay Garg_](https://www.linkedin.com/in/akshaygarg05/)_,_ [_Roger Quero_](https://www.linkedin.com/in/rquero/)\n\n## Introduction\n\nMaximizing immersion for our members is an important goal for the Netflix product and engineering teams to keep our members entertained and fully engaged in our content. Leveraging a good mix of mature and cutting-edge client device technologies to deliver a smooth playback experience with glitch-free in-app transitions is an important step towards achieving this goal. In this article we explain our journey towards productizing a better viewing experience for our members by utilizing features and capabilities in consumer streaming devices.\n\nIf you have a streaming device connected to your TV, such as a Roku Set Top Box (STB) or an Amazon FireTV Stick, you may have come across an option in the device display setting pertaining to content frame rate. Device manufacturers often call this feature “Match Content Frame Rate”, “Auto adjust display refresh rate” or something similar. If you’ve ever wondered what these features are and how they can improve your viewing experience, keep reading — the following sections cover the basics of this feature and explain the details of how the Netflix application uses it.\n\n## Problem\n\nNetflix’s content catalog is composed of video captured and encoded in one of various frame rates ranging from 23.97 to 60 frames per second (fps). When a member chooses to watch a movie or a TV show on a **_source device_** (ex. Set-top box, Streaming stick, Game Console, etc…) the content is delivered and then decoded at its **_native frame rate_**, which is the frame rate it was captured and encoded in. After the decode step, the source device converts it to the HDMI output frame rate which was configured based on the capabilities of the HDMI input port of the connected **_sink device_** (TV, AVR, Monitor etc). In general, the output frame rate over HDMI is automatically set to 50fps for [PAL](https://en.wikipedia.org/wiki/PAL) regions and 60fps for [NTSC](https://en.wikipedia.org/wiki/NTSC) regions.\n\nNetflix offers limited high frame rate content (50fps or 60fps), but the majority of our catalog and viewing hours can be attributed to members watching 23.97 to 30fps content. This essentially means that most of the time, our content goes through a process called **_frame rate conversion_** (aka FRC) on the source device which converts the content from its native frame rate to match the HDMI output frame rate by replicating frames. Figure 1 illustrates a simple FRC algorithm that converts 24fps content to 60fps.\n\n**Figure 1 : 3:2 pulldown technique to convert 24FPS content to 60FPS**\n\nConverting the content and transmitting it over HDMI at the output frame rate sounds logical and straightforward. In fact, FRC works well when the output frame rate is an integer multiple of the native frame rate ( ex. 24→48, 25→50, 30→60, 24→120, etc…). On the other hand, FRC introduces a visual artifact called **Judder** when non-integer multiple conversion is required (ex. 24→60, 25→60, etc…), which manifests as choppy video playback as illustrated below:\n\n**With Judder**\n\n**Without Judder**\n\nIt is important to note that the severity of the judder depends on the replication pattern. For this reason, judder is more prominent in PAL regions because of the process of converting 24fps content to 50fps over HDMI (see Figure 2):\n\n*   Total of 50 frames must be transmitted over HDMI per second\n*   Source device must replicate the original 24 frames to fill in the missing 26 frames\n*   50 output frames from 24 original frames are derived as follows:\n*   22 frames are duplicated ( total of 44 frames )\n*   2 frames are repeated three times ( total of 6 frames )\n\n**Figure 2: Example of a 24 to 50fps frame rate conversion algorithm**\n\nAs a review, judder is more pronounced when the frequency of the number of repeated frames is inconsistent and spread out e.g. in the scenario mentioned above, the frame replication factor varies between 2 and 3 resulting in a more prominent judder.\n\n## Judder Mitigation Solutions\n\nNow that we have a better understanding of the issue, let’s review the solutions that Netflix has invested in. Due to the fragmented nature of device capabilities in the ecosystem, we explored multiple solutions to address this issue for as many devices as possible. Each unique solution leverages existing or new source device capabilities and comes with various tradeoffs.\n\n## Solution #1: Match HDMI frame rate to content Native Frame Rate\n\nThe first solution we explored and recently enabled leverages the capability of existing source & sink devices to change the outgoing frame rate on the HDMI link. Once this feature is enabled in the system settings, devices will match the HDMI output frame rate with the content frame rate, either exactly or an integer multiple, without user intervention.\n\nWhile this sounds like the perfect solution, devices that support older HDMI technologies e.g. HDMI v<2.1, can’t change the frame rate without also changing the HDMI data rate. This results in what is often referred as an “HDMI bonk” which causes the TV to display a blank screen momentarily. Not only is this a disruptive experience for members, but the duration of the blank screen varies depending on how fast the source and sink devices can resynchronize. Figure 3 below is an example of how this transition looks:\n\n**Figure 3: Native frame rate experience with screen blanking**\n\n## Solution #2 : Match HDMI frame rate to content Native Frame Rate w/o screen blanking\n\nImprovements in the recent HDMI standards (HDMI 2.1+) now allow a source device to send the video content at its native frame rate without needing an HDMI resynchronization. This is possible through an innovative technology called [Quick Media Switching](https://www.hdmi.org/spec21sub/quickmediaswitching) (QMS) which is an extension of [Variable Refresh Rate](https://www.hdmi.org/spec21sub/variablerefreshrate) (VRR) targeted for content playback scenarios. QMS allows a source device to maintain a constant data rate on the HDMI link even during transmission of content with different frame rates. It does so by adjusting the amount of non-visible padding data while keeping the amount of visible video data constant. Due to the constant HDMI data rate, the HDMI transmitter and receiver don’t need to resynchronize, leading to a seamless/glitch-free transition as illustrated in Figure 4.\n\nHDMI QMS is positioned to be the ideal solution to address the problem we are presenting. Unfortunately, at present, this technology is relatively new and adoption into source and sink devices will take time.\n\n**Figure 4: Native frame rate experience without screen blanking using HDMI QMS**\n\n## Solution #3: Frame Rate Conversion within Netflix Application\n\nApart from the above HDMI specification dependent solutions, it is possible for an application like Netflix to manipulate the [presentation time stamp](https://en.wikipedia.org/wiki/Presentation_timestamp) value of each video frame to minimize the effect of judder i.e. the application can present video frames to the underlying source device platform at a cadence that can help the source device to minimize the judder associated with FRC on the HDMI output link.\n\nLet us understand this idea with the help of an example. Let’s go back to the same 24 to 50 fps FRC scenario that was covered earlier. But, instead of thinking about the FRC rate per second (24 ⇒ 50 fps), let’s expand the FRC calculation time period to 3 seconds (24\\*3 = 72 ⇒50\\*3 = 150 fps). For content with a native frame rate of 24 fps, the source device needs to get 72 frames from the streaming application in a period of 3 seconds. Now instead of sending 24 frames per second at a regular per second cadence, for each 3 second period the Netflix application can decide to send 25 frames in the first 2 seconds (25 x 2 = 50) and 22 frames in the 3rd second thereby still sending a total of 72 (50+22) frames in 3 seconds. This approach creates an even FRC in the first 2 seconds (25 frames replicated twice evenly) and in the 3rd second the source device can do a 22 to 50 fps FRC which will create less visual judder compared to the 24->50 fps FRC given a more even frame replication pattern. This concept is illustrated in Figure 5 below.\n\n**Figure 5: FRC Algorithm from Solution#3 for 24 to 50 fps conversion**\n\nNOTE: This solution was developed by [David Zheng](https://www.linkedin.com/in/david-weiguo-zheng-7409724/) in the Partner Experience Technology team at Netflix. Watch out for an upcoming article going into further details of this solution.\n\n## How the Netflix Application Uses these Solutions\n\nGiven the possible solutions available to use and the associated benefits and limitations, the Netflix application running on a source device adapts to use one of these approaches based on factors such as source and sink device capabilities, user preferences and the specific use case within the Netflix application. Let’s walk through each of these aspects briefly.\n\n## Device Capability\n\nEvery source device that integrates the Netflix application is required to let the application know if it and the connected sink device have the ability to send and receive video content at its native frame rate. In addition, a source device is required to inform whether it can support QMS and perform a seamless playback start of any content at its native frame rate on the connected HDMI link.\n\nAs discussed in the introduction section, the presence of a system setting like “Match Content Frame Rate” typically indicates that a source device is capable of this feature.\n\n## User Preference\n\nEven if a source device and the connected sink can support Native content frame rate streaming (seamless or non-seamless), a user might have selected not to do this via the source device system settings e.g. “Match Content Frame Rate” set to “Never”. Or they might have indicated a preference of doing this only when the native content frame rate play start can happen in a seamless manner e.g. “Match Content Frame Rate” set to “Seamless”.\n\nThe Netflix application needs to know this user selection in order to honor their preference. Hence, source devices are expected to relay this user preference to the Netflix application to help with this run-time decision making.\n\n## Netflix Use Case\n\nIn spite of source device capability and the user preferences collectively indicating that the Native Content Frame Rate streaming should be enabled, the Netflix application can decide to disable this feature for specific member experiences. As an example, when the user is browsing Netflix content in the home UI, we cannot play Netflix trailers in their Native frame rate due to the following reasons:\n\n*   If using Solution # 1, when the Netflix trailers are encoded in varying content frame rates, switching between trailers will result in screen blanking, thereby making the UI browsing unusable.\n*   If using Solution # 2, sending Netflix trailers in their Native frame rate would mean that the associated UI components (movement of cursor, asset selection etc) would also be displayed at the reduced frame rate and this will result in a sluggish UI browsing experience. This is because on HDMI output from the source device, both graphics (Netflix application UI) and video components will go out at the same frame rate (native content frame rate of the trailer) after being blended together on the source device.\n\nTo handle these issues we follow an approach as shown in Figure 6 below where we enable the Native Frame Rate playback experience only when the user selects a title and watches it in full screen with minimal graphical UI elements.\n\n**Figure 6: Native Frame Rate usage within Netflix application**\n\n## Conclusion\n\nThis article presented features that aim to improve the content playback experience on HDMI source devices. The breadth of available technical solutions, user selectable preferences, device capabilities and the application of each of these permutations in the context of various in-app member journeys represent a typical engineering and product decision framework at Netflix. Here at Netflix, our goal is to maximize immersion for our members through introduction of new features that will improve their viewing experience and keep them fully engaged in our content.\n\n## Acknowledgements\n\nWe would like to acknowledge the hard work of a number of teams that came together to deliver the features being discussed in this document. These include Core UI and JS Player development, Netflix Application Software development, AV Test and Tooling (earlier [article](https://netflixtechblog.com/hdmi-scaling-netflix-certification-8e9cb3ec524f) from this team), Partner Engineering and Product teams in the [Consumer Engineering](https://jobs.netflix.com/team?slug=client-and-ui-engineering) organization and our data science friends in the [Data Science and Engineering](https://jobs.netflix.com/team?slug=data-science-and-engineering) organization at Netflix. Diagrams in this article are courtesy of our Partner Enterprise Platform XD team."
    },
    {
      "url": "https://netflixtechblog.com/ensuring-the-successful-launch-of-ads-on-netflix-f99490fdf1ba?source=collection_home---4------12-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/ensuring-the-successful-launch-of-ads-on-netflix-f99490fdf1ba?gi=2565fc7e177b&source=collection_home---4------12-----------------------",
        "loadedTime": "2023-12-06T00:03:39.833Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/ensuring-the-successful-launch-of-ads-on-netflix-f99490fdf1ba",
        "title": "Ensuring the Successful Launch of Ads on Netflix | by Netflix Technology Blog | Netflix TechBlog",
        "description": "Netflix used replay traffic to simulate the projected load of its new Basic with ads tier before launch. By mimicking member viewing behavior and incorporating abrupt regional shifts, they tested new ad systems and algorithms at scale.",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Ensuring the Successful Launch of Ads on Netflix\nBy Jose Fernandez, Ed Barker, Hank Jacobs\nIntroduction\nIn November 2022, we introduced a brand new tier — Basic with ads. This tier extended existing infrastructure by adding new backend components and a new remote call to our ads partner on the playback path. As we were gearing up for launch, we wanted to ensure it would go as smoothly as possible. To do this, we devised a novel way to simulate the projected traffic weeks ahead of launch by building upon the traffic migration framework described here. We used this simulation to help us surface problems of scale and validate our Ads algorithms.\nBasic with ads was launched worldwide on November 3rd. In this blog post, we’ll discuss the methods we used to ensure a successful launch, including:\nHow we tested the system\nNetflix technologies involved\nBest practices we developed\nRealistic Test Traffic\nNetflix traffic ebbs and flows throughout the day in a sinusoidal pattern. New content or national events may drive brief spikes, but, by and large, traffic is usually smoothly increasing or decreasing. An exception to this trend is when we redirect traffic between AWS data centers during regional evacuations, which leads to sudden spikes in traffic in multiple regions. Region evacuations can occur at any time, for a variety of reasons.\nWhile evaluating options to test anticipated load and evaluate our ad selection algorithms at scale, we realized that mimicking member viewing behavior in combination with the seasonality of our organic traffic with abrupt regional shifts were important requirements. Replaying real traffic and making it appear as Basic with ads traffic was a better solution than artificially simulating Netflix traffic. Replay traffic enabled us to test our new systems and algorithms at scale before launch, while also making the traffic as realistic as possible.\nThe Setup\nA key objective of this initiative was to ensure that our customers were not impacted. We used member viewing habits to drive the simulation, but customers did not see any ads as a result. Achieving this goal required extensive planning and implementation of measures to isolate the replay traffic environment from the production environment.\nNetflix’s data science team provided projections of what the Basic with ads subscriber count would look like a month after launch. We used this information to simulate a subscriber population through our AB testing platform. When traffic matching our AB test criteria arrived at our playback services, we stored copies of those requests in a Mantis stream.\nNext, we launched a Mantis job that processed all requests in the stream and replayed them in a duplicate production environment created for replay traffic. We set the services in this environment to “replay traffic” mode, which meant that they did not alter state and were programmed to treat the request as being on the ads plan, which activated the components of the ads system.\nThe replay traffic environment generated responses containing a standard playback manifest, a JSON document containing all the necessary information for a Netflix device to start playback. It also included metadata about ads, such as ad placement and impression-tracking events. We stored these responses in a Keystone stream with outputs for Kafka and Elasticsearch. A Kafka consumer retrieved the playback manifests with ad metadata and simulated a device playing the content and triggering the impression-tracking events. We used Elasticsearch dashboards to analyze results.\nUltimately, we accurately simulated the projected Basic with ads traffic weeks ahead of the launch date.\nFig. 2: The Traffic Replay Setup\nThe Rollout\nTo fully replay the traffic, we first validated the idea with a small percentage of traffic. The Mantis query language allowed us to set the percentage of replay traffic to process. We informed our engineering and business partners, including customer support, about the experiment and ramped up traffic incrementally while monitoring the success and error metrics through Lumen dashboards. We continued ramping up and eventually reached 100% replay. At this point we felt confident to run the replay traffic 24/7.\nTo validate handling traffic spikes caused by regional evacuations, we utilized Netflix’s region evacuation exercises which are scheduled regularly. By coordinating with the team in charge of region evacuations and aligning with their calendar, we validated our system and third-party touchpoints at 100% replay traffic during these exercises.\nWe also constructed and checked our ad monitoring and alerting system during this period. Having representative data allowed us to be more confident in our alerting thresholds. The ads team also made necessary modifications to the algorithms to achieve the desired business outcomes for launch.\nFinally, we conducted chaos experiments using the ChAP experimentation platform. This allowed us to validate our fallback logic and our new systems under failure scenarios. By intentionally introducing failure into the simulation, we were able to identify points of weakness and make the necessary improvements to ensure that our ads systems were resilient and able to handle unexpected events.\nThe availability of replay traffic 24/7 enabled us to refine our systems and boost our launch confidence, reducing stress levels for the team.\nTakeaways\nThe above summarizes three months of hard work by a tiger team consisting of representatives from various backend teams and Netflix’s centralized SRE team. This work helped ensure a successful launch of the Basic with ads tier on November 3rd.\nTo briefly recap, here are a few of the things that we took away from this journey:\nAccurately simulating real traffic helps build confidence in new systems and algorithms more quickly.\nLarge scale testing using representative traffic helps to uncover bugs and operational surprises.\nReplay traffic has other applications outside of load testing that can be leveraged to build new products and features at Netflix.\nWhat’s Next\nReplay traffic at Netflix has numerous applications, one of which has proven to be a valuable tool for development and launch readiness. The Resilience team is streamlining this simulation strategy by integrating it into the CHAP experimentation platform, making it accessible for all development teams without the need for extensive infrastructure setup. Keep an eye out for updates on this.",
      "markdown": "## Ensuring the Successful Launch of Ads on Netflix\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----f99490fdf1ba--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----f99490fdf1ba--------------------------------)\n\nBy [Jose Fernandez](https://www.linkedin.com/in/josefernandezmn/), [Ed Barker](https://www.linkedin.com/in/edhbarker/), [Hank Jacobs](https://www.linkedin.com/in/hajacobs/)\n\n## Introduction\n\nIn November 2022, we introduced a brand new tier — [_Basic with ads_](https://about.netflix.com/en/news/announcing-basic-with-ads-us). This tier extended existing infrastructure by adding new backend components and a new remote call to our ads partner on the playback path. As we were gearing up for launch, we wanted to ensure it would go as smoothly as possible. To do this, we devised a novel way to simulate the projected traffic weeks ahead of launch by building upon the traffic migration framework described [here](https://netflixtechblog.com/migrating-critical-traffic-at-scale-with-no-downtime-part-1-ba1c7a1c7835). We used this simulation to help us surface problems of scale and validate our Ads algorithms.\n\n_Basic with ads_ was launched worldwide on November 3rd. In this blog post, we’ll discuss the methods we used to ensure a successful launch, including:\n\n*   How we tested the system\n*   Netflix technologies involved\n*   Best practices we developed\n\n## Realistic Test Traffic\n\nNetflix traffic ebbs and flows throughout the day in a sinusoidal pattern. New content or national events may drive brief spikes, but, by and large, traffic is usually smoothly increasing or decreasing. An exception to this trend is when we redirect traffic between AWS data centers during regional evacuations, which leads to sudden spikes in traffic in multiple regions. Region evacuations can occur at any time, for a variety of reasons.\n\nWhile evaluating options to test anticipated load and evaluate our ad selection algorithms at scale, we realized that mimicking member viewing behavior in combination with the seasonality of our organic traffic with abrupt regional shifts were important requirements. Replaying real traffic and making it appear as _Basic with ads_ traffic was a better solution than artificially simulating Netflix traffic. **Replay traffic enabled us to test our new systems and algorithms at scale before launch, while also making the traffic as realistic as possible.**\n\n## The Setup\n\nA key objective of this initiative was to ensure that our customers were not impacted. We used member viewing habits to drive the simulation, but customers did not see any ads as a result. Achieving this goal required extensive planning and implementation of measures to isolate the replay traffic environment from the production environment.\n\nNetflix’s data science team provided projections of what the _Basic with ads_ subscriber count would look like a month after launch. We used this information to simulate a subscriber population through our [AB testing platform](https://netflixtechblog.com/its-all-a-bout-testing-the-netflix-experimentation-platform-4e1ca458c15). When traffic matching our AB test criteria arrived at our playback services, we stored copies of those requests in a [Mantis stream](https://netflixtechblog.com/stream-processing-with-mantis-78af913f51a6).\n\nNext, we launched a Mantis job that processed all requests in the stream and replayed them in a duplicate production environment created for replay traffic. We set the services in this environment to “replay traffic” mode, which meant that they did not alter state and were programmed to treat the request as being on the ads plan, which activated the components of the ads system.\n\nThe replay traffic environment generated responses containing a standard playback manifest, a JSON document containing all the necessary information for a Netflix device to start playback. It also included metadata about ads, such as ad placement and impression-tracking events. We stored these responses in a [Keystone stream](https://netflixtechblog.com/keystone-real-time-stream-processing-platform-a3ee651812a) with outputs for Kafka and Elasticsearch. A Kafka consumer retrieved the playback manifests with ad metadata and simulated a device playing the content and triggering the impression-tracking events. We used Elasticsearch dashboards to analyze results.\n\n**Ultimately, we accurately simulated the projected _Basic with ads_ traffic weeks ahead of the launch date.**\n\nFig. 2: The Traffic Replay Setup\n\n## The Rollout\n\nTo fully replay the traffic, we first validated the idea with a small percentage of traffic. The [Mantis query language](https://netflix.github.io/mantis/develop/querying/mql/) allowed us to set the percentage of replay traffic to process. We informed our engineering and business partners, including customer support, about the experiment and ramped up traffic incrementally while monitoring the success and error metrics through [Lumen dashboards](https://netflixtechblog.com/lumen-custom-self-service-dashboarding-for-netflix-8c56b541548c). We continued ramping up and eventually reached 100% replay. At this point we felt confident to run the replay traffic 24/7.\n\nTo validate handling traffic spikes caused by regional evacuations, we utilized Netflix’s region evacuation exercises which are scheduled regularly. By coordinating with the team in charge of region evacuations and aligning with their calendar, we validated our system and third-party touchpoints at 100% replay traffic during these exercises.\n\nWe also constructed and checked our ad monitoring and alerting system during this period. Having representative data allowed us to be more confident in our alerting thresholds. The ads team also made necessary modifications to the algorithms to achieve the desired business outcomes for launch.\n\nFinally, we conducted chaos experiments using the [ChAP experimentation platform](https://netflixtechblog.com/chap-chaos-automation-platform-53e6d528371f). This allowed us to validate our fallback logic and our new systems under failure scenarios. By intentionally introducing failure into the simulation, we were able to identify points of weakness and make the necessary improvements to ensure that our ads systems were resilient and able to handle unexpected events.\n\n**The availability of replay traffic 24/7 enabled us to refine our systems and boost our launch confidence, reducing stress levels for the team.**\n\n## Takeaways\n\nThe above summarizes three months of hard work by a tiger team consisting of representatives from various backend teams and [Netflix’s centralized SRE team](https://netflixtechblog.com/keeping-customers-streaming-the-centralized-site-reliability-practice-at-netflix-205cc37aa9fb). This work helped ensure **a successful launch of the _Basic with ads_** tier on November 3rd.\n\nTo briefly recap, here are a few of the things that we took away from this journey:\n\n*   Accurately simulating real traffic helps build confidence in new systems and algorithms more quickly.\n*   Large scale testing using representative traffic helps to uncover bugs and operational surprises.\n*   Replay traffic has other applications outside of load testing that can be leveraged to build new products and features at Netflix.\n\n## What’s Next\n\nReplay traffic at Netflix has numerous applications, one of which has proven to be a valuable tool for development and launch readiness. The Resilience team is streamlining this simulation strategy by integrating it into the [CHAP experimentation platform](https://netflixtechblog.com/chap-chaos-automation-platform-53e6d528371f), making it accessible for all development teams without the need for extensive infrastructure setup. Keep an eye out for updates on this."
    },
    {
      "url": "https://netflixtechblog.com/improved-alerting-with-atlas-streaming-eval-e691c60dc61e?source=collection_home---4------17-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/improved-alerting-with-atlas-streaming-eval-e691c60dc61e?gi=1f97f1e19440&source=collection_home---4------17-----------------------",
        "loadedTime": "2023-12-06T00:03:43.634Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/improved-alerting-with-atlas-streaming-eval-e691c60dc61e",
        "title": "Improved Alerting with Atlas Streaming Eval | by Netflix Technology Blog | Netflix TechBlog",
        "description": "Engineers want their alerting system to be realtime, reliable, and actionable. While actionability is subjective and may vary by use-case, reliability is non-negotiable. In other words, false…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Ruchir Jha, Brian Harrington, Yingwu Zhao\nTL;DR\nStreaming alert evaluation scales much better than the traditional approach of polling time-series databases.\nIt allows us to overcome high dimensionality/cardinality limitations of the time-series database.\nIt opens doors to support more exciting use-cases.\nEngineers want their alerting system to be realtime, reliable, and actionable. While actionability is subjective and may vary by use-case, reliability is non-negotiable. In other words, false positives are bad but false negatives are the absolute worst!\nA few years ago, we were paged by our SRE team due to our Metrics Alerting System falling behind — critical application health alerts reached engineers 45 minutes late! As we investigated the alerting delay, we found that the number of configured alerts had recently increased dramatically, by 5 times! The alerting system queried Atlas, our time series database on a cron for each configured alert query, and was seeing an elevated throttle rate and excessive retries with backoffs. This, in turn, increased the time between two consecutive checks for an alert, causing a global slowdown for all alerts. On further investigation, we discovered that one user had programmatically created tens of thousands of new alerts. This user represented a platform team at Netflix, and their goal was to build alerting automation for their users.\nWhile we were able to put out the immediate fire by disabling the newly created alerts, this incident raised some critical concerns around the scalability of our alerting system. We also heard from other platform teams at Netflix who wanted to build similar automation for their users who, given our state at the time, wouldn’t have been able to do so without impacting Mean Time To Detect (MTTD) for all others. Rather, we were looking at an order of magnitude increase in the number of alert queries just over the next 6 months!\nSince querying Atlas was the bottleneck, our first instinct was to scale it up to meet the increased alert query demand; however, we soon realized that would increase Atlas cost prohibitively. Atlas is an in-memory time-series database that ingests multiple billions of time-series per day and retains the last two weeks of data. It is already one of the largest services at Netflix both in size and cost. While Atlas is architected around compute & storage separation, and we could theoretically just scale the query layer to meet the increased query demand, every query, regardless of its type, has a data component that needs to be pushed down to the storage layer. To serve the increasing number of push down queries, the in-memory storage layer would need to scale up as well, and it became clear that this would push the already expensive storage costs far higher. Moreover, common database optimizations like caching recently queried data don’t really work for alerting queries because, generally speaking, the last received datapoint is required for correctness. Take for example, this alert query that checks if errors as a % of total RPS exceeds a threshold of 50% for 4 out of the last 5 minutes:\nname,errors,:eq,:sum,\nname,rps,:eq,:sum,\n:div,\n100,:mul,\n50,:gt,\n5,:rolling-count,4,:gt,\nSay if the datapoint received for the last time interval leads to a positive evaluation for this query, relying on stale/cached data would either increase MTTD or result in the perception of a false negative, at least until the missing data is fetched and evaluated. It became clear to us that we needed to solve the scalability problem with a fundamentally different approach. Hence, we started down the path of alert evaluation via real-time streaming metrics.\nHigh Level Architecture\nThe idea, at a high level, was to avoid the need to query the Atlas database almost entirely and transition most alert queries to streaming evaluation.\nAlert queries are submitted either via our Alerting UI or by API clients, which are then saved to a custom config database that supports streaming config updates (full snapshot + update notifications). The Alerting Service receives these config updates and hashes every new or updated alert query for evaluation to one of its nodes by leveraging Edda Slots. The node responsible for evaluating a query, starts by breaking it down into a set of “data expressions” and with them subscribes to an upstream “broker” service. Data expressions define what data needs to be sourced in order to evaluate a query. For the example query listed above, the data expressions are name,errors,:eq,:sum and name,rps,:eq,:sum. The broker service acts as a subscription manager that maps a data expression to a set of subscriptions. In addition, it also maintains a Query Index of all active data expressions which is consulted to discern if an incoming datapoint is of interest to an active subscriber. The internals here are outside the scope of this blog post.\nNext, the Alerting service (via the atlas-eval library) maps the received data points for a data expression to the alert query that needs them. For alert queries that resolve to more than one data expression, we align the incoming data points for each one of those data expressions on the same time boundary before emitting the accumulated values to the final eval step. For the example above, the final eval step would be responsible for computing the ratio and maintaining the rolling-count, which is keeping track of the number of intervals in which the ratio crossed the threshold as shown below:\nThe atlas-eval library supports streaming evaluation for most if not all Query, Data, Math and Stateful operators supported by Atlas today. Certain operators such as offset, integral, des are not supported on the streaming path.\nOK, Results?\nFirst and foremost, we have successfully alleviated our initial scalability problem with the polling based architecture. Today, we run 20X the number of queries we used to run a few years ago, with ease and at a fraction of what it would have cost to scale up the Atlas storage layer to serve the same volume. Multiple platform teams at Netflix programmatically generate and maintain alerts on behalf of their users without having to worry about impacting other users of the system. We are able to maintain strong SLAs around Mean Time To Detect (MTTD) regardless of the number of alerts being evaluated by the system.\nAdditionally, streaming evaluation allowed us to relax restrictions around high cardinality that our users were previously running into — alert queries that were rejected by Atlas Backend before due to cardinality constraints are now getting checked correctly on the streaming path. In addition, we are able to use Atlas Streaming to monitor and alert on some very high cardinality use-cases, such as metrics derived from free-form log data.\nFinally, we switched Telltale, our holistic application health monitoring system, from polling a metrics cache to using realtime Atlas Streaming. The fundamental idea behind Telltale is to detect anomalies on SLI metrics (for example, latency, error rates, etc). When such anomalies are detected, Telltale is able to compute correlations with similar metrics emitted from either upstream or downstream services. In addition, it also computes correlations between SLI metrics and custom metrics like the log derived metrics mentioned above. This has proven to be valuable towards reducing Mean Time to Recover (MTTR). For example, we are able to now correlate increased error rates with increased rate of specific exceptions occurring in logs and even point to an exemplar stacktrace, as shown below:\nOur logs pipeline fingerprints every log message and attaches a (very high cardinality) fingerprint tag to a log events counter that is then emitted to Atlas Streaming. Telltale consumes this metric in a streaming fashion to identify fingerprints that correlate with anomalies seen in SLI metrics. Once an anomaly is found, we query the logs backend with the fingerprint hash to obtain the exemplar stacktrace. What’s more is we are now able to identify correlated anomalies (and exceptions) occurring in services that may be N hops away from the affected service. A system like Telltale becomes more effective as more services are onboarded (and for that matter the full service graph), because otherwise it becomes difficult to root cause the problem, especially in a microservices-based architecture. A few years ago, as noted in this blog, only about a hundred services were using Telltale; thanks to Atlas Streaming we have now managed to onboard thousands of other services at Netflix.\nFinally, we realized that once you remove limits on the number of monitored queries, and start supporting much higher metric dimensionality/cardinality without impacting the cost/performance profile of the system, it opens doors to many exciting new possibilities. For example, to make alerts more actionable, we may now be able to compute correlations between SLI anomalies and custom metrics with high cardinality dimensions, for example an alert on elevated HTTP error rates may be able to point to impacted customer cohorts, by linking to precisely correlated exemplars. This would help developers with reproducibility.\nTransitioning to the streaming path has been a long journey for us. One of the challenges was difficulty in debugging scenarios where the streaming path didn’t agree with what is returned by querying the Atlas database. This is especially true when either the data is not available in Atlas or the query is not supported because of (say) cardinality constraints. This is one of the reasons it has taken us years to get here. That said, early signs indicate that the streaming paradigm may help with tackling a cardinal problem in observability — effective correlation between the metrics & events verticals (logs, and potentially traces in the future), and we are excited to explore the opportunities that this presents for Observability in general.",
      "markdown": "[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----e691c60dc61e--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----e691c60dc61e--------------------------------)\n\n[Ruchir Jha](https://www.linkedin.com/in/ruchir-jha-9a861616/), [Brian Harrington](https://www.linkedin.com/in/brharrington/), [Yingwu Zhao](https://www.linkedin.com/in/yingwu-zhao-62037418/)\n\nTL;DR\n\n*   Streaming alert evaluation scales much better than the traditional approach of polling time-series databases.\n*   It allows us to overcome high dimensionality/cardinality limitations of the time-series database.\n*   It opens doors to support more exciting use-cases.\n\nEngineers want their alerting system to be realtime, reliable, and actionable. While actionability is subjective and may vary by use-case, reliability is non-negotiable. In other words, false positives are bad but false negatives are the absolute worst!\n\nA few years ago, we were paged by our SRE team due to our Metrics Alerting System falling behind — critical application health alerts reached engineers 45 minutes late! As we investigated the alerting delay, we found that the number of configured alerts had recently increased dramatically, by 5 times! The alerting system queried [Atlas](https://github.com/Netflix/atlas), our time series database on a cron for each configured alert query, and was seeing an elevated throttle rate and excessive retries with backoffs. This, in turn, increased the time between two consecutive checks for an alert, causing a global slowdown for all alerts. On further investigation, we discovered that one user had programmatically created tens of thousands of new alerts. This user represented a platform team at Netflix, and their goal was to build alerting automation for their users.\n\nWhile we were able to put out the immediate fire by disabling the newly created alerts, this incident raised some critical concerns around the scalability of our alerting system. We also heard from other platform teams at Netflix who wanted to build similar automation for their users who, given our state at the time, wouldn’t have been able to do so without impacting Mean Time To Detect (MTTD) for all others. Rather, we were looking at an order of magnitude increase in the number of alert queries just over the next 6 months!\n\nSince querying Atlas was the bottleneck, our first instinct was to scale it up to meet the increased alert query demand; however, we soon realized that would increase Atlas cost prohibitively. Atlas is an in-memory time-series database that ingests multiple billions of time-series per day and retains the last two weeks of data. It is already one of the largest services at Netflix both in size and cost. While Atlas [is architected](https://netflix.github.io/atlas-docs/overview/) around compute & storage separation, and we could theoretically just scale the query layer to meet the increased query demand, every query, regardless of its type, has a data component that needs to be pushed down to the storage layer. To serve the increasing number of push down queries, the in-memory storage layer would need to scale up as well, and it became clear that this would push the already expensive storage costs far higher. Moreover, common database optimizations like caching recently queried data don’t really work for alerting queries because, generally speaking, the last received datapoint is required for correctness. Take for example, this alert query that checks if errors as a % of total RPS exceeds a threshold of 50% for 4 out of the last 5 minutes:\n\nname,errors,:eq,:sum,  \nname,rps,:eq,:sum,  \n:div,  \n100,:mul,  \n50,:gt,  \n5,:rolling-count,4,:gt,\n\nSay if the datapoint received for the last time interval leads to a positive evaluation for this query, relying on stale/cached data would either increase MTTD or result in the perception of a false negative, at least until the missing data is fetched and evaluated. It became clear to us that we needed to solve the scalability problem with a fundamentally different approach. Hence, we started down the path of alert evaluation via real-time [streaming metrics](https://github.com/Netflix/atlas/tree/main/atlas-eval).\n\n**High Level Architecture**\n\nThe idea, at a high level, was to avoid the need to query the Atlas database almost entirely and transition most alert queries to streaming evaluation.\n\nAlert queries are submitted either via our Alerting UI or by API clients, which are then saved to a custom config database that supports streaming config updates (full snapshot + update notifications). The Alerting Service receives these config updates and hashes every new or updated alert query for evaluation to one of its nodes by leveraging [Edda Slots](https://netflix.github.io/edda/rest-api/#apiv2group). The node responsible for evaluating a query, starts by breaking it down into a set of “data expressions” and with them subscribes to an upstream “broker” service. Data expressions define what data needs to be sourced in order to evaluate a query. For the example query listed above, the data expressions are name,errors,:eq,:sum and name,rps,:eq,:sum. The broker service acts as a subscription manager that maps a data expression to a set of subscriptions. In addition, it also maintains a Query Index of all active data expressions which is consulted to discern if an incoming datapoint is of interest to an active subscriber. The internals here are outside the scope of this blog post.\n\nNext, the Alerting service (via the [atlas-eval](https://github.com/Netflix/atlas/tree/main/atlas-eval) library) maps the received data points for a data expression to the alert query that needs them. For alert queries that resolve to more than one data expression, we align the incoming data points for each one of those data expressions on the same time boundary before emitting the accumulated values to the final eval step. For the example above, the final eval step would be responsible for computing the ratio and maintaining the [rolling-count](https://netflix.github.io/atlas-docs/asl/ref/rolling-count/), which is keeping track of the number of intervals in which the ratio crossed the threshold as shown below:\n\nThe atlas-eval library supports streaming evaluation for most if not all [Query](https://github.com/Netflix/atlas/wiki/Reference-query), [Data](https://github.com/Netflix/atlas/wiki/Reference-data), [Math](https://github.com/Netflix/atlas/wiki/Reference-math) and [Stateful](https://github.com/Netflix/atlas/wiki/Reference-stateful) operators supported by Atlas today. Certain operators such as [offset](https://netflix.github.io/atlas-docs/asl/ref/offset/), [integral](https://netflix.github.io/atlas-docs/asl/ref/integral/), [des](https://netflix.github.io/atlas-docs/asl/ref/des/) are not supported on the streaming path.\n\n**OK, Results?**\n\nFirst and foremost, we have successfully alleviated our initial scalability problem with the polling based architecture. Today, we run 20X the number of queries we used to run a few years ago, with ease and at a fraction of what it would have cost to scale up the Atlas storage layer to serve the same volume. Multiple platform teams at Netflix programmatically generate and maintain alerts on behalf of their users without having to worry about impacting other users of the system. We are able to maintain strong SLAs around Mean Time To Detect (MTTD) regardless of the number of alerts being evaluated by the system.\n\nAdditionally, streaming evaluation allowed us to relax restrictions around high cardinality that our users were previously running into — alert queries that were rejected by Atlas Backend before due to cardinality constraints are now getting checked correctly on the streaming path. In addition, we are able to use Atlas Streaming to monitor and alert on some very high cardinality use-cases, such as metrics derived from free-form log data.\n\nFinally, we switched [Telltale](https://netflixtechblog.com/telltale-netflix-application-monitoring-simplified-5c08bfa780ba), our holistic application health monitoring system, from polling a metrics cache to using realtime Atlas Streaming. The fundamental idea behind Telltale is to detect anomalies on SLI metrics (for example, latency, error rates, etc). When such anomalies are detected, Telltale is able to compute correlations with similar metrics emitted from either upstream or downstream services. In addition, it also computes correlations between SLI metrics and custom metrics like the log derived metrics mentioned above. This has proven to be valuable towards reducing Mean Time to Recover (MTTR). For example, we are able to now correlate increased error rates with increased rate of specific exceptions occurring in logs and even point to an exemplar stacktrace, as shown below:\n\nOur logs pipeline fingerprints every log message and attaches a (very high cardinality) fingerprint tag to a log events counter that is then emitted to Atlas Streaming. Telltale consumes this metric in a streaming fashion to identify fingerprints that correlate with anomalies seen in SLI metrics. Once an anomaly is found, we query the logs backend with the fingerprint hash to obtain the exemplar stacktrace. What’s more is we are now able to identify correlated anomalies (and exceptions) occurring in services that may be N hops away from the affected service. A system like Telltale becomes more effective as more services are onboarded (and for that matter the full service graph), because otherwise it becomes difficult to root cause the problem, especially in a microservices-based architecture. A few years ago, as noted in this [blog](https://netflixtechblog.com/telltale-netflix-application-monitoring-simplified-5c08bfa780ba), only about a hundred services were using Telltale; thanks to Atlas Streaming we have now managed to onboard thousands of other services at Netflix.\n\nFinally, we realized that once you remove limits on the number of monitored queries, and start supporting much higher metric dimensionality/cardinality without impacting the cost/performance profile of the system, it opens doors to many exciting new possibilities. For example, to make alerts more actionable, we may now be able to compute correlations between SLI anomalies and custom metrics with high cardinality dimensions, for example an alert on elevated HTTP error rates may be able to point to impacted customer cohorts, by linking to precisely correlated exemplars. This would help developers with reproducibility.\n\nTransitioning to the streaming path has been a long journey for us. One of the challenges was difficulty in debugging scenarios where the streaming path didn’t agree with what is returned by querying the Atlas database. This is especially true when either the data is not available in Atlas or the query is not supported because of (say) cardinality constraints. This is one of the reasons it has taken us years to get here. That said, early signs indicate that the streaming paradigm may help with tackling a cardinal problem in observability — effective correlation between the metrics & events verticals (logs, and potentially traces in the future), and we are excited to explore the opportunities that this presents for Observability in general."
    },
    {
      "url": "https://netflixtechblog.com/building-a-media-understanding-platform-for-ml-innovations-9bef9962dcb7?source=collection_home---4------18-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/building-a-media-understanding-platform-for-ml-innovations-9bef9962dcb7?gi=e21524bd8415&source=collection_home---4------18-----------------------",
        "loadedTime": "2023-12-06T00:03:44.125Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/building-a-media-understanding-platform-for-ml-innovations-9bef9962dcb7",
        "title": "Building a Media Understanding Platform for ML Innovations | by Netflix Technology Blog | Netflix TechBlog",
        "description": "Netflix leverages machine learning to create the best media for our members. Earlier we shared the details of one of these algorithms, introduced how our platform team is evolving the media-specific…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Building a Media Understanding Platform for ML Innovations\nBy Guru Tahasildar, Amir Ziai, Jonathan Solórzano-Hamilton, Kelli Griggs, Vi Iyengar\nIntroduction\nNetflix leverages machine learning to create the best media for our members. Earlier we shared the details of one of these algorithms, introduced how our platform team is evolving the media-specific machine learning ecosystem, and discussed how data from these algorithms gets stored in our annotation service.\nMuch of the ML literature focuses on model training, evaluation, and scoring. In this post, we will explore an understudied aspect of the ML lifecycle: integration of model outputs into applications.\nAn example of using Machine Learning to find shots of Eleven in Stranger Things and surfacing the results in studio application for the consumption of Netflix video editors.\nSpecifically, we will dive into the architecture that powers search capabilities for studio applications at Netflix. We discuss specific problems that we have solved using Machine Learning (ML) algorithms, review different pain points that we addressed, and provide a technical overview of our new platform.\nOverview\nAt Netflix, we aim to bring joy to our members by providing them with the opportunity to experience outstanding content. There are two components to this experience. First, we must provide the content that will bring them joy. Second, we must make it effortless and intuitive to choose from our library. We must quickly surface the most stand-out highlights from the titles available on our service in the form of images and videos in the member experience.\nHere is an example of such an asset created for one of our titles:\nThese multimedia assets, or “supplemental” assets, don’t just come into existence. Artists and video editors must create them. We build creator tooling to enable these colleagues to focus their time and energy on creativity. Unfortunately, much of their energy goes into labor-intensive pre-work. A key opportunity is to automate these mundane tasks.\nUse cases\nUse case #1: Dialogue search\nDialogue is a central aspect of storytelling. One of the best ways to tell an engaging story is through the mouths of the characters. Punchy or memorable lines are a prime target for trailer editors. The manual method for identifying such lines is a watchdown (aka breakdown).\nAn editor watches the title start-to-finish, transcribes memorable words and phrases with a timecode, and retrieves the snippet later if the quote is needed. An editor can choose to do this quickly and only jot down the most memorable moments, but will have to rewatch the content if they miss something they need later. Or, they can do it thoroughly and transcribe the entire piece of content ahead of time. In the words of one of our editors:\nWatchdowns / breakdown are very repetitive and waste countless hours of creative time!\nScrubbing through hours of footage (or dozens of hours if working on a series) to find a single line of dialogue is profoundly tedious. In some cases editors need to search across many shows and manually doing it is not feasible. But what if scrubbing and transcribing dialogue is not needed at all?\nIdeally, we want to enable dialogue search that supports the following features:\nSearch across one title, a subset of titles (e.g. all dramas), or the entire catalog\nSearch by character or talent\nMultilingual search\nUse case #2: Visual search\nA picture is worth a thousand words. Visual storytelling can help make complex stories easier to understand, and as a result, deliver a more impactful message.\nArtists and video editors routinely need specific visual elements to include in artworks and trailers. They may scrub for frames, shots, or scenes of specific characters, locations, objects, events (e.g. a car chasing scene in an action movie), or attributes (e.g. a close-up shot). What if we could enable users to find visual elements using natural language?\nHere is an example of the desired output when the user searches for “red race car” across the entire content library.\nUser searching for “red race car”\nUse case #3: Reverse shot search\nNatural-language visual search offers editors a powerful tool. But what if they already have a shot in mind, and they want to find something that just looks similar? For instance, let’s say that an editor has found a visually stunning shot of a plate of food from Chef’s Table, and she’s interested in finding similar shots across the entire show.\nUser provides a query shot to find other similar shots.\nPrior engineering work\nApproach #1: on-demand batch processing\nOur first approach to surface these innovations was a tool to trigger these algorithms on-demand and on a per-show basis. We implemented a batch processing system for users to submit their requests and wait for the system to generate the output. Processing took several hours to complete. Some ML algorithms are computationally intensive. Many of the samples provided had a significant number of frames to process. A typical 1 hour video could contain over 80,000 frames!\nAfter waiting for processing, users downloaded the generated algo outputs for offline consumption. This limited pilot system greatly reduced the time spent by our users to manually analyze the content. Here is a visualization of this flow.\nOn-demand batch processing system flow\nApproach #2: enabling online request with pre-computation\nAfter the success of this approach we decided to add online support for a couple of algorithms. For the first time, users were able to discover matches across the entire catalog, oftentimes finding moments they never knew even existed. They didn’t need any time-consuming local setup and there was no delays since the data was already pre-computed.\nInteractive system with pre-computed data flow\nThe following quote exemplifies the positive reception by our users:\n“We wanted to find all the shots of the dining room in a show. In seconds, we had what normally would have taken 1–2 people hours/a full day to do, look through all the shots of the dining room from all 10 episodes of the show. Incredible!”\nDawn Chenette, Design Lead\nThis approach had several benefits for product engineering. It allowed us to transparently update the algo data without users knowing about it. It also provided insights into query patterns and algorithms that were gaining traction among users. In addition, we were able to perform a handful of A/B tests to validate or negate our hypotheses for tuning the search experience.\nPain points\nOur early efforts to deliver ML insights to creative professionals proved valuable. At the same time we experienced growing engineering pains that limited our ability to scale.\nMaintaining disparate systems posed a challenge. They were first built by different teams on different stacks, so maintenance was expensive. Whenever ML researchers finished a new algorithm they had to integrate it separately into each system. We were near the breaking point with just two systems and a handful of algorithms. We knew this would only worsen as we expanded to more use cases and more researchers.\nThe online application unlocked the interactivity for our users and validated our direction. However, it was not scaling well. Adding new algos and onboarding new use cases was still time consuming and required the effort of too many engineers. These investments in one-to-one integrations were volatile with implementation timelines varying from a few weeks to several months. Due to the bespoke nature of the implementation, we lacked catalog wide searches for all available ML sources.\nIn summary, this model was a tightly-coupled application-to-data architecture, where machine learning algos were mixed with the backend and UI/UX software code stack. To address the variance in the implementation timelines we needed to standardize how different algorithms were integrated — starting from how they were executed to making the data available to all consumers consistently. As we developed more media understanding algos and wanted to expand to additional use cases, we needed to invest in system architecture redesign to enable researchers and engineers from different teams to innovate independently and collaboratively. Media Search Platform (MSP) is the initiative to address these requirements.\nAlthough we were just getting started with media-search, search itself is not new to Netflix. We have a mature and robust search and recommendation functionality exposed to millions of our subscribers. We knew we could leverage learnings from our colleagues who are responsible for building and innovating in this space. In keeping with our “highly aligned, loosely coupled” culture, we wanted to enable engineers to onboard and improve algos quickly and independently, while making it easy for Studio and product applications to integrate with the media understanding algo capabilities.\nMaking the platform modular, pluggable and configurable was key to our success. This approach allowed us to keep the distributed ownership of the platform. It simultaneously provided different specialized teams to contribute relevant components of the platform. We used services already available for other use cases and extended their capabilities to support new requirements.\nNext we will discuss the system architecture and describe how different modules interact with each other for end-to-end flow.\nArchitecture\nSystem Architecture\nNetflix engineers strive to iterate rapidly and prefer the “MVP” (minimum viable product) approach to receive early feedback and minimize the upfront investment costs. Thus, we didn’t build all the modules completely. We scoped the pilot implementation to ensure immediate functionalities were unblocked. At the same time, we kept the design open enough to allow future extensibility. We will highlight a few examples below as we discuss each component separately.\nInterfaces - API & Query\nStarting at the top of the diagram, the platform allows apps to interact with it using either gRPC or GraphQL interfaces. Having diversity in the interfaces is essential to meet the app-developers where they are. At Netflix, gRPC is predominantly used in backend-to-backend communication. With active GraphQL tooling provided by our developer productivity teams, GraphQL has become a de-facto choice for UI — backend integration. You can find more about what the team has built and how it is getting used in these blog posts. In particular, we have been relying on Domain Graph Service Framework for this project.\nDuring the query schema design, we accounted for future use cases and ensured that it will allow future extensions. We aimed to keep the schema generic enough so that it hides implementation details of the actual search systems that are used to execute the query. Additionally it is intuitive and easy to understand yet feature rich so that it can be used to express complex queries. Users have flexibility to perform multimodal search with input being a simple text term, image or short video. As discussed earlier, search could be performed against the entire Netflix catalog, or it could be limited to specific titles. Users may prefer results that are organized in some way such as group by a movie, sorted by timestamp. When there are a large number of matches, we allow users to paginate the results (with configurable page size) instead of fetching all or a fixed number of results.\nSearch Gateway\nThe client generated input query is first given to the Query processing system. Since most of our users are performing targeted queries such as — search for dialogue “friends don’t lie” (from the above example), today this stage performs lightweight processing and provides a hook to integrate A/B testing. In the future we plan to evolve it into a “query understanding system” to support free-form searches to reduce the burden on users and simplify client side query generation.\nThe query processing modifies queries to match the target data set. This includes “embedding” transformation and translation. For queries against embedding based data sources it transforms the input such as text or image to corresponding vector representation. Each data source or algorithm could use a different encoding technique so, this stage ensures that the corresponding encoding is also applied to the provided query. One example why we need different encoding techniques per algorithm is because there is different processing for an image — which has a single frame while video — which contains a sequence of multiple frames.\nWith global expansion we have users where English is not a primary language. All of the text-based models in the platform are trained using English language so we translate non-English text to English. Although the translation is not always perfect it has worked well in our case and has expanded the eligible user base for our tool to non-English speakers.\nOnce the query is transformed and ready for execution, we delegate search execution to one or more of the searcher systems. First we need to federate which query should be routed to which system. This is handled by the Query router and Searcher-proxy module. For the initial implementation we have relied on a single searcher for executing all the queries. Our extensible approach meant the platform could support additional searchers, which have already been used to prototype new algorithms and experiments.\nA search may intersect or aggregate the data from multiple algorithms so this layer can fan out a single query into multiple search executions. We have implemented a “searcher-proxy” inside this layer for each supported searcher. Each proxy is responsible for mapping input query to one expected by the corresponding searcher. It then consumes the raw response from the searcher before handing it over to the Results post-processor component.\nThe Results post-processor works on the results returned by one or more searchers. It can rank results by applying custom scoring, populate search recommendations based on other similar searches. Another functionality we are evaluating with this layer is to dynamically create different views from the same underlying data.\nFor ease of coordination and maintenance we abstracted the query processing and response handling in a module called — Search Gateway.\nSearchers\nAs mentioned above, query execution is handled by the searcher system. The primary searcher used in the current implementation is called Marken — scalable annotation service built at Netflix. It supports different categories of searches including full text and embedding vector based similarity searches. It can store and retrieve temporal (timestamp) as well as spatial (coordinates) data. This service leverages Cassandra and Elasticsearch for data storage and retrieval. When onboarding embedding vector data we performed an extensive benchmarking to evaluate the available datastores. One takeaway here is that even if there is a datastore that specializes in a particular query pattern, for ease of maintainability and consistency we decided to not introduce it.\nWe have identified a handful of common schema types and standardized how data from different algorithms is stored. Each algorithm still has the flexibility to define a custom schema type. We are actively innovating in this space and recently added capability to intersect data from different algorithms. This is going to unlock creative ways of how the data from multiple algorithms can be superimposed on each other to quickly get to the desired results.\nAlgo Execution & Ingestion\nSo far we have focused on how the data is queried but, there is an equally complex machinery powering algorithm execution and the generation of the data. This is handled by our dedicated media ML Platform team. The team specializes in building a suite of media-specific machine learning tooling. It facilitates seamless access to media assets (audio, video, image and text) in addition to media-centric feature storage and compute orchestration.\nFor this project we developed a custom sink that indexes the generated data into Marken according to predefined schemas. Special care is taken when the data is backfilled for the first time so as to avoid overwhelming the system with huge amounts of writes.\nLast but not the least, our UI team has built a configurable, extensible library to simplify integrating this platform with end user applications. Configurable UI makes it easy to customize query generation and response handling as per the needs of individual applications and algorithms. The future work involves building native widgets to minimize the UI work even further.\nSummary\nThe media understanding platform serves as an abstraction layer between machine learning algos and various applications and features. The platform has already allowed us to seamlessly integrate search and discovery capabilities in several applications. We believe future work in maturing different parts will unlock value for more use cases and applications. We hope this post has offered insights into how we approached its evolution. We will continue to share our work in this space, so stay tuned.\nDo these types of challenges interest you? If yes, we’re always looking for engineers and machine learning practitioners to join us.\nAcknowledgements\nSpecial thanks to Vinod Uddaraju, Fernando Amat Gil, Ben Klein, Meenakshi Jindal, Varun Sekhri, Burak Bacioglu, Boris Chen, Jason Ge, Tiffany Low, Vitali Kauhanka, Supriya Vadlamani, Abhishek Soni, Gustavo Carmo, Elliot Chow, Prasanna Padmanabhan, Akshay Modi, Nagendra Kamath, Wenbing Bai, Jackson de Campos, Juan Vimberg, Patrick Strawderman, Dawn Chenette, Yuchen Xie, Andy Yao, and Chen Zheng for designing, developing, and contributing to different parts of the platform.",
      "markdown": "## Building a Media Understanding Platform for ML Innovations\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----9bef9962dcb7--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----9bef9962dcb7--------------------------------)\n\nBy [Guru Tahasildar](https://www.linkedin.com/in/gurutahasildar/), [Amir Ziai](https://www.linkedin.com/in/amirziai/), [Jonathan Solórzano-Hamilton](https://www.linkedin.com/in/peachpie/), [Kelli Griggs](https://www.linkedin.com/in/kelli-griggs-32990125/), [Vi Iyengar](https://www.linkedin.com/in/vi-pallavika-iyengar-144abb1b/)\n\n## Introduction\n\nNetflix leverages machine learning to create the best media for our members. Earlier we shared the details of one of these [algorithms](https://netflixtechblog.com/match-cutting-at-netflix-finding-cuts-with-smooth-visual-transitions-31c3fc14ae59), introduced how our platform team is evolving the [media-specific machine learning ecosystem](https://netflixtechblog.com/scaling-media-machine-learning-at-netflix-f19b400243), and discussed how data from these algorithms gets stored in our [annotation service](https://netflixtechblog.com/scalable-annotation-service-marken-f5ba9266d428).\n\nMuch of the ML literature focuses on model training, evaluation, and scoring. In this post, we will explore an understudied aspect of the ML lifecycle: integration of model outputs into applications.\n\nAn example of using Machine Learning to find shots of Eleven in [Stranger Things](https://www.netflix.com/title/80057281) and surfacing the results in studio application for the consumption of Netflix video editors.\n\nSpecifically, we will dive into the architecture that powers search capabilities for studio applications at Netflix. We discuss specific problems that we have solved using Machine Learning (ML) algorithms, review different pain points that we addressed, and provide a technical overview of our new platform.\n\n## Overview\n\nAt Netflix, we aim to bring joy to our members by providing them with the opportunity to experience outstanding content. There are two components to this experience. First, we must provide the content that will bring them joy. Second, we must make it effortless and intuitive to choose from our library. We must quickly surface the most stand-out highlights from the titles available on our service in the form of images and videos in the member experience.\n\nHere is an example of such an asset created for one of our titles:\n\nThese multimedia assets, or “supplemental” assets, don’t just come into existence. Artists and video editors must create them. We build creator tooling to enable these colleagues to focus their time and energy on creativity. Unfortunately, much of their energy goes into labor-intensive pre-work. A key opportunity is to automate these mundane tasks.\n\n## Use cases\n\n## Use case #1: Dialogue search\n\nDialogue is a central aspect of storytelling. One of the best ways to tell an engaging story is through the mouths of the characters. Punchy or memorable lines are a prime target for trailer editors. The manual method for identifying such lines is a watchdown (aka breakdown).\n\nAn editor watches the title start-to-finish, transcribes memorable words and phrases with a timecode, and retrieves the snippet later if the quote is needed. An editor can choose to do this quickly and only jot down the most memorable moments, but will have to rewatch the content if they miss something they need later. Or, they can do it thoroughly and transcribe the entire piece of content ahead of time. In the words of one of our editors:\n\n> Watchdowns / breakdown are very repetitive and waste countless hours of creative time!\n\nScrubbing through hours of footage (or dozens of hours if working on a series) to find a single line of dialogue is profoundly tedious. In some cases editors need to search across many shows and manually doing it is not feasible. But what if scrubbing and transcribing dialogue is not needed at all?\n\nIdeally, we want to enable dialogue search that supports the following features:\n\n*   Search across one title, a subset of titles (e.g. all dramas), or the entire catalog\n*   Search by character or talent\n*   Multilingual search\n\n## Use case #2: Visual search\n\nA picture is worth a thousand words. Visual storytelling can help make complex stories easier to understand, and as a result, deliver a more impactful message.\n\nArtists and video editors routinely need specific visual elements to include in artworks and trailers. They may scrub for frames, shots, or scenes of specific characters, locations, objects, events (e.g. a car chasing scene in an action movie), or attributes (e.g. a close-up shot). What if we could enable users to find visual elements using natural language?\n\nHere is an example of the desired output when the user searches for “red race car” across the entire content library.\n\nUser searching for “red race car”\n\n## Use case #3: Reverse shot search\n\nNatural-language visual search offers editors a powerful tool. But what if they already have a shot in mind, and they want to find something that just _looks_ similar? For instance, let’s say that an editor has found a visually stunning shot of a plate of food from [Chef’s Table](https://www.netflix.com/title/80007945), and she’s interested in finding similar shots across the entire show.\n\nUser provides a query shot to find other similar shots.\n\n## Prior engineering work\n\n## Approach #1: on-demand batch processing\n\nOur first approach to surface these innovations was a tool to trigger these algorithms on-demand and on a per-show basis. We implemented a batch processing system for users to submit their requests and wait for the system to generate the output. Processing took several hours to complete. Some ML algorithms are computationally intensive. Many of the samples provided had a significant number of frames to process. A typical 1 hour video could contain over 80,000 frames!\n\nAfter waiting for processing, users downloaded the generated algo outputs for offline consumption. This limited pilot system greatly reduced the time spent by our users to manually analyze the content. Here is a visualization of this flow.\n\nOn-demand batch processing system flow\n\n## Approach #2: enabling online request with pre-computation\n\nAfter the success of this approach we decided to add online support for a couple of algorithms. For the first time, users were able to discover matches across the entire catalog, oftentimes finding moments they never knew even existed. They didn’t need any time-consuming local setup and there was no delays since the data was already pre-computed.\n\nInteractive system with pre-computed data flow\n\nThe following quote exemplifies the positive reception by our users:\n\n> “We wanted to find all the shots of the dining room in a show. In seconds, we had what normally would have taken 1–2 people hours/a full day to do, look through all the shots of the dining room from all 10 episodes of the show. Incredible!”  \n> [Dawn Chenette](https://www.linkedin.com/in/dawn-ec/), Design Lead\n\nThis approach had several benefits for product engineering. It allowed us to transparently update the algo data without users knowing about it. It also provided insights into query patterns and algorithms that were gaining traction among users. In addition, we were able to perform a handful of A/B tests to validate or negate our hypotheses for tuning the search experience.\n\n## Pain points\n\nOur early efforts to deliver ML insights to creative professionals proved valuable. At the same time we experienced growing engineering pains that limited our ability to scale.\n\nMaintaining disparate systems posed a challenge. They were first built by different teams on different stacks, so maintenance was expensive. Whenever ML researchers finished a new algorithm they had to integrate it separately into each system. We were near the breaking point with just two systems and a handful of algorithms. We knew this would only worsen as we expanded to more use cases and more researchers.\n\nThe online application unlocked the interactivity for our users and validated our direction. However, it was not scaling well. Adding new algos and onboarding new use cases was still time consuming and required the effort of too many engineers. These investments in one-to-one integrations were volatile with implementation timelines varying from a few weeks to several months. Due to the bespoke nature of the implementation, we lacked catalog wide searches for all available ML sources.\n\nIn summary, this model was a tightly-coupled application-to-data architecture, where machine learning algos were mixed with the backend and UI/UX software code stack. To address the variance in the implementation timelines we needed to standardize how different algorithms were integrated — starting from how they were executed to making the data available to all consumers consistently. As we developed more media understanding algos and wanted to expand to additional use cases, we needed to invest in system architecture redesign to enable researchers and engineers from different teams to innovate independently and collaboratively. Media Search Platform (MSP) is the initiative to address these requirements.\n\nAlthough we were just getting started with _media-search_, search itself is not new to Netflix. We have a mature and robust search and recommendation functionality exposed to millions of our subscribers. We knew we could leverage learnings from our colleagues who are responsible for building and innovating in this space. In keeping with our “[highly aligned, loosely coupled](https://jobs.netflix.com/culture)” culture, we wanted to enable engineers to onboard and improve algos quickly and independently, while making it easy for Studio and product applications to integrate with the media understanding algo capabilities.\n\nMaking the platform modular, pluggable and configurable was key to our success. This approach allowed us to keep the distributed ownership of the platform. It simultaneously provided different specialized teams to contribute relevant components of the platform. We used services already available for other use cases and extended their capabilities to support new requirements.\n\nNext we will discuss the system architecture and describe how different modules interact with each other for end-to-end flow.\n\n## Architecture\n\nSystem Architecture\n\nNetflix engineers strive to iterate rapidly and prefer the “MVP” (minimum viable product) approach to receive early feedback and minimize the upfront investment costs. Thus, we didn’t build all the modules completely. We scoped the pilot implementation to ensure immediate functionalities were unblocked. At the same time, we kept the design open enough to allow future extensibility. We will highlight a few examples below as we discuss each component separately.\n\n## Interfaces - API & Query\n\nStarting at the top of the diagram, the platform allows apps to interact with it using either gRPC or GraphQL interfaces. Having diversity in the interfaces is essential to meet the app-developers where they are. At Netflix, gRPC is predominantly used in backend-to-backend communication. With active GraphQL tooling provided by our developer productivity teams, GraphQL has become a de-facto choice for UI — backend integration. You can find more about what the team has built and how it is getting used in [these blog posts](https://netflixtechblog.com/tagged/graphql). In particular, we have been relying on [Domain Graph Service](https://netflixtechblog.com/open-sourcing-the-netflix-domain-graph-service-framework-graphql-for-spring-boot-92b9dcecda18) Framework for this project.\n\nDuring the query schema design, we accounted for future use cases and ensured that it will allow future extensions. We aimed to keep the schema generic enough so that it hides implementation details of the actual search systems that are used to execute the query. Additionally it is intuitive and easy to understand yet feature rich so that it can be used to express complex queries. Users have flexibility to perform multimodal search with input being a simple text term, image or short video. As discussed earlier, search could be performed against the entire Netflix catalog, or it could be limited to specific titles. Users may prefer results that are organized in some way such as group by a movie, sorted by timestamp. When there are a large number of matches, we allow users to paginate the results (with configurable page size) instead of fetching all or a fixed number of results.\n\n## Search Gateway\n\nThe client generated input query is first given to the Query processing system. Since most of our users are performing targeted queries such as — search for dialogue “friends don’t lie” (from the above example), today this stage performs lightweight processing and provides a hook to integrate A/B testing. In the future we plan to evolve it into a “query understanding system” to support free-form searches to reduce the burden on users and simplify client side query generation.\n\nThe query processing modifies queries to match the target data set. This includes “embedding” transformation and translation. For queries against embedding based data sources it transforms the input such as text or image to corresponding vector representation. Each data source or algorithm could use a different encoding technique so, this stage ensures that the corresponding encoding is also applied to the provided query. One example why we need different encoding techniques per algorithm is because there is different processing for an image — which has a single frame while video — which contains a sequence of multiple frames.\n\nWith global expansion we have users where English is not a primary language. All of the text-based models in the platform are trained using English language so we translate non-English text to English. Although the translation is not always perfect it has worked well in our case and has expanded the eligible user base for our tool to non-English speakers.\n\nOnce the query is transformed and ready for execution, we delegate search execution to one or more of the searcher systems. First we need to federate which query should be routed to which system. This is handled by the Query router and Searcher-proxy module. For the initial implementation we have relied on a single searcher for executing all the queries. Our extensible approach meant the platform could support additional searchers, which have already been used to prototype new algorithms and experiments.\n\nA search may intersect or aggregate the data from multiple algorithms so this layer can fan out a single query into multiple search executions. We have implemented a “searcher-proxy” inside this layer for each supported searcher. Each proxy is responsible for mapping input query to one expected by the corresponding searcher. It then consumes the raw response from the searcher before handing it over to the Results post-processor component.\n\nThe Results post-processor works on the results returned by one or more searchers. It can rank results by applying custom scoring, populate search recommendations based on other similar searches. Another functionality we are evaluating with this layer is to dynamically create different _views_ from the same underlying data.\n\nFor ease of coordination and maintenance we abstracted the query processing and response handling in a module called — Search Gateway.\n\n## Searchers\n\nAs mentioned above, query execution is handled by the searcher system. The primary searcher used in the current implementation is called [_Marken_ — scalable annotation service](https://netflixtechblog.com/scalable-annotation-service-marken-f5ba9266d428) built at Netflix. It supports different categories of searches including full text and embedding vector based similarity searches. It can store and retrieve temporal (timestamp) as well as spatial (coordinates) data. This service leverages Cassandra and Elasticsearch for data storage and retrieval. When onboarding embedding vector data we performed an extensive benchmarking to evaluate the available datastores. One takeaway here is that even if there is a datastore that specializes in a particular query pattern, for ease of maintainability and consistency we decided to not introduce it.\n\nWe have identified a handful of common schema types and standardized how data from different algorithms is stored. Each algorithm still has the flexibility to define a custom schema type. We are actively innovating in this space and recently added capability to intersect data from different algorithms. This is going to unlock creative ways of how the data from multiple algorithms can be superimposed on each other to quickly get to the desired results.\n\n## Algo Execution & Ingestion\n\nSo far we have focused on how the data is queried but, there is an equally complex machinery powering algorithm execution and the generation of the data. This is handled by our dedicated media ML Platform team. The team specializes in building a suite of [media-specific machine learning](https://netflixtechblog.com/scaling-media-machine-learning-at-netflix-f19b400243) tooling. It facilitates seamless access to media assets (audio, video, image and text) in addition to media-centric feature storage and compute orchestration.\n\nFor this project we developed a custom sink that indexes the generated data into Marken according to predefined schemas. Special care is taken when the data is backfilled for the first time so as to avoid overwhelming the system with huge amounts of writes.\n\nLast but not the least, our UI team has built a configurable, extensible library to simplify integrating this platform with end user applications. Configurable UI makes it easy to customize query generation and response handling as per the needs of individual applications and algorithms. The future work involves building native widgets to minimize the UI work even further.\n\n## Summary\n\nThe media understanding platform serves as an abstraction layer between machine learning algos and various applications and features. The platform has already allowed us to seamlessly integrate search and discovery capabilities in several applications. We believe future work in maturing different parts will unlock value for more use cases and applications. We hope this post has offered insights into how we approached its evolution. We will continue to share our work in this space, so stay tuned.\n\nDo these types of challenges interest you? If yes, we’re always looking for [engineers](https://jobs.netflix.com/search?q=software+engineer) and [machine learning practitioners](https://jobs.netflix.com/search?q=%22machine+learning%22) to join us.\n\n## Acknowledgements\n\nSpecial thanks to [Vinod Uddaraju](https://www.linkedin.com/in/vinodvarmauddaraju/), [Fernando Amat Gil](https://www.linkedin.com/in/fernando-amat-6110931/), [Ben Klein](https://www.linkedin.com/in/benjamin-klein-usa/), [Meenakshi Jindal](https://www.linkedin.com/in/meenakshijindal/), [Varun Sekhri](https://www.linkedin.com/in/varun-sekhri-087a213/), [Burak Bacioglu](https://www.linkedin.com/in/burakbacioglu/), [Boris Chen](https://www.linkedin.com/in/boris-chen-b921a214/), [Jason Ge](https://www.linkedin.com/in/jasonge27/), [Tiffany Low](https://www.linkedin.com/in/tiffany-low/), [Vitali Kauhanka](https://www.linkedin.com/in/vitalikauhanka/), [Supriya Vadlamani](https://www.linkedin.com/in/supriya-vadlamani/), [Abhishek Soni](https://www.linkedin.com/in/abhisheks0ni/), [Gustavo Carmo](https://www.linkedin.com/in/gucarmo/), [Elliot Chow](https://www.linkedin.com/in/ellchow/), [Prasanna Padmanabhan](https://www.linkedin.com/in/prasannapadmanabhan/), [Akshay Modi](https://www.linkedin.com/in/akshay-naresh-modi/), [Nagendra Kamath](https://www.linkedin.com/in/nagendrak/), [Wenbing Bai](https://www.linkedin.com/in/wenbingbai/), [Jackson de Campos](https://www.linkedin.com/in/jacksondecampos/), [Juan Vimberg](https://www.linkedin.com/in/jivimberg/), [Patrick Strawderman](https://www.linkedin.com/in/patrickstrawderman/), [Dawn Chenette](https://www.linkedin.com/in/dawn-ec/), [Yuchen Xie](https://www.linkedin.com/in/yuchen-xie-788a3818/), [Andy Yao](https://www.linkedin.com/in/yaoandy/), and [Chen Zheng](https://www.linkedin.com/in/chen-zheng-a70434/) for designing, developing, and contributing to different parts of the platform."
    },
    {
      "url": "https://netflixtechblog.com/migrating-critical-traffic-at-scale-with-no-downtime-part-1-ba1c7a1c7835?source=collection_home---4------16-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/migrating-critical-traffic-at-scale-with-no-downtime-part-1-ba1c7a1c7835?gi=96888f03f036&source=collection_home---4------16-----------------------",
        "loadedTime": "2023-12-06T00:03:45.232Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/migrating-critical-traffic-at-scale-with-no-downtime-part-1-ba1c7a1c7835",
        "title": "Migrating Critical Traffic At Scale with No Downtime — Part 1 | by Netflix Technology Blog | Netflix TechBlog",
        "description": "Hundreds of millions of customers tune into Netflix every day, expecting an uninterrupted and immersive streaming experience. Behind the scenes, a myriad of systems and services are involved in…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Migrating Critical Traffic At Scale with No Downtime — Part 1\nShyam Gala, Javier Fernandez-Ivern, Anup Rokkam Pratap, Devang Shah\nHundreds of millions of customers tune into Netflix every day, expecting an uninterrupted and immersive streaming experience. Behind the scenes, a myriad of systems and services are involved in orchestrating the product experience. These backend systems are consistently being evolved and optimized to meet and exceed customer and product expectations.\nWhen undertaking system migrations, one of the main challenges is establishing confidence and seamlessly transitioning the traffic to the upgraded architecture without adversely impacting the customer experience. This blog series will examine the tools, techniques, and strategies we have utilized to achieve this goal.\nThe backend for the streaming product utilizes a highly distributed microservices architecture; hence these migrations also happen at different points of the service call graph. It can happen on an edge API system servicing customer devices, between the edge and mid-tier services, or from mid-tiers to data stores. Another relevant factor is that the migration could be happening on APIs that are stateless and idempotent, or it could be happening on stateful APIs.\nWe have categorized the tools and techniques we have used to facilitate these migrations in two high-level phases. The first phase involves validating functional correctness, scalability, and performance concerns and ensuring the new systems’ resilience before the migration. The second phase involves migrating the traffic over to the new systems in a manner that mitigates the risk of incidents while continually monitoring and confirming that we are meeting crucial metrics tracked at multiple levels. These include Quality-of-Experience(QoE) measurements at the customer device level, Service-Level-Agreements (SLAs), and business-level Key-Performance-Indicators(KPIs).\nThis blog post will provide a detailed analysis of replay traffic testing, a versatile technique we have applied in the preliminary validation phase for multiple migration initiatives. In a follow-up blog post, we will focus on the second phase and look deeper at some of the tactical steps that we use to migrate the traffic over in a controlled manner.\nReplay Traffic Testing\nReplay traffic refers to production traffic that is cloned and forked over to a different path in the service call graph, allowing us to exercise new/updated systems in a manner that simulates actual production conditions. In this testing strategy, we execute a copy (replay) of production traffic against a system’s existing and new versions to perform relevant validations. This approach has a handful of benefits.\nReplay traffic testing enables sandboxed testing at scale without significantly impacting production traffic or customer experience.\nUtilizing cloned real traffic, we can exercise the diversity of inputs from a wide range of devices and device application software versions in production. This is particularly important for complex APIs that have many high cardinality inputs. Replay traffic provides the reach and coverage required to test the ability of the system to handle infrequently used input combinations and edge cases.\nThis technique facilitates validation on multiple fronts. It allows us to assert functional correctness and provides a mechanism to load test the system and tune the system and scaling parameters for optimal functioning.\nBy simulating a real production environment, we can characterize system performance over an extended period while considering the expected and unexpected traffic pattern shifts. It provides a good read on the availability and latency ranges under different production conditions.\nProvides a platform to ensure that relevant operational insights, metrics, logging, and alerting are in place before migration.\nReplay Solution\nThe replay traffic testing solution comprises two essential components.\nTraffic Duplication and Correlation: The initial step requires the implementation of a mechanism to clone and fork production traffic to the newly established pathway, along with a process to record and correlate responses from the original and alternative routes.\nComparative Analysis and Reporting: Following traffic duplication and correlation, we need a framework to compare and analyze the responses recorded from the two paths and get a comprehensive report for the analysis.\nReplay Testing Framework\nWe have tried different approaches for the traffic duplication and recording step through various migrations, making improvements along the way. These include options where replay traffic generation is orchestrated on the device, on the server, and via a dedicated service. We will examine these alternatives in the upcoming sections.\nDevice Driven\nIn this option, the device makes a request on the production path and the replay path, then discards the response on the replay path. These requests are executed in parallel to minimize any potential delay on the production path. The selection of the replay path on the backend can be driven by the URL the device uses when making the request or by utilizing specific request parameters in routing logic at the appropriate layer of the service call graph. The device also includes a unique identifier with identical values on both paths, which is used to correlate the production and replay responses. The responses can be recorded at the most optimal location in the service call graph or by the device itself, depending on the particular migration.\nDevice Driven Replay\nThe device-driven approach’s obvious downside is that we are wasting device resources. There is also a risk of impact on device QoE, especially on low-resource devices. Adding forking logic and complexity to the device code can create dependencies on device application release cycles that generally run at a slower cadence than service release cycles, leading to bottlenecks in the migration. Moreover, allowing the device to execute untested server-side code paths can inadvertently expose an attack surface area for potential misuse.\nServer Driven\nTo address the concerns of the device-driven approach, the other option we have used is to handle the replay concerns entirely on the backend. The replay traffic is cloned and forked in the appropriate service upstream of the migrated service. The upstream service calls the existing and new replacement services concurrently to minimize any latency increase on the production path. The upstream service records the responses on the two paths along with an identifier with a common value that is used to correlate the responses. This recording operation is also done asynchronously to minimize any impact on the latency on the production path.\nServer Driven Replay\nThe server-driven approach’s benefit is that the entire complexity of replay logic is encapsulated on the backend, and there is no wastage of device resources. Also, since this logic resides on the server side, we can iterate on any required changes faster. However, we are still inserting the replay-related logic alongside the production code that is handling business logic, which can result in unnecessary coupling and complexity. There is also an increased risk that bugs in the replay logic have the potential to impact production code and metrics.\nDedicated Service\nThe latest approach that we have used is to completely isolate all components of replay traffic into a separate dedicated service. In this approach, we record the requests and responses for the service that needs to be updated or replaced to an offline event stream asynchronously. Quite often, this logging of requests and responses is already happening for operational insights. Subsequently, we use Mantis, a distributed stream processor, to capture these requests and responses and replay the requests against the new service or cluster while making any required adjustments to the requests. After replaying the requests, this dedicated service also records the responses from the production and replay paths for offline analysis.\nDedicated Replay Service\nThis approach centralizes the replay logic in an isolated, dedicated code base. Apart from not consuming device resources and not impacting device QoE, this approach also reduces any coupling between production business logic and replay traffic logic on the backend. It also decouples any updates on the replay framework away from the device and service release cycles.\nAnalyzing Replay Traffic\nOnce we have run replay traffic and recorded a statistically significant volume of responses, we are ready for the comparative analysis and reporting component of replay traffic testing. Given the scale of the data being generated using replay traffic, we record the responses from the two sides to a cost-effective cold storage facility using technology like Apache Iceberg. We can then create offline distributed batch processing jobs to correlate & compare the responses across the production and replay paths and generate detailed reports on the analysis.\nNormalization\nDepending on the nature of the system being migrated, the responses might need some preprocessing before being compared. For example, if some fields in the responses are timestamps, those will differ. Similarly, if there are unsorted lists in the responses, it might be best to sort them before comparing. In certain migration scenarios, there may be intentional alterations to the response generated by the updated service or component. For instance, a field that was a list in the original path is represented as key-value pairs in the new path. In such cases, we can apply specific transformations to the response on the replay path to simulate the expected changes. Based on the system and the associated responses, there might be other specific normalizations that we might apply to the response before we compare the responses.\nComparison\nAfter normalizing, we diff the responses on the two sides and check whether we have matching or mismatching responses. The batch job creates a high-level summary that captures some key comparison metrics. These include the total number of responses on both sides, the count of responses joined by the correlation identifier, matches and mismatches. The summary also records the number of passing/ failing responses on each path. This summary provides an excellent high-level view of the analysis and the overall match rate across the production and replay paths. Additionally, for mismatches, we record the normalized and unnormalized responses from both sides to another big data table along with other relevant parameters, such as the diff. We use this additional logging to debug and identify the root cause of issues driving the mismatches. Once we discover and address those issues, we can use the replay testing process iteratively to bring down the mismatch percentage to an acceptable number.\nLineage\nWhen comparing responses, a common source of noise arises from the utilization of non-deterministic or non-idempotent dependency data for generating responses on the production and replay pathways. For instance, envision a response payload that delivers media streams for a playback session. The service responsible for generating this payload consults a metadata service that provides all available streams for the given title. Various factors can lead to the addition or removal of streams, such as identifying issues with a specific stream, incorporating support for a new language, or introducing a new encode. Consequently, there is a potential for discrepancies in the sets of streams used to determine payloads on the production and replay paths, resulting in divergent responses.\nA comprehensive summary of data versions or checksums for all dependencies involved in generating a response, referred to as a lineage, is compiled to address this challenge. Discrepancies can be identified and discarded by comparing the lineage of both production and replay responses in the automated jobs analyzing the responses. This approach mitigates the impact of noise and ensures accurate and reliable comparisons between production and replay responses.\nComparing Live Traffic\nAn alternative method to recording responses and performing the comparison offline is to perform a live comparison. In this approach, we do the forking of the replay traffic on the upstream service as described in the `Server Driven` section. The service that forks and clones the replay traffic directly compares the responses on the production and replay path and records relevant metrics. This option is feasible if the response payload isn’t very complex, such that the comparison doesn’t significantly increase latencies or if the services being migrated are not on the critical path. Logging is selective to cases where the old and new responses do not match.\nReplay Traffic Analysis\nLoad Testing\nBesides functional testing, replay traffic allows us to stress test the updated system components. We can regulate the load on the replay path by controlling the amount of traffic being replayed and the new service’s horizontal and vertical scale factors. This approach allows us to evaluate the performance of the new services under different traffic conditions. We can see how the availability, latency, and other system performance metrics, such as CPU consumption, memory consumption, garbage collection rate, etc, change as the load factor changes. Load testing the system using this technique allows us to identify performance hotspots using actual production traffic profiles. It helps expose memory leaks, deadlocks, caching issues, and other system issues. It enables the tuning of thread pools, connection pools, connection timeouts, and other configuration parameters. Further, it helps in the determination of reasonable scaling policies and estimates for the associated cost and the broader cost/risk tradeoff.\nStateful Systems\nWe have extensively utilized replay testing to build confidence in migrations involving stateless and idempotent systems. Replay testing can also validate migrations involving stateful systems, although additional measures must be taken. The production and replay paths must have distinct and isolated data stores that are in identical states before enabling the replay of traffic. Additionally, all different request types that drive the state machine must be replayed. In the recording step, apart from the responses, we also want to capture the state associated with that specific response. Correspondingly in the analysis phase, we want to compare both the response and the related state in the state machine. Given the overall complexity of using replay testing with stateful systems, we have employed other techniques in such scenarios. We will look at one of them in the follow-up blog post in this series.\nSummary\nWe have adopted replay traffic testing at Netflix for numerous migration projects. A recent example involved leveraging replay testing to validate an extensive re-architecture of the edge APIs that drive the playback component of our product. Another instance included migrating a mid-tier service from REST to gRPC. In both cases, replay testing facilitated comprehensive functional testing, load testing, and system tuning at scale using real production traffic. This approach enabled us to identify elusive issues and rapidly build confidence in these substantial redesigns.\nUpon concluding replay testing, we are ready to start introducing these changes in production. In an upcoming blog post, we will look at some of the techniques we use to roll out significant changes to the system to production in a gradual risk-controlled way while building confidence via metrics at different levels.",
      "markdown": "## Migrating Critical Traffic At Scale with No Downtime — Part 1\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----ba1c7a1c7835--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----ba1c7a1c7835--------------------------------)\n\n[Shyam Gala](https://www.linkedin.com/in/shyam-gala-5891224/), [Javier Fernandez-Ivern](https://www.linkedin.com/in/ivern/), [Anup Rokkam Pratap](https://www.linkedin.com/in/rokkampratap/), [Devang Shah](https://www.linkedin.com/in/shahdewang/)\n\nHundreds of millions of customers tune into Netflix every day, expecting an uninterrupted and immersive streaming experience. Behind the scenes, a myriad of systems and services are involved in orchestrating the product experience. These backend systems are consistently being evolved and optimized to meet and exceed customer and product expectations.\n\n**When undertaking system migrations, one of the main challenges is establishing confidence and seamlessly transitioning the traffic to the upgraded architecture without adversely impacting the customer experience. This blog series will examine the tools, techniques, and strategies we have utilized to achieve this goal.**\n\nThe backend for the streaming product utilizes a highly distributed microservices architecture; hence these migrations also happen at different points of the service call graph. It can happen on an edge API system servicing customer devices, between the edge and mid-tier services, or from mid-tiers to data stores. Another relevant factor is that the migration could be happening on APIs that are stateless and idempotent, or it could be happening on stateful APIs.\n\nWe have categorized the tools and techniques we have used to facilitate these migrations in two high-level phases. The first phase involves validating functional correctness, scalability, and performance concerns and ensuring the new systems’ resilience before the migration. The second phase involves migrating the traffic over to the new systems in a manner that mitigates the risk of incidents while continually monitoring and confirming that we are meeting crucial metrics tracked at multiple levels. These include Quality-of-Experience(QoE) measurements at the customer device level, Service-Level-Agreements (SLAs), and business-level Key-Performance-Indicators(KPIs).\n\n**This blog post will provide a detailed analysis of replay traffic testing, a versatile technique we have applied in the preliminary validation phase for multiple migration initiatives**. **In a** [**follow-up blog post**](https://netflixtechblog.medium.com/migrating-critical-traffic-at-scale-with-no-downtime-part-2-4b1c8c7155c1)**, we will focus on the second phase and look deeper at some of the tactical steps that we use to migrate the traffic over in a controlled manner.**\n\n## Replay Traffic Testing\n\nReplay traffic refers to production traffic that is cloned and forked over to a different path in the service call graph, allowing us to exercise new/updated systems in a manner that simulates actual production conditions. In this testing strategy, we execute a copy (replay) of production traffic against a system’s existing and new versions to perform relevant validations. This approach has a handful of benefits.\n\n*   Replay traffic testing enables **sandboxed testing at scale** without significantly impacting production traffic or customer experience.\n*   Utilizing cloned real traffic, we can exercise the diversity of inputs from a wide range of devices and device application software versions in production. This is particularly important for complex APIs that have many high cardinality inputs. Replay traffic provides the **reach and coverage** required to test the ability of the system to handle infrequently used input combinations and edge cases.\n*   This technique facilitates **validation on multiple fronts**. It allows us to assert functional correctness and provides a mechanism to load test the system and tune the system and scaling parameters for optimal functioning.\n*   By simulating a real production environment, we can **characterize system performance** over an extended period while considering the expected and unexpected traffic pattern shifts. It provides a good read on the availability and latency ranges under different production conditions.\n*   Provides a platform to ensure that relevant **operational insights**, metrics, logging, and alerting are in place before migration.\n\n## Replay Solution\n\nThe replay traffic testing solution comprises two essential components.\n\n1.  Traffic Duplication and Correlation: The initial step requires the implementation of a mechanism to clone and fork production traffic to the newly established pathway, along with a process to record and correlate responses from the original and alternative routes.\n2.  Comparative Analysis and Reporting: Following traffic duplication and correlation, we need a framework to compare and analyze the responses recorded from the two paths and get a comprehensive report for the analysis.\n\nReplay Testing Framework\n\nWe have tried different approaches for the traffic duplication and recording step through various migrations, making improvements along the way. These include options where replay traffic generation is orchestrated on the device, on the server, and via a dedicated service. We will examine these alternatives in the upcoming sections.\n\n**Device Driven**\n\nIn this option, the device makes a request on the production path and the replay path, then discards the response on the replay path. These requests are executed in parallel to minimize any potential delay on the production path. The selection of the replay path on the backend can be driven by the URL the device uses when making the request or by utilizing specific request parameters in routing logic at the appropriate layer of the service call graph. The device also includes a unique identifier with identical values on both paths, which is used to correlate the production and replay responses. The responses can be recorded at the most optimal location in the service call graph or by the device itself, depending on the particular migration.\n\nDevice Driven Replay\n\nThe device-driven approach’s obvious downside is that we are wasting device resources. There is also a risk of impact on device QoE, especially on low-resource devices. Adding forking logic and complexity to the device code can create dependencies on device application release cycles that generally run at a slower cadence than service release cycles, leading to bottlenecks in the migration. Moreover, allowing the device to execute untested server-side code paths can inadvertently expose an attack surface area for potential misuse.\n\n**Server Driven**\n\nTo address the concerns of the device-driven approach, the other option we have used is to handle the replay concerns entirely on the backend. The replay traffic is cloned and forked in the appropriate service upstream of the migrated service. The upstream service calls the existing and new replacement services concurrently to minimize any latency increase on the production path. The upstream service records the responses on the two paths along with an identifier with a common value that is used to correlate the responses. This recording operation is also done asynchronously to minimize any impact on the latency on the production path.\n\nServer Driven Replay\n\nThe server-driven approach’s benefit is that the entire complexity of replay logic is encapsulated on the backend, and there is no wastage of device resources. Also, since this logic resides on the server side, we can iterate on any required changes faster. However, we are still inserting the replay-related logic alongside the production code that is handling business logic, which can result in unnecessary coupling and complexity. There is also an increased risk that bugs in the replay logic have the potential to impact production code and metrics.\n\n**Dedicated Service**\n\nThe latest approach that we have used is to completely isolate all components of replay traffic into a separate dedicated service. In this approach, we record the requests and responses for the service that needs to be updated or replaced to an offline event stream asynchronously. Quite often, this logging of requests and responses is already happening for operational insights. Subsequently, we use [Mantis](https://netflixtechblog.com/stream-processing-with-mantis-78af913f51a6), a distributed stream processor, to capture these requests and responses and replay the requests against the new service or cluster while making any required adjustments to the requests. After replaying the requests, this dedicated service also records the responses from the production and replay paths for offline analysis.\n\nDedicated Replay Service\n\nThis approach centralizes the replay logic in an isolated, dedicated code base. Apart from not consuming device resources and not impacting device QoE, this approach also reduces any coupling between production business logic and replay traffic logic on the backend. It also decouples any updates on the replay framework away from the device and service release cycles.\n\n## Analyzing Replay Traffic\n\nOnce we have run replay traffic and recorded a statistically significant volume of responses, we are ready for the comparative analysis and reporting component of replay traffic testing. Given the scale of the data being generated using replay traffic, we record the responses from the two sides to a cost-effective cold storage facility using technology like [Apache Iceberg](https://iceberg.apache.org/). We can then create offline distributed batch processing jobs to correlate & compare the responses across the production and replay paths and generate detailed reports on the analysis.\n\n**Normalization**\n\nDepending on the nature of the system being migrated, the responses might need some preprocessing before being compared. For example, if some fields in the responses are timestamps, those will differ. Similarly, if there are unsorted lists in the responses, it might be best to sort them before comparing. In certain migration scenarios, there may be intentional alterations to the response generated by the updated service or component. For instance, a field that was a list in the original path is represented as key-value pairs in the new path. In such cases, we can apply specific transformations to the response on the replay path to simulate the expected changes. Based on the system and the associated responses, there might be other specific normalizations that we might apply to the response before we compare the responses.\n\n**Comparison**\n\nAfter normalizing, we diff the responses on the two sides and check whether we have matching or mismatching responses. The batch job creates a high-level summary that captures some key comparison metrics. These include the total number of responses on both sides, the count of responses joined by the correlation identifier, matches and mismatches. The summary also records the number of passing/ failing responses on each path. This summary provides an excellent high-level view of the analysis and the overall match rate across the production and replay paths. Additionally, for mismatches, we record the normalized and unnormalized responses from both sides to another big data table along with other relevant parameters, such as the diff. We use this additional logging to debug and identify the root cause of issues driving the mismatches. Once we discover and address those issues, we can use the replay testing process iteratively to bring down the mismatch percentage to an acceptable number.\n\n**Lineage**\n\nWhen comparing responses, a common source of noise arises from the utilization of non-deterministic or non-idempotent dependency data for generating responses on the production and replay pathways. For instance, envision a response payload that delivers media streams for a playback session. The service responsible for generating this payload consults a metadata service that provides all available streams for the given title. Various factors can lead to the addition or removal of streams, such as identifying issues with a specific stream, incorporating support for a new language, or introducing a new encode. Consequently, there is a potential for discrepancies in the sets of streams used to determine payloads on the production and replay paths, resulting in divergent responses.\n\nA comprehensive summary of data versions or checksums for all dependencies involved in generating a response, referred to as a lineage, is compiled to address this challenge. Discrepancies can be identified and discarded by comparing the lineage of both production and replay responses in the automated jobs analyzing the responses. This approach mitigates the impact of noise and ensures accurate and reliable comparisons between production and replay responses.\n\n## Comparing Live Traffic\n\nAn alternative method to recording responses and performing the comparison offline is to perform a live comparison. In this approach, we do the forking of the replay traffic on the upstream service as described in the \\`Server Driven\\` section. The service that forks and clones the replay traffic directly compares the responses on the production and replay path and records relevant metrics. This option is feasible if the response payload isn’t very complex, such that the comparison doesn’t significantly increase latencies or if the services being migrated are not on the critical path. Logging is selective to cases where the old and new responses do not match.\n\nReplay Traffic Analysis\n\n## Load Testing\n\nBesides functional testing, replay traffic allows us to stress test the updated system components. We can regulate the load on the replay path by controlling the amount of traffic being replayed and the new service’s horizontal and vertical scale factors. This approach allows us to evaluate the performance of the new services under different traffic conditions. We can see how the availability, latency, and other system performance metrics, such as CPU consumption, memory consumption, garbage collection rate, etc, change as the load factor changes. Load testing the system using this technique allows us to identify performance hotspots using actual production traffic profiles. It helps expose memory leaks, deadlocks, caching issues, and other system issues. It enables the tuning of thread pools, connection pools, connection timeouts, and other configuration parameters. Further, it helps in the determination of reasonable scaling policies and estimates for the associated cost and the broader cost/risk tradeoff.\n\n## Stateful Systems\n\nWe have extensively utilized replay testing to build confidence in migrations involving stateless and idempotent systems. Replay testing can also validate migrations involving stateful systems, although additional measures must be taken. The production and replay paths must have distinct and isolated data stores that are in identical states before enabling the replay of traffic. Additionally, all different request types that drive the state machine must be replayed. In the recording step, apart from the responses, we also want to capture the state associated with that specific response. Correspondingly in the analysis phase, we want to compare both the response and the related state in the state machine. Given the overall complexity of using replay testing with stateful systems, we have employed other techniques in such scenarios. We will look at one of them in the follow-up blog post in this series.\n\n## Summary\n\nWe have adopted replay traffic testing at Netflix for numerous migration projects. A recent example involved leveraging replay testing to validate an extensive re-architecture of the edge APIs that drive the playback component of our product. Another instance included migrating a mid-tier service from REST to gRPC. In both cases, replay testing facilitated comprehensive functional testing, load testing, and system tuning at scale using real production traffic. This approach enabled us to identify elusive issues and rapidly build confidence in these substantial redesigns.\n\nUpon concluding replay testing, we are ready to start introducing these changes in production. In an [upcoming blog post](https://netflixtechblog.medium.com/migrating-critical-traffic-at-scale-with-no-downtime-part-2-4b1c8c7155c1), we will look at some of the techniques we use to roll out significant changes to the system to production in a gradual risk-controlled way while building confidence via metrics at different levels."
    },
    {
      "url": "https://netflixtechblog.com/data-ingestion-pipeline-with-operation-management-3c5c638740a8?source=collection_home---4------19-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/data-ingestion-pipeline-with-operation-management-3c5c638740a8?gi=f21911c7f526&source=collection_home---4------19-----------------------",
        "loadedTime": "2023-12-06T00:03:48.038Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/data-ingestion-pipeline-with-operation-management-3c5c638740a8",
        "title": "Data ingestion pipeline with Operation Management (Marken) | Netflix TechBlog",
        "description": "Data ingestion APIs/pipeline used by machine learning teams at Netflix to ingest data in Annotation Service (Marken)",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Data ingestion pipeline with Operation Management\nIntroduction\nAt Netflix, to promote and recommend the content to users in the best possible way there are many Media Algorithm teams which work hand in hand with content creators and editors. Several of these algorithms aim to improve different manual workflows so that we show the personalized promotional image, trailer or the show to the user.\nThese media focused machine learning algorithms as well as other teams generate a lot of data from the media files, which we described in our previous blog, are stored as annotations in Marken. We designed a unique concept called Annotation Operations which allows teams to create data pipelines and easily write annotations without worrying about access patterns of their data from different applications.\nGoals\nAnnotation Operations\nLets pick an example use case of identifying objects (like trees, cars etc.) in a video file. As described in the above picture\nDuring the first run of the algorithm it identified 500 objects in a particular Video file. These 500 objects were stored as annotations of a specific schema type, let’s say Objects, in Marken.\nThe Algorithm team improved their algorithm. Now when we re-ran the algorithm on the same video file it created 600 annotations of schema type Objects and stored them in our service.\nNotice that we cannot update the annotations from previous runs because we don’t know how many annotations a new algorithm run will result into. It is also very expensive for us to keep track of which annotation needs to be updated.\nThe goal is that when the consumer comes and searches for annotations of type Objects for the given video file then the following should happen.\nBefore Algo run 1, if they search they should not find anything.\nAfter the completion of Algo run 1, the query should find the first set of 500 annotations.\nDuring the time when Algo run 2 was creating the set of 600 annotations, clients search should still return the older 500 annotations.\nWhen all of the 600 annotations are successfully created, they should replace the older set of 500.\nSo now when clients search annotations for Objects then they should get 600 annotations.\nDoes this remind you of something? This seems very similar (not exactly same) to a distributed transaction.\nTypically, an algorithm run can have 2k-5k annotations. There are many naive solutions possible for this problem for example:\nWrite different runs in different databases. This is obviously very expensive.\nWrite algo runs into files. But we cannot search or present low latency retrievals from files\nEtc.\nInstead our challenge was to implement this feature on top of Cassandra and ElasticSearch databases because that’s what Marken uses. The solution which we present in this blog is not limited to annotations and can be used for any other domain which uses ES and Cassandra as well.\nMarken Architecture\nMarken’s architecture diagram is as follows. We refer the reader to our previous blog article for details. We use Cassandra as a source of truth where we store the annotations while we index annotations in ElasticSearch to provide rich search functionalities.\nMarken Architecture\nOur goal was to help teams at Netflix to create data pipelines without thinking about how that data is available to the readers or the client teams. Similarly, client teams don’t have to worry about when or how the data is written. This is what we call decoupling producer flows from clients of the data.\nLifecycle of a movie goes through a lot of creative stages. We have many temporary files which are delivered before we get to the final file of the movie. Similarly, a movie has many different languages and each of those languages can have different files delivered. Teams generally want to run algorithms and create annotations using all those media files.\nSince algorithms can be run on a different permutations of how the media files are created and delivered we can simplify an algorithm run as follows\nAnnotation Schema Type — identifies the schema for the annotation generated by the Algorithm.\nAnnotation Schema Version — identifies the schema version of the annotation generated by the Algorithm.\nPivotId — a unique string identifier which identifies the file or method which is used to generate the annotations. This could be the SHA hash of the file or simply the movie Identifier number.\nGiven above we can describe the data model for an annotation operation as follows.\n{\n\"annotationOperationKeys\": [\n{\n\"annotationType\": \"string\", ❶\n\"annotationTypeVersion\": “integer”,\n\"pivotId\": \"string\",\n\"operationNumber\": “integer” ❷\n}\n],\n\"id\": \"UUID\",\n\"operationStatus\": \"STARTED\", ❸\n\"isActive\": true ❹\n}\nWe already explained AnnotationType, AnnotationTypeVersion and PivotId above.\nOperationNumber is an auto incremented number for each new operation.\nOperationStatus — An operation goes through three phases, Started, Finished and Canceled.\nIsActive — Whether an operation and its associated annotations are active and searchable.\nAs you can see from the data model that the producer of an annotation has to choose an AnnotationOperationKey which lets them define how they want UPSERT annotations in an AnnotationOperation. Inside, AnnotationOperationKey the important field is pivotId and how it is generated.\nCassandra Tables\nOur source of truth for all objects in Marken in Cassandra. To store Annotation Operations we have the following main tables.\nAnnotationOperationById — It stores the AnnotationOperations\nAnnotationIdByAnnotationOperationId — it stores the Ids of all annotations in an operation.\nSince Cassandra is NoSql, we have more tables which help us create reverse indices and run admin jobs so that we can scan all annotation operations whenever there is a need.\nElasticSearch\nEach annotation in Marken is also indexed in ElasticSearch for powering various searches. To record the relationship between annotation and operation we also index two fields\nannotationOperationId — The ID of the operation to which this annotation belongs\nisAnnotationOperationActive — Whether the operation is in an ACTIVE state.\nAPIs\nWe provide three APIs to our users. In following sections we describe the APIs and the state management done within the APIs.\nStartAnnotationOperation\nWhen this API is called we store the operation with its OperationKey (tuple of annotationType, annotationType Version and pivotId) in our database. This new operation is marked to be in STARTED state. We store all OperationIDs which are in STARTED state in a distributed cache (EVCache) for fast access during searches.\nStartAnnotationOperation\nUpsertAnnotationsInOperation\nUsers call this API to upsert the annotations in an Operation. They pass annotations along with the OperationID. We store the annotations and also record the relationship between the annotation IDs and the Operation ID in Cassandra. During this phase operations are in isAnnotationOperationActive = ACTIVE and operationStatus = STARTED state.\nNote that typically in one operation run there can be 2K to 5k annotations which can be created. Clients can call this API from many different machines or threads for fast upserts.\nUpsertAnnotationsInOperation\nFinishAnnotationOperation\nOnce the annotations have been created in an operation clients call FinishAnnotationOperation which changes following\nMarks the current operation (let’s say with ID2) to be operationStatus = FINISHED and isAnnotationOperationActive=ACTIVE.\nWe remove the ID2 from the Memcache since it is not in STARTED state.\nAny previous operation (let’s say with ID1) which was ACTIVE is now marked isAnnotationOperationActive=FALSE in Cassandra.\nFinally, we call updateByQuery API in ElasticSearch. This API finds all Elasticsearch documents with ID1 and marks isAnnotationOperationActive=FALSE.\nFinishAnnotationOperation\nSearch API\nThis is the key part for our readers. When a client calls our search API we must exclude\nany annotations which are from isAnnotationOperationActive=FALSE operations or\nfor which Annotation operations are currently in STARTED state. We do that by excluding the following from all queries in our system.\nTo achieve above\nWe add a filter in our ES query to exclude isAnnotationOperationStatus is FALSE.\nWe query EVCache to find out all operations which are in STARTED state. Then we exclude all those annotations with annotationId found in memcache. Using memcache allows us to keep latencies for our search low (most of our queries are less than 100ms).\nError handling\nCassandra is our source of truth so if an error happens we fail the client call. However, once we commit to Cassandra we must handle Elasticsearch errors. In our experience, all errors have happened when the Elasticsearch database is having some issue. In the above case, we created a retry logic for updateByQuery calls to ElasticSearch. If the call fails we push a message to SQS so we can retry in an automated fashion after some interval.\nFuture work\nIn near term, we want to write a high level abstraction single API which can be called by our clients instead of calling three APIs. For example, they can store the annotations in a blob storage like S3 and give us a link to the file as part of the single API.",
      "markdown": "## Data ingestion pipeline with Operation Management\n\n## **Introduction**\n\nAt Netflix, to promote and recommend the content to users in the best possible way there are many Media Algorithm teams which work hand in hand with content creators and editors. Several of these algorithms aim to improve different manual workflows so that we show the personalized promotional image, trailer or the show to the user.\n\nThese media focused machine learning algorithms as well as other teams generate a lot of data from the media files, which we described in our [previous blog](https://netflixtechblog.com/scalable-annotation-service-marken-f5ba9266d428), are stored as annotations in Marken. We designed a unique concept called Annotation Operations which allows teams to create data pipelines and easily write annotations without worrying about access patterns of their data from different applications.\n\n## **Goals**\n\nAnnotation Operations\n\nLets pick an example use case of identifying objects (like trees, cars etc.) in a video file. As described in the above picture\n\n*   During the first run of the algorithm it identified 500 objects in a particular Video file. These 500 objects were stored as annotations of a specific schema type, let’s say Objects, in Marken.\n*   The Algorithm team improved their algorithm. Now when we re-ran the algorithm on the same video file it created 600 annotations of schema type Objects and stored them in our service.\n\nNotice that we cannot update the annotations from previous runs because we don’t know how many annotations a new algorithm run will result into. It is also very expensive for us to keep track of which annotation needs to be updated.\n\nThe goal is that when the consumer comes and searches for annotations of type Objects for the given video file then the following should happen.\n\n*   Before Algo run 1, if they search they should not find anything.\n*   After the completion of Algo run 1, the query should find the first set of 500 annotations.\n*   During the time when Algo run 2 was creating the set of 600 annotations, clients search should still return the older 500 annotations.\n*   When all of the 600 annotations are successfully created, they should replace the older set of 500.\n*   So now when clients search annotations for Objects then they should get 600 annotations.\n\nDoes this remind you of something? This seems very similar (not exactly same) to a distributed transaction.\n\nTypically, an algorithm run can have 2k-5k annotations. There are many naive solutions possible for this problem for example:\n\n*   Write different runs in different databases. This is obviously very expensive.\n*   Write algo runs into files. But we cannot search or present low latency retrievals from files\n*   Etc.\n\nInstead our challenge was to implement this feature on top of Cassandra and ElasticSearch databases because that’s what Marken uses. The solution which we present in this blog is not limited to annotations and can be used for any other domain which uses ES and Cassandra as well.\n\n## **Marken Architecture**\n\nMarken’s architecture diagram is as follows. We refer the reader to our previous [blog article](https://netflixtechblog.com/scalable-annotation-service-marken-f5ba9266d428) for details. We use Cassandra as a source of truth where we store the annotations while we index annotations in ElasticSearch to provide rich search functionalities.\n\nMarken Architecture\n\nOur goal was to help teams at Netflix to create data pipelines without thinking about how that data is available to the readers or the client teams. Similarly, client teams don’t have to worry about when or how the data is written. This is what we call decoupling producer flows from clients of the data.\n\nLifecycle of a movie goes through a lot of creative stages. We have many temporary files which are delivered before we get to the final file of the movie. Similarly, a movie has many different languages and each of those languages can have different files delivered. Teams generally want to run algorithms and create annotations using all those media files.\n\nSince algorithms can be run on a different permutations of how the media files are created and delivered we can simplify an algorithm run as follows\n\n*   Annotation Schema Type — identifies the schema for the annotation generated by the Algorithm.\n*   Annotation Schema Version — identifies the schema version of the annotation generated by the Algorithm.\n*   PivotId — a unique string identifier which identifies the file or method which is used to generate the annotations. This could be the SHA hash of the file or simply the movie Identifier number.\n\nGiven above we can describe the data model for an annotation operation as follows.\n\n{  \n  \"annotationOperationKeys\": \\[  \n    {  \n      \"annotationType\": \"string\",   ❶  \n      \"annotationTypeVersion\": “integer”,  \n      \"pivotId\": \"string\",  \n      \"operationNumber\": “integer”    ❷  \n    }  \n  \\],  \n  \"id\": \"UUID\",  \n  \"operationStatus\": \"STARTED\",   ❸  \n  \"isActive\": true   ❹  \n}\n\n1.  We already explained AnnotationType, AnnotationTypeVersion and PivotId above.\n2.  OperationNumber is an auto incremented number for each new operation.\n3.  OperationStatus — An operation goes through three phases, Started, Finished and Canceled.\n4.  IsActive — Whether an operation and its associated annotations are active and searchable.\n\nAs you can see from the data model that the producer of an annotation has to choose an AnnotationOperationKey which lets them define how they want UPSERT annotations in an AnnotationOperation. Inside, AnnotationOperationKey the important field is pivotId and how it is generated.\n\n## **Cassandra Tables**\n\nOur source of truth for all objects in Marken in Cassandra. To store Annotation Operations we have the following main tables.\n\n*   AnnotationOperationById — It stores the AnnotationOperations\n*   AnnotationIdByAnnotationOperationId — it stores the Ids of all annotations in an operation.\n\nSince Cassandra is NoSql, we have more tables which help us create reverse indices and run admin jobs so that we can scan all annotation operations whenever there is a need.\n\n## **ElasticSearch**\n\nEach annotation in Marken is also indexed in ElasticSearch for powering various searches. To record the relationship between annotation and operation we also index two fields\n\n*   annotationOperationId — The ID of the operation to which this annotation belongs\n*   isAnnotationOperationActive — Whether the operation is in an ACTIVE state.\n\n## **APIs**\n\nWe provide three APIs to our users. In following sections we describe the APIs and the state management done within the APIs.\n\n## **StartAnnotationOperation**\n\nWhen this API is called we store the operation with its OperationKey (tuple of annotationType, annotationType Version and pivotId) in our database. This new operation is marked to be in STARTED state. We store all OperationIDs which are in STARTED state in a distributed cache (EVCache) for fast access during searches.\n\nStartAnnotationOperation\n\n## **UpsertAnnotationsInOperation**\n\nUsers call this API to upsert the annotations in an Operation. They pass annotations along with the OperationID. We store the annotations and also record the relationship between the annotation IDs and the Operation ID in Cassandra. During this phase operations are in isAnnotationOperationActive = ACTIVE and operationStatus = STARTED state.\n\nNote that typically in one operation run there can be 2K to 5k annotations which can be created. Clients can call this API from many different machines or threads for fast upserts.\n\nUpsertAnnotationsInOperation\n\n## **FinishAnnotationOperation**\n\nOnce the annotations have been created in an operation clients call FinishAnnotationOperation which changes following\n\n*   Marks the current operation (let’s say with ID2) to be operationStatus = FINISHED and isAnnotationOperationActive=ACTIVE.\n*   We remove the ID2 from the Memcache since it is not in STARTED state.\n*   Any previous operation (let’s say with ID1) which was ACTIVE is now marked isAnnotationOperationActive=FALSE in Cassandra.\n*   Finally, we call updateByQuery API in ElasticSearch. This API finds all Elasticsearch documents with ID1 and marks isAnnotationOperationActive=FALSE.\n\nFinishAnnotationOperation\n\n## **Search API**\n\nThis is the key part for our readers. When a client calls our search API we must exclude\n\n*   any annotations which are from isAnnotationOperationActive=FALSE operations or\n*   for which Annotation operations are currently in STARTED state. We do that by excluding the following from all queries in our system.\n\nTo achieve above\n\n1.  We add a filter in our ES query to exclude isAnnotationOperationStatus is FALSE.\n2.  We query EVCache to find out all operations which are in STARTED state. Then we exclude all those annotations with annotationId found in memcache. Using memcache allows us to keep latencies for our search low (most of our queries are less than 100ms).\n\n## **Error handling**\n\nCassandra is our source of truth so if an error happens we fail the client call. However, once we commit to Cassandra we must handle Elasticsearch errors. In our experience, all errors have happened when the Elasticsearch database is having some issue. In the above case, we created a retry logic for updateByQuery calls to ElasticSearch. If the call fails we push a message to SQS so we can retry in an automated fashion after some interval.\n\n## **Future work**\n\nIn near term, we want to write a high level abstraction single API which can be called by our clients instead of calling three APIs. For example, they can store the annotations in a blob storage like S3 and give us a link to the file as part of the single API."
    },
    {
      "url": "https://netflixtechblog.com/abac-on-spicedb-enabling-netflixs-complex-identity-types-c118f374fa89?source=collection_home---4------15-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/abac-on-spicedb-enabling-netflixs-complex-identity-types-c118f374fa89?gi=ff5b1d13e006&source=collection_home---4------15-----------------------",
        "loadedTime": "2023-12-06T00:03:47.648Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://authzed.com/blog/abac-on-spicedb-enabling-netflix-complex-identity-types",
        "title": "ABAC on SpiceDB: Enabling Netflix’s Complex Identity Types | by Netflix Technology Blog | Netflix TechBlog",
        "description": "The authorization team at Netflix recently sponsored work to add Attribute Based Access Control (ABAC) support to AuthZed’s open source Google Zanzibar inspired authorization system, SpiceDB. Netflix…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "ABAC on SpiceDB: Enabling Netflix’s Complex Identity Types\nBy Chris Wolfe, Joey Schorr, and Victor Roldán Betancort\nIntroduction\nThe authorization team at Netflix recently sponsored work to add Attribute Based Access Control (ABAC) support to AuthZed’s open source Google Zanzibar inspired authorization system, SpiceDB. Netflix required attribute support in SpiceDB to support core Netflix application identity constructs. This post discusses why Netflix wanted ABAC support in SpiceDB, how Netflix collaborated with AuthZed, the end result–SpiceDB Caveats, and how Netflix may leverage this new feature.\nNetflix is always looking for security, ergonomic, or efficiency improvements, and this extends to authorization tools. Google Zanzibar is exciting to Netflix as it makes it easier to produce authorization decision objects and reverse indexes for resources a principal can access.\nLast year, while experimenting with Zanzibar approaches to authorization, Netflix found SpiceDB, the open source Google Zanzibar inspired permission system, and built a prototype to experiment with modeling. The prototype uncovered trade-offs required to implement Attribute Based Access Control in SpiceDB, which made it poorly suited to Netflix’s core requirements for application identities.\nWhy did Netflix Want Caveated Relationships?\nNetflix application identities are fundamentally attribute based: e.g. an instance of the Data Processor runs in eu-west-1 in the test environment with a public shard.\nAuthorizing these identities is done not only by application name, but by specifying specific attributes on which to match. An application owner might want to craft a policy like “Application members of the EU data processors group can access a PI decryption key”. This is one normal relationship in SpiceDB. But, they might also want to specify a policy for compliance reasons that only allows access to the PI key from data processor instances running in the EU within a sensitive shard. Put another way, an identity should only be considered to have the “is member of the EU-data-processors group” if certain identity attributes (like region==eu) match in addition to the application name. This is a Caveated SpiceDB relationship.\nNetflix Modeling Challenges Before Caveats\nSpiceDB, being a Relationship Based Access Control (ReBAC) system, expected authorization checks to be performed against the existence of a specific relationship between objects. Users fit this model — they have a single user ID to describe who they are. As described above, Netflix applications do not fit this model. Their attributes are used to scope permissions to varying degrees.\nNetflix ran into significant difficulties in trying to fit their existing policy model into relations. To do so Netflix’s design required:\nAn event based mechanism that could ingest information about application autoscaling groups. An autoscaling group isn’t the lowest level of granularity, but it’s relatively close to the lowest level where we’d typically see authorization policy applied.\nIngest the attributes describing the autoscaling group and write them as separate relations. That is for the data-processor, Netflix would need to write relations describing the region, environment, account, application name, etc.\nAt authZ check time, provide the attributes for the identity to check, e.g. “can app bar in us-west-2 access this document.” SpiceDB is then responsible for figuring out which relations map back to the autoscaling group, e.g. name, environment, region, etc.\nA cleanup process to prune stale relationships from the database.\nWhat was problematic about this design? Aside from being complicated, there were a few specific things that made Netflix uncomfortable. The most salient being that it wasn’t resilient to an absence of relationship data, e.g. if a new autoscaling group started and reporting its presence to SpiceDB had not yet happened, the autoscaling group members would be missing necessary permissions to run. All this meant that Netflix would have to write and prune the relationship state with significant freshness requirements. This would be a significant departure from its existing policy based system.\nWhile working through this, Netflix hopped into the SpiceDB Discord to chat about possible solutions and found an open community issue: the caveated relationships proposal.\nThe Beginning of SpiceDB Caveats\nThe SpiceDB community had already explored integrating SpiceDB with Open Policy Agent (OPA) and concluded it strayed too far from Zanzibar’s core promise of global horizontal scalability with strong consistency. With Netflix’s support, the AuthZed team pondered a Zanzibar-native approach to Attribute-Based Access Control.\nThe requirements were captured and published as the caveated relationships proposal on GitHub for feedback from the SpiceDB community. The community’s excitement and interest became apparent through comments, reactions, and conversations on the SpiceDB Discord server. Clearly, Netflix wasn’t the only one facing challenges when reconciling SpiceDB with policy-based approaches, so Netflix decided to help! By sponsoring the project, Netflix was able to help AuthZed prioritize engineering effort and accelerate adding Caveats to SpiceDB.\nBuilding SpiceDB Caveats\nQuick Intro to SpiceDB\nThe SpiceDB Schema Language lays the rules for how to build, traverse, and interpret SpiceDB’s Relationship Graph to make authorization decisions. SpiceDB Relationships, e.g., document:readme writer user:emilia, are stored as relationships that represent a graph within a datastore like CockroachDB or PostgreSQL. SpiceDB walks the graph and decomposes it into subproblems. These subproblems are assigned through consistent hashing and dispatched to a node in a cluster running SpiceDB. Over time, each node caches a subset of subproblems to support a distributed cache, reduce the datastore load, and achieve SpiceDB’s horizontal scalability.\nSpiceDB Caveats Design\nThe fundamental challenge with policies is that their input arguments can change the authorization result as understood by a centralized relationships datastore. If SpiceDB were to cache subproblems that have been “tainted” with policy variables, the likelihood those are reused for other requests would decrease and thus severely affect the cache hit rate. As you’d suspect, this would jeopardize one of the pillars of the system: its ability to scale.\nOnce you accept that adding input arguments to the distributed cache isn’t efficient, you naturally gravitate toward the first question: what if you keep those inputs out of the cached subproblems? They are only known at request-time, so let’s add them as a variable in the subproblem! The cost of propagating those variables, assembling them, and executing the logic pales compared to fetching relationships from the datastore.\nThe next question was: how do you integrate the policy decisions into the relationships graph? The SpiceDB Schema Languages’ core concepts are Relations and Permissions; these are how a developer defines the shape of their relationships and how to traverse them. Naturally, being a graph, it’s fitting to add policy logic at the edges or the nodes. That leaves at least two obvious options: policy at the Relation level, or policy at the Permission level.\nAfter iterating on both options to get a feel for the ergonomics and expressiveness the choice was policy at the relation level. After all, SpiceDB is a Relationship Based Access Control (ReBAC) system. Policy at the relation level allows you to parameterize each relationship, which brought about the saying “this relationship exists, but with a Caveat!.” With this approach, SpiceDB could do request-time relationship vetoing like so:\ndefinition human {}\ncaveat the_answer(received int) {\nreceived == 42\n}\ndefinition the_answer_to_life_the_universe_and_everything {\nrelation humans: human with the_answer\npermission enlightenment = humans\nNetflix and AuthZed discussed the concept of static versus dynamic Caveats as well. A developer would define static Caveat expressions in the SpiceDB Schema, while dynamic Caveats would have expressions defined at run time. The discussion centered around typed versus dynamic programming languages, but given SpiceDB’s Schema Language was designed for type safety, it seemed coherent with the overall design to continue with static Caveats. To support runtime-provided policies, the choice was to introduce expressions as arguments to a Caveat. Keeping the SpiceDB Schema easy to understand was a key driver for this decision.\nFor defining Caveats, the main requirement was to provide an expression language with first-class support for partially-evaluated expressions. Google’s CEL seemed like the obvious choice: a protobuf-native expression language that evaluates in linear time, with first-class support for partial results that can be run at the edge, and is not turing complete. CEL expressions are type-safe, so they wouldn’t cause as many errors at runtime and can be stored in the datastore as a compiled protobuf. Given the near-perfect requirement match, it does make you wonder what Google’s Zanzibar has been up to since the white paper!\nTo execute the logic, SpiceDB would have to return a third response CAVEATED, in addition to ALLOW and DENY, to signal that a result of a CheckPermission request depends on computing an unresolved chain of CEL expressions.\nSpiceDB Caveats needed to allow static input variables to be stored before evaluation to represent the multi-dimensional nature of Netflix application identities. Today, this is called “Caveat context,” defined by the values written in a SpiceDB Schema alongside a Relation and those provided by the client. Think of build time variables as an expansion of a templated CEL expression, and those take precedence over request-time arguments. Here is an example:\ncaveat the_answer(received int, expected int) {\nreceived == expected\n}\nLastly, to deal with scenarios where there are multiple Caveated subproblems, the decision was to collect up a final CEL expression tree before evaluating it. The result of the final evaluation can be ALLOW, DENY, or CAVEATED. Things get trickier with wildcards and SpiceDB APIs, but let’s save that for another post! If the response is CAVEATED, the client receives a list of missing variables needed to properly evaluate the expression.\nTo sum up! The primary design decisions were:\nCaveats defined at the Relation-level, not the Permission-level\nKeep Caveats in line with SpiceDB Schema’s type-safe nature\nSupport well-typed values provided by the caller\nUse Google’s CEL to define Caveat expressions\nIntroduce a new result type: CAVEATED\nHow do SpiceDB Caveats Change Authorizing Netflix Identities?\nSpiceDB Caveats simplify this approach by allowing Netflix to specify authorization policy as they have in the past for applications. Instead of needing to have the entire state of the authorization world persisted as relations, the system can have relations and attributes of the identity used at authorization check time.\nNow Netflix can write a Caveat similar to match_fine , described below, that takes lists of expected attributes, e.g. region, account, etc. This Caveat would allow the specific application named by the relation as long as the context of the authorization check had an observed account, stack, detail, region, and extended attribute values that matched the values in their expected counterparts. This playground has a live version of the schema, relations, etc. with which to experiment.\nA movie resource with the replicate permission and a relation using the match_fine caveat\ndefinition app {}\ncaveat match_fine(\nexpected_accounts list<string>,\nexpected_regions list<string>,\nexpected_stacks list<string>,\nexpected_details list<string>,\nexpected_ext_attrs map<any>,\nobserved_account string,\nobserved_region string,\nobserved_stack string,\nobserved_detail string,\nobserved_ext_attrs map<any>\n) {\nobserved_account in expected_accounts &&\nobserved_region in expected_regions &&\nobserved_stack in expected_stacks &&\nobserved_detail in expected_details &&\nexpected_ext_attrs.isSubtreeOf(observed_ext_attrs)\n}\ndefinition movie {\nrelation replicator: app with match_fine\npermission replicate = replicator\n}\nUsing this SpiceDB Schema we can write a relation to restrict access to the replicator application. It should only be allowed to run when\nIt is in the highrisk or birdie accounts\nAND in either us-west-1 or us-east-1\nAND it has stack bg\nAND it has detail casser\nAND its extended attributes contain the key-value pair ‘foo: bar’\nmovie:newspecial#replicator@app:mover[match_fine:{\"expected_accounts\":[\"highrisk\",\"birdie\"],\"expected_regions\":[\"us-west-1\",\"us-east-1\"],\"expected_stacks\":[\"bg\"],\"expected_details\":[\"casser\"],\"expected_ext_attrs\":{\"foo\":\"bar\"}}]\nWith the playground we can also make assertions that can mirror the behavior we’d see from the CheckPermission API. These assertions make it clear that our caveats work as expected.\nassertTrue:\n- 'movie:newspecial#replicate@app:mover with {\"observed_account\": \"highrisk\", \"observed_region\": \"us-west-1\", \"observed_stack\": \"bg\", \"observed_detail\": \"casser\", \"observed_ext_attrs\": {\"foo\": \"bar\"}}'\nassertFalse:\n- 'movie:newspecial#replicate@app:mover with {\"observed_account\": \"lowrisk\", \"observed_region\": \"us-west-1\", \"observed_stack\": \"bg\", \"observed_detail\": \"casser\", \"observed_ext_attrs\": {\"foo\": \"bar\"}}'\n- 'movie:newspecial#replicate@app:purger with {\"observed_account\": \"highrisk\", \"observed_region\": \"us-west-1\", \"observed_stack\": \"bg\", \"observed_detail\": \"casser\", \"observed_ext_attrs\": {\"foo\": \"bar\"}}'\nClosing\nNetflix and AuthZed are both excited about the collaboration’s outcome. Netflix has another authorization tool it can employ and SpiceDB users have another option with which to perform rich authorization checks. Bridging the gap between policy based authorization and ReBAC is a powerful paradigm that is already benefiting companies looking to Zanzibar based implementations for modernizing their authorization stack.\nAcknowledgments\nChris Wolfe\nJoey Schorr\nVictor Roldán Betancort\nCheston Lee\nAdditional Reading\nWhat is Google Zanzibar\nAnnotated Google Zanzibar Paper\nSpiceDB, an Open Source, Google Zanzibar Inspired Authorization System\nGoogle’s CEL\nSpiceDB Glossary\nTop-3 Most Used SpiceDB Caveat Patterns (authzed.com)\nHow Permissions are Answered in SpiceDB\nNetflix Complex Identities Example Schema",
      "markdown": "## ABAC on SpiceDB: Enabling Netflix’s Complex Identity Types\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----c118f374fa89--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----c118f374fa89--------------------------------)\n\nBy [Chris Wolfe](https://www.linkedin.com/in/chris-w-0a884022/), [Joey Schorr](https://www.linkedin.com/in/joseph-s-4324904/), and [Victor Roldán Betancort](https://www.linkedin.com/in/vroldanbet/)\n\n## Introduction\n\nThe authorization team at Netflix recently sponsored work to add Attribute Based Access Control (ABAC) support to AuthZed’s [open source Google Zanzibar inspired](https://github.com/authzed/spicedb) authorization system, [SpiceDB](https://authzed.com/products/spicedb). Netflix required attribute support in SpiceDB to support core Netflix application identity constructs. This post discusses why Netflix wanted ABAC support in SpiceDB, how Netflix collaborated with AuthZed, the end result–[SpiceDB Caveats](https://authzed.com/docs/reference/caveats), and how Netflix may leverage this new feature.\n\nNetflix is always looking for security, ergonomic, or efficiency improvements, and this extends to authorization tools. [Google Zanzibar](https://authzed.com/blog/what-is-google-zanzibar) is exciting to Netflix as it makes it easier to produce authorization decision objects and reverse indexes for resources a principal can access.\n\nLast year, while experimenting with Zanzibar approaches to authorization, Netflix found SpiceDB, the [open source Google Zanzibar inspired permission system](https://github.com/authzed/spicedb), and built a prototype to experiment with modeling. The prototype uncovered trade-offs required to implement Attribute Based Access Control in SpiceDB, which made it poorly suited to Netflix’s core requirements for application identities.\n\n## Why did Netflix Want Caveated Relationships?\n\nNetflix application identities are fundamentally attribute based: e.g. an instance of the Data Processor runs in eu-west-1 in the test environment with a public shard.\n\nAuthorizing these identities is done not only by application name, but by specifying specific attributes on which to match. An application owner might want to craft a policy like “Application members of the EU data processors group can access a PI decryption key”. This is one normal relationship in SpiceDB. But, they might also want to specify a policy for compliance reasons that only allows access to the PI key from data processor instances running in the EU within a sensitive shard. Put another way, an identity should only be considered to have the “is member of the `EU-data-processors` group” if certain identity attributes (like region==eu) match in addition to the application name. This is a Caveated SpiceDB relationship.\n\n## Netflix Modeling Challenges Before Caveats\n\nSpiceDB, being a Relationship Based Access Control (ReBAC) system, expected authorization checks to be performed against the existence of a specific relationship between objects. Users fit this model — they have a single user ID to describe who they are. As described above, Netflix applications do not fit this model. Their attributes are used to scope permissions to varying degrees.\n\nNetflix ran into significant difficulties in trying to fit their existing policy model into relations. To do so Netflix’s design required:\n\n*   An event based mechanism that could ingest information about application autoscaling groups. An autoscaling group isn’t the lowest level of granularity, but it’s relatively close to the lowest level where we’d typically see authorization policy applied.\n*   Ingest the attributes describing the autoscaling group and write them as separate relations. That is for the data-processor, Netflix would need to write relations describing the region, environment, account, application name, etc.\n*   At authZ check time, provide the attributes for the identity to check, e.g. “can app bar in us-west-2 access this document.” SpiceDB is then responsible for figuring out which relations map back to the autoscaling group, e.g. name, environment, region, etc.\n*   A cleanup process to prune stale relationships from the database.\n\nWhat was problematic about this design? Aside from being complicated, there were a few specific things that made Netflix uncomfortable. The most salient being that i**t wasn’t resilient to an absence of relationship data, e.g. if a new autoscaling group started and reporting its presence to SpiceDB had not yet happened, the autoscaling group members would be missing necessary permissions to run**. All this meant that Netflix would have to write and prune the relationship state with significant freshness requirements. This would be a significant departure from its existing policy based system.\n\nWhile working through this, Netflix hopped into the SpiceDB Discord to chat about possible solutions and found an open community issue: the [caveated relationships proposal](https://github.com/authzed/spicedb/issues/386).\n\n## The Beginning of SpiceDB Caveats\n\nThe SpiceDB community had already explored [integrating SpiceDB with Open Policy Agent (OPA)](https://github.com/authzed/spicedb/issues/158) and concluded it strayed too far from Zanzibar’s core promise of global horizontal scalability with strong consistency. With Netflix’s support, the AuthZed team pondered a Zanzibar-native approach to Attribute-Based Access Control.\n\nThe requirements were captured and published as the [caveated relationships proposal on GitHub](https://github.com/authzed/spicedb/issues/386) for feedback from the SpiceDB community. The community’s excitement and interest became apparent through comments, reactions, and conversations on the [SpiceDB Discord server](https://authzed.com/discord). Clearly, Netflix wasn’t the only one facing challenges when reconciling SpiceDB with policy-based approaches, so Netflix decided to help! By sponsoring the project, Netflix was able to help AuthZed prioritize engineering effort and accelerate adding Caveats to SpiceDB.\n\n## Building SpiceDB Caveats\n\n## Quick Intro to SpiceDB\n\nThe [SpiceDB Schema Language](https://authzed.com/docs/reference/schema-lang) lays the rules for how to build, traverse, and interpret SpiceDB’s Relationship Graph to make authorization decisions. SpiceDB Relationships, e.g., `document:readme writer user:emilia`, are stored as relationships that represent a graph within a datastore like CockroachDB or PostgreSQL. SpiceDB walks the graph and decomposes it into subproblems. These subproblems are assigned through [consistent hashing](https://authzed.com/blog/consistent-hash-load-balancing-grpc/) and dispatched to a node in a cluster running SpiceDB. Over time, each node caches a subset of subproblems to support a distributed cache, reduce the datastore load, and achieve SpiceDB’s horizontal scalability.\n\n## SpiceDB Caveats Design\n\nThe fundamental challenge with policies is that their input arguments can change the authorization result as understood by a centralized relationships datastore. If SpiceDB were to cache subproblems that have been “tainted” with policy variables, the likelihood those are reused for other requests would decrease and thus severely affect the cache hit rate. As you’d suspect, this would jeopardize one of the pillars of the system: its ability to scale.\n\nOnce you accept that adding input arguments to the distributed cache isn’t efficient, you naturally gravitate toward the first question: what if you keep those inputs out of the cached subproblems? They are only known at request-time, so let’s add them as a variable in the subproblem! The cost of propagating those variables, assembling them, and executing the logic pales compared to fetching relationships from the datastore.\n\nThe next question was: how do you integrate the policy decisions into the relationships graph? The SpiceDB Schema Languages’ core concepts are [Relations](https://authzed.com/docs/reference/glossary#relation) and [Permissions](https://authzed.com/docs/reference/glossary#permission); these are how a developer defines the shape of their relationships and how to traverse them. Naturally, being a graph, it’s fitting to add policy logic at the edges or the nodes. That leaves at least two obvious options: **policy at the Relation level, or policy at the Permission level.**\n\nAfter iterating on both options to get a feel for the ergonomics and expressiveness the choice was **policy at the relation level**. After all, SpiceDB is a Relationship Based Access Control (ReBAC) system. Policy at the relation level allows you to parameterize each relationship, which brought about the saying “this relationship exists, but with a Caveat!.” With this approach, SpiceDB could do request-time relationship vetoing like so:\n\ndefinition human {}\n\ncaveat the\\_answer(received int) {  \n  received == 42  \n}  \ndefinition the\\_answer\\_to\\_life\\_the\\_universe\\_and\\_everything {  \n  relation humans: human with the\\_answer  \n  permission enlightenment = humans\n\nNetflix and AuthZed discussed the concept of static versus dynamic Caveats as well. A developer would define static Caveat expressions in the SpiceDB Schema, while dynamic Caveats would have expressions defined at run time. The discussion centered around typed versus dynamic programming languages, but given SpiceDB’s Schema Language was designed for type safety, it seemed coherent with the overall design to continue with static Caveats. To support runtime-provided policies, the choice was to introduce expressions as arguments to a Caveat. Keeping the SpiceDB Schema easy to understand was a key driver for this decision.\n\nFor defining Caveats, the main requirement was to provide an expression language with first-class support for partially-evaluated expressions. [Google’s CEL](https://github.com/google/cel-spec) seemed like the obvious choice: a protobuf-native expression language that evaluates in linear time, with first-class support for partial results that can be run at the edge, and is not turing complete. CEL expressions are type-safe, so they wouldn’t cause as many errors at runtime and can be stored in the datastore as a compiled protobuf. Given the near-perfect requirement match, it does make you wonder what Google’s Zanzibar has been up to since the white paper!\n\nTo execute the logic, SpiceDB would have to return a third response `CAVEATED`, in addition to `ALLOW` and `DENY`, to signal that a result of a CheckPermission request depends on computing an unresolved chain of CEL expressions.\n\nSpiceDB Caveats needed to allow static input variables to be stored before evaluation to represent the multi-dimensional nature of Netflix application identities. Today, this is called “Caveat context,” defined by the values written in a SpiceDB Schema alongside a Relation and those provided by the client. Think of build time variables as an expansion of a templated CEL expression, and those take precedence over request-time arguments. Here is an example:\n\ncaveat the\\_answer(received int, expected int) {  \n  received == expected  \n}\n\nLastly, to deal with scenarios where there are multiple Caveated subproblems, the decision was to collect up a final CEL expression tree before evaluating it. The result of the final evaluation can be `ALLOW`, `DENY`, or `CAVEATED`. Things get trickier with wildcards and SpiceDB APIs, but let’s save that for another post! If the response is `CAVEATED`, the client receives a list of missing variables needed to properly evaluate the expression.\n\nTo sum up! The primary design decisions were:\n\n*   Caveats defined at the Relation-level, not the Permission-level\n*   Keep Caveats in line with SpiceDB Schema’s type-safe nature\n*   Support well-typed values provided by the caller\n*   Use Google’s CEL to define Caveat expressions\n*   Introduce a new result type: `CAVEATED`\n\n## How do SpiceDB Caveats Change Authorizing Netflix Identities?\n\n[SpiceDB Caveats](https://authzed.com/docs/reference/caveats) simplify this approach by allowing Netflix to specify authorization policy as they have in the past for applications. Instead of needing to have the entire state of the authorization world persisted as relations, the system can have relations and attributes of the identity used at authorization check time.\n\nNow Netflix can write a Caveat similar to `match_fine` , described below, that takes lists of expected attributes, e.g. region, account, etc. This Caveat would allow the specific application named by the relation as long as the context of the authorization check had an observed account, stack, detail, region, and extended attribute values that matched the values in their expected counterparts. This [playground](https://play.authzed.com/s/51q8FOZ1PlzG/assertions) has a live version of the schema, relations, etc. with which to experiment.\n\nA movie resource with the replicate permission and a relation using the match\\_fine caveat\n\ndefinition app {}\n\ncaveat match\\_fine(  \n  expected\\_accounts list<string>,  \n  expected\\_regions list<string>,  \n  expected\\_stacks list<string>,  \n  expected\\_details list<string>,  \n  expected\\_ext\\_attrs map<any>,  \n  observed\\_account string,  \n  observed\\_region string,  \n  observed\\_stack string,  \n  observed\\_detail string,  \n  observed\\_ext\\_attrs map<any>  \n) {  \n  observed\\_account in expected\\_accounts &&  \n  observed\\_region in expected\\_regions &&  \n  observed\\_stack in expected\\_stacks &&  \n  observed\\_detail in expected\\_details &&  \n  expected\\_ext\\_attrs.isSubtreeOf(observed\\_ext\\_attrs)  \n}\n\ndefinition movie {  \n  relation replicator: app with match\\_fine  \n  permission replicate = replicator  \n}\n\nUsing this SpiceDB Schema we can write a relation to restrict access to the replicator application. It should only be allowed to run when\n\n*   It is in the `highrisk` or `birdie` accounts\n*   AND in either `us-west-1` or `us-east-1`\n*   AND it has stack `bg`\n*   AND it has detail `casser`\n*   AND its extended attributes contain the key-value pair ‘foo: bar’\n\nmovie:newspecial#replicator@app:mover\\[match\\_fine:{\"expected\\_accounts\":\\[\"highrisk\",\"birdie\"\\],\"expected\\_regions\":\\[\"us-west-1\",\"us-east-1\"\\],\"expected\\_stacks\":\\[\"bg\"\\],\"expected\\_details\":\\[\"casser\"\\],\"expected\\_ext\\_attrs\":{\"foo\":\"bar\"}}\\]\n\nWith the playground we can also make assertions that can mirror the behavior we’d see from the CheckPermission API. These assertions make it clear that our caveats work as expected.\n\nassertTrue:  \n\\- 'movie:newspecial#replicate@app:mover with {\"observed\\_account\": \"highrisk\", \"observed\\_region\": \"us-west-1\", \"observed\\_stack\": \"bg\", \"observed\\_detail\": \"casser\", \"observed\\_ext\\_attrs\": {\"foo\": \"bar\"}}'  \nassertFalse:  \n\\- 'movie:newspecial#replicate@app:mover with {\"observed\\_account\": \"lowrisk\", \"observed\\_region\": \"us-west-1\", \"observed\\_stack\": \"bg\", \"observed\\_detail\": \"casser\", \"observed\\_ext\\_attrs\": {\"foo\": \"bar\"}}'  \n\\- 'movie:newspecial#replicate@app:purger with {\"observed\\_account\": \"highrisk\", \"observed\\_region\": \"us-west-1\", \"observed\\_stack\": \"bg\", \"observed\\_detail\": \"casser\", \"observed\\_ext\\_attrs\": {\"foo\": \"bar\"}}'\n\n## Closing\n\nNetflix and AuthZed are both excited about the collaboration’s outcome. Netflix has another authorization tool it can employ and SpiceDB users have another option with which to perform rich authorization checks. Bridging the gap between policy based authorization and ReBAC is a powerful paradigm that is already benefiting companies looking to Zanzibar based implementations for modernizing their authorization stack.\n\n## Acknowledgments\n\n*   [Chris Wolfe](https://www.linkedin.com/in/chris-w-0a884022/)\n*   [Joey Schorr](https://www.linkedin.com/in/joseph-s-4324904/)\n*   [Victor Roldán Betancort](https://www.linkedin.com/in/vroldanbet/)\n*   [Cheston Lee](https://www.linkedin.com/in/chestonlee/)\n\n## Additional Reading\n\n*   [What is Google Zanzibar](https://authzed.com/blog/what-is-google-zanzibar)\n*   [Annotated Google Zanzibar Paper](https://authzed.com/zanzibar)\n*   [SpiceDB, an Open Source, Google Zanzibar Inspired Authorization System](https://github.com/authzed/spicedb)\n*   [Google’s CEL](https://github.com/google/cel-spec)\n*   SpiceDB [Glossary](https://authzed.com/docs/reference/glossary)\n*   [Top-3 Most Used SpiceDB Caveat Patterns (authzed.com)](https://authzed.com/blog/top-three-caveat-use-cases/)\n*   [How Permissions are Answered in SpiceDB](https://authzed.com/blog/check-it-out)\n*   [Netflix Complex Identities Example Schema](https://play.authzed.com/s/51q8FOZ1PlzG)"
    },
    {
      "url": "https://netflixtechblog.com/scaling-media-machine-learning-at-netflix-f19b400243?source=collection_home---4------20-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/scaling-media-machine-learning-at-netflix-f19b400243?gi=b2cbcb4b3ece&source=collection_home---4------20-----------------------",
        "loadedTime": "2023-12-06T00:03:48.368Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/scaling-media-machine-learning-at-netflix-f19b400243",
        "title": "Scaling Media Machine Learning at Netflix | by Netflix Technology Blog | Netflix TechBlog",
        "description": "We tackle some of the unique challenges of scaling multimodal machine learning models that operate on media assets (video, audio, and text).",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Scaling Media Machine Learning at Netflix\nBy Gustavo Carmo, Elliot Chow, Nagendra Kamath, Akshay Modi, Jason Ge, Wenbing Bai, Jackson de Campos, Lingyi Liu, Pablo Delgado, Meenakshi Jindal, Boris Chen, Vi Iyengar, Kelli Griggs, Amir Ziai, Prasanna Padmanabhan, and Hossein Taghavi\nIntroduction\nIn 2007, Netflix started offering streaming alongside its DVD shipping services. As the catalog grew and users adopted streaming, so did the opportunities for creating and improving our recommendations. With a catalog spanning thousands of shows and a diverse member base spanning millions of accounts, recommending the right show to our members is crucial.\nWhy should members care about any particular show that we recommend? Trailers and artworks provide a glimpse of what to expect in that show. We have been leveraging machine learning (ML) models to personalize artwork and to help our creatives create promotional content efficiently.\nOur goal in building a media-focused ML infrastructure is to reduce the time from ideation to productization for our media ML practitioners. We accomplish this by paving the path to:\nAccessing and processing media data (e.g. video, image, audio, and text)\nTraining large-scale models efficiently\nProductizing models in a self-serve fashion in order to execute on existing and newly arriving assets\nStoring and serving model outputs for consumption in promotional content creation\nIn this post, we will describe some of the challenges of applying machine learning to media assets, and the infrastructure components that we have built to address them. We will then present a case study of using these components in order to optimize, scale, and solidify an existing pipeline. Finally, we’ll conclude with a brief discussion of the opportunities on the horizon.\nInfrastructure challenges and components\nIn this section, we highlight some of the unique challenges faced by media ML practitioners, along with the infrastructure components that we have devised to address them.\nFigure 1 — Media Machine Learning Infrastructure\nMedia Access: Jasper\nIn the early days of media ML efforts, it was very hard for researchers to access media data. Even after gaining access, one needed to deal with the challenges of homogeneity across different assets in terms of decoding performance, size, metadata, and general formatting.\nTo streamline this process, we standardized media assets with pre-processing steps that create and store dedicated quality-controlled derivatives with associated snapshotted metadata. In addition, we provide a unified library that enables ML practitioners to seamlessly access video, audio, image, and various text-based assets.\nMedia Feature Storage: Amber Feature Store\nMedia feature computation tends to be expensive and time-consuming. Many ML practitioners independently computed identical features against the same asset in their ML pipelines.\nTo reduce costs and promote reuse, we have built a feature store in order to memoize features/embeddings tied to media entities. This feature store is equipped with a data replication system that enables copying data to different storage solutions depending on the required access patterns.\nCompute Triggering and Orchestration: Amber Compute\nProductized models must run over newly arriving assets for scoring. In order to satisfy this requirement, ML practitioners had to develop bespoke triggering and orchestration components per pipeline. Over time, these bespoke components became the source of many downstream errors and were difficult to maintain.\nAmber is a suite of multiple infrastructure components that offers triggering capabilities to initiate the computation of algorithms with recursive dependency resolution.\nTraining Performance\nMedia model training poses multiple system challenges in storage, network, and GPUs. We have developed a large-scale GPU training cluster based on Ray, which supports multi-GPU / multi-node distributed training. We precompute the datasets, offload the preprocessing to CPU instances, optimize model operators within the framework, and utilize a high-performance file system to resolve the data loading bottleneck, increasing the entire training system throughput 3–5 times.\nServing and Searching\nMedia feature values can be optionally synchronized to other systems depending on necessary query patterns. One of these systems is Marken, a scalable service used to persist feature values as annotations, which are versioned and strongly typed constructs associated with Netflix media entities such as videos and artwork.\nThis service provides a user-friendly query DSL for applications to perform search operations over these annotations with specific filtering and grouping. Marken provides unique search capabilities on temporal and spatial data by time frames or region coordinates, as well as vector searches that are able to scale up to the entire catalog.\nML practitioners interact with this infrastructure mostly using Python, but there is a plethora of tools and platforms being used in the systems behind the scenes. These include, but are not limited to, Conductor, Dagobah, Metaflow, Titus, Iceberg, Trino, Cassandra, Elastic Search, Spark, Ray, MezzFS, S3, Baggins, FSx, and Java/Scala-based applications with Spring Boot.\nCase study: scaling match cutting using the media ML infra\nThe Media Machine Learning Infrastructure is empowering various scenarios across Netflix, and some of them are described here. In this section, we showcase the use of this infrastructure through the case study of Match Cutting.\nBackground\nMatch Cutting is a video editing technique. It’s a transition between two shots that uses similar visual framing, composition, or action to fluidly bring the viewer from one scene to the next. It is a powerful visual storytelling tool used to create a connection between two scenes.\nFigure 2 — a series of frame match cuts from Wednesday.\nIn an earlier post, we described how we’ve used machine learning to find candidate pairs. In this post, we will focus on the engineering and infrastructure challenges of delivering this feature.\nWhere we started\nInitially, we built Match Cutting to find matches across a single title (i.e. either a movie or an episode within a show). An average title has 2k shots, which means that we need to enumerate and process ~2M pairs.\nFigure 3- The original Match Cutting pipeline before leveraging media ML infrastructure components.\nThis entire process was encapsulated in a single Metaflow flow. Each step was mapped to a Metaflow step, which allowed us to control the amount of resources used per step.\nStep 1\nWe download a video file and produce shot boundary metadata. An example of this data is provided below:\nSB = {0: [0, 20], 1: [20, 30], 2: [30, 85], …}\nEach key in the SB dictionary is a shot index and each value represents the frame range corresponding to that shot index. For example, for the shot with index 1 (the second shot), the value captures the shot frame range [20, 30], where 20 is the start frame and 29 is the end frame (i.e. the end of the range is exclusive while the start is inclusive).\nUsing this data, we then materialized individual clip files (e.g. clip0.mp4, clip1.mp4, etc) corresponding to each shot so that they can be processed in Step 2.\nStep 2\nThis step works with the individual files produced in Step 1 and the list of shot boundaries. We first extract a representation (aka embedding) of each file using a video encoder (i.e. an algorithm that converts a video to a fixed-size vector) and use that embedding to identify and remove duplicate shots.\nIn the following example SB_deduped is the result of deduplicating SB:\n# the second shot (index 1) was removed and so was clip1.mp4\nSB_deduped = {0: [0, 20], 2: [30, 85], …}\nSB_deduped along with the surviving files are passed along to step 3.\nStep 3\nWe compute another representation per shot, depending on the flavor of match cutting.\nStep 4\nWe enumerate all pairs and compute a score for each pair of representations. These scores are stored along with the shot metadata:\n[\n# shots with indices 12 and 729 have a high matching score\n{shot1: 12, shot2: 729, score: 0.96},\n# shots with indices 58 and 419 have a low matching score\n{shot1: 58, shot2: 410, score: 0.02},\n…\n]\nStep 5\nFinally, we sort the results by score in descending order and surface the top-K pairs, where K is a parameter.\nThe problems we faced\nThis pattern works well for a single flavor of match cutting and finding matches within the same title. As we started venturing beyond single-title and added more flavors, we quickly faced a few problems.\nLack of standardization\nThe representations we extract in Steps 2 and Step 3 are sensitive to the characteristics of the input video files. In some cases such as instance segmentation, the output representation in Step 3 is a function of the dimensions of the input file.\nNot having a standardized input file format (e.g. same encoding recipes and dimensions) created matching quality issues when representations across titles with different input files needed to be processed together (e.g. multi-title match cutting).\nWasteful repeated computations\nSegmentation at the shot level is a common task used across many media ML pipelines. Also, deduplicating similar shots is a common step that a subset of those pipelines share.\nWe realized that memoizing these computations not only reduces waste but also allows for congruence between algo pipelines that share the same preprocessing step. In other words, having a single source of truth for shot boundaries helps us guarantee additional properties for the data generated downstream. As a concrete example, knowing that algo A and algo B both used the same shot boundary detection step, we know that shot index i has identical frame ranges in both. Without this knowledge, we’ll have to check if this is actually true.\nGaps in media-focused pipeline triggering and orchestration\nOur stakeholders (i.e. video editors using match cutting) need to start working on titles as quickly as the video files land. Therefore, we built a mechanism to trigger the computation upon the landing of new video files. This triggering logic turned out to present two issues:\nLack of standardization meant that the computation was sometimes re-triggered for the same video file due to changes in metadata, without any content change.\nMany pipelines independently developed similar bespoke components for triggering computation, which created inconsistencies.\nAdditionally, decomposing the pipeline into modular pieces and orchestrating computation with dependency semantics did not map to existing workflow orchestrators such as Conductor and Meson out of the box. The media machine learning domain needed to be mapped with some level of coupling between media assets metadata, media access, feature storage, feature compute and feature compute triggering, in a way that new algorithms could be easily plugged with predefined standards.\nThis is where Amber comes in, offering a Media Machine Learning Feature Development and Productization Suite, gluing all aspects of shipping algorithms while permitting the interdependency and composability of multiple smaller parts required to devise a complex system.\nEach part is in itself an algorithm, which we call an Amber Feature, with its own scope of computation, storage, and triggering. Using dependency semantics, an Amber Feature can be plugged into other Amber Features, allowing for the composition of a complex mesh of interrelated algorithms.\nMatch Cutting across titles\nStep 4 entails a computation that is quadratic in the number of shots. For instance, matching across a series with 10 episodes with an average of 2K shots per episode translates into 200M comparisons. Matching across 1,000 files (across multiple shows) would take approximately 200 trillion computations.\nSetting aside the sheer number of computations required momentarily, editors may be interested in considering any subset of shows for matching. The naive approach is to pre-compute all possible subsets of shows. Even assuming that we only have 1,000 video files, this means that we have to pre-compute 2¹⁰⁰⁰ subsets, which is more than the number of atoms in the observable universe!\nIdeally, we want to use an approach that avoids both issues.\nWhere we landed\nThe Media Machine Learning Infrastructure provided many of the building blocks required for overcoming these hurdles.\nStandardized video encodes\nThe entire Netflix catalog is pre-processed and stored for reuse in machine learning scenarios. Match Cutting benefits from this standardization as it relies on homogeneity across videos for proper matching.\nShot segmentation and deduplication reuse\nVideos are matched at the shot level. Since breaking videos into shots is a very common task across many algorithms, the infrastructure team provides this canonical feature that can be used as a dependency for other algorithms. With this, we were able to reuse memoized feature values, saving on compute costs and guaranteeing coherence of shot segments across algos.\nOrchestrating embedding computations\nWe have used Amber’s feature dependency semantics to tie the computation of embeddings to shot deduplication. Leveraging Amber’s triggering, we automatically initiate scoring for new videos as soon as the standardized video encodes are ready. Amber handles the computation in the dependency chain recursively.\nFeature value storage\nWe store embeddings in Amber, which guarantees immutability, versioning, auditing, and various metrics on top of the feature values. This also allows other algorithms to be built on top of the Match Cutting output as well as all the intermediate embeddings.\nPair computation and sink to Marken\nWe have also used Amber’s synchronization mechanisms to replicate data from the main feature value copies to Marken, which is used for serving.\nMedia Search Platform\nUsed to serve high-scoring pairs to video editors in internal applications via Marken.\nThe following figure depicts the new pipeline using the above-mentioned components:\nFigure 4 — Match cutting pipeline built using media ML infrastructure components. Interactions between algorithms are expressed as a feature mesh, and each Amber Feature encapsulates triggering and compute.\nConclusion and Future Work\nThe intersection of media and ML holds numerous prospects for innovation and impact. We examined some of the unique challenges that media ML practitioners face and presented some of our early efforts in building a platform that accommodates the scaling of ML solutions.\nIn addition to the promotional media use cases we discussed, we are extending the infrastructure to facilitate a growing set of use cases. Here are just a few examples:\nML-based VFX tooling\nImproving recommendations using a suite of content understanding models\nEnriching content understanding ML and creative tooling by leveraging personalization signals and insights\nIn future posts, we’ll dive deeper into more details about the solutions built for each of the components we have briefly described in this post.\nIf you’re interested in media ML, we’re always looking for engineers and ML researchers and engineers to join us!\nAcknowledgments\nSpecial thanks to Ben Klein, Fernando Amat Gil, Varun Sekhri, Guru Tahasildar, and Burak Bacioglu for contributing to ideas, designs, and discussions.",
      "markdown": "## **Scaling Media Machine Learning at Netflix**\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----f19b400243--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----f19b400243--------------------------------)\n\nBy [Gustavo Carmo](https://www.linkedin.com/in/gucarmo/), [Elliot Chow](https://www.linkedin.com/in/ellchow/), [Nagendra Kamath](https://www.linkedin.com/in/nagendrak), [Akshay Modi](https://www.linkedin.com/in/akshay-naresh-modi), [Jason Ge](https://www.linkedin.com/in/jasonge27), [Wenbing Bai](https://www.linkedin.com/in/wenbingbai), [Jackson de Campos](https://www.linkedin.com/in/jacksondecampos), [Lingyi Liu](https://www.linkedin.com/in/lingyi-liu-4b866016/), [Pablo Delgado](https://www.linkedin.com/in/pabloadelgado), [Meenakshi Jindal](https://www.linkedin.com/in/meenakshijindal), [Boris Chen](https://www.linkedin.com/in/boris-chen-b921a214/), [Vi Iyengar](https://www.linkedin.com/in/vi-pallavika-iyengar-144abb1b/), [Kelli Griggs](https://www.linkedin.com/in/kelli-griggs-32990125/), [Amir Ziai](https://linkedin.com/in/amirziai), [Prasanna Padmanabhan](https://www.linkedin.com/in/prasannapadmanabhan), and [Hossein Taghavi](https://www.linkedin.com/in/mhtaghavi/)\n\n## Introduction\n\nIn 2007, Netflix started offering streaming alongside its DVD shipping services. As the catalog grew and users adopted streaming, so did the opportunities for creating and improving our recommendations. With a catalog spanning thousands of shows and a diverse member base spanning millions of accounts, recommending the right show to our members is crucial.\n\nWhy should members care about any particular show that we recommend? Trailers and artworks provide a glimpse of what to expect in that show. We have been leveraging machine learning (ML) models to [personalize artwork](https://netflixtechblog.com/artwork-personalization-c589f074ad76) and to help our [creatives create promotional content](https://netflixtechblog.com/new-series-creating-media-with-machine-learning-5067ac110bcd) efficiently.\n\nOur goal in building a media-focused ML infrastructure is to reduce the time from ideation to productization for our media ML practitioners. We accomplish this by paving the path to:\n\n*   **Accessing** and processing **media data** (e.g. video, image, audio, and text)\n*   **Training** large-scale models efficiently\n*   **Productizing** models in a self-serve fashion in order to execute on existing and newly arriving assets\n*   **Storing** and **serving** model outputs for consumption in promotional content creation\n\nIn this post, we will describe some of the challenges of applying machine learning to media assets, and the infrastructure components that we have built to address them. We will then present a case study of using these components in order to optimize, scale, and solidify an existing pipeline. Finally, we’ll conclude with a brief discussion of the opportunities on the horizon.\n\n## Infrastructure challenges and components\n\nIn this section, we highlight some of the unique challenges faced by media ML practitioners, along with the infrastructure components that we have devised to address them.\n\nFigure 1 — Media Machine Learning Infrastructure\n\n## _Media Access: Jasper_\n\nIn the early days of media ML efforts, it was very hard for researchers to access media data. Even after gaining access, one needed to deal with the challenges of homogeneity across different assets in terms of decoding performance, size, metadata, and general formatting.\n\nTo streamline this process, we standardized media assets with pre-processing steps that create and store dedicated quality-controlled derivatives with associated snapshotted metadata. In addition, we provide a unified library that enables ML practitioners to seamlessly access video, audio, image, and various text-based assets.\n\n## **_Media Feature Storage: Amber Feature Store_**\n\nMedia feature computation tends to be expensive and time-consuming. Many ML practitioners independently computed identical features against the same asset in their ML pipelines.\n\nTo reduce costs and promote reuse, we have built a feature store in order to memoize features/embeddings tied to media entities. This feature store is equipped with a data replication system that enables copying data to different storage solutions depending on the required access patterns.\n\n## **_Compute Triggering and Orchestration: Amber Compute_**\n\nProductized models must run over newly arriving assets for scoring. In order to satisfy this requirement, ML practitioners had to develop bespoke triggering and orchestration components per pipeline. Over time, these bespoke components became the source of many downstream errors and were difficult to maintain.\n\nAmber is a suite of multiple infrastructure components that offers triggering capabilities to initiate the computation of algorithms with recursive dependency resolution.\n\n## **_Training Performance_**\n\nMedia model training poses multiple system challenges in storage, network, and GPUs. We have developed a large-scale GPU training cluster based on [Ray](https://www.ray.io/), which supports multi-GPU / multi-node distributed training. We precompute the datasets, offload the preprocessing to CPU instances, optimize model operators within the framework, and utilize a high-performance file system to resolve the data loading bottleneck, increasing the entire training system throughput 3–5 times.\n\n## **_Serving and Searching_**\n\nMedia feature values can be optionally synchronized to other systems depending on necessary query patterns. One of these systems is [Marken](https://netflixtechblog.com/scalable-annotation-service-marken-f5ba9266d428), a scalable service used to persist feature values as annotations, which are versioned and strongly typed constructs associated with Netflix media entities such as videos and artwork.\n\nThis service provides a user-friendly query DSL for applications to perform search operations over these annotations with specific filtering and grouping. [Marken](https://netflixtechblog.com/scalable-annotation-service-marken-f5ba9266d428) provides unique search capabilities on temporal and spatial data by time frames or region coordinates, as well as vector searches that are able to scale up to the entire catalog.\n\nML practitioners interact with this infrastructure mostly using Python, but there is a plethora of tools and platforms being used in the systems behind the scenes. These include, but are not limited to, [Conductor](https://conductor.netflix.com/), [Dagobah](https://www.youtube.com/watch?v=V2E1PdboYLk), [Metaflow](https://metaflow.org/), [Titus](https://netflix.github.io/titus/), [Iceberg](https://github.com/Netflix/iceberg), Trino, Cassandra, Elastic Search, Spark, Ray, [MezzFS](https://netflixtechblog.com/mezzfs-mounting-object-storage-in-netflixs-media-processing-platform-cda01c446ba), S3, [Baggins](https://www.infoq.com/presentations/netflix-drive/), [FSx](https://aws.amazon.com/fsx/), and Java/Scala-based applications with Spring Boot.\n\n## Case study: scaling match cutting using the media ML infra\n\nThe _Media Machine Learning Infrastructure_ is empowering various scenarios across Netflix, and some of them are described [here](https://netflixtechblog.medium.com/new-series-creating-media-with-machine-learning-5067ac110bcd). In this section, we showcase the use of this infrastructure through the case study of [_Match Cutting_](https://netflixtechblog.com/match-cutting-at-netflix-finding-cuts-with-smooth-visual-transitions-31c3fc14ae59).\n\n## Background\n\n_Match Cutting_ is a video editing technique. It’s a transition between two [shots](https://en.wikipedia.org/wiki/Shot_(filmmaking)#:~:text=In%20filmmaking%20and%20video%20production,express%20emotion%2C%20ideas%20and%20movement.) that uses similar visual framing, composition, or action to fluidly bring the viewer from one scene to the next. It is a powerful visual storytelling tool used to create a connection between two scenes.\n\nFigure 2 — a series of frame match cuts from [Wednesday](https://www.netflix.com/title/81231974).\n\nIn [an earlier post](https://netflixtechblog.com/match-cutting-at-netflix-finding-cuts-with-smooth-visual-transitions-31c3fc14ae59), we described how we’ve used machine learning to find candidate pairs. In this post, we will focus on the engineering and infrastructure challenges of delivering this feature.\n\n## Where we started\n\nInitially, we built _Match Cutting_ to find matches across a single title (i.e. either a movie or an episode within a show). An average title has 2k shots, which means that we need to enumerate and process ~2M pairs.\n\nFigure 3- The original Match Cutting pipeline before leveraging media ML infrastructure components.\n\nThis entire process was encapsulated in a single [Metaflow](https://metaflow.org/) flow. Each step was mapped to a Metaflow [step](https://docs.metaflow.org/metaflow/basics#what-should-be-a-step), which allowed us to control the amount of resources used per step.\n\n**Step 1**\n\nWe download a video file and produce [shot](https://en.wikipedia.org/wiki/Shot_(filmmaking)) boundary metadata. An example of this data is provided below:\n\nSB = {0: \\[0, 20\\], 1: \\[20, 30\\], 2: \\[30, 85\\], …}\n\nEach key in the `SB` dictionary is a shot index and each value represents the frame range corresponding to that shot index. For example, for the shot with index `1` (the second shot), the value captures the shot frame range `[20, 30]`, where `20` is the start frame and `29` is the end frame (i.e. the end of the range is exclusive while the start is inclusive).\n\nUsing this data, we then materialized individual clip files (e.g. `clip0.mp4`, `clip1.mp4`, etc) corresponding to each shot so that they can be processed in Step 2.\n\n**Step 2**\n\nThis step works with the individual files produced in _Step 1_ and the list of shot boundaries. We first extract a representation (aka embedding) of each file using a video encoder (i.e. an algorithm that converts a video to a fixed-size vector) and use that embedding to identify and remove duplicate shots.\n\nIn the following example `SB_deduped` is the result of deduplicating `SB`:\n\n\\# the second shot (index 1) was removed and so was clip1.mp4  \nSB\\_deduped = {0: \\[0, 20\\], 2: \\[30, 85\\], …}\n\n`SB_deduped` along with the surviving files are passed along to step 3.\n\n**Step 3**\n\nWe compute another representation per shot, depending on the flavor of match cutting.\n\n**Step 4**\n\nWe enumerate all pairs and compute a score for each pair of representations. These scores are stored along with the shot metadata:\n\n\\[  \n  # shots with indices 12 and 729 have a high matching score  \n  {shot1: 12, shot2: 729, score: 0.96},  \n  # shots with indices 58 and 419 have a low matching score  \n  {shot1: 58, shot2: 410, score: 0.02},  \n  …  \n\\]\n\n**Step 5**\n\nFinally, we sort the results by score in descending order and surface the top-`K` pairs, where `K` is a parameter.\n\n## The problems we faced\n\nThis pattern works well for a single flavor of match cutting and finding matches within the same title. As we started venturing beyond single-title and added more flavors, we quickly faced a few problems.\n\n**Lack of standardization**\n\nThe representations we extract in _Steps 2_ and _Step 3_ are sensitive to the characteristics of the input video files. In some cases such as instance segmentation, the output representation in _Step 3_ is a function of the dimensions of the input file.\n\nNot having a standardized input file format (e.g. same encoding recipes and dimensions) created matching quality issues when representations across titles with different input files needed to be processed together (e.g. multi-title match cutting).\n\n**Wasteful repeated computations**\n\nSegmentation at the shot level is a common task used across many media ML pipelines. Also, deduplicating similar shots is a common step that a subset of those pipelines share.\n\nWe realized that memoizing these computations not only reduces waste but also allows for congruence between algo pipelines that share the same preprocessing step. In other words, having a single source of truth for shot boundaries helps us guarantee additional properties for the data generated downstream. As a concrete example, knowing that algo `A` and algo `B` both used the same shot boundary detection step, we know that shot index `i` has identical frame ranges in both. Without this knowledge, we’ll have to check if this is actually true.\n\n**Gaps in media-focused pipeline triggering and orchestration**\n\nOur stakeholders (i.e. video editors using match cutting) need to start working on titles as quickly as the video files land. Therefore, we built a mechanism to trigger the computation upon the landing of new video files. This triggering logic turned out to present two issues:\n\n1.  Lack of standardization meant that the computation was sometimes re-triggered for the same video file due to changes in metadata, without any content change.\n2.  Many pipelines independently developed similar bespoke components for triggering computation, which created inconsistencies.\n\nAdditionally, decomposing the pipeline into modular pieces and orchestrating computation with dependency semantics did not map to existing workflow orchestrators such as [Conductor](https://conductor.netflix.com/) and [Meson](https://netflixtechblog.com/meson-workflow-orchestration-for-netflix-recommendations-fc932625c1d9) out of the box. The media machine learning domain needed to be mapped with some level of coupling between media assets metadata, media access, feature storage, feature compute and feature compute triggering, in a way that new algorithms could be easily plugged with predefined standards.\n\nThis is where _Amber_ comes in, offering a _Media Machine Learning Feature Development and Productization Suite,_ gluing all aspects of shipping algorithms while permitting the interdependency and composability of multiple smaller parts required to devise a complex system.\n\nEach part is in itself an algorithm, which we call an _Amber Feature_, with its own scope of computation, storage, and triggering. Using dependency semantics, an _Amber Feature_ can be plugged into other _Amber Features_, allowing for the composition of a complex mesh of interrelated algorithms.\n\n**Match Cutting across titles**\n\n_Step 4_ entails a computation that is quadratic in the number of shots. For instance, matching across a series with 10 episodes with an average of 2K shots per episode translates into 200M comparisons. Matching across 1,000 files (across multiple shows) would take approximately 200 trillion computations.\n\nSetting aside the sheer number of computations required momentarily, editors may be interested in considering any subset of shows for matching. The naive approach is to pre-compute all possible subsets of shows. Even assuming that we only have 1,000 video files, this means that we have to pre-compute 2¹⁰⁰⁰ subsets, which is more than the [number of atoms in the observable universe](https://en.wikipedia.org/wiki/Observable_universe#Matter_content%E2%80%94number_of_atoms)!\n\nIdeally, we want to use an approach that avoids both issues.\n\n## **Where we landed**\n\nThe _Media Machine Learning Infrastructure_ provided many of the building blocks required for overcoming these hurdles.\n\n**Standardized video encodes**\n\nThe entire Netflix catalog is pre-processed and stored for reuse in machine learning scenarios. _Match Cutting_ benefits from this standardization as it relies on homogeneity across videos for proper matching.\n\n**Shot segmentation and deduplication reuse**\n\nVideos are matched at the shot level. Since breaking videos into shots is a very common task across many algorithms, the infrastructure team provides this canonical feature that can be used as a dependency for other algorithms. With this, we were able to reuse memoized feature values, saving on compute costs and guaranteeing coherence of shot segments across algos.\n\n**Orchestrating embedding computations**\n\nWe have used _Amber_’s feature dependency semantics to tie the computation of embeddings to shot deduplication. Leveraging _Amber_’s triggering, we automatically initiate scoring for new videos as soon as the standardized video encodes are ready. _Amber_ handles the computation in the dependency chain recursively.\n\n**Feature value storage**\n\nWe store embeddings in _Amber_, which guarantees immutability, versioning, auditing, and various metrics on top of the feature values. This also allows other algorithms to be built on top of the _Match Cutting_ output as well as all the intermediate embeddings.\n\n**Pair computation and sink to** [**Marken**](https://netflixtechblog.com/scalable-annotation-service-marken-f5ba9266d428)\n\nWe have also used _Amber’s_ synchronization mechanisms to replicate data from the main feature value copies to [_Marken_](https://netflixtechblog.com/scalable-annotation-service-marken-f5ba9266d428), which is used for serving.\n\n**Media Search Platform**\n\nUsed to serve high-scoring pairs to video editors in internal applications via [_Marken_](https://netflixtechblog.com/scalable-annotation-service-marken-f5ba9266d428).\n\nThe following figure depicts the new pipeline using the above-mentioned components:\n\nFigure 4 — Match cutting pipeline built using media ML infrastructure components. Interactions between algorithms are expressed as a feature mesh, and each Amber Feature encapsulates triggering and compute.\n\n## Conclusion and Future Work\n\nThe intersection of media and ML holds numerous prospects for innovation and impact. We examined some of the unique challenges that media ML practitioners face and presented some of our early efforts in building a platform that accommodates the scaling of ML solutions.\n\nIn addition to the promotional media use cases we discussed, we are extending the infrastructure to facilitate a growing set of use cases. Here are just a few examples:\n\n*   ML-based VFX tooling\n*   Improving recommendations using a suite of content understanding models\n*   Enriching content understanding ML and creative tooling by leveraging personalization signals and insights\n\nIn future posts, we’ll dive deeper into more details about the solutions built for each of the components we have briefly described in this post.\n\nIf you’re interested in media ML, we’re always looking for [engineers](https://jobs.netflix.com/search?q=engineer) and ML researchers and engineers to join us!\n\n## **Acknowledgments**\n\nSpecial thanks to [Ben Klein](https://www.linkedin.com/in/benjamin-klein-usa/), [Fernando Amat Gil](https://www.linkedin.com/in/fernando-amat-6110931/), [Varun Sekhri](https://www.linkedin.com/in/varun-sekhri-087a213/), [Guru Tahasildar](https://www.linkedin.com/in/gurutahasildar/), and [Burak Bacioglu](https://www.linkedin.com/in/burakbacioglu/) for contributing to ideas, designs, and discussions."
    },
    {
      "url": "https://netflixtechblog.com/discovering-creative-insights-in-promotional-artwork-295e4d788db5?source=collection_home---4------21-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/discovering-creative-insights-in-promotional-artwork-295e4d788db5?gi=e9f233790bb1&source=collection_home---4------21-----------------------",
        "loadedTime": "2023-12-06T00:03:51.987Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/discovering-creative-insights-in-promotional-artwork-295e4d788db5",
        "title": "Discovering Creative Insights in Promotional Artwork | by Netflix Technology Blog | Netflix TechBlog",
        "description": "When members are shown a title on Netflix, the displayed artwork, trailers, and synopses are personalized. That means members see the assets that are most likely to help them make an informed choice…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Discovering Creative Insights in Promotional Artwork\nBy Grace Tang, Aneesh Vartakavi, Julija Bagdonaite, Cristina Segalin, and Vi Iyengar\nWhen members are shown a title on Netflix, the displayed artwork, trailers, and synopses are personalized. That means members see the assets that are most likely to help them make an informed choice. These assets are a critical source of information for the member to make a decision to watch, or not watch, a title. The stories on Netflix are multidimensional and there are many ways that a single story could appeal to different members. We want to show members the images, trailers, and synopses that are most helpful to them for making a watch decision.\nIn a previous blog post we explained how our artwork personalization algorithm can pick the best image for each member, but how do we create a good set of images to choose from? What data would you like to have if you were designing an asset suite?\nIn this blog post, we talk about two approaches to create effective artwork. Broadly, they are:\nThe top-down approach, where we preemptively identify image properties to investigate, informed by our initial beliefs.\nThe bottom-up approach, where we let the data naturally surface important trends.\nThe role of promotional artwork\nGreat promotional media helps viewers discover titles they’ll love. In addition to helping members quickly find titles already aligned with their tastes, they help members discover new content. We want to make artwork that is compelling and personally relevant, but we also want to represent the title authentically. We don’t want to make clickbait.\nHere’s an example: Purple Hearts is a film about an aspiring singer-songwriter who commits to a marriage of convenience with a soon-to-deploy Marine. This title has storylines that might appeal to both fans of romance as well as military and war themes. This is reflected in our artwork suite for this title.\nImages for the title “Purple Hearts”\nCreative Insights\nTo create suites that are relevant, attractive, and authentic, we’ve relied on creative strategists and designers with intimate knowledge of the titles to recommend and create the right art for upcoming titles. To supplement their domain expertise, we’ve built a suite of tools to help them look for trends. By inspecting past asset performance from thousands of titles that have already been launched on Netflix, we achieve a beautiful intersection of art & science. However, there are some downsides to this approach: It is tedious to manually scrub through this large collection of data, and looking for trends this way could be subjective and vulnerable to confirmation bias.\nCreators often have years of experience and expert knowledge on what makes a good piece of art. However, it is still useful to test our assumptions, especially in the context of the specific canvases we use on the Netflix product. For example, certain traditional art styles that are effective in traditional media like movie posters might not translate well to the Netflix UI in your living room. Compared to a movie poster or physical billboard, Netflix artwork on TV screens and mobile phones have very different size, aspect ratios, and amount of attention paid to them. As a consequence, we need to conduct research into the effectiveness of artwork on our unique user interfaces instead of extrapolating from established design principles.\nGiven these challenges, we develop data-driven recommendations and surface them to creators in an actionable, user-friendly way. These insights complement their extensive domain expertise in order to help them to create more effective asset suites. We do this in two ways, a top-down approach that can find known features that have worked well in the past, and a bottom-up approach that surfaces groups of images with no prior knowledge or assumptions.\nTop-down approach\nIn our top-down approach, we describe an image using attributes and find features that make images successful. We collaborate with experts to identify a large set of features based on their prior knowledge and experience, and model them using Computer Vision and Machine Learning techniques. These features range from low level features like color and texture, to higher level features like the number of faces, composition, and facial expressions.\nAn example of the features we might capture for this image include: number of people (two), where they’re facing (facing each other), emotion (neutral to positive), saturation (low), objects present (military uniform)\nWe can use pre-trained models/APIs to create some of these features, like face detection and object labeling. We also build internal datasets and models for features where pre-trained models are not sufficient. For example, common Computer Vision models can tell us that an image contains two people facing each other with happy facial expressions — are they friends, or in a romantic relationship? We have built human-in-the-loop tools to help experts train ML models rapidly and efficiently, enabling them to build custom models for subjective and complex attributes.\nOnce we describe an image with features, we employ various predictive and causal methods to extract insights about which features are most important for effective artwork, which are leveraged to create artwork for upcoming titles. An example insight is that when we look across the catalog, we found that single person portraits tend to perform better than images featuring more than one person.\nSingle Character Portraits\nBottom-up approach\nThe top-down approach can deliver clear actionable insights supported by data, but these insights are limited to the features we are able to identify beforehand and model computationally. We balance this using a bottom-up approach where we do not make any prior guesses, and let the data surface patterns and features. In practice, we surface clusters of similar images and have our creative experts derive insights, patterns and inspiration from these groups.\nOne such method we use for image clustering is leveraging large pre-trained convolutional neural networks to model image similarity. Features from the early layers often model low level similarity like colors, edges, textures and shape, while features from the final layers group images depending on the task (eg. similar objects if the model is trained for object detection). We could then use an unsupervised clustering algorithm (like k-means) to find clusters within these images.\nUsing our example title above, one of the characters in Purple Hearts is in the Marines. Looking at clusters of images from similar titles, we see a cluster that contains imagery commonly associated with images of military and war, featuring characters in military uniform.\nAn example cluster of imagery related to military and war.\nSampling some images from the cluster above, we see many examples of soldiers or officers in uniform, some holding weapons, with serious facial expressions, looking off camera. A creator could find this pattern of images within the cluster below, confirm that the pattern has worked well in the past using performance data, and use this as inspiration to create final artwork.\nA creator can draw inspiration from images in the cluster to the left, and use this to create effective artwork for new titles, such as the image for Purple Hearts on the right.\nSimilarly, the title has a romance storyline, so we find a cluster of images that show romance. From such a cluster, a creator could infer that showing close physical proximity and body language convey romance, and use this as inspiration to create the artwork below.\nOn the flip side, creatives can also use these clusters to learn what not to do. For example, here are images within the same cluster with military and war imagery above. If, hypothetically speaking, they were presented with historical evidence that these kinds of images didn’t perform well for a given canvas, a creative strategist could infer that highly saturated silhouettes don’t work as well in this context, confirm it with a test to establish a causal relationship, and decide not to use it for their title.\nA creator can also spot patterns that didn’t work in the past, and avoid using it for future titles.\nMember clustering\nAnother complementary technique is member clustering, where we group members based on their preferences. We can group them by viewing behavior, or also leverage our image personalization algorithm to find groups of members that positively responded to the same image asset. As we observe these patterns across many titles, we can learn to predict which user clusters might be interested in a title, and we can also learn which assets might resonate with these user clusters.\nAs an example, let’s say we are able to cluster Netflix members into two broad clusters — one that likes romance, and another that enjoys action. We can look at how these two groups of members responded to a title after its release. We might find that 80% of viewers of Purple Hearts belong to the romance cluster, while 20% belong to the action cluster. Furthermore, we might find that a representative romance fan (eg. the cluster centroid) responds most positively to images featuring the star couple in an embrace. Meanwhile, viewers in the action cluster respond most strongly to images featuring a soldier on the battlefield. As we observe these patterns across many titles, we can learn to predict which user clusters might be interested in similar upcoming titles, and we can also learn which themes might resonate with these user clusters. Insights like these can guide artwork creation strategy for future titles.\nConclusion\nOur goal is to empower creatives with data-driven insights to create better artwork. Top-down and bottom-up methods approach this goal from different angles, and provide insights with different tradeoffs.\nTop-down features have the benefit of being clearly explainable and testable. On the other hand, it is relatively difficult to model the effects of interactions and combinations of features. It is also challenging to capture complex image features, requiring custom models. For example, there are many visually distinct ways to convey a theme of “love”: heart emojis, two people holding hands, or people gazing into each others’ eyes and so on, which are all very visually different. Another challenge with top-down approaches is that our lower level features could miss the true underlying trend. For example, we might detect that the colors green and blue are effective features for nature documentaries, but what is really driving effectiveness may be the portrayal of natural settings like forests or oceans.\nIn contrast, bottom-up methods model complex high-level features and their combinations, but their insights are less explainable and subjective. Two users may look at the same cluster of images and extract different insights. However, bottom-up methods are valuable because they can surface unexpected patterns, providing inspiration and leaving room for creative exploration and interpretation without being prescriptive.\nThe two approaches are complementary. Unsupervised clusters can give rise to observable trends that we can then use to create new testable top-down hypotheses. Conversely, top-down labels can be used to describe unsupervised clusters to expose common themes within clusters that we might not have spotted at first glance. Our users synthesize information from both sources to design better artwork.\nThere are many other important considerations that our current models don’t account for. For example, there are factors outside of the image itself that might affect its effectiveness, like how popular a celebrity is locally, cultural differences in aesthetic preferences or how certain themes are portrayed, what device a member is using at the time and so on. As our member base becomes increasingly global and diverse, these are factors we need to account for in order to create an inclusive and personalized experience.\nAcknowledgements\nThis work would not have been possible without our cross-functional partners in the creative innovation space. We would like to specifically thank Ben Klein and Amir Ziai for helping to build the technology we describe here.",
      "markdown": "## Discovering Creative Insights in Promotional Artwork\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----295e4d788db5--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----295e4d788db5--------------------------------)\n\nBy [Grace Tang](https://www.linkedin.com/in/tsmgrace/), [Aneesh Vartakavi](https://www.linkedin.com/in/aneeshvartakavi/), [Julija Bagdonaite](https://www.linkedin.com/in/jbagdonaite/), [Cristina Segalin](https://www.linkedin.com/in/cristinasegalin/), and [Vi Iyengar](https://www.linkedin.com/in/vi-pallavika-iyengar-144abb1b/)\n\nWhen members are shown a title on Netflix, the displayed artwork, trailers, and synopses are personalized. That means members see the assets that are most likely to help them make an informed choice. These assets are a critical source of information for the member to make a decision to watch, or not watch, a title. The stories on Netflix are multidimensional and there are many ways that a single story could appeal to different members. We want to show members the images, trailers, and synopses that are most helpful to them for making a watch decision.\n\nIn a [previous blog post](https://netflixtechblog.com/artwork-personalization-c589f074ad76) we explained how our artwork personalization algorithm can pick the best image for each member, but how do we create a good set of images to choose from? What data would you like to have if you were designing an asset suite?\n\nIn this blog post, we talk about two approaches to create effective artwork. Broadly, they are:\n\n1.  The top-down approach, where we preemptively identify image properties to investigate, informed by our initial beliefs.\n2.  The bottom-up approach, where we let the data naturally surface important trends.\n\n## **The role of promotional artwork**\n\nGreat promotional media helps viewers discover titles they’ll love. In addition to helping members quickly find titles already aligned with their tastes, they help members discover new content. We want to make artwork that is compelling and personally relevant, but we also want to represent the title authentically. We don’t want to make clickbait.\n\nHere’s an example: [_Purple Hearts_](https://www.netflix.com/title/81043665) is a film about an aspiring singer-songwriter who commits to a marriage of convenience with a soon-to-deploy Marine. This title has storylines that might appeal to both fans of romance as well as military and war themes. This is reflected in our artwork suite for this title.\n\nImages for the title “Purple Hearts”\n\n## **Creative Insights**\n\nTo create suites that are relevant, attractive, and authentic, we’ve relied on creative strategists and designers with intimate knowledge of the titles to recommend and create the right art for upcoming titles. To supplement their domain expertise, we’ve built a suite of tools to help them look for trends. By inspecting past asset performance from thousands of titles that have already been launched on Netflix, we achieve a beautiful intersection of art & science. However, there are some downsides to this approach: It is tedious to manually scrub through this large collection of data, and looking for trends this way could be subjective and vulnerable to confirmation bias.\n\nCreators often have years of experience and expert knowledge on what makes a good piece of art. However, it is still useful to test our assumptions, especially in the context of the specific canvases we use on the Netflix product. For example, certain traditional art styles that are effective in traditional media like movie posters might not translate well to the Netflix UI in your living room. Compared to a movie poster or physical billboard, Netflix artwork on TV screens and mobile phones have very different size, aspect ratios, and amount of attention paid to them. As a consequence, we need to conduct research into the effectiveness of artwork on our unique user interfaces instead of extrapolating from established design principles.\n\nGiven these challenges, we develop data-driven recommendations and surface them to creators in an actionable, user-friendly way. These insights complement their extensive domain expertise in order to help them to create more effective asset suites. We do this in two ways, a top-down approach that can find known features that have worked well in the past, and a bottom-up approach that surfaces groups of images with no prior knowledge or assumptions.\n\n## **Top-down approach**\n\nIn our top-down approach, we describe an image using attributes and find features that make images successful. We collaborate with experts to identify a large set of features based on their prior knowledge and experience, and model them using Computer Vision and Machine Learning techniques. These features range from low level features like color and texture, to higher level features like the number of faces, composition, and facial expressions.\n\nAn example of the features we might capture for this image include: number of people (two), where they’re facing (facing each other), emotion (neutral to positive), saturation (low), objects present (military uniform)\n\nWe can use pre-trained models/APIs to create some of these features, like face detection and object labeling. We also build internal datasets and models for features where pre-trained models are not sufficient. For example, common Computer Vision models can tell us that an image contains two people facing each other with happy facial expressions — are they friends, or in a romantic relationship? We have built human-in-the-loop tools to help experts train ML models rapidly and efficiently, enabling them to build custom models for subjective and complex attributes.\n\nOnce we describe an image with features, we employ various [predictive and causal methods](https://netflixtechblog.medium.com/causal-machine-learning-for-creative-insights-4b0ce22a8a96) to extract insights about which features are most important for effective artwork, which are leveraged to create artwork for upcoming titles. An example insight is that when we look across the catalog, we found that single person portraits tend to perform better than images featuring more than one person.\n\nSingle Character Portraits\n\n**Bottom-up approach**\n\nThe top-down approach can deliver clear actionable insights supported by data, but these insights are limited to the features we are able to identify beforehand and model computationally. We balance this using a bottom-up approach where we do not make any prior guesses, and let the data surface patterns and features. In practice, we surface clusters of similar images and have our creative experts derive insights, patterns and inspiration from these groups.\n\nOne such method we use for image clustering is leveraging large pre-trained convolutional neural networks to model image similarity. Features from the early layers often model low level similarity like colors, edges, textures and shape, while features from the final layers group images depending on the task (eg. similar objects if the model is trained for object detection). We could then use an unsupervised clustering algorithm (like k-means) to find clusters within these images.\n\nUsing our example title above, one of the characters in _Purple Hearts_ is in the Marines. Looking at clusters of images from similar titles, we see a cluster that contains imagery commonly associated with images of military and war, featuring characters in military uniform.\n\nAn example cluster of imagery related to military and war.\n\nSampling some images from the cluster above, we see many examples of soldiers or officers in uniform, some holding weapons, with serious facial expressions, looking off camera. A creator could find this pattern of images within the cluster below, confirm that the pattern has worked well in the past using performance data, and use this as inspiration to create final artwork.\n\nA creator can draw inspiration from images in the cluster to the left, and use this to create effective artwork for new titles, such as the image for Purple Hearts on the right.\n\nSimilarly, the title has a romance storyline, so we find a cluster of images that show romance. From such a cluster, a creator could infer that showing close physical proximity and body language convey romance, and use this as inspiration to create the artwork below.\n\nOn the flip side, creatives can also use these clusters to learn what _not_ to do. For example, here are images within the same cluster with military and war imagery above. If, hypothetically speaking, they were presented with historical evidence that these kinds of images didn’t perform well for a given canvas, a creative strategist could infer that highly saturated silhouettes don’t work as well in this context, confirm it with a test to establish a causal relationship, and decide not to use it for their title.\n\nA creator can also spot patterns that didn’t work in the past, and avoid using it for future titles.\n\n**Member clustering**\n\nAnother complementary technique is member clustering, where we group members based on their preferences. We can group them by viewing behavior, or also leverage our image personalization algorithm to find groups of members that positively responded to the same image asset. As we observe these patterns across many titles, we can learn to predict which user clusters might be interested in a title, and we can also learn which assets might resonate with these user clusters.\n\nAs an example, let’s say we are able to cluster Netflix members into two broad clusters — one that likes romance, and another that enjoys action. We can look at how these two groups of members responded to a title after its release. We might find that 80% of viewers of _Purple Hearts_ belong to the romance cluster, while 20% belong to the action cluster. Furthermore, we might find that a representative romance fan (eg. the cluster centroid) responds most positively to images featuring the star couple in an embrace. Meanwhile, viewers in the action cluster respond most strongly to images featuring a soldier on the battlefield. As we observe these patterns across many titles, we can learn to predict which user clusters might be interested in similar upcoming titles, and we can also learn which themes might resonate with these user clusters. Insights like these can guide artwork creation strategy for future titles.\n\n**Conclusion**\n\nOur goal is to empower creatives with data-driven insights to create better artwork. Top-down and bottom-up methods approach this goal from different angles, and provide insights with different tradeoffs.\n\nTop-down features have the benefit of being clearly explainable and testable. On the other hand, it is relatively difficult to model the effects of interactions and combinations of features. It is also challenging to capture complex image features, requiring custom models. For example, there are many visually distinct ways to convey a theme of “love”: heart emojis, two people holding hands, or people gazing into each others’ eyes and so on, which are all very visually different. Another challenge with top-down approaches is that our lower level features could miss the true underlying trend. For example, we might detect that the colors green and blue are effective features for nature documentaries, but what is really driving effectiveness may be the portrayal of natural settings like forests or oceans.\n\nIn contrast, bottom-up methods model complex high-level features and their combinations, but their insights are less explainable and subjective. Two users may look at the same cluster of images and extract different insights. However, bottom-up methods are valuable because they can surface unexpected patterns, providing inspiration and leaving room for creative exploration and interpretation without being prescriptive.\n\nThe two approaches are complementary. Unsupervised clusters can give rise to observable trends that we can then use to create new testable top-down hypotheses. Conversely, top-down labels can be used to describe unsupervised clusters to expose common themes within clusters that we might not have spotted at first glance. Our users synthesize information from both sources to design better artwork.\n\nThere are many other important considerations that our current models don’t account for. For example, there are factors outside of the image itself that might affect its effectiveness, like how popular a celebrity is locally, cultural differences in aesthetic preferences or how certain themes are portrayed, what device a member is using at the time and so on. As our member base becomes increasingly global and diverse, these are factors we need to account for in order to create an inclusive and personalized experience.\n\n**Acknowledgements**\n\nThis work would not have been possible without our cross-functional partners in the creative innovation space. We would like to specifically thank Ben Klein and Amir Ziai for helping to build the technology we describe here."
    },
    {
      "url": "https://netflixtechblog.com/scalable-annotation-service-marken-f5ba9266d428?source=collection_home---4------22-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/scalable-annotation-service-marken-f5ba9266d428?gi=b5d6ace84301&source=collection_home---4------22-----------------------",
        "loadedTime": "2023-12-06T00:03:52.562Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/scalable-annotation-service-marken-f5ba9266d428",
        "title": "Scalable Annotation Service — Marken | Netflix TechBlog",
        "description": "In Marken (Scalable Annotation Service at Netflix), an annotation is a piece of metadata which can be attached to an object from any domain.",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Scalable Annotation Service — Marken\nIntroduction\nAt Netflix, we have hundreds of micro services each with its own data models or entities. For example, we have a service that stores a movie entity’s metadata or a service that stores metadata about images. All of these services at a later point want to annotate their objects or entities. Our team, Asset Management Platform, decided to create a generic service called Marken which allows any microservice at Netflix to annotate their entity.\nAnnotations\nSometimes people describe annotations as tags but that is a limited definition. In Marken, an annotation is a piece of metadata which can be attached to an object from any domain. There are many different kinds of annotations our client applications want to generate. A simple annotation, like below, would describe that a particular movie has violence.\nMovie Entity with id 1234 has violence.\nBut there are more interesting cases where users want to store temporal (time-based) data or spatial data. In Pic 1 below, we have an example of an application which is used by editors to review their work. They want to change the color of gloves to rich black so they want to be able to mark up that area, in this case using a blue circle, and store a comment for it. This is a typical use case for a creative review application.\nAn example for storing both time and space based data would be an ML algorithm that can identify characters in a frame and wants to store the following for a video\nIn a particular frame (time)\nIn some area in image (space)\nA character name (annotation data)\nPic 1 : Editors requesting changes by drawing shapes like the blue circle shown above.\nGoals for Marken\nWe wanted to create an annotation service which will have the following goals.\nAllows to annotate any entity. Teams should be able to define their data model for annotation.\nAnnotations can be versioned.\nThe service should be able to serve real-time, aka UI, applications so CRUD and search operations should be achieved with low latency.\nAll data should be also available for offline analytics in Hive/Iceberg.\nSchema\nSince the annotation service would be used by anyone at Netflix we had a need to support different data models for the annotation object. A data model in Marken can be described using schema — just like how we create schemas for database tables etc.\nOur team, Asset Management Platform, owns a different service that has a json based DSL to describe the schema of a media asset. We extended this service to also describe the schema of an annotation object.\n{\n\"type\": \"BOUNDING_BOX\", ❶\n\"version\": 0, ❷\n\"description\": \"Schema describing a bounding box\",\n\"keys\": {\n\"properties\": { ❸\n\"boundingBox\": {\n\"type\": \"bounding_box\",\n\"mandatory\": true\n},\n\"boxTimeRange\": {\n\"type\": \"time_range\",\n\"mandatory\": true\n}\n}\n}\n}\nIn the above example, the application wants to represent in a video a rectangular area which spans a range of time.\nSchema’s name is BOUNDING_BOX\nSchemas can have versions. This allows users to make add/remove properties in their data model. We don’t allow incompatible changes, for example, users can not change the data type of a property.\nThe data stored is represented in the “properties” section. In this case, there are two properties\nboundingBox, with type “bounding_box”. This is basically a rectangular area.\nboxTimeRange, with type “time_range”. This allows us to specify start and end time for this annotation.\nGeometry Objects\nTo represent spatial data in an annotation we used the Well Known Text (WKT) format. We support following objects\nPoint\nLine\nMultiLine\nBoundingBox\nLinearRing\nOur model is extensible allowing us to easily add more geometry objects as needed.\nTemporal Objects\nSeveral applications have a requirement to store annotations for videos that have time in it. We allow applications to store time as frame numbers or nanoseconds.\nTo store data in frames clients must also store frames per second. We call this a SampleData with following components:\nsampleNumber aka frame number\nsampleNumerator\nsampleDenominator\nAnnotation Object\nJust like schema, an annotation object is also represented in JSON. Here is an example of annotation for BOUNDING_BOX which we discussed above.\n{ \n\"annotationId\": { ❶\n\"id\": \"188c5b05-e648-4707-bf85-dada805b8f87\",\n\"version\": \"0\"\n},\n\"associatedId\": { ❷\n\"entityType\": \"MOVIE_ID\",\n\"id\": \"1234\"\n},\n\"annotationType\": \"ANNOTATION_BOUNDINGBOX\", ❸\n\"annotationTypeVersion\": 1,\n\"metadata\": { ❹\n\"fileId\": \"identityOfSomeFile\",\n\"boundingBox\": {\n\"topLeftCoordinates\": {\n\"x\": 20,\n\"y\": 30\n},\n\"bottomRightCoordinates\": {\n\"x\": 40,\n\"y\": 60\n}\n},\n\"boxTimeRange\": {\n\"startTimeInNanoSec\": 566280000000,\n\"endTimeInNanoSec\": 567680000000\n}\n}\n}\nThe first component is the unique id of this annotation. An annotation is an immutable object so the identity of the annotation always includes a version. Whenever someone updates this annotation we automatically increment its version.\nAn annotation must be associated with some entity which belongs to some microservice. In this case, this annotation was created for a movie with id “1234”\nWe then specify the schema type of the annotation. In this case it is BOUNDING_BOX.\nActual data is stored in the metadata section of json. Like we discussed above there is a bounding box and time range in nanoseconds.\nBase schemas\nJust like in Object Oriented Programming, our schema service allows schemas to be inherited from each other. This allows our clients to create an “is-a-type-of” relationship between schemas. Unlike Java, we support multiple inheritance as well.\nWe have several ML algorithms which scan Netflix media assets (images and videos) and create very interesting data for example identifying characters in frames or identifying match cuts. This data is then stored as annotations in our service.\nAs a platform service we created a set of base schemas to ease creating schemas for different ML algorithms. One base schema (TEMPORAL_SPATIAL_BASE) has the following optional properties. This base schema can be used by any derived schema and not limited to ML algorithms.\nTemporal (time related data)\nSpatial (geometry data)\nAnd another one BASE_ALGORITHM_ANNOTATION which has the following optional properties which is typically used by ML algorithms.\nlabel (String)\nconfidenceScore (double) — denotes the confidence of the generated data from the algorithm.\nalgorithmVersion (String) — version of the ML algorithm.\nBy using multiple inheritance, a typical ML algorithm schema derives from both TEMPORAL_SPATIAL_BASE and BASE_ALGORITHM_ANNOTATION schemas.\n{\n\"type\": \"BASE_ALGORITHM_ANNOTATION\",\n\"version\": 0,\n\"description\": \"Base Schema for Algorithm based Annotations\",\n\"keys\": {\n\"properties\": {\n\"confidenceScore\": {\n\"type\": \"decimal\",\n\"mandatory\": false,\n\"description\": \"Confidence Score\",\n},\n\"label\": {\n\"type\": \"string\",\n\"mandatory\": false,\n\"description\": \"Annotation Tag\",\n},\n\"algorithmVersion\": {\n\"type\": \"string\",\n\"description\": \"Algorithm Version\"\n}\n}\n}\n}\nArchitecture\nGiven the goals of the service we had to keep following in mind.\nOur service will be used by a lot of internal UI applications hence the latency for CRUD and search operations must be low.\nBesides applications we will have ML algorithm data stored. Some of this data can be on the frame level for videos. So the amount of data stored can be large. The databases we pick should be able to scale horizontally.\nWe also anticipated that the service will have high RPS.\nSome other goals came from search requirements.\nAbility to search the temporal and spatial data.\nAbility to search with different associated and additional associated Ids as described in our Annotation Object data model.\nFull text searches on many different fields in the Annotation Object\nStem search support\nAs time progressed the requirements for search only increased and we will discuss these requirements in detail in a different section.\nGiven the requirements and the expertise in our team we decided to choose Cassandra as the source of truth for storing annotations. For supporting different search requirements we chose ElasticSearch. Besides to support various features we have bunch of internal auxiliary services for eg. zookeeper service, internationalization service etc.\nMarken architecture\nAbove picture represents the block diagram of the architecture for our service. On the left we show data pipelines which are created by several of our client teams to automatically ingest new data into our service. The most important of such a data pipeline is created by the Machine Learning team.\nOne of the key initiatives at Netflix, Media Search Platform, now uses Marken to store annotations and perform various searches explained below. Our architecture makes it possible to easily onboard and ingest data from Media algorithms. This data is used by various teams for eg. creators of promotional media (aka trailers, banner images) to improve their workflows.\nSearch\nSuccess of Annotation Service (data labels) depends on the effective search of those labels without knowing much of input algorithms details. As mentioned above, we use the base schemas for every new annotation type (depending on the algorithm) indexed into the service. This helps our clients to search across the different annotation types consistently. Annotations can be searched either by simply data labels or with more added filters like movie id.\nWe have defined a custom query DSL to support searching, sorting and grouping of the annotation results. Different types of search queries are supported using the Elasticsearch as a backend search engine.\nFull Text Search — Clients may not know the exact labels created by the ML algorithms. As an example, the label can be ‘shower curtain’. With full text search, clients can find the annotation by searching using label ‘curtain’ . We also support fuzzy search on the label values. For example, if the clients want to search ‘curtain’ but they wrongly typed ‘curtian` — annotation with the ‘curtain’ label will be returned.\nStem Search — With global Netflix content supported in different languages, our clients have the requirement to support stem search for different languages. Marken service contains subtitles for a full catalog of titles in Netflix which can be in many different languages. As an example for stem search , `clothing` and `clothes` can be stemmed to the same root word `cloth`. We use ElasticSearch to support stem search for 34 different languages.\nTemporal Annotations Search — Annotations for videos are more relevant if it is defined along with the temporal (time range with start and end time) information. Time range within video is also mapped to the frame numbers. We support labels search for the temporal annotations within the provided time range/frame number also.\nSpatial Annotation Search — Annotations for video or image can also include the spatial information. For example a bounding box which defines the location of the labeled object in the annotation.\nTemporal and Spatial Search — Annotation for video can have both time range and spatial coordinates. Hence, we support queries which can search annotations within the provided time range and spatial coordinates range.\nSemantics Search — Annotations can be searched after understanding the intent of the user provided query. This type of search provides results based on the conceptually similar matches to the text in the query, unlike the traditional tag based search which is expected to be exact keyword matches with the annotation labels. ML algorithms also ingest annotations with vectors instead of actual labels to support this type of search. User provided text is converted into a vector using the same ML model, and then search is performed with the converted text-to-vector to find the closest vectors with the searched vector. Based on the clients feedback, such searches provide more relevant results and don’t return empty results in case there are no annotations which exactly match to the user provided query labels. We support semantic search using Open Distro for ElasticSearch . We will cover more details on Semantic Search support in a future blog article.\nSemantic search\nRange Intersection — We recently started supporting the range intersection queries across multiple annotation types for a specific title in the real time. This allows the clients to search with multiple data labels (resulted from different algorithms so they are different annotation types) within video specific time range or the complete video, and get the list of time ranges or frames where the provided set of data labels are present. A common example of this query is to find the `James in the indoor shot drinking wine`. For such queries, the query processor finds the results of both data labels (James, Indoor shot) and vector search (drinking wine); and then finds the intersection of resulting frames in-memory.\nSearch Latency\nOur client applications are studio UI applications so they expect low latency for the search queries. As highlighted above, we support such queries using Elasticsearch. To keep the latency low, we have to make sure that all the annotation indices are balanced, and hotspot is not created with any algorithm backfill data ingestion for the older movies. We followed the rollover indices strategy to avoid such hotspots (as described in our blog for asset management application) in the cluster which can cause spikes in the cpu utilization and slow down the query response. Search latency for the generic text queries are in milliseconds. Semantic search queries have comparatively higher latency than generic text searches. Following graph shows the average search latency for generic search and semantic search (including KNN and ANN search) latencies.\nAverage search latencySemantic search latency\nScaling\nOne of the key challenges while designing the annotation service is to handle the scaling requirements with the growing Netflix movie catalog and ML algorithms. Video content analysis plays a crucial role in the utilization of the content across the studio applications in the movie production or promotion. We expect the algorithm types to grow widely in the coming years. With the growing number of annotations and its usage across the studio applications, prioritizing scalability becomes essential.\nData ingestions from the ML data pipelines are generally in bulk specifically when a new algorithm is designed and annotations are generated for the full catalog. We have set up a different stack (fleet of instances) to control the data ingestion flow and hence provide consistent search latency to our consumers. In this stack, we are controlling the write throughput to our backend databases using Java threadpool configurations.\nCassandra and Elasticsearch backend databases support horizontal scaling of the service with growing data size and queries. We started with a 12 nodes cassandra cluster, and scaled up to 24 nodes to support current data size. This year, annotations are added approximately for the Netflix full catalog. Some titles have more than 3M annotations (most of them are related to subtitles). Currently the service has around 1.9 billion annotations with data size of 2.6TB.\nAnalytics\nAnnotations can be searched in bulk across multiple annotation types to build data facts for a title or across multiple titles. For such use cases, we persist all the annotation data in iceberg tables so that annotations can be queried in bulk with different dimensions without impacting the real time applications CRUD operations latency.\nOne of the common use cases is when the media algorithm teams read subtitle data in different languages (annotations containing subtitles on a per frame basis) in bulk so that they can refine the ML models they have created.\nFuture work\nThere is a lot of interesting future work in this area.\nOur data footprint keeps increasing with time. Several times we have data from algorithms which are revised and annotations related to the new version are more accurate and in-use. So we need to do cleanups for large amounts of data without affecting the service.\nIntersection queries over a large scale of data and returning results with low latency is an area where we want to invest more time.\nAcknowledgements\nBurak Bacioglu and other members of the Asset Management Platform contributed in the design and development of Marken.",
      "markdown": "## **Scalable Annotation Service — Marken**\n\n## Introduction\n\nAt Netflix, we have hundreds of micro services each with its own data models or entities. For example, we have a service that stores a movie entity’s metadata or a service that stores metadata about images. All of these services at a later point want to annotate their objects or entities. Our team, Asset Management Platform, decided to create a generic service called Marken which allows any microservice at Netflix to annotate their entity.\n\n## Annotations\n\nSometimes people describe annotations as tags but that is a limited definition. In Marken, an annotation is a piece of metadata which can be attached to an object from any domain. There are many different kinds of annotations our client applications want to generate. A simple annotation, like below, would describe that a particular movie has violence.\n\n*   Movie Entity with id 1234 has violence.\n\nBut there are more interesting cases where users want to store temporal (time-based) data or spatial data. In Pic 1 below, we have an example of an application which is used by editors to review their work. They want to change the color of gloves to **rich black** so they want to be able to mark up that area, in this case using a blue circle, and store a comment for it. This is a typical use case for a creative review application.\n\nAn example for storing both time and space based data would be an ML algorithm that can identify characters in a frame and wants to store the following for a video\n\n*   In a particular frame (time)\n*   In some area in image (space)\n*   A character name (annotation data)\n\nPic 1 : Editors requesting changes by drawing shapes like the blue circle shown above.\n\n## Goals for Marken\n\nWe wanted to create an annotation service which will have the following goals.\n\n*   Allows to annotate any entity. Teams should be able to define their data model for annotation.\n*   Annotations can be versioned.\n*   The service should be able to serve real-time, aka UI, applications so CRUD and search operations should be achieved with low latency.\n*   All data should be also available for offline analytics in Hive/Iceberg.\n\n## Schema\n\nSince the annotation service would be used by anyone at Netflix we had a need to support different data models for the annotation object. A data model in Marken can be described using schema — just like how we create schemas for database tables etc.\n\nOur team, Asset Management Platform, owns a different service that has a json based DSL to describe the schema of a media asset. We extended this service to also describe the schema of an annotation object.\n\n{  \n      \"type\": \"BOUNDING\\_BOX\", ❶  \n      \"version\": 0, ❷  \n      \"description\": \"Schema describing a bounding box\",  \n      \"keys\": {  \n        \"properties\": { ❸  \n          \"boundingBox\": {  \n            \"type\": \"bounding\\_box\",  \n            \"mandatory\": true  \n          },  \n          \"boxTimeRange\": {  \n             \"type\": \"time\\_range\",  \n             \"mandatory\": true  \n          }  \n      }  \n    }  \n}\n\nIn the above example, the application wants to represent in a video a rectangular area which spans a range of time.\n\n1.  Schema’s name is BOUNDING\\_BOX\n2.  Schemas can have versions. This allows users to make add/remove properties in their data model. We don’t allow incompatible changes, for example, users can not change the data type of a property.\n3.  The data stored is represented in the “properties” section. In this case, there are two properties\n4.  boundingBox, with type “bounding\\_box”. This is basically a rectangular area.\n5.  boxTimeRange, with type “time\\_range”. This allows us to specify start and end time for this annotation.\n\n## Geometry Objects\n\nTo represent spatial data in an annotation we used the [Well Known Text (WKT)](https://en.wikipedia.org/wiki/Well-known_text_representation_of_geometry) format. We support following objects\n\n*   Point\n*   Line\n*   MultiLine\n*   BoundingBox\n*   LinearRing\n\nOur model is extensible allowing us to easily add more geometry objects as needed.\n\n## **Temporal Objects**\n\nSeveral applications have a requirement to store annotations for videos that have time in it. We allow applications to store time as frame numbers or nanoseconds.\n\nTo store data in frames clients must also store frames per second. We call this a SampleData with following components:\n\n*   sampleNumber aka frame number\n*   sampleNumerator\n*   sampleDenominator\n\n## Annotation Object\n\nJust like schema, an annotation object is also represented in JSON. Here is an example of annotation for BOUNDING\\_BOX which we discussed above.\n\n{    \n  \"annotationId\": { ❶  \n    \"id\": \"188c5b05-e648-4707-bf85-dada805b8f87\",  \n    \"version\": \"0\"  \n  },  \n  \"associatedId\": { ❷  \n    \"entityType\": \"MOVIE\\_ID\",  \n    \"id\": \"1234\"  \n  },  \n  \"annotationType\": \"ANNOTATION\\_BOUNDINGBOX\", ❸  \n  \"annotationTypeVersion\": 1,  \n  \"metadata\": { ❹  \n    \"fileId\": \"identityOfSomeFile\",  \n    \"boundingBox\": {  \n      \"topLeftCoordinates\": {  \n        \"x\": 20,  \n        \"y\": 30  \n      },  \n      \"bottomRightCoordinates\": {  \n        \"x\": 40,  \n        \"y\": 60  \n      }  \n  },  \n  \"boxTimeRange\": {  \n    \"startTimeInNanoSec\": 566280000000,  \n    \"endTimeInNanoSec\": 567680000000  \n  }  \n }  \n}\n\n1.  The first component is the unique id of this annotation. An annotation is an immutable object so the identity of the annotation always includes a version. Whenever someone updates this annotation we automatically increment its version.\n2.  An annotation must be associated with some entity which belongs to some microservice. In this case, this annotation was created for a movie with id “1234”\n3.  We then specify the schema type of the annotation. In this case it is BOUNDING\\_BOX.\n4.  Actual data is stored in the `metadata` section of json. Like we discussed above there is a bounding box and time range in nanoseconds.\n\n## Base schemas\n\nJust like in Object Oriented Programming, our schema service allows schemas to be inherited from each other. This allows our clients to create an “is-a-type-of” relationship between schemas. Unlike Java, we support multiple inheritance as well.\n\nWe have several ML algorithms which scan Netflix media assets (images and videos) and create very interesting data for example identifying characters in frames or identifying [match cuts](https://netflixtechblog.com/match-cutting-at-netflix-finding-cuts-with-smooth-visual-transitions-31c3fc14ae59). This data is then stored as annotations in our service.\n\nAs a platform service we created a set of base schemas to ease creating schemas for different ML algorithms. One base schema (TEMPORAL\\_SPATIAL\\_BASE) has the following optional properties. This base schema can be used by any derived schema and not limited to ML algorithms.\n\n*   Temporal (time related data)\n*   Spatial (geometry data)\n\nAnd another one BASE\\_ALGORITHM\\_ANNOTATION which has the following optional properties which is typically used by ML algorithms.\n\n*   `label` (String)\n*   `confidenceScore` (double) — denotes the confidence of the generated data from the algorithm.\n*   `algorithmVersion` (String) — version of the ML algorithm.\n\nBy using multiple inheritance, a typical ML algorithm schema derives from both TEMPORAL\\_SPATIAL\\_BASE and BASE\\_ALGORITHM\\_ANNOTATION schemas.\n\n{  \n  \"type\": \"BASE\\_ALGORITHM\\_ANNOTATION\",  \n  \"version\": 0,  \n  \"description\": \"Base Schema for Algorithm based Annotations\",  \n  \"keys\": {  \n    \"properties\": {  \n      \"confidenceScore\": {  \n        \"type\": \"decimal\",  \n        \"mandatory\": false,  \n        \"description\": \"Confidence Score\",  \n      },  \n      \"label\": {  \n        \"type\": \"string\",  \n        \"mandatory\": false,  \n        \"description\": \"Annotation Tag\",  \n      },  \n      \"algorithmVersion\": {  \n        \"type\": \"string\",  \n        \"description\": \"Algorithm Version\"  \n      }  \n    }  \n  }  \n}\n\n## Architecture\n\nGiven the goals of the service we had to keep following in mind.\n\n*   Our service will be used by a lot of internal UI applications hence the latency for CRUD and search operations must be low.\n*   Besides applications we will have ML algorithm data stored. Some of this data can be on the frame level for videos. So the amount of data stored can be large. The databases we pick should be able to scale horizontally.\n*   We also anticipated that the service will have high RPS.\n\nSome other goals came from search requirements.\n\n*   Ability to search the temporal and spatial data.\n*   Ability to search with different associated and additional associated Ids as described in our Annotation Object data model.\n*   Full text searches on many different fields in the Annotation Object\n*   Stem search support\n\nAs time progressed the requirements for search only increased and we will discuss these requirements in detail in a different section.\n\nGiven the requirements and the expertise in our team we decided to choose Cassandra as the source of truth for storing annotations. For supporting different search requirements we chose ElasticSearch. Besides to support various features we have bunch of internal auxiliary services for eg. zookeeper service, internationalization service etc.\n\nMarken architecture\n\nAbove picture represents the block diagram of the architecture for our service. On the left we show data pipelines which are created by several of our client teams to automatically ingest new data into our service. The most important of such a data pipeline is created by the Machine Learning team.\n\nOne of the key initiatives at Netflix, Media Search Platform, now uses Marken to store annotations and perform various searches explained below. Our architecture makes it possible to easily onboard and ingest data from Media algorithms. This data is used by various teams for eg. creators of promotional media (aka trailers, banner images) to improve their workflows.\n\n## Search\n\nSuccess of Annotation Service (data labels) depends on the effective search of those labels without knowing much of input algorithms details. As mentioned above, we use the base schemas for every new annotation type (depending on the algorithm) indexed into the service. This helps our clients to search across the different annotation types consistently. Annotations can be searched either by simply data labels or with more added filters like movie id.\n\nWe have defined a custom query DSL to support searching, sorting and grouping of the annotation results. Different types of search queries are supported using the [Elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html) as a backend search engine.\n\n*   **Full Text Search** — Clients may not know the exact labels created by the ML algorithms. As an example, the label can be _‘shower curtain’._ With full text search, clients can find the annotation by searching using label _‘curtain’_ . We also support fuzzy search on the label values. For example, if the clients want to search _‘curtain’_ but they wrongly typed _‘curtian_\\` — annotation with the _‘curtain’_ label will be returned.\n*   **Stem Search** — With global Netflix content supported in different languages, our clients have the requirement to support stem search for different languages. Marken service contains subtitles for a full catalog of titles in Netflix which can be in many different languages. As an example for stem search , \\`_clothing_\\` and \\`_clothes_\\` can be stemmed to the same root word \\`_cloth_\\`. We use ElasticSearch to support stem search for 34 different languages.\n*   **Temporal Annotations Search** — Annotations for videos are more relevant if it is defined along with the temporal (time range with start and end time) information. Time range within video is also mapped to the frame numbers. We support labels search for the temporal annotations within the provided time range/frame number also.\n*   **Spatial Annotation Search** — Annotations for video or image can also include the spatial information. For example a bounding box which defines the location of the labeled object in the annotation.\n*   **Temporal and Spatial Search** — Annotation for video can have both time range and spatial coordinates. Hence, we support queries which can search annotations within the provided time range and spatial coordinates range.\n*   **Semantics Search** — Annotations can be searched after understanding the intent of the user provided query. This type of search provides results based on the conceptually similar matches to the text in the query, unlike the traditional tag based search which is expected to be exact keyword matches with the annotation labels. ML algorithms also ingest annotations with vectors instead of actual labels to support this type of search. User provided text is converted into a vector using the same ML model, and then search is performed with the converted text-to-vector to find the closest vectors with the searched vector. Based on the clients feedback, such searches provide more relevant results and don’t return empty results in case there are no annotations which exactly match to the user provided query labels. We support semantic search using [Open Distro for ElasticSearch](https://opendistro.github.io/for-elasticsearch-docs/docs/knn/) . We will cover more details on Semantic Search support in a future blog article.\n\nSemantic search\n\n*   **Range Intersection** — We recently started supporting the range intersection queries across multiple annotation types for a specific title in the real time. This allows the clients to search with multiple data labels (resulted from different algorithms so they are different annotation types) within video specific time range or the complete video, and get the list of time ranges or frames where the provided set of data labels are present. A common example of this query is to find the \\`James in the indoor shot drinking wine\\`. For such queries, the query processor finds the results of both data labels (James, Indoor shot) and vector search (drinking wine); and then finds the intersection of resulting frames in-memory.\n\n## Search Latency\n\nOur client applications are studio UI applications so they expect low latency for the search queries. As highlighted above, we support such queries using Elasticsearch. To keep the latency low, we have to make sure that all the annotation indices are balanced, and hotspot is not created with any algorithm backfill data ingestion for the older movies. We followed the rollover indices strategy to avoid such hotspots (as described in our [blog](https://netflixtechblog.medium.com/elasticsearch-indexing-strategy-in-asset-management-platform-amp-99332231e541) for asset management application) in the cluster which can cause spikes in the cpu utilization and slow down the query response. Search latency for the generic text queries are in milliseconds. Semantic search queries have comparatively higher latency than generic text searches. Following graph shows the average search latency for generic search and semantic search (including [KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) and [ANN](https://opendistro.github.io/for-elasticsearch-docs/docs/knn/approximate-knn/) search) latencies.\n\nAverage search latency\n\nSemantic search latency\n\n## Scaling\n\nOne of the key challenges while designing the annotation service is to handle the scaling requirements with the growing Netflix movie catalog and ML algorithms. Video content analysis plays a crucial role in the utilization of the content across the studio applications in the movie production or promotion. We expect the algorithm types to grow widely in the coming years. With the growing number of annotations and its usage across the studio applications, prioritizing scalability becomes essential.\n\nData ingestions from the ML data pipelines are generally in bulk specifically when a new algorithm is designed and annotations are generated for the full catalog. We have set up a different stack (fleet of instances) to control the data ingestion flow and hence provide consistent search latency to our consumers. In this stack, we are controlling the write throughput to our backend databases using Java threadpool configurations.\n\nCassandra and Elasticsearch backend databases support horizontal scaling of the service with growing data size and queries. We started with a 12 nodes cassandra cluster, and scaled up to 24 nodes to support current data size. This year, annotations are added approximately for the Netflix full catalog. Some titles have more than 3M annotations (most of them are related to subtitles). Currently the service has around 1.9 billion annotations with data size of 2.6TB.\n\n## Analytics\n\nAnnotations can be searched in bulk across multiple annotation types to build data facts for a title or across multiple titles. For such use cases, we persist all the annotation data in [iceberg](https://iceberg.apache.org/) tables so that annotations can be queried in bulk with different dimensions without impacting the real time applications CRUD operations latency.\n\nOne of the common use cases is when the media algorithm teams read subtitle data in different languages (annotations containing subtitles on a per frame basis) in bulk so that they can refine the ML models they have created.\n\n## Future work\n\nThere is a lot of interesting future work in this area.\n\n1.  Our data footprint keeps increasing with time. Several times we have data from algorithms which are revised and annotations related to the new version are more accurate and in-use. So we need to do cleanups for large amounts of data without affecting the service.\n2.  Intersection queries over a large scale of data and returning results with low latency is an area where we want to invest more time.\n\n## Acknowledgements\n\n[Burak Bacioglu](https://www.linkedin.com/in/burakbacioglu/) and other members of the Asset Management Platform contributed in the design and development of Marken."
    },
    {
      "url": "https://netflixtechblog.com/causal-machine-learning-for-creative-insights-4b0ce22a8a96?source=collection_home---4------23-----------------------",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/causal-machine-learning-for-creative-insights-4b0ce22a8a96?gi=093caa482b6c&source=collection_home---4------23-----------------------",
        "loadedTime": "2023-12-06T00:03:59.285Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/causal-machine-learning-for-creative-insights-4b0ce22a8a96",
        "title": "Causal Machine Learning for Creative Insights | by Netflix Technology Blog | Netflix TechBlog",
        "description": "At Netflix, we want our viewers to easily find TV shows and movies that resonate and engage. Our creative team helps make this happen by designing promotional artwork that best represents each title…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "A framework to identify the causal impact of successful visual components.\nBy Billur Engin, Yinghong Lan, Grace Tang, Cristina Segalin, Kelli Griggs, Vi Iyengar\nIntroduction\nAt Netflix, we want our viewers to easily find TV shows and movies that resonate and engage. Our creative team helps make this happen by designing promotional artwork that best represents each title featured on our platform. What if we could use machine learning and computer vision to support our creative team in this process? Through identifying the components that contribute to a successful artwork — one that leads a member to choose and watch it — we can give our creative team data-driven insights to incorporate into their creative strategy, and help in their selection of which artwork to feature.\nWe are going to make an assumption that the presence of a specific component will lead to an artwork’s success. We will discuss a causal framework that will help us find and summarize the successful components as creative insights, and hypothesize and estimate their impact.\nThe Challenge\nGiven Netflix’s vast and increasingly diverse catalog, it is a challenge to design experiments that both work within an A/B test framework and are representative of all genres, plots, artists, and more. In the past, we have attempted to design A/B tests where we investigate one aspect of artwork at a time, often within one particular genre. However, this approach has a major drawback: it is not scalable because we either have to label images manually or create new asset variants differing only in the feature under investigation. The manual nature of these tasks means that we cannot test many titles at a time. Furthermore, given the multidimensional nature of artwork, we might be missing many other possible factors that might explain an artwork’s success, such as figure orientation, the color of the background, facial expressions, etc. Since we want to ensure that our testing framework allows for maximum creative freedom, and avoid any interruption to the design process, we decided to try an alternative approach.\nFigure. Given the multidimensional nature of artwork, it is challenging to design an A/B test to investigate one aspect of artwork at a given time. We could be missing many other possible factors that might explain an artwork’s success, such as figure orientation, the color of the background, facial expressions, etc.\nThe Causal Framework\nThanks to our Artwork Personalization System and vision algorithms (some of which are exemplified here), we have a rich dataset of promotional artwork components and user engagement data to build a causal framework. Utilizing this dataset, we have developed the framework to test creative insights and estimate their causal impact on an artwork’s performance via the dataset generated through our recommendation system. In other words, we can learn which attributes led to a title’s successful selection based on its artwork.\nLet’s first explore the workflow of the causal framework, as well as the data and success metrics that power it.\nWe represent the success of an artwork with the take rate: the probability of an average user to watch the promoted title after seeing its promotional artwork, adjusted for the popularity of the title. Every show on our platform has multiple promotional artwork assets. Using Netflix’s Artwork Personalization, we serve these assets to hundreds of millions of members everyday. To power this recommendation system, we look at user engagement patterns and see whether or not these engagements with artworks resulted in a successful title selection.\nWith the capability to annotate a given image (some of which are mentioned in an earlier post), an artwork asset in this case, we use a series of computer vision algorithms to gather objective image metadata, latent representation of the image, as well as some of the contextual metadata that a given image contains. This process allows our dataset to consist of both the image features and user data, all in an effort to understand which image components lead to successful user engagement. We also utilize machine learning algorithms, consumer insights¹, and correlational analysis for discovering high-level associations between image features and an artwork’s success. These statistically significant associations become our hypotheses for the next phase.\nOnce we have a specific hypothesis, we can test it by deploying causal machine learning algorithms. This framework reduces our experimental effort to uncover causal relationships, while taking into account confounding among the high-level variables (i.e. the variables that may influence both the treatment / intervention and outcome).\nThe Hypothesis and Assumptions\nWe will use the following hypothesis in the rest of the script: presence of a face in an artwork causally improves the asset performance. (We know that faces work well in artwork, especially images with an expressive facial emotion that’s in line with the tone of the title.)\nHere are two promotional artwork assets from Unbreakable Kimmy Schmidt. We know that the image on the left performed better than the image on the right. However, the difference between them is not only the presence of a face. There are many other variances, like the difference in background, text placement, font size, face size, etc. Causal Machine Learning makes it possible for us to understand an artwork’s performance based on the causal impact of its treatment.\nTo make sure our hypothesis is fit for the causal framework, it’s important we go over the identification assumptions.\nConsistency: The treatment component is sufficiently well-defined.\nWe use machine learning algorithms to predict whether or not the artwork contains a face. That’s why the first assumption we make is that our face detection algorithm is mostly accurate (~92% average precision).\nPositivity / Probabilistic Assignment: Every unit (an artwork) has some chance of getting treated.\nWe calculate the propensity score (the probability of receiving the treatment based on certain baseline characteristics) of having a face for samples with different covariates. If a certain subset of artwork (such as artwork from a certain genre) has close to a 0 or 1 propensity score for having a face, then we discard these samples from our analysis.\nIndividualistic Assignment / SUTVA (stable unit treatment value assumption): The potential outcomes of a unit do not depend on the treatments assigned to others.\nCreatives make the decision to create artwork with or without faces based on considerations limited to the title of interest itself. This decision is not dependent on whether other assets have a face in them or not.\nConditional exchangeability (Unconfoundedness): There are no unmeasured confounders.\nThis assumption is by definition not testable. Given a dataset, we can’t know if there has been an unobserved confounder. However, we can test the sensitivity of our conclusions toward the violation of this assumption in various different ways.\nThe Models\nNow that we have established our hypothesis to be a causal inference problem, we can focus on the Causal Machine Learning Application. Predictive Machine Learning (ML) models are great at finding patterns and associations in order to predict outcomes, however they are not great at explaining cause-effect relationships, as their model structure does not reflect causality (the relationship between cause and effect). As an example, let’s say we looked at the price of Broadway theater tickets and the number of tickets sold. An ML algorithm may find a correlation between price increases and ticket sales. If we have used this algorithm for decision making, we could falsely conclude that increasing the ticket price leads to higher ticket sales if we do not consider the confounder of show popularity, which clearly impacts both ticket prices and sales. It is understandable that a Broadway musical ticket may be more expensive if the show is a hit, however simply increasing ticket prices to gain more customers is counter-intuitive.\nCausal ML helps us estimate treatment effects from observational data, where it is challenging to conduct clean randomizations. Back-to-back publications on Causal ML, such as Double ML, Causal Forests, Causal Neural Networks, and many more, showcased a toolset for investigating treatment effects, via combining domain knowledge with ML in the learning system. Unlike predictive ML models, Causal ML explicitly controls for confounders, by modeling both treatment of interest as a function of confounders (i.e., propensity scores) as well as the impact of confounders on the outcome of interest. In doing so, Causal ML isolates out the causal impact of treatment on outcome. Moreover, the estimation steps of Causal ML are carefully set up to achieve better error bounds for the estimated treatment effects, another consideration often overlooked in predictive ML. Compared to more traditional Causal Inference methods anchored on linear models, Causal ML leverages the latest ML techniques to not only better control for confounders (when propensity or outcome models are hard to capture by linear models) but also more flexibly estimate treatment effects (when treatment effect heterogeneity is nonlinear). In short, by utilizing machine learning algorithms, Causal ML provides researchers with a framework for understanding causal relationships with flexible ML methods.\nY : outcome variable (take rate)\nT : binary treatment variable (presence of a face or not)\nW: a vector of covariates (features of the title and artwork)\nX ⊆ W: a vector of covariates (a subset of W) along which treatment effect heterogeneity is evaluated\nLet’s dive more into the causal ML (Double ML to be specific) application steps for creative insights.\nBuild a propensity model to predict treatment probability (T) given the W covariates.\n2. Build a potential outcome model to predict Y given the W covariates.\n3. Residualization of\nThe treatment (observed T — predicted T via propensity model)\nThe outcome (observed Y — predicted Y via potential outcome model)\n4. Fit a third model on the residuals to predict the average treatment effect (ATE) or conditional average treatment effect (CATE).\nWhere 𝜖 and η are stochastic errors and we assume that E[ 𝜖|T,W] = 0 , E[ η|W] = 0.\nFor the estimation of the nuisance functions (i.e., the propensity score model and the outcome model), we have implemented the propensity model as a classifier (as we have a binary treatment variable — the presence of face) and the potential outcome model as a regressor (as we have a continuous outcome variable — adjusted take rate). We have used grid search for tuning the XGBoosting classifier & regressor hyperparameters. We have also used k-fold cross-validation to avoid overfitting. Finally, we have used a causal forest on the residuals of treatment and the outcome variables to capture the ATE, as well as CATE on different genres and countries.\nMediation and Moderation\nATE will reveal the impact of the treatment — in this case, having a face in the artwork — across the board. The result will answer the question of whether it is worth applying this approach for all of our titles across our catalog, regardless of potential conditioning variables e.g. genre, country, etc. Another advantage of our multi-feature dataset is that we get to deep dive into the relationships between attributes. To do this, we can employ two methods: mediation and moderation.\nIn their classic paper, Baron & Kenny define a moderator as “a qualitative (e.g., sex, race, class) or quantitative (e.g., level of reward) variable that affects the direction and/or strength of the relation between an independent or predictor variable and a dependent or criterion variable.”. We can investigate suspected moderators to uncover Conditional Average Treatment Effects (CATE). For example, we might suspect that the effect of the presence of a face in artwork varies across genres (e.g. certain genres, like nature documentaries, probably benefit less from the presence of a human face since titles in those genres tend to focus more on non-human subject matter). We can investigate these relationships by including an interaction term between the suspected moderator and the independent variable. If the interaction term is significant, we can conclude that the third variable is a moderator of the relationship between the independent and dependent variables.\nMediation, on the other hand, occurs when a third variable explains the relationship between an independent and dependent variable. To quote Baron & Kenny once more, “whereas moderator variables specify when certain effects will hold, mediators speak to how or why such effects occur.”\nFor example, we observed that the presence of more than 3 people tends to negatively impact performance. It could be that higher numbers of faces make it harder for a user to focus on any one face in the asset. However, since face count and face size tend to be negatively correlated (since we fit more information in an image of fixed size, each individual piece of information tends to be smaller), one could also hypothesize that the negative correlation with face count is not driven so much from the number of people featured in the artwork, but rather the size of each individual person’s face, which may affect how visible each person is. To test this, we can run a mediation analysis to see if face size is mediating the effect of face count on the asset’s performance.\nThe steps of the mediation analysis are as follows: We have already detected a correlation between the independent variable (number of faces) and the outcome variable (user engagement) — in other words, we observed that a higher number of faces is associated with lower user engagement. But, we also observe that the number of faces is negatively correlated with average face size — faces tend to be smaller when more faces are fit into the same fixed-size canvas. To find out the degree to which face size mediates the effect of face count, we regress user engagement on both average face size and the number of faces. If 1) face size is a significant predictor of engagement, and 2) the significance of the predictive contribution of the number of people drops, we can conclude that face size mediates the effect of the number of people in artwork user engagement. If the coefficient for the number of people is no longer significant, it shows that face size fully mediates the effect of the number of faces on engagement.\nIn this dataset, we found that face size only partially mediates the effect of face count on asset effectiveness. This implies that both factors have an impact on asset effectiveness — fewer faces tend to be more effective even if we control for the effect of face size.\nSensitivity Analysis\nAs alluded to above, the conditional exchangeability assumption (unconfoundedness) is not testable by definition. It is thus crucial to evaluate how sensitive our findings and insights are to the violation of this assumption. Inspired by prior work, we conducted a suite of sensitivity analyses that stress-tested this assumption from multiple different angles. In addition, we leveraged ideas from academic research (most notably the E-value) and concluded that our estimates are robust even when the unconfoundedness assumption is violated. We are actively working on designing and implementing a standardized framework for sensitivity analysis and will share the various applications in an upcoming blog post — stay tuned for a more detailed discussion!\nFinally, we also compared our estimated treatment effects with known effects for specific genres that were derived with other different methods, validating our estimates with consistency across different methods\nConclusion\nUsing the causal machine learning framework, we can potentially test and identify the various components of promotional artwork and gain invaluable creative insights. With this post, we just started to scratch the surface of this interesting challenge. In the upcoming posts in this series, we will share alternative machine learning and computer vision approaches that can provide insights from a causal perspective. These insights will guide and assist our team of talented strategists and creatives to select and generate the most attractive artwork, leveraging the attributes that these models selected, down to a specific genre. Ultimately this will give Netflix members a better and more personalized experience.\nIf these types of challenges interest you, please let us know! We are always looking for great people who are inspired by causal inference, machine learning, and computer vision to join our team.\nContributions\nThe authors contributed to the post as follows.\nBillur Engin was the main driver of this blog post, she worked on the causal machine learning theory and its application in the artwork space. Yinghong Lan contributed equally to the causal machine learning theory. Grace Tang worked on the mediation analysis. Cristina Segalin engineered and extracted the visual features at scale from artworks used in the analysis. Grace Tang and Cristina Segalin initiated and conceptualized the problem space that is being used as the illustrative example in this post (studying factors affecting user engagement with a broad multivariate analysis of artwork features), curated the data, and performed initial statistical analysis and construction of predictive models supporting this work.\nAcknowledgments\nWe would like to thank Shiva Chaitanya for reviewing this work, and a special thanks to Shaun Wright , Luca Aldag, Sarah Soquel Morhaim, and Anna Pulido who helped make this possible.\nFootnotes\n¹The Consumer Insights team at Netflix seeks to understand members and non-members through a wide range of quantitative and qualitative research methods.",
      "markdown": "[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----4b0ce22a8a96--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----4b0ce22a8a96--------------------------------)\n\n**A framework to identify the causal impact of successful visual components.**\n\nBy [Billur Engin](https://www.linkedin.com/in/billurengin/), [Yinghong Lan](https://www.linkedin.com/in/yinghong-lan-2368656b/), [Grace Tang](https://www.linkedin.com/in/tsmgrace/), [Cristina Segalin](https://www.linkedin.com/in/cristinasegalin/), [Kelli Griggs](https://www.linkedin.com/in/kelli-griggs-32990125/), [Vi Iyengar](https://www.linkedin.com/in/vi-pallavika-iyengar-144abb1b/)\n\n**Introduction**\n\nAt Netflix, we want our viewers to easily find TV shows and movies that resonate and engage. Our creative team helps make this happen by designing promotional artwork that best represents each title featured on our platform. What if we could use machine learning and computer vision to support our creative team in this process? Through identifying the components that contribute to a successful artwork — one that leads a member to choose and watch it — we can give our creative team data-driven insights to incorporate into their creative strategy, and help in their selection of which artwork to feature.\n\nWe are going to make an assumption that the presence of a specific component will lead to an artwork’s success. We will discuss a causal framework that will help us find and summarize the successful components as creative insights, and hypothesize and estimate their impact.\n\n**The Challenge**\n\nGiven Netflix’s vast and increasingly diverse catalog, it is a challenge to design experiments that both work within an A/B test framework and are representative of all genres, plots, artists, and more. In the past, we have attempted to design A/B tests where we investigate one aspect of artwork at a time, often within one particular genre. However, this approach has a major drawback: it is not scalable because we either have to label images manually or create new asset variants differing only in the feature under investigation. The manual nature of these tasks means that we cannot test many titles at a time. Furthermore, given the multidimensional nature of artwork, we might be missing many other possible factors that might explain an artwork’s success, such as figure orientation, the color of the background, facial expressions, etc. Since we want to ensure that our testing framework allows for maximum creative freedom, and avoid any interruption to the design process, we decided to try an alternative approach.\n\n**Figure.** Given the multidimensional nature of artwork, it is challenging to design an A/B test to investigate one aspect of artwork at a given time. We could be missing many other possible factors that might explain an artwork’s success, such as figure orientation, the color of the background, facial expressions, etc.\n\n**The Causal Framework**\n\nThanks to our [Artwork Personalization System](https://netflixtechblog.com/artwork-personalization-c589f074ad76) and vision algorithms (some of which are [exemplified here](https://netflixtechblog.com/ava-the-art-and-science-of-image-discovery-at-netflix-a442f163af6)), we have a rich dataset of promotional artwork components and user engagement data to build a causal framework. Utilizing this dataset, we have developed the framework to test creative insights and estimate their causal impact on an artwork’s performance via the dataset generated through our recommendation system. In other words, we can learn which attributes led to a title’s successful selection based on its artwork.\n\nLet’s first explore the workflow of the causal framework, as well as the data and success metrics that power it.\n\nWe represent the success of an artwork with the take rate: the probability of an average user to watch the promoted title after seeing its promotional artwork, adjusted for the popularity of the title. Every show on our platform has multiple promotional artwork assets. Using Netflix’s [Artwork Personalization](https://netflixtechblog.com/artwork-personalization-c589f074ad76), we serve these assets to hundreds of millions of members everyday. To power this recommendation system, we look at user engagement patterns and see whether or not these engagements with artworks resulted in a successful title selection.\n\nWith the capability to annotate a given image (some of which are mentioned in [an earlier post](https://netflixtechblog.com/ava-the-art-and-science-of-image-discovery-at-netflix-a442f163af6#:~:text=editorial%20image%20candidates-,frame%20annotation,-As%20part%20of)), an artwork asset in this case, we use a series of computer vision algorithms to gather objective image metadata, latent representation of the image, as well as some of the contextual metadata that a given image contains. This process allows our dataset to consist of both the image features and user data, all in an effort to understand which image components lead to successful user engagement. We also utilize machine learning algorithms, consumer insights¹, and correlational analysis for discovering high-level associations between image features and an artwork’s success. These statistically significant associations become our hypotheses for the next phase.\n\nOnce we have a specific hypothesis, we can test it by deploying causal machine learning algorithms. This framework reduces our experimental effort to uncover causal relationships, while taking into account confounding among the high-level variables (i.e. the variables that may influence both the treatment / intervention and outcome).\n\n**The Hypothesis and Assumptions**\n\nWe will use the following hypothesis in the rest of the script: _presence of a face in an artwork causally improves the asset performance_. (We know that [faces work well in artwork](https://about.netflix.com/en/news/the-power-of-a-picture#:~:text=emotions%20are%20an%20efficient%20way%20of%20conveying%20complex%20nuances), especially [images with an expressive facial emotion that’s in line with the tone of the title.](https://netflixtechblog.com/selecting-the-best-artwork-for-videos-through-a-b-testing-f6155c4595f6#:~:text=Unbreakable%20Kimmy%20Schmidt-,conclusion,-Over%20the%20course))\n\nHere are two promotional artwork assets from _Unbreakable Kimmy Schmidt_. We know that the image on the left performed better than the image on the right. However, the difference between them is not only the presence of a face. There are many other variances, like the difference in background, text placement, font size, face size, etc. Causal Machine Learning makes it possible for us to understand an artwork’s performance based on the causal impact of its treatment.\n\nTo make sure our hypothesis is fit for the causal framework, it’s important we go over the _identification assumptions_.\n\n*   **Consistency:** The treatment component is sufficiently well-defined.\n\nWe use machine learning algorithms to predict whether or not the artwork contains a face. That’s why the first assumption we make is that our face detection algorithm is mostly accurate (~92% average precision).\n\n*   **Positivity / Probabilistic Assignment:** Every unit (an artwork) has some chance of getting treated.\n\nWe calculate the propensity score (the probability of receiving the treatment based on certain baseline characteristics) of having a face for samples with different covariates. If a certain subset of artwork (such as artwork from a certain genre) has close to a 0 or 1 propensity score for having a face, then we discard these samples from our analysis.\n\n*   **Individualistic Assignment / SUTVA (stable unit treatment value assumption):** The potential outcomes of a unit do not depend on the treatments assigned to others.\n\nCreatives make the decision to create artwork with or without faces based on considerations limited to the title of interest itself. This decision is not dependent on whether other assets have a face in them or not.\n\n*   **Conditional exchangeability (Unconfoundedness):** There are no unmeasured confounders.\n\nThis assumption is by definition not testable. Given a dataset, we can’t know if there has been an unobserved confounder. However, we can test the sensitivity of our conclusions toward the violation of this assumption in various different ways.\n\n**The Models**\n\nNow that we have established our hypothesis to be a causal inference problem, we can focus on the Causal Machine Learning Application. Predictive Machine Learning (ML) models are great at finding patterns and associations in order to predict outcomes, however they are not great at explaining cause-effect relationships, as their model structure does not reflect causality (the relationship between cause and effect). As an example, let’s say we looked at the price of Broadway theater tickets and the number of tickets sold. An ML algorithm may find a correlation between price increases and ticket sales. If we have used this algorithm for decision making, we could falsely conclude that increasing the ticket price leads to higher ticket sales if we do not consider the confounder of show popularity, which clearly impacts both ticket prices and sales. It is understandable that a Broadway musical ticket may be more expensive if the show is a hit, however simply increasing ticket prices to gain more customers is counter-intuitive.\n\nCausal ML helps us estimate treatment effects from observational data, where it is challenging to conduct clean randomizations. Back-to-back publications on Causal ML, such as [Double ML](https://arxiv.org/abs/1608.00060), [Causal Forests](https://arxiv.org/abs/1510.04342), [Causal Neural Networks](https://arxiv.org/pdf/1906.02120.pdf), and many more, showcased a toolset for investigating treatment effects, via combining domain knowledge with ML in the learning system. Unlike predictive ML models, Causal ML explicitly controls for confounders, by modeling both treatment of interest as a function of confounders (i.e., propensity scores) as well as the impact of confounders on the outcome of interest. In doing so, Causal ML isolates out the _causal_ impact of treatment on outcome. Moreover, the estimation steps of Causal ML are carefully set up to achieve better error bounds for the estimated treatment effects, another consideration often overlooked in predictive ML. Compared to more traditional Causal Inference methods anchored on linear models, Causal ML leverages the latest ML techniques to not only better control for confounders (when propensity or outcome models are hard to capture by linear models) but also more flexibly estimate treatment effects (when treatment effect heterogeneity is nonlinear). In short, by utilizing machine learning algorithms, Causal ML provides researchers with a framework for understanding causal relationships with flexible ML methods.\n\nY : outcome variable (take rate)  \nT : binary treatment variable (presence of a face or not)  \nW: a vector of covariates (features of the title and artwork)  \nX ⊆ W: a vector of covariates (a subset of W) along which treatment effect heterogeneity is evaluated\n\nLet’s dive more into the causal ML (Double ML to be specific) application steps for creative insights.\n\n1.  Build a propensity model to predict treatment probability (T) given the W covariates.\n\n2\\. Build a potential outcome model to predict Y given the W covariates.\n\n3\\. Residualization of\n\n*   The treatment (observed T — predicted T via propensity model)\n*   The outcome (observed Y — predicted Y via potential outcome model)\n\n4\\. Fit a third model on the residuals to predict the average treatment effect (ATE) or conditional average treatment effect (CATE).\n\nWhere 𝜖 and η are stochastic errors and we assume that **E\\[ 𝜖|T,W\\] = 0** , **E\\[ η|W\\] = 0**.\n\nFor the estimation of the nuisance functions (i.e., the propensity score model and the outcome model), we have implemented the propensity model as a classifier (as we have a binary treatment variable — the presence of face) and the potential outcome model as a regressor (as we have a continuous outcome variable — adjusted take rate). We have used grid search for tuning the XGBoosting classifier & regressor hyperparameters. We have also used k-fold cross-validation to avoid overfitting. Finally, we have used a causal forest on the residuals of treatment and the outcome variables to capture the ATE, as well as CATE on different genres and countries.\n\n**Mediation and Moderation**\n\nATE will reveal the impact of the treatment — in this case, having a face in the artwork — across the board. The result will answer the question of whether it is worth applying this approach for all of our titles across our catalog, regardless of potential conditioning variables e.g. genre, country, etc. Another advantage of our multi-feature dataset is that we get to deep dive into the relationships between attributes. To do this, we can employ two methods: mediation and moderation.\n\nIn their classic paper, [Baron & Kenny](https://www2.psych.ubc.ca/~schaller/528Readings/BaronKenny1986.pdf) define a moderator as “a qualitative (e.g., sex, race, class) or quantitative (e.g., level of reward) variable that affects the direction and/or strength of the relation between an independent or predictor variable and a dependent or criterion variable.”. We can investigate suspected moderators to uncover Conditional Average Treatment Effects (CATE). For example, we might suspect that the effect of the presence of a face in artwork varies across genres (e.g. certain genres, like nature documentaries, probably benefit less from the presence of a human face since titles in those genres tend to focus more on non-human subject matter). We can investigate these relationships by including an interaction term between the suspected moderator and the independent variable. If the interaction term is significant, we can conclude that the third variable is a moderator of the relationship between the independent and dependent variables.\n\nMediation, on the other hand, occurs when a third variable explains the relationship between an independent and dependent variable. To quote Baron & Kenny once more, “whereas moderator variables specify when certain effects will hold, mediators speak to how or why such effects occur.”\n\nFor example, we observed that the [presence of more than 3 people tends to negatively impact performance](https://about.netflix.com/en/news/the-power-of-a-picture#:~:text=dropped%20when%20it%20contained%20more%20than%203%20people). It could be that higher numbers of faces make it harder for a user to focus on any one face in the asset. However, since face count and face size tend to be negatively correlated (since we fit more information in an image of fixed size, each individual piece of information tends to be smaller), one could also hypothesize that the negative correlation with face count is not driven so much from the number of people featured in the artwork, but rather the size of each individual person’s face, which may affect how visible each person is. To test this, we can run a mediation analysis to see if face size is mediating the effect of face count on the asset’s performance.\n\nThe steps of the mediation analysis are as follows: We have already detected a correlation between the independent variable (number of faces) and the outcome variable (user engagement) — in other words, we observed that a higher number of faces is associated with lower user engagement. But, we also observe that the number of faces is negatively correlated with average face size — faces tend to be smaller when more faces are fit into the same fixed-size canvas. To find out the degree to which face size mediates the effect of face count, we regress user engagement on both average face size and the number of faces. If 1) face size is a significant predictor of engagement, and 2) the significance of the predictive contribution of the number of people drops, we can conclude that face size mediates the effect of the number of people in artwork user engagement. If the coefficient for the number of people is no longer significant, it shows that face size _fully_ mediates the effect of the number of faces on engagement.\n\nIn this dataset, we found that face size only partially mediates the effect of face count on asset effectiveness. This implies that both factors have an impact on asset effectiveness — fewer faces tend to be more effective even if we control for the effect of face size.\n\n**Sensitivity Analysis**\n\nAs alluded to above, the conditional exchangeability assumption (unconfoundedness) is not testable by definition. It is thus crucial to evaluate how sensitive our findings and insights are to the violation of this assumption. Inspired by prior [work](https://medium.com/data-science-at-microsoft/causal-inference-part-3-of-3-model-validation-and-applications-c84764156a29), we conducted a suite of sensitivity analyses that stress-tested this assumption from multiple different angles. In addition, we leveraged ideas from academic research (most notably the [E-value](https://www.acpjournals.org/doi/abs/10.7326/m16-2607)) and concluded that our estimates are robust even when the unconfoundedness assumption is violated. We are actively working on designing and implementing a standardized framework for sensitivity analysis and will share the various applications in an upcoming blog post — stay tuned for a more detailed discussion!\n\nFinally, we also compared our estimated treatment effects with known effects for specific genres that were derived with other different methods, validating our estimates with consistency across different methods\n\n**Conclusion**\n\nUsing the causal machine learning framework, we can potentially test and identify the various components of promotional artwork and gain invaluable creative insights. With this post, we just started to scratch the surface of this interesting challenge. In the upcoming posts in this series, we will share alternative machine learning and computer vision approaches that can provide insights from a causal perspective. These insights will guide and assist our team of talented strategists and creatives to select and generate the most attractive artwork, leveraging the attributes that these models selected, down to a specific genre. Ultimately this will give Netflix members a better and more personalized experience.\n\nIf these types of challenges interest you, please let us know! We are always looking for great people who are inspired by causal inference, [machine learning](https://jobs.netflix.com/search?q=%22machine+learning%22), and [computer vision](https://jobs.netflix.com/search?q=%22computer+vision%22) to join our team.\n\n**Contributions**\n\nThe authors contributed to the post as follows.\n\nBillur Engin was the main driver of this blog post, she worked on the causal machine learning theory and its application in the artwork space. Yinghong Lan contributed equally to the causal machine learning theory. Grace Tang worked on the mediation analysis. Cristina Segalin engineered and extracted the visual features at scale from artworks used in the analysis. Grace Tang and Cristina Segalin initiated and conceptualized the problem space that is being used as the illustrative example in this post (studying factors affecting user engagement with a broad multivariate analysis of artwork features), curated the data, and performed initial statistical analysis and construction of predictive models supporting this work.\n\n**Acknowledgments**\n\nWe would like to thank [Shiva Chaitanya](https://www.linkedin.com/in/shiva-chaitanya-05a93b5/) for reviewing this work, and a special thanks to [Shaun Wright](https://www.linkedin.com/in/shaun-wright-28b74248/) , [Luca Aldag](https://www.linkedin.com/in/luca-aldag/), [Sarah Soquel Morhaim](https://www.linkedin.com/in/sarah-soquel-morhaim-3875831a3/), and [Anna Pulid](https://www.linkedin.com/in/anna-pulido-61025063/)o who helped make this possible.\n\n**Footnotes**\n\n¹The Consumer Insights team at Netflix seeks to understand members and non-members through a wide range of quantitative and qualitative research methods."
    },
    {
      "url": "https://netflixtechblog.com/archive",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/archive?gi=e8f0a4a528e8",
        "loadedTime": "2023-12-06T00:04:03.266Z",
        "referrerUrl": "https://netflixtechblog.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/archive",
        "title": "Archive of stories published by Netflix TechBlog",
        "description": "Read top stories published by Netflix TechBlog. Learn about Netflix’s world class engineering efforts, company culture, product developments and more.",
        "author": null,
        "keywords": null,
        "languageCode": null
      },
      "screenshotUrl": null,
      "text": "Netflix Technology Blog in Netflix TechBlog\nDec 7, 2017\nArtwork Personalization at Netflix\nArtwork is the first instance of personalizing not just what we…\nRead more…\n142 responses\nNetflix Technology Blog in Netflix TechBlog\nAug 16, 2018\nBeyond Interactive: Notebook Innovation at Netflix\nRead more…\n41 responses\nNetflix Technology Blog in Netflix TechBlog\nNov 9, 2020\nHow Netflix Scales its API with GraphQL Federation (Part 1)\nRead more…\n23 responses\nNetflix Technology Blog in Netflix TechBlog\nMar 10, 2020\nReady for changes with Hexagonal Architecture\nA story on how we leveraged Hexagonal Architecture…\nRead more…\n31 responses\nNetflix Technology Blog in Netflix TechBlog\nNov 30, 2015\nLinux Performance Analysis in 60,000 Milliseconds\nYou log in to a Linux server with a performance issue: what do you check in the first minute?\nAt Netflix we have a massive EC2 Linux cloud, and numerous performance analysis tools to monitor and investigate its…\nRead more…\n11 responses\nNetflix Technology Blog in Netflix TechBlog\nMay 17, 2018\nFull Cycle Developers at Netflix — Operate What You Build\nRead more…\n41 responses\nNetflix Technology Blog in Netflix TechBlog\nDec 11, 2020\nLife of a Netflix Partner Engineer — The case of extra 40 ms\nBy: John Blair, Netflix Partner…\nRead more…\n30 responses\nNetflix Technology Blog in Netflix TechBlog\nApr 29, 2019\nPython at Netflix\nBy Pythonistas at Netflix, coordinated by Amjith Ramanujam and edited by Ellen Livengood\nRead more…\n18 responses\nNetflix Technology Blog in Netflix TechBlog\nJan 15, 2013\nOptimizing the Netflix API\nhow we redesigned our API to help UI teams meet their needs\nRead more…\n3 responses\nNetflix Technology Blog in Netflix TechBlog\nFeb 13, 2020\nAVIF for Next-Generation Image Coding\nBy Aditya Mavlankar, Jan De Cock¹, Cyril Concolato, Kyle Swanson, Anush Moorthy and Anne Aaron\nRead more…\n11 responses",
      "markdown": "[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog?source=collection_archive---------0-----------------------) in [Netflix TechBlog](https://netflixtechblog.com/?source=collection_archive---------0-----------------------)\n\n[Dec 7, 2017](https://netflixtechblog.com/artwork-personalization-c589f074ad76?source=collection_archive---------0-----------------------)\n\n[\n\n![](https://cdn-images-1.medium.com/freeze/fit/t/60/18/1*xwD8rVHPapbfmrl6AIbQbA.png?q=20)\n\n![](https://cdn-images-1.medium.com/fit/t/1600/480/1*xwD8rVHPapbfmrl6AIbQbA.png)\n\n### Artwork Personalization at Netflix\n\n#### Artwork is the first instance of personalizing not just what we…\n\n\n\n](https://netflixtechblog.com/artwork-personalization-c589f074ad76?source=collection_archive---------0-----------------------)\n\n[Read more…](https://netflixtechblog.com/artwork-personalization-c589f074ad76?source=collection_archive---------0-----------------------)\n\n[142 responses](https://netflixtechblog.com/artwork-personalization-c589f074ad76?source=collection_archive---------0-----------------------#--responses)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog?source=collection_archive---------1-----------------------) in [Netflix TechBlog](https://netflixtechblog.com/?source=collection_archive---------1-----------------------)\n\n[Aug 16, 2018](https://netflixtechblog.com/notebook-innovation-591ee3221233?source=collection_archive---------1-----------------------)\n\n[\n\n![](https://cdn-images-1.medium.com/freeze/focal/60/18/21/33/1*uW5qwTm4FQ3OauOWu_MCcA.gif?q=20)\n\n### Beyond Interactive: Notebook Innovation at Netflix\n\n\n\n](https://netflixtechblog.com/notebook-innovation-591ee3221233?source=collection_archive---------1-----------------------)\n\n[Read more…](https://netflixtechblog.com/notebook-innovation-591ee3221233?source=collection_archive---------1-----------------------)\n\n[41 responses](https://netflixtechblog.com/notebook-innovation-591ee3221233?source=collection_archive---------1-----------------------#--responses)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog?source=collection_archive---------2-----------------------) in [Netflix TechBlog](https://netflixtechblog.com/?source=collection_archive---------2-----------------------)\n\n[Nov 9, 2020](https://netflixtechblog.com/how-netflix-scales-its-api-with-graphql-federation-part-1-ae3557c187e2?source=collection_archive---------2-----------------------)\n\n[\n\n![Studio Edge Architecture Diagram](https://cdn-images-1.medium.com/freeze/fit/t/60/18/1*jc5AUnnZFDb9g2-vg8_BhQ.png?q=20)\n\n![Studio Edge Architecture Diagram](https://cdn-images-1.medium.com/fit/t/1600/480/1*jc5AUnnZFDb9g2-vg8_BhQ.png)\n\n### How Netflix Scales its API with GraphQL Federation (Part 1)\n\n\n\n](https://netflixtechblog.com/how-netflix-scales-its-api-with-graphql-federation-part-1-ae3557c187e2?source=collection_archive---------2-----------------------)\n\n[Read more…](https://netflixtechblog.com/how-netflix-scales-its-api-with-graphql-federation-part-1-ae3557c187e2?source=collection_archive---------2-----------------------)\n\n[23 responses](https://netflixtechblog.com/how-netflix-scales-its-api-with-graphql-federation-part-1-ae3557c187e2?source=collection_archive---------2-----------------------#--responses)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog?source=collection_archive---------3-----------------------) in [Netflix TechBlog](https://netflixtechblog.com/?source=collection_archive---------3-----------------------)\n\n[Mar 10, 2020](https://netflixtechblog.com/ready-for-changes-with-hexagonal-architecture-b315ec967749?source=collection_archive---------3-----------------------)\n\n[\n\n![](https://cdn-images-1.medium.com/freeze/focal/60/18/49/49/1*NfFzI7Z-E3ypn8ahESbDzw.png?q=20)\n\n![](https://cdn-images-1.medium.com/focal/1600/480/49/49/1*NfFzI7Z-E3ypn8ahESbDzw.png)\n\n### Ready for changes with Hexagonal Architecture\n\n#### A story on how we leveraged Hexagonal Architecture…\n\n\n\n](https://netflixtechblog.com/ready-for-changes-with-hexagonal-architecture-b315ec967749?source=collection_archive---------3-----------------------)\n\n[Read more…](https://netflixtechblog.com/ready-for-changes-with-hexagonal-architecture-b315ec967749?source=collection_archive---------3-----------------------)\n\n[31 responses](https://netflixtechblog.com/ready-for-changes-with-hexagonal-architecture-b315ec967749?source=collection_archive---------3-----------------------#--responses)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog?source=collection_archive---------4-----------------------) in [Netflix TechBlog](https://netflixtechblog.com/?source=collection_archive---------4-----------------------)\n\n[Nov 30, 2015](https://netflixtechblog.com/linux-performance-analysis-in-60-000-milliseconds-accc10403c55?source=collection_archive---------4-----------------------)\n\n[\n\n### Linux Performance Analysis in 60,000 Milliseconds\n\nYou log in to a Linux server with a performance issue: what do you check in the first minute?\n\nAt Netflix we have a massive EC2 Linux cloud, and numerous performance analysis tools to monitor and investigate its…\n\n\n\n](https://netflixtechblog.com/linux-performance-analysis-in-60-000-milliseconds-accc10403c55?source=collection_archive---------4-----------------------)\n\n[Read more…](https://netflixtechblog.com/linux-performance-analysis-in-60-000-milliseconds-accc10403c55?source=collection_archive---------4-----------------------)\n\n[11 responses](https://netflixtechblog.com/linux-performance-analysis-in-60-000-milliseconds-accc10403c55?source=collection_archive---------4-----------------------#--responses)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog?source=collection_archive---------5-----------------------) in [Netflix TechBlog](https://netflixtechblog.com/?source=collection_archive---------5-----------------------)\n\n[May 17, 2018](https://netflixtechblog.com/full-cycle-developers-at-netflix-a08c31f83249?source=collection_archive---------5-----------------------)\n\n[\n\n![](https://cdn-images-1.medium.com/freeze/focal/60/18/25/36/1*LhKUxthL3goY9oS0t5GBoQ.png?q=20)\n\n![](https://cdn-images-1.medium.com/focal/1600/480/25/36/1*LhKUxthL3goY9oS0t5GBoQ.png)\n\n### Full Cycle Developers at Netflix — Operate What You Build\n\n\n\n](https://netflixtechblog.com/full-cycle-developers-at-netflix-a08c31f83249?source=collection_archive---------5-----------------------)\n\n[Read more…](https://netflixtechblog.com/full-cycle-developers-at-netflix-a08c31f83249?source=collection_archive---------5-----------------------)\n\n[41 responses](https://netflixtechblog.com/full-cycle-developers-at-netflix-a08c31f83249?source=collection_archive---------5-----------------------#--responses)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog?source=collection_archive---------6-----------------------) in [Netflix TechBlog](https://netflixtechblog.com/?source=collection_archive---------6-----------------------)\n\n[Dec 11, 2020](https://netflixtechblog.com/life-of-a-netflix-partner-engineer-the-case-of-extra-40-ms-b4c2dd278513?source=collection_archive---------6-----------------------)\n\n[\n\n![A diagram showing content downloaded to a device into a streaming buffer, then copied into the device decode buffer.](https://cdn-images-1.medium.com/freeze/fit/t/60/18/1*TftatmXUCJnKtnyKisOCrQ.png?q=20)\n\n![A diagram showing content downloaded to a device into a streaming buffer, then copied into the device decode buffer.](https://cdn-images-1.medium.com/fit/t/1600/480/1*TftatmXUCJnKtnyKisOCrQ.png)\n\n### Life of a Netflix Partner Engineer — The case of extra 40 ms\n\n#### By: John Blair, Netflix Partner…\n\n\n\n](https://netflixtechblog.com/life-of-a-netflix-partner-engineer-the-case-of-extra-40-ms-b4c2dd278513?source=collection_archive---------6-----------------------)\n\n[Read more…](https://netflixtechblog.com/life-of-a-netflix-partner-engineer-the-case-of-extra-40-ms-b4c2dd278513?source=collection_archive---------6-----------------------)\n\n[30 responses](https://netflixtechblog.com/life-of-a-netflix-partner-engineer-the-case-of-extra-40-ms-b4c2dd278513?source=collection_archive---------6-----------------------#--responses)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog?source=collection_archive---------7-----------------------) in [Netflix TechBlog](https://netflixtechblog.com/?source=collection_archive---------7-----------------------)\n\n[Apr 29, 2019](https://netflixtechblog.com/python-at-netflix-bba45dae649e?source=collection_archive---------7-----------------------)\n\n[\n\n### Python at Netflix\n\n_By Pythonistas at Netflix, coordinated by Amjith Ramanujam and edited by Ellen Livengood_\n\n![](https://cdn-images-1.medium.com/freeze/max/60/1*PPIp7twJJUknfohZqtL8pQ.png?q=20)\n\n![](https://cdn-images-1.medium.com/max/1200/1*PPIp7twJJUknfohZqtL8pQ.png)\n\n\n\n\n\n\n\n](https://netflixtechblog.com/python-at-netflix-bba45dae649e?source=collection_archive---------7-----------------------)\n\n[Read more…](https://netflixtechblog.com/python-at-netflix-bba45dae649e?source=collection_archive---------7-----------------------)\n\n[18 responses](https://netflixtechblog.com/python-at-netflix-bba45dae649e?source=collection_archive---------7-----------------------#--responses)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog?source=collection_archive---------8-----------------------) in [Netflix TechBlog](https://netflixtechblog.com/?source=collection_archive---------8-----------------------)\n\n[Jan 15, 2013](https://netflixtechblog.com/optimizing-the-netflix-api-5c9ac715cf19?source=collection_archive---------8-----------------------)\n\n[\n\n![](https://cdn-images-1.medium.com/freeze/focal/60/18/42/23/0*hH8iqS2HLJI9DUbz.png?q=20)\n\n![](https://cdn-images-1.medium.com/focal/1600/480/42/23/0*hH8iqS2HLJI9DUbz.png)\n\n### Optimizing the Netflix API\n\n#### how we redesigned our API to help UI teams meet their needs\n\n\n\n](https://netflixtechblog.com/optimizing-the-netflix-api-5c9ac715cf19?source=collection_archive---------8-----------------------)\n\n[Read more…](https://netflixtechblog.com/optimizing-the-netflix-api-5c9ac715cf19?source=collection_archive---------8-----------------------)\n\n[3 responses](https://netflixtechblog.com/optimizing-the-netflix-api-5c9ac715cf19?source=collection_archive---------8-----------------------#--responses)\n\n[![Go to the profile of Netflix Technology Blog](https://cdn-images-1.medium.com/fit/c/72/72/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)](https://netflixtechblog.com/@netflixtechblog)\n\n[Netflix Technology Blog](https://netflixtechblog.com/@netflixtechblog?source=collection_archive---------9-----------------------) in [Netflix TechBlog](https://netflixtechblog.com/?source=collection_archive---------9-----------------------)\n\n[Feb 13, 2020](https://netflixtechblog.com/avif-for-next-generation-image-coding-b1d75675fe4?source=collection_archive---------9-----------------------)\n\n[\n\n![](https://cdn-images-1.medium.com/freeze/fit/t/60/18/1*7ssIFKx8fyHbRGD8aw4DUQ.png?q=20)\n\n![](https://cdn-images-1.medium.com/fit/t/1600/480/1*7ssIFKx8fyHbRGD8aw4DUQ.png)\n\n### AVIF for Next-Generation Image Coding\n\nBy Aditya Mavlankar, Jan De Cock¹, Cyril Concolato, Kyle Swanson, Anush Moorthy and Anne Aaron\n\n\n\n](https://netflixtechblog.com/avif-for-next-generation-image-coding-b1d75675fe4?source=collection_archive---------9-----------------------)\n\n[Read more…](https://netflixtechblog.com/avif-for-next-generation-image-coding-b1d75675fe4?source=collection_archive---------9-----------------------)\n\n[11 responses](https://netflixtechblog.com/avif-for-next-generation-image-coding-b1d75675fe4?source=collection_archive---------9-----------------------#--responses)"
    },
    {
      "url": "https://netflixtechblog.com/vmaf-the-journey-continues-44b51ee9ed12",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/vmaf-the-journey-continues-44b51ee9ed12?gi=74368b4c4f9d",
        "loadedTime": "2023-12-06T00:05:10.237Z",
        "referrerUrl": "https://netflixtechblog.com/all-of-netflixs-hdr-video-streaming-is-now-dynamically-optimized-e9e0cb15f2ba?source=collection_home---4------0-----------------------",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/vmaf-the-journey-continues-44b51ee9ed12",
        "title": "VMAF: The Journey Continues. by Zhi Li, Christos Bampis, Julie… | by Netflix Technology Blog | Netflix TechBlog",
        "description": "How will Netflix members rate the quality of this video — poor, average or excellent? \nWhich video clip looks better — encoded with Codec A or Codec B?\nFor this episode, at 1000 kbps, is it better to…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "VMAF: The Journey Continues\nby Zhi Li, Christos Bampis, Julie Novak, Anne Aaron, Kyle Swanson, Anush Moorthy and Jan De Cock\nHow will Netflix members rate the quality of this video — poor, average or excellent? \nWhich video clip looks better — encoded with Codec A or Codec B?\nFor this episode, at 1000 kbps, is it better to encode with HD resolution, with some blockiness, or will SD look better?\nThese were example questions we asked ourselves as we worked towards delivering the best quality of experience for Netflix members. A few years ago, we realized that we were unable to answer these questions effectively by simply relying on “golden eyes.” Expert viewing was not scalable across content, encoding recipes, and the overall output of our encoding pipeline. It was possible to deploy existing video quality metrics, such as PSNR and SSIM at scale, but they fell short of accurately capturing human perception. Thus, we embarked on a journey to develop an automated way to answer the question, “How will a Netflix member rate the quality of this encode?” This was the birth of VMAF.\nVideo Multi-method Assessment Fusion, or VMAF for short, is a video quality metric that combines human vision modeling with machine learning. The project started as research collaboration between our team and Prof. C.-C. Jay Kuo from University of Southern California. His research group had previously worked on perceptual metrics for images, and together, we worked on extending the ideas to video. Over time, we have collaborated with other research partners such as Prof. Alan Bovik from the University of Texas at Austin and Prof. Patrick Le Callet from Université de Nantes with the goal of improving VMAF accuracy related to human subjective perception, and broaden its scope to cover more use cases. In June 2016, we open-sourced VMAF on Github, and also published the first VMAF techblog. In this new techblog, we want to share our journey.\nIndustry Adoption\nOutside of Netflix, the video community is finding VMAF a valuable tool for quality assessment. Because of industry adoption, the project is benefitting from broader contribution from researchers, video-related companies and the open-source community.\nVMAF has been integrated into 3rd-party video analysis tools (for example, FFmpeg, Elecard StreamEye, MSU Video Quality Measurement Tool and arewecompressedyet), putting it side-by-side with more established metrics such as PSNR and SSIM.\nIn industry trade shows and meet-ups such as NAB, Video@Scale and Demuxed, demos and presentations are given using VMAF scores to compare quality and efficiency of various encoding techniques.\nThe Video Quality Experts Group (VQEG) is an international consortium of video quality assessment experts. In the recent Los Gatos, Krakow and Madrid VQEG meetings, VMAF was evaluated in multiple discussions.\nWe are pleased to see that other research groups have cross-verified the perceptual accuracy of VMAF. Rassool (RealNetworks) reports high correlation between VMAF and DMOS scores for 4K content. Barman et al. (Kingston University) tested several quality assessment metrics on gaming content and concluded that VMAF was the best in predicting the subjective scores. Lee et al. (Yonsei University) applied quality metrics for multi-resolution adaptive streaming and showed that VMAF and EPSNR demonstrated the highest correlation with perceptual quality. VMAF and VQM were the best performing quality metrics in the study of Gutiérrez et al. (Université de Nantes) where MOS scores were generated for HD and UHD content. We have also read studies where it is claimed that VMAF does not perform as expected. We invite industry and researchers to evaluate the latest VMAF models and encourage them to share with us counterexamples and corner cases that can potentially improve the next VMAF version. We also give best practices of using VMAF at a later section to address some of the concerns.\nVMAF can be used as an optimization criterion for better encoding decisions, and we have seen reports of other companies applying VMAF for this purpose.\nVMAF at Netflix\nCodec Comparisons\nTraditionally, codec comparisons share the same methodology: PSNR values are calculated for a number of video sequences, each encoded at predefined resolutions and fixed quantization settings according to a set of test conditions. Subsequently, rate-quality curves are constructed, and average differences between those curves (BD-rate) are calculated. Such settings work well for small differences in codecs, or for evaluating tools within the same codec. For our use case — video streaming — the use of PSNR is ill-suited, since it correlates poorly with perceptual quality.\nVMAF fills the gap, and can capture larger differences between codecs, as well as scaling artifacts, in a way that’s better correlated with perceptual quality. It enables us to compare codecs in the regions which are truly relevant, i.e. on the convex hull of attainable rate-quality points. Comparing the convex hulls between different codecs and/or different configurations gives a comparison of the Pareto front of both options, in the rate-quality regions that practically matter. Some of our team’s recent work on codec comparisons was published in a tech blog on shot-based encodes, and in academic papers at the Picture Coding Symposium 2018 and SPIE Applications of Digital Image Processing XLI.\nEncoding Decisions\nVMAF is used throughout our production pipeline, not only to measure the outcome of our encoding process, but also to guide our encodes towards the best possible quality. An important example of how VMAF is used within encoding is in our Dynamic Optimizer, where encoding decisions for each individual shot are guided by bitrate and quality measurements for each encoder option. VMAF scores are essential in this optimization process to get accurate quality measurements, and to select the final resolution/bitrate points on the convex hull.\nA/B Experimentation\nResearchers in different business areas — TV UI teams and streaming client teams, for example — are constantly innovating to improve streaming quality. With VMAF, we have a tool that allows us to run system-wide A/B tests and quantify the impact on members’ video quality. For example, a researcher changes the adaptive streaming algorithm or deploys new encodes, runs an experiment, and compares VMAF between the old and new algorithms or encodes. This metric is well-suited for assessing quality in experiments because of its consistency across content and accuracy in reflecting human perception of quality. For example, a VMAF score of 85 will mean “good” quality for all titles, as opposed to using bitrate, where the same bitrate may indicate different quality between titles.\nWhat We’ve Been Working On\nSpeed Optimization\nWhen we first released VMAF on Github back in June 2016, it had its core feature extraction library written in C and the control code in Python, with the main goal of supporting algorithm experimentation and fast prototyping. Upon user’s request, we soon added a stand-alone C++ executable, which can be deployed more easily in the production environment. In December 2016, we added AVX optimization to VMAF’s convolution function, which is the most computationally heavy operation in VMAF. This led to around 3x speedup of VMAF’s execution time. Most recently in June 2018, we added frame-level multithreading, a long-due feature (special shout out to DonTequila). We also introduced the feature of frame skipping, allowing VMAF to be computed on every one of N frames. This is the first time that VMAF can be computed in real time, even in 4K, albeit with a slight accuracy loss.\nlibvmaf\nWith help from the FFmpeg community, we packaged VMAF into a C library called libvmaf. The library offers an interface to incorporate VMAF measurements into your C/C++ code. VMAF is now included as a filter in FFmpeg. The FFmpeg libvmaf filter is now a convenient paved path for running VMAF on compressed video bitstreams as input.\nAccuracy Improvement\nSince we open-sourced VMAF, we have been continuously improving its prediction accuracy. Over time, we have fixed a number of undesirable cases found in the elementary metrics and the machine learning model, yielding more accurate prediction overall. For example, the elementary metrics are modified to yield improved consistency with luminance masking; motion scores at the scene boundaries are updated to avoid overshoot due to scene changes; the QP-VMAF monotonicity is now maintained when extrapolating into high QP regions. Clearly, a VMAF model’s accuracy also heavily depends on the coverage and accuracy of the subjective scores that it is trained on. We have collected a subjective dataset with a broadened scope compared to our previous dataset, including more diverse content and source artifacts such as film grain and camera noise, and more comprehensive coverage of encoding resolutions and compression parameters. We have also developed a new data cleaning technique to remove human bias and inconsistency from the raw data, and open-sourced it on Github. The new approach uses maximum likelihood estimation to jointly optimize its parameters based on the available information and eliminates the need for explicit subject rejection.\nViewing Condition Adaptation\nThe VMAF framework allows training of prediction models tailored to specific viewing conditions, no matter whether it is on a mobile phone or on a UHD TV. The original model released when we open-sourced VMAF was based on the assumption that the viewers sit in front of a 1080p display in a living room-like environment with the viewing distance of 3x the screen height (3H). This is a setup that is generally useful for many scenarios. In applying this model to the mobile phone viewing, however, we found that it did not accurately reflect how a viewer perceives quality on a phone. In particular, due to smaller screen size and longer viewing distance relative to the screen height (>3H), viewers perceive high-quality videos with smaller noticeable differences. For example, on a mobile phone, there is less distinction between 720p and 1080p videos compared to other devices. With this in mind, we trained and released a VMAF phone model.\nAn example VMAF-bitrate relationship for the default model and the phone model is shown above. It can be interpreted that the same distorted video would be perceived as having a higher quality when viewed on a phone screen than on a HD TV, and that the VMAF score differences between the 720p and 1080p videos are smaller using the phone model.\nMost recently, we added a new 4K VMAF model which predicts the subjective quality of video displayed on a 4K TV and viewed from a distance of 1.5H. A viewing distance of 1.5H is the maximum distance for the average viewer to appreciate the sharpness of 4K content. The 4K model is similar to the default model in the sense that both models capture quality at the critical angular frequency of 1/60 degree/pixel. However, the 4K model assumes a wider viewing angle, which affects the foveal vs peripheral vision that the subject uses.\nQuantifying Prediction Uncertainty\nVMAF is trained on a set of representative video genres and distortions. Due to limitations in the size of lab-based subjective experiments, the selection of video sequences does not cover the entire space of perceptual video qualities. Therefore, VMAF predictions need to be associated with a confidence interval (CI) that expresses the inherent uncertainty of the training process. Towards this end, we recently introduced a method to accompany VMAF prediction scores with a 95% CI, which quantifies the level of confidence that the prediction lies within the interval. The CI is established through bootstrapping on the prediction residuals using the full training data. Essentially, it trains multiple models, using “resampling with replacement”, on the residuals of prediction. Each of the models will introduce a slightly different prediction. The variability of these predictions quantifies the level of confidence — the closer these predictions are, the more reliable the prediction using the full data will be.\nThe example plot above based on the NFLX Public Dataset showcases a 95% CI associated with each data point. It is interesting to note that points on the higher-score end tend to have a tighter CI than points on the lower-score end. This can be explained by the fact that in the dataset to train the VMAF model, there are more dense data points on the higher end than the lower. Notably, the bootstrapping technique will not necessarily improve the accuracy of the trained model, but will lend a statistical meaning to its predictions.\nBest Practices\nOftentimes we have been asked whether a particular way of calculating VMAF score is appropriate, or how to interpret the scores obtained. This section highlights some of the best practices of using VMAF.\nInterpreting VMAF Scores\nVMAF scores range from 0 to 100, with 0 indicating the lowest quality, and 100 the highest. A good way to think about a VMAF score is to linearly map it to the human opinion scale under which condition the training scores are obtained. As an example, the default model v0.6.1 is trained using scores collected by the Absolute Category Rating (ACR) methodology using a 1080p display with viewing distance of 3H. Viewers voted the video quality on the scale of “bad”, “poor”, “fair”, “good” and “excellent”, and roughly speaking, “bad” is mapped to the VMAF scale 20 and “excellent” to 100. Thus, a VMAF score of 70 can be interpreted as a vote between “good” and “fair” by an average viewer under the 1080p and 3H condition. Another factor to consider is that the best and the worst votes a viewer can give is calibrated by the highest- and lowest-quality videos in the entire set (during training, and before the actual test starts, subjects are typically accustomed to the experiment’s scale). In the case of the default model v0.6.1, the best and the worst videos are the ones compressed at 1080p with a low quantization parameter (QP) and the ones at 240p with a high QP, respectively.\nComputing VMAF at the Right Resolution\nA typical encoding pipeline for adaptive streaming introduces two types of artifacts — compression artifacts (due to lossy compression) and scaling artifacts (for low bitrates, source video is downsampled before compression, and later upsampled on the display device). When using VMAF to evaluate perceptual quality, both types of artifacts must be taken into account. For example, when a source is 1080p but the encode is 480p, the correct way of calculating VMAF on the pair is to upsample the encode to 1080p to match the source’s resolution. If, instead, the source is downsampled to 480p to match the encode, the obtained VMAF score will not capture the scaling artifacts. This is especially important when using VMAF as the quality criterion for per-title encoding, where the construction of the convex hull is crucial for selecting the optimal encoding parameters.\nThe above example illustrates the convex hull forming when VMAF is calculated correctly (left) and incorrectly (right). When VMAF is calculated with encode upsampled to match the source resolution, one can easily identify intersections among curves from different resolutions. On the other hand, if the source is downsampled to match the encode resolution, the low-resolution encodes will yield undeservingly high scores, and no intersection pattern among the curves can be spotted.\nPicking An Upsampling Algorithm\nWhen upsampling an encode to match the source resolution, many upsampling algorithms are available, including bilinear, bicubic, lanczos, or even the more advanced neural net-based methods. It is untrue that the higher quality the upsampling algorithm is, the better. In principle, one should pick an algorithm that can best match the display device’s. In many cases, the precise circuit for upsampling the video in a display is unknown. In this case, we recommend using bicubic upsampling as the approximation in general.\nInterpreting VMAF Score When Resolution Is Not 1080p\nA frequently asked question is: if both the source and the encoded video have a resolution lower than 1080p, can a 1080p model (e.g. the default model v0.6.1) still apply? Note that the default model measures quality at the critical angular frequency of 1/60 degree/pixel. One way to think about the default model is that it is a “1/60 degree/pixel” model, which means that it assumes that 60 pixels are packed into one degree of viewing angle. If applying the geometry, one can find that it equally applies to 1080p video with 3H, or 720p video with 4.5H, or 480p video with 6.75H. In other words, if applying the 1080p model to 720p / 480p videos, the resulting scores can be interpreted as the ones obtained with viewing distance of 4.5H / 6.75H, respectively. At such long viewing distances, a lot of artifacts are hidden from the human eye, yielding much higher scores.\nNote that this interpretation is without the calibration of other factors such as the eye focal length, foveal vision, and viewer’s expectation on SD vs. HD videos.\nTemporal Pooling\nVMAF produces one score per video frame. It is often useful to generate a summary score per longer time duration, for example, for a video segment of a few seconds, or even for a two-hour movie. Although sophisticated techniques exist to temporally pool the per-frame scores, our empirical results suggest that simple arithmetic mean (AM) is the best way of averaging, in that it yields highest correlation with subjective scores.\nAnother useful averaging technique is harmonic mean (HM). Oftentimes it produces a summary score very similar to AM, except that in the presence of outliers, HM emphasizes the impact of small values.\nThe examples above demonstrate the differences between AM / HM in the absence / presence of small-value outliers. Temporal pooling based on HM is useful when one wants to optimize quality based on VMAF while avoiding the occasional low-quality video encodes.\nMetrics for A/B Experimentation\nTo understand the effects of different treatments in A/B tests, we need metrics to summarize per-frame VMAF scores into per session metrics (i.e. one number per session). To add to the challenge, since in adaptive streaming the best bitrates to stream are selected based on a variety of factors (such as throughput), each session will have a different “combination” of per-frame VMAF values based on how long each stream was played and during which time period of the session. To reach a single VMAF summary number per session, we must 1) determine an appropriate level of aggregation and 2) build upon these aggregates with metrics that reflect different aspects of perceptual quality.\nIf we aggregate the per-frame VMAF scores to one average number per stream, we will miss drops in quality that happen during the session. If we do not aggregate at all and use per-frame values, the computational complexity for creating final summary metrics based on per-frame values for every session will be too high. To this end, we recommend going with intervals in the granularity of seconds to strike a balance between analytic accuracy and ease of implementation.\nIn order to understand the effects of different treatments in A/B tests, we recommend metrics which capture three key aspects of quality: aggregate quality, startplay quality, and variability. These could be the average VMAF over the entire session, average VMAF over the first N seconds, and the number of times the VMAF value drops below a certain threshold relative to the previous values.\nThe Journey Continues\nFrom an internal project to help deliver the best video quality to Netflix’s members, to an open-source project that starts attracting a community of users and contributors, VMAF has been constantly evolving and continuously finding new areas of applications. We are pleased to see that inside and outside Netflix, VMAF has been applied to codec comparison, encoding optimization, A/B experimentation, video analysis, post-processing optimization, and many more areas. Independent researchers have helped cross-verify the perceptual accuracy of VMAF. The open-source community has helped speed up the tool, create easy-to-use interfaces, and moderate the Github repo.\nBut we are just getting started.\nThrough conversations with researchers and VMAF adopters, we have realized that there are many areas that the current VMAF can improve upon. To give a few examples:\nVMAF uses a simple temporal feature, which is the average low-pass filtered differences between adjacent frames. Potentially, VMAF could benefit from more sophisticated models that can better measure the temporal perceptual effects.\nVMAF does not fully capture the benefits of perceptual optimization options found in many codecs, although it is moving in the right direction compared to PSNR.\nCurrently, VMAF does not use chroma features, and does not fully express the perceptual advantage of HDR / WCG videos.\nThe VMAF model works the best with videos of a few seconds. It does not capture long-term effects such as recency and primacy, as well as rebuffering events.\nIn the coming years, we strive to continue improving VMAF, and we invite industry, academia and the open-source community to collaborate with us.\nAcknowledgments\nWe would like to acknowledge the following individuals for their help with the VMAF project: Prof. C.-C Jay Kuo (University of Southern California), Joe Yuchieh Lin, Eddy Chi-Hao Wu, Haiqiang Wang, Prof. Patrick Le Callet (Université de Nantes), Jing Li, Yoann Baveye, Lukas Krasula, Prof. Alan Bovik (University of Texas at Austin), Todd Goodall, Ioannis Katsavounidis, members of the Video Algorithms team, Martin Tingley and Lavanya Sharan of Streaming S&A, and the open-source contributors to the VMAF project.",
      "markdown": "## VMAF: The Journey Continues\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----44b51ee9ed12--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----44b51ee9ed12--------------------------------)\n\n_by Zhi Li, Christos Bampis, Julie Novak, Anne Aaron, Kyle Swanson, Anush Moorthy and Jan De Cock_\n\n_How will Netflix members rate the quality of this video — poor, average or excellent?  \nWhich video clip looks better — encoded with Codec A or Codec B?  \nFor this episode, at 1000 kbps, is it better to encode with HD resolution, with some blockiness, or will SD look better?_\n\nThese were example questions we asked ourselves as we worked towards delivering the best quality of experience for Netflix members. A few years ago, we realized that we were unable to answer these questions effectively by simply relying on “golden eyes.” Expert viewing was not scalable across content, encoding recipes, and the overall output of our encoding pipeline. It was possible to deploy existing video quality metrics, such as PSNR and SSIM at scale, but they fell short of accurately capturing human perception. Thus, we embarked on a journey to develop an automated way to answer the question, “How will a Netflix member rate the quality of this encode?” This was the birth of VMAF.\n\nVideo Multi-method Assessment Fusion, or VMAF for short, is a video quality metric that combines human vision modeling with machine learning. The project started as research collaboration between our team and Prof. C.-C. Jay Kuo from University of Southern California. His research group had previously worked on perceptual metrics for images, and together, we worked on extending the ideas to video. Over time, we have collaborated with other research partners such as Prof. Alan Bovik from the University of Texas at Austin and Prof. Patrick Le Callet from Université de Nantes with the goal of improving VMAF accuracy related to human subjective perception, and broaden its scope to cover more use cases. In June 2016, we [open-sourced VMAF on Github](https://github.com/Netflix/vmaf), and also [published the first VMAF techblog](https://medium.com/netflix-techblog/toward-a-practical-perceptual-video-quality-metric-653f208b9652). In this new techblog, we want to share our journey.\n\n## Industry Adoption\n\nOutside of Netflix, the video community is finding VMAF a valuable tool for quality assessment. Because of industry adoption, the project is benefitting from broader contribution from researchers, video-related companies and the open-source community.\n\n*   VMAF has been integrated into 3rd-party video analysis tools (for example, [FFmpeg](https://ffmpeg.org/ffmpeg-filters.html#libvmaf), [Elecard StreamEye](https://www.elecard.com/products/video-analysis/video-quality-estimator), [MSU Video Quality Measurement Tool](http://www.compression.ru/video/quality_measure/video_measurement_tool.html) and [arewecompressedyet](https://arewecompressedyet.com/?)), putting it side-by-side with more established metrics such as PSNR and SSIM.\n*   In industry trade shows and meet-ups such as NAB, Video@Scale and Demuxed, demos and presentations are given using VMAF scores to compare quality and efficiency of various encoding techniques.\n*   The [Video Quality Experts Group](https://www.its.bldrdoc.gov/vqeg/about-vqeg.aspx) (VQEG) is an international consortium of video quality assessment experts. In the recent [Los Gatos](https://www.its.bldrdoc.gov/vqeg/meetings/los-gatos-ca-usa-may-8-12-2017.aspx), [Krakow](https://www.its.bldrdoc.gov/vqeg/meetings/krakow-poland-nov-27-to-dec-1-2017.aspx) and [Madrid](https://www.its.bldrdoc.gov/vqeg/meetings/madrid-spain-march-19-23.aspx) VQEG meetings, VMAF was evaluated in multiple discussions.\n\nWe are pleased to see that other research groups have cross-verified the perceptual accuracy of VMAF. [Rassool](https://ieeexplore.ieee.org/document/7986143) (RealNetworks) reports high correlation between VMAF and DMOS scores for 4K content. [Barman et al](http://eprints.kingston.ac.uk/40976/). (Kingston University) tested several quality assessment metrics on gaming content and concluded that VMAF was the best in predicting the subjective scores. [Lee et al.](https://ieeexplore.ieee.org/document/8316385/references#references) (Yonsei University) applied quality metrics for multi-resolution adaptive streaming and showed that VMAF and EPSNR demonstrated the highest correlation with perceptual quality. VMAF and VQM were the best performing quality metrics in the study of Gutiérrez et al. (Université de Nantes) where MOS scores were generated for HD and UHD content. We have also read studies where it is claimed that VMAF does not perform as expected. We invite industry and researchers to evaluate the latest VMAF models and encourage them to share with us counterexamples and corner cases that can potentially improve the next VMAF version. We also give best practices of using VMAF at a later section to address some of the concerns.\n\nVMAF can be used as an optimization criterion for better encoding decisions, and we have seen reports of other companies applying VMAF for this purpose.\n\n## VMAF at Netflix\n\n## Codec Comparisons\n\nTraditionally, codec comparisons share the same methodology: PSNR values are calculated for a number of video sequences, each encoded at predefined resolutions and fixed quantization settings according to a set of test conditions. Subsequently, rate-quality curves are constructed, and average differences between those curves (BD-rate) are calculated. Such settings work well for small differences in codecs, or for evaluating tools within the same codec. For our use case — video streaming — the use of PSNR is ill-suited, since it correlates poorly with perceptual quality.\n\nVMAF fills the gap, and can capture larger differences between codecs, as well as scaling artifacts, in a way that’s better correlated with perceptual quality. It enables us to compare codecs in the regions which are truly relevant, i.e. on the convex hull of attainable rate-quality points. Comparing the convex hulls between different codecs and/or different configurations gives a comparison of the Pareto front of both options, in the rate-quality regions that practically matter. Some of our team’s recent work on codec comparisons was published in a [tech blog on shot-based encodes](https://medium.com/netflix-techblog/optimized-shot-based-encodes-now-streaming-4b9464204830), and in academic papers at the [Picture Coding Symposium 2018](https://ieeexplore.ieee.org/document/8456302) and [SPIE Applications of Digital Image Processing XLI](http://spie.org/Publications/Proceedings/Paper/10.1117/12.2322118).\n\n## Encoding Decisions\n\nVMAF is used throughout our production pipeline, not only to measure the outcome of our encoding process, but also to guide our encodes towards the best possible quality. An important example of how VMAF is used within encoding is in our [Dynamic Optimizer](https://medium.com/netflix-techblog/dynamic-optimizer-a-perceptual-video-encoding-optimization-framework-e19f1e3a277f), where encoding decisions for each individual shot are guided by bitrate and quality measurements for each encoder option. VMAF scores are essential in this optimization process to get accurate quality measurements, and to select the final resolution/bitrate points on the convex hull.\n\n## A/B Experimentation\n\nResearchers in different business areas — TV UI teams and streaming client teams, for example — are constantly innovating to improve streaming quality. With VMAF, we have a tool that allows us to run system-wide A/B tests and quantify the impact on members’ video quality. For example, a researcher changes the adaptive streaming algorithm or [deploys new encodes](https://medium.com/netflix-techblog/optimized-shot-based-encodes-now-streaming-4b9464204830), runs an experiment, and compares VMAF between the old and new algorithms or encodes. This metric is well-suited for assessing quality in experiments because of its consistency across content and accuracy in reflecting human perception of quality. For example, a VMAF score of 85 will mean “good” quality for all titles, as opposed to using bitrate, where the same bitrate may indicate different quality between titles.\n\n## What We’ve Been Working On\n\n## Speed Optimization\n\nWhen we first released VMAF on Github back in June 2016, it had its core feature extraction library written in C and the control code in Python, with the main goal of supporting algorithm experimentation and fast prototyping. Upon user’s request, we soon added a stand-alone C++ executable, which can be deployed more easily in the production environment. In December 2016, we added AVX optimization to VMAF’s convolution function, which is the most computationally heavy operation in VMAF. This led to around 3x speedup of VMAF’s execution time. Most recently in June 2018, we added frame-level multithreading, a long-due feature (special shout out to [DonTequila](https://github.com/DonTequila)). We also introduced the feature of frame skipping, allowing VMAF to be computed on every one of N frames. This is the first time that VMAF can be computed in real time, even in 4K, albeit with a slight accuracy loss.\n\n## libvmaf\n\nWith help from the FFmpeg community, we packaged VMAF into a C library called libvmaf. The library offers an interface to incorporate VMAF measurements into your C/C++ code. VMAF is now [included as a filter in FFmpeg](https://ffmpeg.org/ffmpeg-filters.html#libvmaf). The FFmpeg libvmaf filter is now a convenient paved path for running VMAF on compressed video bitstreams as input.\n\n## Accuracy Improvement\n\nSince we open-sourced VMAF, we have been continuously improving its prediction accuracy. Over time, we have fixed a number of undesirable cases found in the elementary metrics and the machine learning model, yielding more accurate prediction overall. For example, the elementary metrics are modified to yield improved consistency with luminance masking; motion scores at the scene boundaries are updated to avoid overshoot due to scene changes; the QP-VMAF monotonicity is now maintained when extrapolating into high QP regions. Clearly, a VMAF model’s accuracy also heavily depends on the coverage and accuracy of the subjective scores that it is trained on. We have collected a subjective dataset with a broadened scope compared to our previous dataset, including more diverse content and source artifacts such as film grain and camera noise, and more comprehensive coverage of encoding resolutions and compression parameters. We have also developed a new [data cleaning technique](https://arxiv.org/abs/1611.01715) to remove human bias and inconsistency from the raw data, and [open-sourced it on Github](https://github.com/Netflix/sureal). The new approach uses maximum likelihood estimation to jointly optimize its parameters based on the available information and eliminates the need for explicit subject rejection.\n\n## Viewing Condition Adaptation\n\nThe VMAF framework allows training of prediction models tailored to specific viewing conditions, no matter whether it is on a mobile phone or on a UHD TV. The original model released when we open-sourced VMAF was based on the assumption that the viewers sit in front of a 1080p display in a living room-like environment with the viewing distance of 3x the screen height (3H). This is a setup that is generally useful for many scenarios. In applying this model to the mobile phone viewing, however, we found that it did not accurately reflect how a viewer perceives quality on a phone. In particular, due to smaller screen size and longer viewing distance relative to the screen height (>3H), viewers perceive high-quality videos with smaller noticeable differences. For example, on a mobile phone, there is less distinction between 720p and 1080p videos compared to other devices. With this in mind, we trained and released a VMAF phone model.\n\nAn example VMAF-bitrate relationship for the default model and the phone model is shown above. It can be interpreted that the same distorted video would be perceived as having a higher quality when viewed on a phone screen than on a HD TV, and that the VMAF score differences between the 720p and 1080p videos are smaller using the phone model.\n\nMost recently, we added a new 4K VMAF model which predicts the subjective quality of video displayed on a 4K TV and viewed from a distance of 1.5H. A viewing distance of 1.5H is the maximum distance for the average viewer to appreciate the sharpness of 4K content. The 4K model is similar to the default model in the sense that both models capture quality at the critical angular frequency of 1/60 degree/pixel. However, the 4K model assumes a wider viewing angle, which affects the foveal vs peripheral vision that the subject uses.\n\n## Quantifying Prediction Uncertainty\n\nVMAF is trained on a set of representative video genres and distortions. Due to limitations in the size of lab-based subjective experiments, the selection of video sequences does not cover the entire space of perceptual video qualities. Therefore, VMAF predictions need to be associated with a confidence interval (CI) that expresses the inherent uncertainty of the training process. Towards this end, we recently introduced a method to accompany VMAF prediction scores with a 95% CI, which quantifies the level of confidence that the prediction lies within the interval. The CI is established through [bootstrapping on the prediction residuals](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)#Resampling_residuals) using the full training data. Essentially, it trains multiple models, using “resampling with replacement”, on the residuals of prediction. Each of the models will introduce a slightly different prediction. The variability of these predictions quantifies the level of confidence — the closer these predictions are, the more reliable the prediction using the full data will be.\n\nThe example plot above based on the [NFLX Public Dataset](https://github.com/Netflix/vmaf/blob/master/resource/doc/datasets.md) showcases a 95% CI associated with each data point. It is interesting to note that points on the higher-score end tend to have a tighter CI than points on the lower-score end. This can be explained by the fact that in the dataset to train the VMAF model, there are more dense data points on the higher end than the lower. Notably, the bootstrapping technique will not necessarily improve the accuracy of the trained model, but will lend a statistical meaning to its predictions.\n\n## Best Practices\n\nOftentimes we have been asked whether a particular way of calculating VMAF score is appropriate, or how to interpret the scores obtained. This section highlights some of the best practices of using VMAF.\n\n## Interpreting VMAF Scores\n\nVMAF scores range from 0 to 100, with 0 indicating the lowest quality, and 100 the highest. A good way to think about a VMAF score is to linearly map it to the human opinion scale under which condition the training scores are obtained. As an example, the default model v0.6.1 is trained using scores collected by the Absolute Category Rating (ACR) methodology using a 1080p display with viewing distance of 3H. Viewers voted the video quality on the scale of “bad”, “poor”, “fair”, “good” and “excellent”, and roughly speaking, “bad” is mapped to the VMAF scale 20 and “excellent” to 100. Thus, a VMAF score of 70 can be interpreted as a vote between “good” and “fair” by an average viewer under the 1080p and 3H condition. Another factor to consider is that the best and the worst votes a viewer can give is calibrated by the highest- and lowest-quality videos in the entire set (during training, and before the actual test starts, subjects are typically accustomed to the experiment’s scale). In the case of the default model v0.6.1, the best and the worst videos are the ones compressed at 1080p with a low quantization parameter (QP) and the ones at 240p with a high QP, respectively.\n\n## Computing VMAF at the Right Resolution\n\nA typical encoding pipeline for adaptive streaming introduces two types of artifacts — compression artifacts (due to lossy compression) and scaling artifacts (for low bitrates, source video is downsampled before compression, and later upsampled on the display device). When using VMAF to evaluate perceptual quality, both types of artifacts must be taken into account. For example, when a source is 1080p but the encode is 480p, the correct way of calculating VMAF on the pair is to upsample the encode to 1080p to match the source’s resolution. If, instead, the source is downsampled to 480p to match the encode, the obtained VMAF score will not capture the scaling artifacts. This is especially important when using VMAF as the quality criterion for [per-title encoding](https://medium.com/netflix-techblog/per-title-encode-optimization-7e99442b62a2), where the construction of the convex hull is crucial for selecting the optimal encoding parameters.\n\nThe above example illustrates the convex hull forming when VMAF is calculated correctly (left) and incorrectly (right). When VMAF is calculated with encode upsampled to match the source resolution, one can easily identify intersections among curves from different resolutions. On the other hand, if the source is downsampled to match the encode resolution, the low-resolution encodes will yield undeservingly high scores, and no intersection pattern among the curves can be spotted.\n\n## Picking An Upsampling Algorithm\n\nWhen upsampling an encode to match the source resolution, many upsampling algorithms are available, including bilinear, bicubic, lanczos, or even the more advanced neural net-based methods. It is untrue that the higher quality the upsampling algorithm is, the better. In principle, one should pick an algorithm that can best match the display device’s. In many cases, the precise circuit for upsampling the video in a display is unknown. In this case, we recommend using bicubic upsampling as the approximation in general.\n\n## Interpreting VMAF Score When Resolution Is Not 1080p\n\nA frequently asked question is: if both the source and the encoded video have a resolution lower than 1080p, can a 1080p model (e.g. the default model v0.6.1) still apply? Note that the default model measures quality at the critical angular frequency of 1/60 degree/pixel. One way to think about the default model is that it is a “1/60 degree/pixel” model, which means that it assumes that 60 pixels are packed into one degree of viewing angle. If applying the geometry, one can find that it equally applies to 1080p video with 3H, or 720p video with 4.5H, or 480p video with 6.75H. In other words, if applying the 1080p model to 720p / 480p videos, the resulting scores can be interpreted as the ones obtained with viewing distance of 4.5H / 6.75H, respectively. At such long viewing distances, a lot of artifacts are hidden from the human eye, yielding much higher scores.\n\nNote that this interpretation is without the calibration of other factors such as the eye focal length, foveal vision, and viewer’s expectation on SD vs. HD videos.\n\n## Temporal Pooling\n\nVMAF produces one score per video frame. It is often useful to generate a summary score per longer time duration, for example, for a video segment of a few seconds, or even for a two-hour movie. Although sophisticated techniques exist to temporally pool the per-frame scores, our empirical results suggest that simple arithmetic mean (AM) is the best way of averaging, in that it yields highest correlation with subjective scores.\n\nAnother useful averaging technique is [harmonic mean (HM)](https://en.wikipedia.org/wiki/Harmonic_mean). Oftentimes it produces a summary score very similar to AM, except that in the presence of outliers, HM emphasizes the impact of small values.\n\nThe examples above demonstrate the differences between AM / HM in the absence / presence of small-value outliers. Temporal pooling based on HM is useful when one wants to optimize quality based on VMAF while avoiding the occasional low-quality video encodes.\n\n## Metrics for A/B Experimentation\n\nTo understand the effects of different treatments in A/B tests, we need metrics to summarize per-frame VMAF scores into per session metrics (i.e. one number per session). To add to the challenge, since in adaptive streaming the best bitrates to stream are selected based on a variety of factors (such as throughput), each session will have a different “combination” of per-frame VMAF values based on how long each stream was played and during which time period of the session. To reach a single VMAF summary number per session, we must 1) determine an appropriate level of aggregation and 2) build upon these aggregates with metrics that reflect different aspects of perceptual quality.\n\nIf we aggregate the per-frame VMAF scores to one average number per stream, we will miss drops in quality that happen during the session. If we do not aggregate at all and use per-frame values, the computational complexity for creating final summary metrics based on per-frame values for every session will be too high. To this end, we recommend going with intervals in the granularity of seconds to strike a balance between analytic accuracy and ease of implementation.\n\nIn order to understand the effects of different treatments in A/B tests, we recommend metrics which capture three key aspects of quality: aggregate quality, startplay quality, and variability. These could be the average VMAF over the entire session, average VMAF over the first N seconds, and the number of times the VMAF value drops below a certain threshold relative to the previous values.\n\n## The Journey Continues\n\nFrom an internal project to help deliver the best video quality to Netflix’s members, to an open-source project that starts attracting a community of users and contributors, VMAF has been constantly evolving and continuously finding new areas of applications. We are pleased to see that inside and outside Netflix, VMAF has been applied to codec comparison, encoding optimization, A/B experimentation, video analysis, post-processing optimization, and many more areas. Independent researchers have helped cross-verify the perceptual accuracy of VMAF. The open-source community has helped speed up the tool, create easy-to-use interfaces, and moderate the Github repo.\n\nBut we are just getting started.\n\nThrough conversations with researchers and VMAF adopters, we have realized that there are many areas that the current VMAF can improve upon. To give a few examples:\n\n*   VMAF uses a simple temporal feature, which is the average low-pass filtered differences between adjacent frames. Potentially, VMAF could benefit from more sophisticated models that can better measure the temporal perceptual effects.\n*   VMAF does not fully capture the benefits of perceptual optimization options found in many codecs, although it is moving in the right direction compared to PSNR.\n*   Currently, VMAF does not use chroma features, and does not fully express the perceptual advantage of HDR / WCG videos.\n*   The VMAF model works the best with videos of a few seconds. It does not capture long-term effects such as recency and primacy, as well as rebuffering events.\n\nIn the coming years, we strive to continue improving VMAF, and we invite industry, academia and the open-source community to collaborate with us.\n\n## Acknowledgments\n\nWe would like to acknowledge the following individuals for their help with the VMAF project: Prof. C.-C Jay Kuo (University of Southern California), Joe Yuchieh Lin, Eddy Chi-Hao Wu, Haiqiang Wang, Prof. Patrick Le Callet (Université de Nantes), Jing Li, Yoann Baveye, Lukas Krasula, Prof. Alan Bovik (University of Texas at Austin), Todd Goodall, Ioannis Katsavounidis, members of the Video Algorithms team, Martin Tingley and Lavanya Sharan of Streaming S&A, and the open-source contributors to the VMAF project."
    },
    {
      "url": "https://netflixtechblog.com/dynamic-optimizer-a-perceptual-video-encoding-optimization-framework-e19f1e3a277f",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/dynamic-optimizer-a-perceptual-video-encoding-optimization-framework-e19f1e3a277f?gi=89235bb3f1e2",
        "loadedTime": "2023-12-06T00:05:12.677Z",
        "referrerUrl": "https://netflixtechblog.com/all-of-netflixs-hdr-video-streaming-is-now-dynamically-optimized-e9e0cb15f2ba?source=collection_home---4------0-----------------------",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/dynamic-optimizer-a-perceptual-video-encoding-optimization-framework-e19f1e3a277f",
        "title": "Dynamic optimizer — a perceptual video encoding optimization framework | by Netflix Technology Blog | Netflix TechBlog",
        "description": "Video encoding has fueled academic research over the past 25 years and enabled compelling products and services. Many companies are built around video encoding and transmission — Netflix and Google’s…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Dynamic optimizer — a perceptual video encoding optimization framework\nBy Ioannis Katsavounidis, Sr. Research Scientist, Video Algorithms\nMotivation\nVideo encoding has fueled academic research over the past 25 years and enabled compelling products and services. Many companies are built around video encoding and transmission — Netflix and Google’s YouTube are two prime examples of video-centric companies. The fundamentals of video encoding haven’t changed for all these years, as modern video streams are produced with the same type of encoding parameters that have been used since MPEG-1 [1]: a certain frame resolution is chosen, together with a group-of-pictures (GOP) structure that imposes periodic Intra pictures, and a target bitrate that is (approximately) met by either a single-pass or two-passes over the input video frames.\nCompanies have struggled to fine-tune additional parameters in video codecs, creating what is commonly referred to in the industry as a good “recipe”. These recipes have been typically created and customized by human inspection of resulting encodes on a selected set of a few titles and have been kept fixed for a very long time.\nAt the same time, improvements in core video codec tools have led to spectacular reduction in bitrate savings — one can achieve the same quality with an HEVC [2] encoder, while using just a fraction (about 30%) of the bits required by MPEG-1. This improvement, although, which has always been measured using mean-squared-error (MSE), wasn’t always accompanied by equally impressive results, when encodes were evaluated by human observers. The magic number to claim when a new codec was being developed has been “50%”. H264/AVC [3] claimed 50% less bits than MPEG-2 [4] and HEVC claimed 50% less bits than AVC. Yet, in practical systems, these savings never quite materialized — the best estimates on what benefits one sees from an incremental change in video codecs is closer to 40% [5].\nParallel to the standardization efforts at ISO and ITU, Google has been developing their own family of royalty-free video codecs; its latest addition was VP9 [6], first introduced in 2013 and finalized in 2014. VP9 built on the earlier success of VP8 and the line of “True Motion” codecs developed by On2 Technologies, acquired by Google in 2010.\nKeeping in mind that most of the video codec improvements carried a very heavy computational overhead on both decoding and — mainly — encoding complexity, one already understands that the newer and more efficient codecs required an ever increasing complexity in order to be deployed in a commercial video transmission service. One typically sees a factor between 5–10 in encoder complexity increase with each generation of video codecs — while the corresponding increase in decoding complexity is typically by a factor of 2.\nIf one accepts the increased complexity that comes with newer and more efficient codecs, a bigger question is: what can we do at the system level, for example, in the way we connect video frames as input to an encoder or how we use the output of a video decoder to render it on a screen, to further improve the video quality as perceived by human observers who consume all these hours of video today?\nThe keywords in this new approach, presented here, are the following:\nPerceptual: the whole purpose of video encoding is to compress visual information in a way that makes it pleasing to the human eye; Mean-squared-error (MSE), typically used for encoder decisions, is a number that doesn’t always correlate very nicely with human perception.\nComplexity: just like we spend an increasing amount of complexity within a video codec, we can afford to spend some outside of it, as well.\nLook-ahead: unlike broadcast TV, where one is forced to make encoding decisions on-the-fly or with minimal delay, in video-on-demand services, video sequences are available in their entirety and can thus be pre-analyzed multiple times to improve quality.\nShot-based encoding\nFor the remainder of this tech blog, we assume the reader is familiar with the basics of adaptive streaming, such as\nMultiple coded representations of the same visual content in different resolutions and/or qualities, using a basic unit of processing, referred to as “streaming segment”\nDelivery of encoded segments from a server, as requested by a streaming client, that belong to different representations in order to accommodate varying channel conditions (bitstream switching)\nTemporal alignment of segments among different coded representations of the same visual content to allow bitstream switching\nInterested readers can refer to a number of available adaptive streaming tutorials, such as this Wiki page [7].\nChunked encoding\nIn a previous Netflix tech-blog [8], published in Dec. 2015, we described how encoding on the cloud benefits greatly from “chunked” encoding. This translates into breaking a long video sequence, e.g. of 1 hour duration, in multiple chunks, each of a certain duration — for example, 20 chunks, each 3 min. long. We then perform encoding of each chunk independently with a certain encoding recipe, concatenate or “assemble” the encodes and thus obtain an encoded version of the entire video sequence.\nAmong the advantages of chunked encoding, the most important is that it allows for a robust system to be built on the cloud using software video encoding. If and when cloud instances fail to complete a certain encode, it requires re-processing the corresponding chunk only, instead of restarting an entire hour-long video encode. One can also see the reduction in end-to-end delay, since different chunks can be encoded in parallel; thus achieving almost infinite scalability in the overall encoding system.\nThere are some penalties that come with chunked encoding — namely the fact that a video encoder operating over the full hour-long sequence, especially in two-pass mode, can preview what is following and therefore do better long-term bitrate allocation; thus achieving better overall quality at the same bitrate. Yet, the advantages that come from chunked encoding outweigh these penalties.\nPer-title and per-chunk encode optimization\nAt Netflix, we have been constantly improving video quality for our members all over the world. One major milestone in our continuous efforts has been “Per-title encode optimization”, described in great detail in our techblog, posted in Dec. 2015 [9]. Per-title encode optimization introduced the concept of customizing encoding according to complexity, which translates to proper resolution and bitrate selection for each video sequence we have in our catalog. This provided significant improvement over our previous fixed resolution/bitrate ladder generation, by taking into account the characteristics of video — amount of motion, level of detail, colorfulness — and optimizing coding efficiency by selecting encoding parameters that better fit each title. Another important milestone has been “per-chunk encode optimization”, introduced in Dec. 2016 as part of our “Mobile encodes for downloads” initiative, explained in more detail in this Netflix tech blog [10]. The concept of equalizing rate-distortion slopes, discussed in more detail in a subsequent section, was also used in that work and provided significant improvements. In fact, one can consider the current work a natural extension of the “Per-title encode optimization” and “Per-chunk encode optimization”; we can call it “Perceptual per-shot encode optimization”.\nFrom chunks to shots\nIn an ideal world, one would like to chunk a video and impose different sets of parameters to each chunk, in a way to optimize the final assembled video. The first step in achieving this perfect bit allocation is to split video in its natural atoms, consisting of frames that are very similar to each other and thus behave similarly to changes to encoding parameters — these are the “shots” that make up a long video sequence. Shots are portions of video with a relatively short duration, coming from the same camera under fairly constant lighting and environment conditions. It captures the same or similar visual content, for example, the face of an actor standing in front of a tree and — most important — it is uniform in its behavior when changing coding parameters. The natural boundaries of shots are established by relatively simple algorithms, called shot-change detection algorithms, which check the amount of differences between pixels that belong to consecutive frames, as well as other statistics. When that amount of difference exceeds certain fixed or dynamically adapted threshold, a new shot boundary is announced.\nThere are cases, such as cross-fades or other visual effects that can be applied on the boundary between two consecutive shots, which can be dealt with by more sophisticated algorithms.\nThe end result of a shot-change detection algorithm is a list of shots and their timestamps. One can use the resulting shots as the basic encoding block, instead of a fixed-length chunk. That provides for a few really unique opportunities:\nThe placement of Intra frames can now be “consistently irregular”, a term that means (a) Intra frames can be placed in a “random” place, for example the first 4 Intra frames can be at times 0, 2, 5, 7 secs. and (b) Yet, temporal positions are always aligned among encodes of the same title, in order words, the location of the first 4 Intra frames remains at 0, 2, 5, 7 secs. for all encodes of this title.\nThe irregular placement of Intra frames results in minimum coding overhead; keep in mind that Intra frames are the least efficient among the 3 different types (I/P/B) used in video coding, and thus one wishes to minimize their presence in an encoded video.\nSeeking in a long video sequence now leads to natural points of interest, which are signaled by shot boundaries.\nThere is no prediction penalty when encoding shots independently: if one instead places an Intra frame in the middle of a shot, this breaks the shot into parts that, when coded independently instead of a single unit, require more bits, since pixels after the Intra frame can’t reference their similar counterparts in frames before the Intra frame.\nAny significant encoding parameter change between consecutive shots is much less likely to be noticed by the human eye, since the disruption incurred by the different visual content in different shots is far more disruptive to human visual system than any possible encoding parameter — such as resolution/quality — change.\nWithin a homogeneous set of frames, such as those that belong to the same shot, there is much less need to use rate-control, since very simple coding schemes, such as the fixed-quantization parameter (“fixed QP”) mode, supported by virtually all existing video encoders, offers a very consistent video quality, with almost minimal bitrate variation. In fact, “fixed QP” has always been used during development of video codecs, since almost all sequences used for testing in MPEG, ITU and other standards bodies, consist of single-shot video chunks.\nFig. 1: “Consistently irregular” Intra (key) frame placement on shot boundaries. Key frames are temporally aligned with shot boundaries for all encodes\nVMAF as a perceptual video quality metric\nIn another Netflix tech-blog [11], published in June 2016, we explained the Video Multi-method Assessment Fusion (VMAF) quality metric, developed in-house and then open-sourced for the entire video community to benefit.\nKey features of VMAF are the following:\nIt is a full-reference metric, which means it can be applied wherever the original, undistorted version of a video sequence is available, as well as a distorted one.\nIt takes into account both compression and up/down scaling artifacts, by upsampling decoded video frames to a common reference frame size (1920x1080). In this way, one can use VMAF to assess quality of encoded video at different resolutions. In particular, it can be used to compare encoded versions of the same title at different resolutions and help decide which one is better.\nIt relies on existing image quality metrics (VIF, DLM), properly modified to cover multiple scales of resolution, as well as the amount of motion between consecutive video frames in a video sequence as features that are input in a machine-learned set of weights. The final score is the result of combining these elementary features in a support vector machine (SVM) regressor.\nCalibration and training of the weights used in VMAF has been performed by collecting subjective data from actual observers who provided the ground-truth data that VMAF was then fit against. The content used to train VMAF is a representative subset of the Netflix catalog, therefore it is understood that its performance has been tuned to our use-case. Yet, the VMAF framework is general and allows for others to retrain it for their own use-case. In fact, a large number of researchers have validated the accuracy of VMAF using their own subjective datasets.\nFig. 2: How VMAF works: pixel level data are pooled to create frame-level features; different spatial and temporal features are fused using SVM regression to create frame-level quality score; consecutive frame scores are pooled to produce final video sequence VMAF score\nConvex hull of Rate-Distortion curve\nA seminal paper by Ortega and Ramchandran [12] in 1998 showed how to address optimality when dealing with multiple choices in image and video coding.\nAssuming that an image consists of N units that need to be coded\nYou encode each unit separately, obtaining a (rate, distortion) pair for each possibility, called “operating point”\nYou place all available operating points on a rate-distortion graph\nYou extract its convex hull, the shell that outlines its boundary\nYou pick one point on the convex hull from each unit such that points from different units have (roughly) equal distortion-rate slope.\nFig. 3: Example of operational points and their R-D convex hull, together with source R-D curve. Reproduced from [12]\nPutting it together\nOne can thus consider the following system:\nA long video sequence is split in shots\nFig. 4: Thumbnails from representative shots from “El Fuente” test sequence; this title consists of 96 shots\nEach shot is encoded multiple times with different encoding parameters, such as resolutions and qualities (QPs)\nEach encode is evaluated using VMAF, which together with its bitrate produces an (R,D) point. One can convert VMAF quality to distortion using different mappings; we tested against the following two, linearly and inversely proportional mappings, which give rise to different temporal aggregation strategies, discussed in the subsequent section\nFig. 5: Encoding a shot using a set of parameters, such as resolution and QP, and obtaining a single (R,D) point for it.\nThe convex hull of (R,D) points for each shot is calculated. In the following example figures, distortion is inverse of (VMAF+1)\nFig. 6: Multiple (R,D) points for a certain shot from “El Fuente”, obtained at various encoding resolutions and QPs using VP9 (libvpx)Fig. 7: Convex hull of (R, D) operating points for the same shot from “El Fuente” using VP9 (libvpx)\nPoints from the convex hull, one from each shot, are combined to create an encode for the entire video sequence by following the constant-slope principle and building end-to-end paths in a Trellis\nFig. 8: Combining shot encodes to produce optimal encodes; example Trellis paths show fixed QP encoding, minimizing bitrate for a given average quality or maximizing quality for a given average bitrate. Selected shot encodes have approximately equal slope in (R,D) space.\nOne produces as many aggregate encodes (final operating points) by varying the slope parameter of the R-D curve as necessary in order to cover a desired bitrate/quality range\nFinal result is a complete R-D or rate-quality (R-Q) curve for the entire video sequence\nFig 9: Final R-Q curve obtained for the entire “El Fuente” video sequence. Baseline is the best possible fixed-QP encoding; dynamic optimizer reduces bitrate by an average of 30% in this case\nThis complete system is called “Dynamic Optimizer” and the framework produces Netflix’s newest generation of encodes.\nTesting methodology — Results\n10 representative titles from the Netflix catalog were selected and encoded using the VP9-libvpx video codec.\nIn terms of temporal aggregation, we have implemented various pooling methods, two of them corresponding to the quality-to-distortion mappings introduced earlier, i.e. linear and inversely proportional mapping. We refer to them as arithmetic mean average VMAF (LVMAF) and harmonic mean averaged VMAF (HVMAF).\nThese two methods, LVMAF and HVMAF temporal quality aggregation, produced very high quality encoded sequences — allowing for more aggressive or more conservative temporal quality fluctuations in the combined video sequence, respectively.\nVP9 encoding recipe\nEncoding parameters used in VP9-libvpx were taken from a previous study; its findings were presented at Netflix’s “Open house on royalty-free codecs” held in Oct. 2016. Based on that study, the best configuration to use is “fixed-QP, AQ-mode=0, CPU=0, best”, shown to produce highest quality both in terms of PSNR and VMAF quality metrics. We reproduce key results from that study in the Appendix.\nWe compared results obtained by the dynamic optimizer against the best possible fixed-QP VP9-libvpx encoding. The methodology followed and various parameters chosen for this experiment are summarized in the following table.\nTable 1: Parameters used for Dynamic Optimizer Experiment 1\nThe corresponding gains obtained by the dynamic optimizer, expressed both in terms of % bitrate savings at the same visual quality and in terms of improvement in HVMAF scores at the same average bitrate, are as follows:\nTable 2: Experiment 1 Results. R-D performance comparison around 256kbps between dynamic optimizer and best possible fixed-QP encoding for 10 representative titles from the Netflix catalog.\nThe result was an average bitrate savings of 17.1% over the best possible fixed-QP encoding of the entire video sequence when using HVMAF as quality metric. The improvement when using PSNR is even higher: 22.5% bitrate savings on average.\nIn this comparison, computational complexity remained constant between the baseline and dynamic optimizer results, since obtaining the convex hull of fixed-QP encodes for an entire sequence requires the same complexity as that for the dynamic optimizer. Thus, this represents a lower-bound on the amount of improvement introduced by the dynamic optimizer.\nIf we use a more common baseline, such as the 2-pass VBR configuration with CPU=1, good, AQ-mode=2 encoding recipe in VP9-libvpx, the improvement by the dynamic optimizer is much larger: over 50% bitrate savings on average, in terms of HVMAF. One needs to keep in mind, although, that computational complexity of the dynamic optimizer solution is much higher in that case.\nHow good is it for other video codecs?\nBased on what was presented earlier, one can immediately understand that there is nothing codec-specific in the dynamic optimizer framework. In order to confirm this, a set of shorter clips were encoded with H.264/AVC, HEVC and VP9-libvpx, with the following experimental set-up:\nTable 3: Parameters used for Dynamic Optimizer Experiment 2Fig. 10: Experiment 2 Results. BD-rate improvement by the dynamic optimizer over fixed-QP/CRF encoding for AVC-High (x264), VP9 (libvpx) and HEVC (x265) in terms of VMAF and PSNR.Table 4: Experiment 2 Results. BD-rate improvement by the dynamic optimizer over fixed-QP/CRF encoding using different video codecs and quality metrics\nOne can notice that the dynamic optimizer improves all three codecs by approximately 28–38%. Keep in mind that these improvements are not comparing performance between codecs but rather how each one of these codecs can be improved by using the dynamic optimizer framework. A more thorough comparison of state-of-the-art video codecs, using the dynamic optimizer as high-level encoding framework, will be published in the upcoming weeks.\nDynamic optimizer summary\nDynamic optimizer is an optimization framework for video encoding. Its key features are the following:\nShot-based encoding\nMultiple encoding at different resolutions and quality parameters\nPerceptual assessment and tuning of quality by using VMAF as its core metric\nMassively parallel processing, ideal for cloud-based video encoding software pipelines\nIts key advantages are the following:\nIt can be applied to any existing or future video codec, which qualifies it as a video encoding optimization framework\nIt can help future codec development by identifying “perceptually relevant” ranges of encoding resolutions and qualities (QPs) for each test video sequence, which can be used while developing and evaluating performance of new coding tools\nIt removes much of the rate-control factor in video codec implementations, thus allowing for much more fair comparisons of video codecs\nIt is orthogonal to improvements one can bring in the shot-encoding recipe, such as better I-B-P coding structure, spatially adaptive QP selection; any improvements performed at the shot level are additive to those brought by the dynamic optimizer\nIts complexity can be scaled up or down, depending on the amount of compute resources, offering a trade-off between complexity and rate-distortion efficiency\nIt produces fully compliant bitstreams\nIt can be used with VMAF, PSNR or any other video quality metric.\nIt benefits both standalone bitstream creation, intended for downloading and offline consumption, as well as full bitrate ladder creation, used for adapting streaming\nCloud Implementation\nWe’ve implemented the dynamic optimizer framework in our encoding pipeline, leveraging our scalable cloud infrastructure and under-utilized cloud instances during non-peak streaming hours [13],[14]. We’ve applied this encoding system to AVC-High and VP9 streams, improving our members’ video quality as well as saving bandwidth. Stay tuned for another tech blog describing our implementation and results!\nAcknowledgement\nThis work is the collective result of the entire Video Algorithms team at Netflix. I would like to personally thank Anne Aaron, Chao Chen, Jan De Cock, Rich Gerber, Liwei Guo, Zhi Li, Megha Manohara, Aditya Mavlankar, Anush Moorthy, Andrey Norkin, Kyle Swanson and David Ronca for all their contributions.\nReferences\n[1] ISO/IEC 11172–2:1993 “Information technology — Coding of moving pictures and associated audio for digital storage media at up to about 1,5 Mbit/s — Part 2: Video”\n[2] ISO/IEC 23008–2:2013 “Information technology — High efficiency coding and media delivery in heterogeneous environments — Part 2: Video”\n[3] ISO/IEC 14496–10:2014 “Information technology — Coding of audio-visual object — Part 10: Advanced Video Coding”\n[4] ISO/IEC 13818–2:2013 “Information technology — Generic coding of moving pictures and associated audio information — Part 2: Video”\n[5] J. De Cock, A. Mavlankar, A. Moorthy and A. Aaron, “A large-scale video codec comparison of x264, x265 and libvpx for practical VOD applications”, Proc. of the SPIE 9971, Applications of Digital Image Processing XXXIX, 997116 (27 Sep. 2016)\n[6] A. Grange, P. de Rivaz, and J. Hunt, “VP9 Bitstream and Decoding Process Specification”, Google, 2016\n[7] “Adaptive bitrate streaming”, Wikipedia — The Free Encyclopedia, https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming\n[8] A. Aaron and D. Ronca, “High quality video encoding at scale,” The NETFLIX tech blog, Dec. 9, 2015, link: http://techblog.netflix.com/2015/12/high-quality-video-encoding-at-scale.html\n[9] A. Aaron, Z. Li, M. Manohara, J. De Cock and D. Ronca, “Per-title encode optimization”, The NETFLIX tech blog, Dec. 14, 2015, link: http://techblog.netflix.com/2015/12/per-title-encode-optimization.html\n[10] A. Norkin, J. De Cock, A. Mavlankar and A. Aaron, “More Efficient Mobile Encodes for Netflix Downloads”, The NETFLIX tech blog, Dec. 1, 2016, link: https://medium.com/netflix-techblog/more-efficient-mobile-encodes-for-netflix-downloads-625d7b082909\n[11] Z. Li, A. Aaron, I. Katsavounidis, A. Moorthy, and M. Manohara, “Toward a practical perceptual video quality metric,” The NETFLIX tech blog, June 5, 2016, link: http://techblog.netflix.com/2016/06/toward-practical-perceptual-video.html\n[12] A. Ortega and K. Ramchandran, “Rate-distortion methods for image and video compression: An overview,” IEEE Signal Processing Magazine, vol. 15, no. 6, pp. 23–50, 1998\n[13] A. Park, D. Derlinger and C. Watson “Creating your own EC2 spot market,” The NETFLIX tech blog, Sep. 28, 2015, link: http://techblog.netflix.com/2015/09/creating-your-own-ec2-spot-market.html\n[14] R. Wong, D. Derlinger, A. Shiroor, N. Mareddy, F. San Miguel, R. Gallardo and M. Prabhu “Creating your own EC2 spot market — part 2,” The NETFLIX tech blog, Nov. 23, 2015, link: http://techblog.netflix.com/2015/11/creating-your-own-ec2-spot-market-part-2.html\nAppendix: VP9 recipe testing\nEncoding parameters used in VP9-libvpx were taken from a previous study; its findings were presented at Netflix’s “Open house on royalty-free codecs” in Oct. 2016. Based on that study, which used the same set of 10 full-titles for testing as those chosen for the first experiment reported earlier, the best configuration to use is “fixed-QP, AQ-mode=0, CPU=0, best”, shown to produce highest quality both in terms of PSNR and VMAF quality metrics. The following figures show the effect in terms of average BD-rate loss when choosing different parameters in VP9-libvpx encoding.\nFig. 11: Effect of AQ-mode=0/1/2 in VP9-libvpx encoding; AQ-mode=0 yields better results in all metrics (from Oct. 2016 presentation on “VP9 Encoding Opportunities @ Netflix”, part of Netflix’s “Open House on Royalty Free Codecs”)Fig. 12: Effect of CPU0/best in VP9-libvpx encoding; CPU0/best is the slowest, but provides consistently better results than other settings (from Oct. 2016 presentation on “VP9 Encoding Opportunities @ Netflix”, part of Netflix “Open House on Royalty Free Codecs”)Fig. 13: BD-rate loss of 2-pass VBR over fixed-QP encoding in VP9-libvpx (from Oct. 2016 presentation on “VP9 Encoding Opportunities @ Netflix”, part of Netflix’s “Open House on Royalty Free Codecs”)",
      "markdown": "## Dynamic optimizer — a perceptual video encoding optimization framework\n\n## _By Ioannis Katsavounidis, Sr. Research Scientist, Video Algorithms_\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----e19f1e3a277f--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----e19f1e3a277f--------------------------------)\n\n## Motivation\n\nVideo encoding has fueled academic research over the past 25 years and enabled compelling products and services. Many companies are built around video encoding and transmission — Netflix and Google’s YouTube are two prime examples of video-centric companies. The fundamentals of video encoding haven’t changed for all these years, as modern video streams are produced with the same type of encoding parameters that have been used since MPEG-1 \\[[1](#1846)\\]: a certain frame resolution is chosen, together with a group-of-pictures (GOP) structure that imposes periodic Intra pictures, and a target bitrate that is (approximately) met by either a single-pass or two-passes over the input video frames.\n\nCompanies have struggled to fine-tune additional parameters in video codecs, creating what is commonly referred to in the industry as a good “recipe”. These recipes have been typically created and customized by human inspection of resulting encodes on a selected set of a few titles and have been kept fixed for a very long time.\n\nAt the same time, improvements in core video codec tools have led to spectacular reduction in bitrate savings — one can achieve the same quality with an HEVC \\[[2](#cf4f)\\] encoder, while using just a fraction (about 30%) of the bits required by MPEG-1. This improvement, although, which has always been measured using mean-squared-error (MSE), wasn’t always accompanied by equally impressive results, when encodes were evaluated by human observers. The magic number to claim when a new codec was being developed has been “50%”. H264/AVC \\[[3](#6d1a)\\] claimed 50% less bits than MPEG-2 \\[[4](#8836)\\] and HEVC claimed 50% less bits than AVC. Yet, in practical systems, these savings never quite materialized — the best estimates on what benefits one sees from an incremental change in video codecs is closer to 40% \\[[5](#d0f2)\\].\n\nParallel to the standardization efforts at ISO and ITU, Google has been developing their own family of royalty-free video codecs; its latest addition was VP9 \\[[6](#698b)\\], first introduced in 2013 and finalized in 2014. VP9 built on the earlier success of VP8 and the line of “True Motion” codecs developed by On2 Technologies, acquired by Google in 2010.\n\nKeeping in mind that most of the video codec improvements carried a very heavy computational overhead on both decoding and — mainly — encoding complexity, one already understands that the newer and more efficient codecs required an ever increasing complexity in order to be deployed in a commercial video transmission service. One typically sees a factor between 5–10 in encoder complexity increase with each generation of video codecs — while the corresponding increase in decoding complexity is typically by a factor of 2.\n\nIf one accepts the increased complexity that comes with newer and more efficient codecs, a bigger question is: what can we do at the system level, for example, in the way we connect video frames as input to an encoder or how we use the output of a video decoder to render it on a screen, to further improve the video quality as perceived by human observers who consume all these hours of video today?\n\nThe keywords in this new approach, presented here, are the following:\n\n*   **Perceptual**: the whole purpose of video encoding is to compress visual information in a way that makes it pleasing to the human eye; Mean-squared-error (MSE), typically used for encoder decisions, is a number that doesn’t always correlate very nicely with human perception.\n*   **Complexity**: just like we spend an increasing amount of complexity within a video codec, we can afford to spend some outside of it, as well.\n*   **Look-ahead**: unlike broadcast TV, where one is forced to make encoding decisions on-the-fly or with minimal delay, in video-on-demand services, video sequences are available in their entirety and can thus be pre-analyzed multiple times to improve quality.\n\n## Shot-based encoding\n\nFor the remainder of this tech blog, we assume the reader is familiar with the basics of adaptive streaming, such as\n\n*   Multiple coded representations of the same visual content in different resolutions and/or qualities, using a basic unit of processing, referred to as “streaming segment”\n*   Delivery of encoded segments from a server, as requested by a streaming client, that belong to different representations in order to accommodate varying channel conditions (bitstream switching)\n*   Temporal alignment of segments among different coded representations of the same visual content to allow bitstream switching\n\nInterested readers can refer to a number of available adaptive streaming tutorials, such as [this Wiki page](https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming) \\[[7](#f324)\\].\n\n## Chunked encoding\n\nIn a previous Netflix tech-blog \\[[8](http://techblog.netflix.com/2015/12/high-quality-video-encoding-at-scale.html)\\], published in Dec. 2015, we described how encoding on the cloud benefits greatly from “chunked” encoding. This translates into breaking a long video sequence, e.g. of 1 hour duration, in multiple chunks, each of a certain duration — for example, 20 chunks, each 3 min. long. We then perform encoding of each chunk independently with a certain encoding recipe, concatenate or “assemble” the encodes and thus obtain an encoded version of the entire video sequence.\n\nAmong the advantages of chunked encoding, the most important is that it allows for a robust system to be built on the cloud using software video encoding. If and when cloud instances fail to complete a certain encode, it requires re-processing the corresponding chunk only, instead of restarting an entire hour-long video encode. One can also see the reduction in end-to-end delay, since different chunks can be encoded in parallel; thus achieving almost infinite scalability in the overall encoding system.\n\nThere are some penalties that come with chunked encoding — namely the fact that a video encoder operating over the full hour-long sequence, especially in two-pass mode, can preview what is following and therefore do better long-term bitrate allocation; thus achieving better overall quality at the same bitrate. Yet, the advantages that come from chunked encoding outweigh these penalties.\n\n## Per-title and per-chunk encode optimization\n\nAt Netflix, we have been constantly improving video quality for our members all over the world. One major milestone in our continuous efforts has been “Per-title encode optimization”, described in great detail in our techblog, posted in Dec. 2015 \\[[9](https://techblog.netflix.com/2015/12/per-title-encode-optimization.html)\\]. Per-title encode optimization introduced the concept of customizing encoding according to complexity, which translates to proper resolution and bitrate selection for each video sequence we have in our catalog. This provided significant improvement over our previous fixed resolution/bitrate ladder generation, by taking into account the characteristics of video — amount of motion, level of detail, colorfulness — and optimizing coding efficiency by selecting encoding parameters that better fit each title. Another important milestone has been “per-chunk encode optimization”, introduced in Dec. 2016 as part of our “Mobile encodes for downloads” initiative, explained in more detail in [this](http://techblog.netflix.com/2016/12/more-efficient-mobile-encodes-for.html) Netflix tech blog \\[[10](#2c28)\\]. The concept of equalizing rate-distortion slopes, discussed in more detail in a subsequent section, was also used in that work and provided significant improvements. In fact, one can consider the current work a natural extension of the “Per-title encode optimization” and “Per-chunk encode optimization”; we can call it “Perceptual per-shot encode optimization”.\n\n## From chunks to shots\n\nIn an ideal world, one would like to chunk a video and impose different sets of parameters to each chunk, in a way to optimize the final assembled video. The first step in achieving this perfect bit allocation is to split video in its natural atoms, consisting of frames that are very similar to each other and thus behave similarly to changes to encoding parameters — these are the “shots” that make up a long video sequence. Shots are portions of video with a relatively short duration, coming from the same camera under fairly constant lighting and environment conditions. It captures the same or similar visual content, for example, the face of an actor standing in front of a tree and — most important — it is uniform in its behavior when changing coding parameters. The natural boundaries of shots are established by relatively simple algorithms, called shot-change detection algorithms, which check the amount of differences between pixels that belong to consecutive frames, as well as other statistics. When that amount of difference exceeds certain fixed or dynamically adapted threshold, a new shot boundary is announced.\n\nThere are cases, such as cross-fades or other visual effects that can be applied on the boundary between two consecutive shots, which can be dealt with by more sophisticated algorithms.\n\nThe end result of a shot-change detection algorithm is a list of shots and their timestamps. One can use the resulting shots as the basic encoding block, instead of a fixed-length chunk. That provides for a few really unique opportunities:\n\n1.  The placement of Intra frames can now be “consistently irregular”, a term that means **(a)** Intra frames can be placed in a “random” place, for example the first 4 Intra frames can be at times 0, 2, 5, 7 secs. and (**b**) Yet, temporal positions are always aligned among encodes of the same title, in order words, the location of the first 4 Intra frames remains at 0, 2, 5, 7 secs. for **all** encodes of this title.\n2.  The irregular placement of Intra frames results in minimum coding overhead; keep in mind that Intra frames are the least efficient among the 3 different types (I/P/B) used in video coding, and thus one wishes to minimize their presence in an encoded video.\n3.  Seeking in a long video sequence now leads to natural points of interest, which are signaled by shot boundaries.\n4.  There is no prediction penalty when encoding shots independently: if one instead places an Intra frame in the middle of a shot, this breaks the shot into parts that, when coded independently instead of a single unit, require more bits, since pixels after the Intra frame can’t reference their similar counterparts in frames before the Intra frame.\n5.  Any significant encoding parameter change between consecutive shots is much less likely to be noticed by the human eye, since the disruption incurred by the different visual content in different shots is far more disruptive to human visual system than any possible encoding parameter — such as resolution/quality — change.\n6.  Within a homogeneous set of frames, such as those that belong to the same shot, there is much less need to use rate-control, since very simple coding schemes, such as the fixed-quantization parameter (“fixed QP”) mode, supported by virtually all existing video encoders, offers a very consistent video quality, with almost minimal bitrate variation. In fact, “fixed QP” has always been used during development of video codecs, since almost all sequences used for testing in MPEG, ITU and other standards bodies, consist of single-shot video chunks.\n\nFig. 1: “Consistently irregular” Intra (key) frame placement on shot boundaries. Key frames are temporally aligned with shot boundaries for all encodes\n\n## VMAF as a perceptual video quality metric\n\nIn another Netflix tech-blog \\[[11](http://techblog.netflix.com/2016/06/toward-practical-perceptual-video.html)\\], published in June 2016, we explained the Video Multi-method Assessment Fusion (VMAF) quality metric, developed in-house and then open-sourced for the entire video community to benefit.\n\nKey features of VMAF are the following:\n\n1.  It is a full-reference metric, which means it can be applied wherever the original, undistorted version of a video sequence is available, as well as a distorted one.\n2.  It takes into account both compression and up/down scaling artifacts, by upsampling decoded video frames to a common reference frame size (1920x1080). In this way, one can use VMAF to assess quality of encoded video at different resolutions. In particular, it can be used to compare encoded versions of the same title at different resolutions and help decide which one is better.\n3.  It relies on existing image quality metrics (VIF, DLM), properly modified to cover multiple scales of resolution, as well as the amount of motion between consecutive video frames in a video sequence as features that are input in a machine-learned set of weights. The final score is the result of combining these elementary features in a support vector machine (SVM) regressor.\n4.  Calibration and training of the weights used in VMAF has been performed by collecting subjective data from actual observers who provided the ground-truth data that VMAF was then fit against. The content used to train VMAF is a representative subset of the Netflix catalog, therefore it is understood that its performance has been tuned to our use-case. Yet, the VMAF framework is general and allows for others to retrain it for their own use-case. In fact, a large number of researchers have validated the accuracy of VMAF using their own subjective datasets.\n\nFig. 2: How VMAF works: pixel level data are pooled to create frame-level features; different spatial and temporal features are fused using SVM regression to create frame-level quality score; consecutive frame scores are pooled to produce final video sequence VMAF score\n\n## Convex hull of Rate-Distortion curve\n\nA seminal paper by Ortega and Ramchandran \\[[12](#b3ac)\\] in 1998 showed how to address optimality when dealing with multiple choices in image and video coding.\n\nAssuming that an image consists of N units that need to be coded\n\n*   You encode each unit separately, obtaining a (rate, distortion) pair for each possibility, called “operating point”\n*   You place all available operating points on a rate-distortion graph\n*   You extract its convex hull, the shell that outlines its boundary\n*   You pick one point on the convex hull from each unit such that points from different units have (roughly) equal distortion-rate slope.\n\nFig. 3: Example of operational points and their R-D convex hull, together with source R-D curve. Reproduced from \\[[12](#b3ac)\\]\n\n## Putting it together\n\nOne can thus consider the following system:\n\n*   A long video sequence is split in shots\n\nFig. 4: Thumbnails from representative shots from “El Fuente” test sequence; this title consists of 96 shots\n\n*   Each shot is encoded multiple times with different encoding parameters, such as resolutions and qualities (QPs)\n*   Each encode is evaluated using VMAF, which together with its bitrate produces an (R,D) point. One can convert VMAF quality to distortion using different mappings; we tested against the following two, linearly and inversely proportional mappings, which give rise to different temporal aggregation strategies, discussed in the subsequent section\n\nFig. 5: Encoding a shot using a set of parameters, such as resolution and QP, and obtaining a single (R,D) point for it.\n\n*   The convex hull of (R,D) points for each shot is calculated. In the following example figures, distortion is inverse of (VMAF+1)\n\nFig. 6: Multiple (R,D) points for a certain shot from “El Fuente”, obtained at various encoding resolutions and QPs using VP9 (libvpx)\n\nFig. 7: Convex hull of (R, D) operating points for the same shot from “El Fuente” using VP9 (libvpx)\n\n*   Points from the convex hull, one from each shot, are combined to create an encode for the entire video sequence by following the constant-slope principle and building end-to-end paths in a Trellis\n\nFig. 8: Combining shot encodes to produce optimal encodes; example Trellis paths show fixed QP encoding, minimizing bitrate for a given average quality or maximizing quality for a given average bitrate. Selected shot encodes have approximately equal slope in (R,D) space.\n\n*   One produces as many aggregate encodes (final operating points) by varying the slope parameter of the R-D curve as necessary in order to cover a desired bitrate/quality range\n*   Final result is a complete R-D or rate-quality (R-Q) curve for the entire video sequence\n\nFig 9: Final R-Q curve obtained for the entire “El Fuente” video sequence. Baseline is the best possible fixed-QP encoding; dynamic optimizer reduces bitrate by an average of 30% in this case\n\nThis complete system is called “Dynamic Optimizer” and the framework produces Netflix’s newest generation of encodes.\n\n## Testing methodology — Results\n\n10 representative titles from the Netflix catalog were selected and encoded using the VP9-libvpx video codec.\n\nIn terms of temporal aggregation, we have implemented various pooling methods, two of them corresponding to the quality-to-distortion mappings introduced earlier, i.e. linear and inversely proportional mapping. We refer to them as arithmetic mean average VMAF (LVMAF) and harmonic mean averaged VMAF (HVMAF).\n\nThese two methods, LVMAF and HVMAF temporal quality aggregation, produced very high quality encoded sequences — allowing for more aggressive or more conservative temporal quality fluctuations in the combined video sequence, respectively.\n\n## VP9 encoding recipe\n\nEncoding parameters used in VP9-libvpx were taken from a previous study; its findings were presented at Netflix’s “Open house on royalty-free codecs” held in Oct. 2016. Based on that study, the best configuration to use is “fixed-QP, AQ-mode=0, CPU=0, best”, shown to produce highest quality both in terms of PSNR and VMAF quality metrics. We reproduce key results from that study in the Appendix.\n\nWe compared results obtained by the dynamic optimizer against the best possible fixed-QP VP9-libvpx encoding. The methodology followed and various parameters chosen for this experiment are summarized in the following table.\n\nTable 1: Parameters used for Dynamic Optimizer Experiment 1\n\nThe corresponding gains obtained by the dynamic optimizer, expressed both in terms of % bitrate savings at the same visual quality and in terms of improvement in HVMAF scores at the same average bitrate, are as follows:\n\nTable 2: Experiment 1 Results. R-D performance comparison around 256kbps between dynamic optimizer and best possible fixed-QP encoding for 10 representative titles from the Netflix catalog.\n\nThe result was an average bitrate savings of 17.1% over the best possible fixed-QP encoding of the entire video sequence when using HVMAF as quality metric. The improvement when using PSNR is even higher: 22.5% bitrate savings on average.\n\nIn this comparison, computational complexity remained constant between the baseline and dynamic optimizer results, since obtaining the convex hull of fixed-QP encodes for an entire sequence requires the same complexity as that for the dynamic optimizer. Thus, this represents a lower-bound on the amount of improvement introduced by the dynamic optimizer.\n\nIf we use a more common baseline, such as the 2-pass VBR configuration with CPU=1, good, AQ-mode=2 encoding recipe in VP9-libvpx, the improvement by the dynamic optimizer is much larger: over 50% bitrate savings on average, in terms of HVMAF. One needs to keep in mind, although, that computational complexity of the dynamic optimizer solution is much higher in that case.\n\n## How good is it for other video codecs?\n\nBased on what was presented earlier, one can immediately understand that there is nothing codec-specific in the dynamic optimizer framework. In order to confirm this, a set of shorter clips were encoded with H.264/AVC, HEVC and VP9-libvpx, with the following experimental set-up:\n\nTable 3: Parameters used for Dynamic Optimizer Experiment 2\n\nFig. 10: Experiment 2 Results. BD-rate improvement by the dynamic optimizer over fixed-QP/CRF encoding for AVC-High (x264), VP9 (libvpx) and HEVC (x265) in terms of VMAF and PSNR.\n\nTable 4: Experiment 2 Results. BD-rate improvement by the dynamic optimizer over fixed-QP/CRF encoding using different video codecs and quality metrics\n\nOne can notice that the dynamic optimizer improves all three codecs by approximately 28–38%. Keep in mind that these improvements are not comparing performance between codecs but rather how each one of these codecs can be improved by using the dynamic optimizer framework. A more thorough comparison of state-of-the-art video codecs, using the dynamic optimizer as high-level encoding framework, will be published in the upcoming weeks.\n\n## Dynamic optimizer summary\n\nDynamic optimizer is an optimization framework for video encoding. Its key features are the following:\n\n*   Shot-based encoding\n*   Multiple encoding at different resolutions and quality parameters\n*   Perceptual assessment and tuning of quality by using VMAF as its core metric\n*   Massively parallel processing, ideal for cloud-based video encoding software pipelines\n\nIts key advantages are the following:\n\n*   It can be applied to any existing or future video codec, which qualifies it as a video encoding optimization framework\n*   It can help future codec development by identifying “perceptually relevant” ranges of encoding resolutions and qualities (QPs) for each test video sequence, which can be used while developing and evaluating performance of new coding tools\n*   It removes much of the rate-control factor in video codec implementations, thus allowing for much more fair comparisons of video codecs\n*   It is orthogonal to improvements one can bring in the shot-encoding recipe, such as better I-B-P coding structure, spatially adaptive QP selection; any improvements performed at the shot level are additive to those brought by the dynamic optimizer\n*   Its complexity can be scaled up or down, depending on the amount of compute resources, offering a trade-off between complexity and rate-distortion efficiency\n*   It produces fully compliant bitstreams\n*   It can be used with VMAF, PSNR or any other video quality metric.\n*   It benefits both standalone bitstream creation, intended for downloading and offline consumption, as well as full bitrate ladder creation, used for adapting streaming\n\n## Cloud Implementation\n\nWe’ve implemented the dynamic optimizer framework in our encoding pipeline, leveraging our scalable cloud infrastructure and under-utilized cloud instances during non-peak streaming hours \\[[13](http://techblog.netflix.com/2015/09/creating-your-own-ec2-spot-market.html)\\],\\[[14](http://techblog.netflix.com/2015/11/creating-your-own-ec2-spot-market-part-2.html)\\]. We’ve applied this encoding system to AVC-High and VP9 streams, improving our members’ video quality as well as saving bandwidth. Stay tuned for another tech blog describing our implementation and results!\n\n## Acknowledgement\n\nThis work is the collective result of the entire Video Algorithms team at Netflix. I would like to personally thank Anne Aaron, Chao Chen, Jan De Cock, Rich Gerber, Liwei Guo, Zhi Li, Megha Manohara, Aditya Mavlankar, Anush Moorthy, Andrey Norkin, Kyle Swanson and David Ronca for all their contributions.\n\n## References\n\n\\[1\\] ISO/IEC 11172–2:1993 “Information technology — Coding of moving pictures and associated audio for digital storage media at up to about 1,5 Mbit/s — Part 2: Video”\n\n\\[2\\] ISO/IEC 23008–2:2013 “Information technology — High efficiency coding and media delivery in heterogeneous environments — Part 2: Video”\n\n\\[3\\] ISO/IEC 14496–10:2014 “Information technology — Coding of audio-visual object — Part 10: Advanced Video Coding”\n\n\\[4\\] ISO/IEC 13818–2:2013 “Information technology — Generic coding of moving pictures and associated audio information — Part 2: Video”\n\n\\[5\\] J. De Cock, A. Mavlankar, A. Moorthy and A. Aaron, “A large-scale video codec comparison of x264, x265 and libvpx for practical VOD applications”, Proc. of the SPIE 9971, Applications of Digital Image Processing XXXIX, 997116 (27 Sep. 2016)\n\n\\[6\\] A. Grange, P. de Rivaz, and J. Hunt, “VP9 Bitstream and Decoding Process Specification”, Google, 2016\n\n\\[7\\] “Adaptive bitrate streaming”, Wikipedia — The Free Encyclopedia, [https://en.wikipedia.org/wiki/Adaptive\\_bitrate\\_streaming](https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming)\n\n\\[8\\] A. Aaron and D. Ronca, “High quality video encoding at scale,” The NETFLIX tech blog, Dec. 9, 2015, link: [http://techblog.netflix.com/2015/12/high-quality-video-encoding-at-scale.html](http://techblog.netflix.com/2015/12/high-quality-video-encoding-at-scale.html)\n\n\\[9\\] A. Aaron, Z. Li, M. Manohara, J. De Cock and D. Ronca, “Per-title encode optimization”, The NETFLIX tech blog, Dec. 14, 2015, link: [http://techblog.netflix.com/2015/12/per-title-encode-optimization.html](http://techblog.netflix.com/2015/12/per-title-encode-optimization.html)\n\n\\[10\\] A. Norkin, J. De Cock, A. Mavlankar and A. Aaron, “More Efficient Mobile Encodes for Netflix Downloads”, The NETFLIX tech blog, Dec. 1, 2016, link: [https://medium.com/netflix-techblog/more-efficient-mobile-encodes-for-netflix-downloads-625d7b082909](https://medium.com/netflix-techblog/more-efficient-mobile-encodes-for-netflix-downloads-625d7b082909)\n\n\\[11\\] Z. Li, A. Aaron, I. Katsavounidis, A. Moorthy, and M. Manohara, “Toward a practical perceptual video quality metric,” The NETFLIX tech blog, June 5, 2016, link: [http://techblog.netflix.com/2016/06/toward-practical-perceptual-video.html](http://techblog.netflix.com/2016/06/toward-practical-perceptual-video.html)\n\n\\[12\\] A. Ortega and K. Ramchandran, “Rate-distortion methods for image and video compression: An overview,” _IEEE Signal Processing Magazine_, vol. 15, no. 6, pp. 23–50, 1998\n\n\\[13\\] A. Park, D. Derlinger and C. Watson “Creating your own EC2 spot market,” The NETFLIX tech blog, Sep. 28, 2015, link: [http://techblog.netflix.com/2015/09/creating-your-own-ec2-spot-market.html](http://techblog.netflix.com/2015/09/creating-your-own-ec2-spot-market.html)\n\n\\[14\\] R. Wong, D. Derlinger, A. Shiroor, N. Mareddy, F. San Miguel, R. Gallardo and M. Prabhu “Creating your own EC2 spot market — part 2,” The NETFLIX tech blog, Nov. 23, 2015, link: [http://techblog.netflix.com/2015/11/creating-your-own-ec2-spot-market-part-2.html](http://techblog.netflix.com/2015/11/creating-your-own-ec2-spot-market-part-2.html)\n\n## Appendix: VP9 recipe testing\n\nEncoding parameters used in VP9-libvpx were taken from a previous study; its findings were presented at Netflix’s “Open house on royalty-free codecs” in Oct. 2016. Based on that study, which used the same set of 10 full-titles for testing as those chosen for the first experiment reported earlier, the best configuration to use is “fixed-QP, AQ-mode=0, CPU=0, best”, shown to produce highest quality both in terms of PSNR and VMAF quality metrics. The following figures show the effect in terms of average BD-rate loss when choosing different parameters in VP9-libvpx encoding.\n\nFig. 11: Effect of AQ-mode=0/1/2 in VP9-libvpx encoding; AQ-mode=0 yields better results in all metrics (from Oct. 2016 presentation on “VP9 Encoding Opportunities @ Netflix”, part of Netflix’s “Open House on Royalty Free Codecs”)\n\nFig. 12: Effect of CPU0/best in VP9-libvpx encoding; CPU0/best is the slowest, but provides consistently better results than other settings (from Oct. 2016 presentation on “VP9 Encoding Opportunities @ Netflix”, part of Netflix “Open House on Royalty Free Codecs”)\n\nFig. 13: BD-rate loss of 2-pass VBR over fixed-QP encoding in VP9-libvpx (from Oct. 2016 presentation on “VP9 Encoding Opportunities @ Netflix”, part of Netflix’s “Open House on Royalty Free Codecs”)"
    },
    {
      "url": "https://netflixtechblog.com/optimized-shot-based-encodes-for-4k-now-streaming-47b516b10bbb",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/optimized-shot-based-encodes-for-4k-now-streaming-47b516b10bbb?gi=46059079e4b6",
        "loadedTime": "2023-12-06T00:05:24.446Z",
        "referrerUrl": "https://netflixtechblog.com/all-of-netflixs-hdr-video-streaming-is-now-dynamically-optimized-e9e0cb15f2ba?source=collection_home---4------0-----------------------",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/optimized-shot-based-encodes-for-4k-now-streaming-47b516b10bbb",
        "title": "Optimized shot-based encodes for 4K: Now streaming! | by Netflix Technology Blog | Netflix TechBlog",
        "description": "Netflix has an ever-expanding collection of titles which customers can enjoy in 4K resolution with a suitable device and subscription plan. Netflix creates premium bitstreams for those titles in…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Optimized shot-based encodes for 4K: Now streaming!\nby Aditya Mavlankar, Liwei Guo, Anush Moorthy and Anne Aaron\nNetflix has an ever-expanding collection of titles which customers can enjoy in 4K resolution with a suitable device and subscription plan. Netflix creates premium bitstreams for those titles in addition to the catalog-wide 8-bit stream profiles¹. Premium features comprise a title-dependent combination of 10-bit bit-depth, 4K resolution, high frame rate (HFR) and high dynamic range (HDR) and pave the way for an extraordinary viewing experience.\nThe premium bitstreams, launched several years ago, were rolled out with a fixed-bitrate ladder, with fixed 4K resolution bitrates — 8, 10, 12 and 16 Mbps — regardless of content characteristics. Since then, we’ve developed algorithms such as per-title encode optimizations and per-shot dynamic optimization, but these innovations were not back-ported on these premium bitstreams. Moreover, the encoding group of pictures (GoP) duration (or keyframe period) was constant throughout the stream causing additional inefficiency due to shot boundaries not aligning with GoP boundaries.\nAs the number of 4K titles in our catalog continues to grow and more devices support the premium features, we expect these video streams to have an increasing impact on our members and the network. We’ve worked hard over the last year to leapfrog to our most advanced encoding innovations — shot-optimized encoding and 4K VMAF model — and applied those to the premium bitstreams. More specifically, we’ve improved the traditional 4K and 10-bit ladder by employing\nshot-based encoding\ndynamic optimization (DO) similar to that applied on our catalog-wide 8-bit stream profiles\nimproved encoder settings.\nIn this blog post, we present benefits of applying the above-mentioned optimizations to standard dynamic range (SDR) 10-bit and 4K streams (some titles are also HFR). As for HDR, our team is currently developing an HDR extension to VMAF, Netflix’s video quality metric, which will then be used to optimize the HDR streams.\n¹ The 8-bit stream profiles go up to 1080p resolution.\nBitrate versus quality comparison\nFor a sample of titles from the 4K collection, the following plots show the rate-quality comparison of the fixed-bitrate ladder and the optimized ladder. The plots have been arranged in decreasing order of the new highest bitrate — which is now content adaptive and commensurate with the overall complexity of the respective title.\nFig. 1: Example of a thriller-drama episode showing new highest bitrate of 11.8 MbpsFig. 2: Example of a sitcom episode with some action showing new highest bitrate of 8.5 MbpsFig. 3: Example of a sitcom episode with less action showing new highest bitrate of 6.6 MbpsFig. 4: Example of a 4K animation episode showing new highest bitrate of 1.8 Mbps\nThe bitrate as well as quality shown for any point is the average for the corresponding stream, computed over the duration of the title. The annotation next to the point is the corresponding encoding resolution; it should be noted that video received by the client device is decoded and scaled to the device’s display resolution. As for VMAF score computation, for encoding resolutions less than 4K, we follow the VMAF best practice to upscale to 4K assuming bicubic upsampling. Aside from the encoding resolution, each point is also associated with an appropriate pixel aspect ratio (PAR) to achieve a target 16:9 display aspect ratio (DAR). For example, the 640x480 encoding resolution is paired with a 4:3 PAR to achieve 16:9 DAR, consistent with the DAR for other points on the ladder.\nThe last example, showing the new highest bitrate to be 1.8 Mbps, is for a 4K animation title episode which can be very efficiently encoded. It serves as an extreme example of content adaptive ladder optimization — it however should not to be interpreted as all animation titles landing on similar low bitrates.\nThe resolutions and bitrates for the fixed-bitrate ladder are pre-determined; minor deviation in the achieved bitrate is due to rate control in the encoder implementation not hitting the target bitrate precisely. On the other hand, each point on the optimized ladder is associated with optimal bit allocation across all shots with the goal of maximizing a video quality objective function while resulting in the corresponding average bitrate. Consequently, for the optimized encodes, the bitrate varies shot to shot depending on relative complexity and overall bit budget and in theory can reach the respective codec level maximum. Various points are constrained to different codec levels, so receivers with different decoder level capabilities can stream the corresponding subset of points up to the corresponding level.\nThe fixed-bitrate ladder often appears like steps — since it is not title adaptive it switches “late” to most encoding resolutions and as a result the quality stays flat within that resolution even with increasing bitrate. For example, two 1080p points with identical VMAF score or four 4K points with identical VMAF score, resulting in wasted bits and increased storage footprint.\nOn the other hand, the optimized ladder appears closer to a monotonically increasing curve — increasing bitrate results in an increasing VMAF score. As a side note, we do have some additional points, not shown in the plots, that are used in resolution limited scenarios — such as a streaming session limited to 720p or 1080p highest encoding resolution. Such points lie under (or to the right of) the convex hull main ladder curve but allow quality to ramp up in resolution limited scenarios.\nChallenging-to-encode content\nFor the optimized ladders we have logic to detect quality saturation at the high end, meaning an increase in bitrate not resulting in material improvement in quality. Once such a bitrate is reached it is a good candidate for the topmost rung of the ladder. An additional limit can be imposed as a safeguard to avoid excessively high bitrates.\nSometimes we ingest a title that would need more bits at the highest end of the quality spectrum — even higher than the 16 Mbps limit of the fixed-bitrate ladder. For example,\na rock concert with fast-changing lighting effects and other details or\na wildlife documentary with fast action and/or challenging spatial details.\nThis scenario is generally rare. Nevertheless, below plot highlights such a case where the optimized ladder exceeds the fixed-bitrate ladder in terms of the highest bitrate, thereby achieving an improvement in the highest quality.\nAs expected, the quality is higher for the same bitrate, even when compared in the low or medium bitrate regions.\nFig. 5: Example of a movie with action and great amount of rich spatial details showing new highest bitrate of 17.2 Mbps\nVisual examples\nAs an example, we compare the 1.75 Mbps encode from the fixed-bitrate ladder with the 1.45 Mbps encode from the optimized ladder for one of the titles from our 4K collection. Since 4K resolution entails a rather large number of pixels, we show 1024x512 pixel cutouts from the two encodes. The encodes are decoded and scaled to a 4K canvas prior to extracting the cutouts. We toggle between the cutouts so it is convenient to spot differences. We also show the corresponding full frame which helps to get a sense of how the cutout fits in the corresponding video frame.\nFig. 6: Pristine full frame — the purpose is to give a sense of how below cutouts fit in the frameFig. 7: Toggling between 1024x512 pixel cutouts from two encodes as annotated. Corresponding to pristine frame shown in Figure 6.Fig. 8: Pristine full frame — the purpose is to give a sense of how below cutouts fit in the frameFig. 9: Toggling between 1024x512 pixel cutouts from two encodes as annotated. Corresponding to pristine frame shown in Figure 8.Fig. 10: Pristine full frame — the purpose is to give a sense of how below cutouts fit in the frameFig. 11: Toggling between 1024x512 pixel cutouts from two encodes as annotated. Corresponding to pristine frame shown in Figure 10.Fig. 12: Pristine full frame — the purpose is to give a sense of how below cutouts fit in the frameFig. 13: Toggling between 1024x512 pixel cutouts from two encodes as annotated. Corresponding to pristine frame shown in Figure 12.Fig. 14: Pristine full frame — the purpose is to give a sense of how below cutouts fit in the frameFig. 15: Toggling between 1024x512 pixel cutouts from two encodes as annotated. Corresponding to pristine frame shown in Figure 14.\nAs can be seen, the encode from the optimized ladder delivers crisper textures and higher detail for less bits. At 1.45 Mbps it is by no means a perfect 4K rendition, but still very commendable for that bitrate. There exist higher bitrate points on the optimized ladder that deliver impeccable 4K quality, also for less bits compared to the fixed-bitrate ladder.\nCompression and bitrate ladder improvements\nEven before testing the new streams in the field, we observe the following advantages of the optimized ladders vs the fixed ladders, evaluated over 100 sample titles:\nComputing the Bjøntegaard Delta (BD) rate shows 50% gains on average over the fixed-bitrate ladder. Meaning, on average we need 50% less bitrate to achieve the same quality with the optimized ladder.\nThe highest 4K bitrate on average is 8 Mbps which is also a 50% reduction compared to 16 Mbps of the fixed-bitrate ladder.\nAs mobile devices continue to improve, they adopt premium features (other than 4K resolution) like 10-bit and HFR. These video encodes can be delivered to mobile devices as well. The fixed-bitrate ladder starts at 560 kbps which may be too high for some cellular networks. The optimized ladder, on the other hand, has lower bitrate points that are viable in most cellular scenarios.\nThe optimized ladder entails a smaller storage footprint compared to the fixed-bitrate ladder.\nThe new ladder considers adding 1440p resolution (aka QHD) points if they lie on the convex hull of rate-quality tradeoff and most titles seem to get the 1440p treatment. As a result, when averaged over 100 titles, the bitrate required to jump to a resolution higher than 1080p (meaning either QHD or 4K) is 1.7 Mbps compared to 8 Mbps of the fixed-bitrate ladder. When averaged over 100 titles, the bitrate required to jump to 4K resolution is 3.2 Mbps compared to 8 Mbps of the fixed-bitrate ladder.\nBenefits to members\nAt Netflix we perform A/B testing of encoding optimizations to detect any playback issues on client devices as well as gauge the benefits experienced by our members. One set of streaming sessions receives the default encodes and the other set of streaming sessions receives the new encodes. This in turn allows us to compare error rates as well as various metrics related to quality of experience (QoE). Although our streams are standard compliant, the A/B testing can and does sometimes find device-side implementations with minor gaps; in such cases we work with our device partners to find the best remedy.\nOverall, while A/B testing these new encodes, we have seen the following benefits, which are in line with the offline evaluation covered in the previous section:\nFor members with high-bandwidth connections we deliver the same great quality at half the bitrate on average.\nFor members with constrained bandwidth we deliver higher quality at the same (or even lower) bitrate — higher VMAF at the same encoding resolution and bitrate or even higher resolutions than they could stream before. For example, members who were limited by their network to 720p can now be served 1080p or higher resolution instead.\nMost streaming sessions start with a higher initial quality.\nThe number of rebuffers per hour go down by over 65%; members also experience fewer quality drops while streaming.\nThe reduced bitrate together with some Digital Rights Management (DRM) system improvements (not covered in this blog) result in reducing the initial play delay by about 10%.\nNext steps\nWe have started re-encoding the 4K titles in our catalog to generate the optimized streams and we expect to complete in a couple of months. We continue to work on applying similar optimizations to our HDR streams.\nAcknowledgements\nWe thank Lishan Zhu for help rendered during A/B testing.\nThis is a collective effort on the part of our larger team, known as Encoding Technologies, and various other teams that we have crucial partnerships with, such as:\nThe various client device and UI engineering teams that manage the Netflix experience on various platforms\nThe data science and engineering teams that help us run and analyze A/B tests\nThe Open Connect team that manages Netflix’s own content delivery network\nThe Product Edge team that steers the Netflix experience for every client device including the experience served in various encoding A/B tests\nThe Media Cloud Engineering team that manages the compute platform/orchestration that enables us to execute video encoding at scale\nIf you are passionate about video compression research and would like to contribute to this field, we have an open position.",
      "markdown": "## Optimized shot-based encodes for 4K: Now streaming!\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----47b516b10bbb--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----47b516b10bbb--------------------------------)\n\nby [Aditya Mavlankar](https://www.linkedin.com/in/aditya-mavlankar-7139791/), [Liwei Guo](https://www.linkedin.com/in/liwei-guo-a5aa6311/), [Anush Moorthy](https://www.linkedin.com/in/anush-moorthy-b8451142/) and [Anne Aaron](https://www.linkedin.com/in/anne-aaron/)\n\nNetflix has an ever-expanding collection of titles which customers can enjoy in 4K resolution with a suitable device and subscription plan. Netflix creates _premium_ bitstreams for those titles in addition to the catalog-wide 8-bit stream profiles¹. _Premium_ features comprise a title-dependent combination of 10-bit bit-depth, 4K resolution, high frame rate (HFR) and high dynamic range (HDR) and pave the way for an extraordinary viewing experience.\n\nThe premium bitstreams, launched several years ago, were rolled out with a fixed-bitrate ladder, with fixed 4K resolution bitrates — 8, 10, 12 and 16 Mbps — regardless of content characteristics. Since then, we’ve developed algorithms such as [per-title encode optimizations](https://netflixtechblog.com/per-title-encode-optimization-7e99442b62a2) and [per-shot dynamic optimization](https://netflixtechblog.com/dynamic-optimizer-a-perceptual-video-encoding-optimization-framework-e19f1e3a277f), but these innovations were not back-ported on these premium bitstreams. Moreover, the encoding group of pictures (GoP) duration (or keyframe period) was constant throughout the stream causing additional inefficiency due to shot boundaries not aligning with GoP boundaries.\n\nAs the number of 4K titles in our catalog continues to grow and more devices support the premium features, we expect these video streams to have an increasing impact on our members and the network. We’ve worked hard over the last year to leapfrog to our most advanced encoding innovations — shot-optimized encoding and [4K VMAF model](https://netflixtechblog.com/vmaf-the-journey-continues-44b51ee9ed12) — and applied those to the premium bitstreams. More specifically, we’ve improved the traditional 4K and 10-bit ladder by employing\n\n*   shot-based encoding\n*   dynamic optimization (DO) similar to that [applied on our catalog-wide 8-bit stream profiles](https://netflixtechblog.com/optimized-shot-based-encodes-now-streaming-4b9464204830)\n*   improved encoder settings.\n\nIn this blog post, we present benefits of applying the above-mentioned optimizations to standard dynamic range (SDR) 10-bit and 4K streams (some titles are also HFR). As for HDR, our team is currently developing an HDR extension to VMAF, [Netflix’s video quality metric](https://netflixtechblog.com/toward-a-practical-perceptual-video-quality-metric-653f208b9652), which will then be used to optimize the HDR streams.\n\n¹ _The 8-bit stream profiles go up to 1080p resolution._\n\n## Bitrate versus quality comparison\n\nFor a sample of titles from the 4K collection, the following plots show the rate-quality comparison of the fixed-bitrate ladder and the optimized ladder. The plots have been arranged in decreasing order of the new highest bitrate — which is now content adaptive and commensurate with the overall complexity of the respective title.\n\nFig. 1: Example of a thriller-drama episode showing new highest bitrate of 11.8 Mbps\n\nFig. 2: Example of a sitcom episode with some action showing new highest bitrate of 8.5 Mbps\n\nFig. 3: Example of a sitcom episode with less action showing new highest bitrate of 6.6 Mbps\n\nFig. 4: Example of a 4K animation episode showing new highest bitrate of 1.8 Mbps\n\nThe bitrate as well as quality shown for any point is the _average_ for the corresponding stream, computed over the duration of the title. The annotation next to the point is the corresponding encoding resolution; it should be noted that video received by the client device is decoded and scaled to the device’s display resolution. As for VMAF score computation, for encoding resolutions less than 4K, we follow the [VMAF best practice](https://netflixtechblog.com/vmaf-the-journey-continues-44b51ee9ed12) to upscale to 4K assuming bicubic upsampling. Aside from the encoding resolution, each point is also associated with an appropriate pixel aspect ratio (PAR) to achieve a target 16:9 display aspect ratio (DAR). For example, the 640x480 encoding resolution is paired with a 4:3 PAR to achieve 16:9 DAR, consistent with the DAR for other points on the ladder.\n\nThe last example, showing the new highest bitrate to be 1.8 Mbps, is for a 4K animation title episode which can be very efficiently encoded. It serves as an extreme example of content adaptive ladder optimization — it however should not to be interpreted as all animation titles landing on similar low bitrates.\n\nThe resolutions and bitrates for the fixed-bitrate ladder are pre-determined; minor deviation in the achieved bitrate is due to rate control in the encoder implementation not hitting the target bitrate precisely. On the other hand, each point on the optimized ladder is associated with _optimal_ bit allocation across all shots with the goal of maximizing a video quality objective function while resulting in the corresponding average bitrate. Consequently, for the optimized encodes, the bitrate varies shot to shot depending on relative complexity and overall bit budget and in theory can reach the respective codec level maximum. Various points are constrained to different codec levels, so receivers with different decoder level capabilities can stream the corresponding subset of points up to the corresponding level.\n\nThe fixed-bitrate ladder often appears like steps — since it is not title adaptive it switches “late” to most encoding resolutions and as a result the quality stays flat within that resolution even with increasing bitrate. For example, two 1080p points with identical VMAF score or four 4K points with identical VMAF score, resulting in wasted bits and increased storage footprint.\n\nOn the other hand, the optimized ladder appears closer to a monotonically increasing curve — increasing bitrate results in an increasing VMAF score. As a side note, we do have some additional points, not shown in the plots, that are used in resolution limited scenarios — such as a streaming session limited to 720p or 1080p highest encoding resolution. Such points lie under (or to the right of) the convex hull main ladder curve but allow quality to ramp up in resolution limited scenarios.\n\n## Challenging-to-encode content\n\nFor the optimized ladders we have logic to detect quality saturation at the high end, meaning an increase in bitrate not resulting in material improvement in quality. Once such a bitrate is reached it is a good candidate for the topmost rung of the ladder. An additional limit can be imposed as a safeguard to avoid excessively high bitrates.\n\nSometimes we ingest a title that would need more bits at the highest end of the quality spectrum — even higher than the 16 Mbps limit of the fixed-bitrate ladder. For example,\n\n*   a rock concert with fast-changing lighting effects and other details or\n*   a wildlife documentary with fast action and/or challenging spatial details.\n\nThis scenario is generally rare. Nevertheless, below plot highlights such a case where the optimized ladder exceeds the fixed-bitrate ladder in terms of the highest bitrate, thereby achieving an improvement in the highest quality.\n\nAs expected, the quality is higher for the same bitrate, even when compared in the low or medium bitrate regions.\n\nFig. 5: Example of a movie with action and great amount of rich spatial details showing new highest bitrate of 17.2 Mbps\n\n## Visual examples\n\nAs an example, we compare the 1.75 Mbps encode from the fixed-bitrate ladder with the 1.45 Mbps encode from the optimized ladder for one of the titles from our 4K collection. Since 4K resolution entails a rather large number of pixels, we show 1024x512 pixel cutouts from the two encodes. The encodes are decoded and scaled to a 4K canvas prior to extracting the cutouts. We toggle between the cutouts so it is convenient to spot differences. We also show the corresponding full frame which helps to get a sense of how the cutout fits in the corresponding video frame.\n\nFig. 6: Pristine full frame — the purpose is to give a sense of how below cutouts fit in the frame\n\nFig. 7: Toggling between 1024x512 pixel cutouts from two encodes as annotated. Corresponding to pristine frame shown in Figure 6.\n\nFig. 8: Pristine full frame — the purpose is to give a sense of how below cutouts fit in the frame\n\nFig. 9: Toggling between 1024x512 pixel cutouts from two encodes as annotated. Corresponding to pristine frame shown in Figure 8.\n\nFig. 10: Pristine full frame — the purpose is to give a sense of how below cutouts fit in the frame\n\nFig. 11: Toggling between 1024x512 pixel cutouts from two encodes as annotated. Corresponding to pristine frame shown in Figure 10.\n\nFig. 12: Pristine full frame — the purpose is to give a sense of how below cutouts fit in the frame\n\nFig. 13: Toggling between 1024x512 pixel cutouts from two encodes as annotated. Corresponding to pristine frame shown in Figure 12.\n\nFig. 14: Pristine full frame — the purpose is to give a sense of how below cutouts fit in the frame\n\nFig. 15: Toggling between 1024x512 pixel cutouts from two encodes as annotated. Corresponding to pristine frame shown in Figure 14.\n\nAs can be seen, the encode from the optimized ladder delivers crisper textures and higher detail for less bits. At 1.45 Mbps it is by no means a perfect 4K rendition, but still very commendable for that bitrate. There exist higher bitrate points on the optimized ladder that deliver impeccable 4K quality, also for less bits compared to the fixed-bitrate ladder.\n\n## Compression and bitrate ladder improvements\n\nEven before testing the new streams in the field, we observe the following advantages of the optimized ladders vs the fixed ladders, evaluated over 100 sample titles:\n\n*   Computing the [Bjøntegaard Delta (BD) rate](https://www.itu.int/wftp3/av-arch/video-site/0104_Aus/VCEG-M33.doc) shows **_50% gains_** on average over the fixed-bitrate ladder. Meaning, on average we need **_50% less bitrate_** to achieve the same quality with the optimized ladder.\n*   The highest 4K bitrate on average is 8 Mbps which is also a **_50% reduction_** compared to 16 Mbps of the fixed-bitrate ladder.\n*   As mobile devices continue to improve, they adopt premium features (other than 4K resolution) like 10-bit and HFR. These video encodes can be delivered to mobile devices as well. The fixed-bitrate ladder starts at 560 kbps which may be too high for some cellular networks. The optimized ladder, on the other hand, has lower bitrate points that are viable in most cellular scenarios.\n*   The optimized ladder entails a smaller storage footprint compared to the fixed-bitrate ladder.\n*   The new ladder considers adding 1440p resolution (aka QHD) points if they lie on the convex hull of rate-quality tradeoff and most titles seem to get the 1440p treatment. As a result, when averaged over 100 titles, the bitrate required to jump to a resolution higher than 1080p (meaning either QHD or 4K) is **_1.7 Mbps compared to 8 Mbps_** of the fixed-bitrate ladder. When averaged over 100 titles, the bitrate required to jump to 4K resolution is **_3.2 Mbps compared to 8 Mbps_** of the fixed-bitrate ladder.\n\n## Benefits to members\n\nAt Netflix we perform A/B testing of encoding optimizations to detect any playback issues on client devices as well as gauge the benefits experienced by our members. One set of streaming sessions receives the default encodes and the other set of streaming sessions receives the new encodes. This in turn allows us to compare error rates as well as various metrics related to quality of experience (QoE). Although our streams are standard compliant, the A/B testing can and does sometimes find device-side implementations with minor gaps; in such cases we work with our device partners to find the best remedy.\n\nOverall, while A/B testing these new encodes, we have seen the following benefits, which are in line with the offline evaluation covered in the previous section:\n\n*   For members with high-bandwidth connections we deliver **_the same great quality at half the bitrate_** on average.\n*   For members with constrained bandwidth we deliver higher quality at the same (or even lower) bitrate — higher VMAF at the same encoding resolution and bitrate or even higher resolutions than they could stream before. For example, members who were limited by their network to 720p can now be served 1080p or higher resolution instead.\n*   Most streaming sessions start with a higher initial quality.\n*   **_The number of rebuffers per hour go down by over 65%_**; members also experience fewer quality drops while streaming.\n*   The reduced bitrate together with some Digital Rights Management (DRM) system improvements (not covered in this blog) result in **_reducing the initial play delay by about 10%_**.\n\n## Next steps\n\nWe have started re-encoding the 4K titles in our catalog to generate the optimized streams and we expect to complete in a couple of months. We continue to work on applying similar optimizations to our HDR streams.\n\n## Acknowledgements\n\nWe thank Lishan Zhu for help rendered during A/B testing.\n\nThis is a collective effort on the part of our larger team, known as Encoding Technologies, and various other teams that we have crucial partnerships with, such as:\n\n*   The various [client device and UI engineering teams](https://jobs.netflix.com/teams/client-and-ui-engineering) that manage the Netflix experience on various platforms\n*   The [data science and engineering teams](https://jobs.netflix.com/teams/data-science-and-engineering) that help us run and analyze A/B tests\n*   The [Open Connect](https://media.netflix.com/en/company-blog/how-netflix-works-with-isps-around-the-globe-to-deliver-a-great-viewing-experience) team that manages Netflix’s own content delivery network\n*   The [Product Edge](https://www.youtube.com/watch?v=5ju4W9KAzcY) team that steers the Netflix experience for every client device including the experience served in various encoding A/B tests\n*   The [Media Cloud Engineering](https://jobs.netflix.com/teams/studio-technologies) team that manages the compute platform/orchestration that enables us to execute video encoding at scale\n\nIf you are passionate about video compression research and would like to contribute to this field, we have an [open](https://jobs.netflix.com/jobs/867864) position."
    },
    {
      "url": "https://netflixtechblog.com/the-netflix-cosmos-platform-35c14d9351ad",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/the-netflix-cosmos-platform-35c14d9351ad?gi=67479607b185",
        "loadedTime": "2023-12-06T00:05:30.248Z",
        "referrerUrl": "https://netflixtechblog.com/all-of-netflixs-hdr-video-streaming-is-now-dynamically-optimized-e9e0cb15f2ba?source=collection_home---4------0-----------------------",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/the-netflix-cosmos-platform-35c14d9351ad",
        "title": "The Netflix Cosmos Platform. Orchestrated Functions as a… | by Netflix Technology Blog | Netflix TechBlog",
        "description": "Cosmos is a computing platform that combines the best aspects of microservices with asynchronous workflows and serverless functions. Its sweet spot is applications that involve resource-intensive…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "The Netflix Cosmos Platform\nOrchestrated Functions as a Microservice\nby Frank San Miguel on behalf of the Cosmos team\nIntroduction\nCosmos is a computing platform that combines the best aspects of microservices with asynchronous workflows and serverless functions. Its sweet spot is applications that involve resource-intensive algorithms coordinated via complex, hierarchical workflows that last anywhere from minutes to years. It supports both high throughput services that consume hundreds of thousands of CPUs at a time, and latency-sensitive workloads where humans are waiting for the results of a computation.\nA Cosmos service\nThis article will explain why we built Cosmos, how it works and share some of the things we have learned along the way.\nBackground\nThe Media Cloud Engineering and Encoding Technologies teams at Netflix jointly operate a system to process incoming media files from our partners and studios to make them playable on all devices. The first generation of this system went live with the streaming launch in 2007. The second generation added scale but was extremely difficult to operate. The third generation, called Reloaded, has been online for about seven years and has proven to be stable and massively scalable.\nWhen Reloaded was designed, we were a small team of developers operating a constrained compute cluster, and focused on one use case: the video/audio processing pipeline. As time passed the number of developers more than tripled, the breadth and depth of our use cases expanded, and our scale increased more than tenfold. The monolithic architecture significantly slowed down the delivery of new features. We could no longer expect everyone to possess the specialized knowledge that was necessary to build and deploy new features. Dealing with production issues became an expensive chore that placed a tax on all developers because infrastructure code was all mixed up with application code. The centralized data model that had served us well when we were a small team became a liability.\nOur response was to create Cosmos, a platform for workflow-driven, media-centric microservices. The first-order goals were to preserve our current capabilities while offering:\nObservability — via built-in logging, tracing, monitoring, alerting and error classification.\nModularity — An opinionated framework for structuring a service and enabling both compile-time and run-time modularity.\nProductivity — Local development tools including specialized test runners, code generators, and a command line interface.\nDelivery — A fully-managed continuous-delivery system of pipelines, continuous integration jobs, and end to end tests. When you merge your pull request, it makes it to production without manual intervention.\nWhile we were at it, we also made improvements to scalability, reliability, security, and other system qualities.\nOverview\nA Cosmos service is not a microservice but there are similarities. A typical microservice is an API with stateless business logic which is autoscaled based on request load. The API provides strong contracts with its peers while segregating application data and binary dependencies from other systems.\nA typical microservice\nA Cosmos service retains the strong contracts and segregated data/dependencies of a microservice, but adds multi-step workflows and computationally intensive asynchronous serverless functions. In the diagram below of a typical Cosmos service, clients send requests to a Video encoder service API layer. A set of rules orchestrate workflow steps and a set of serverless functions power domain-specific algorithms. Functions are packaged as Docker images and bring their own media-specific binary dependencies (e.g. debian packages). They are scaled based on queue size, and may run on tens of thousands of different containers. Requests may take hours or days to complete.\nA typical Cosmos service\nSeparation of concerns\nCosmos has two axes of separation. On the one hand, logic is divided between API, workflow and serverless functions. On the other hand, logic is separated between application and platform. The platform API provides media-specific abstractions to application developers while hiding the details of distributed computing. For example, a video encoding service is built of components that are scale-agnostic: API, workflow, and functions. They have no special knowledge about the scale at which they run. These domain-specific, scale-agnostic components are built on top of three scale-aware Cosmos subsystems which handle the details of distributing the work:\nOptimus, an API layer mapping external requests to internal business models.\nPlato, a workflow layer for business rule modeling.\nStratum, a serverless layer called for running stateless and computational-intensive functions.\nThe subsystems all communicate with each other asynchronously via Timestone, a high-scale, low-latency priority queuing system. Each subsystem addresses a different concern of a service and can be deployed independently through a purpose-built managed Continuous Delivery process. This separation of concerns makes it easier to write, test, and operate Cosmos services.\nSeparation of Platform and Application\nA Cosmos service request\nTrace graph of a Cosmos service request\nThe picture above is a screenshot from Nirvana, our observability portal. It shows a typical service request in Cosmos (a video encoder service in this case):\nThere is one API call to encode, which includes the video source and a recipe\nThe video is split into 31 chunks, and the 31 encoding functions run in parallel\nThe assemble function is invoked once\nThe index function is invoked once\nThe workflow is complete after 8 minutes\nLayering of services\nCosmos supports decomposition and layering of services. The resulting modular architecture allows teams to concentrate on their area of specialty and control their APIs and release cycles.\nFor example, the video service mentioned above is just one of many used to create streams that can be played on devices. These services, which also include inspection, audio, text, and packaging, are orchestrated using higher-level services. The largest and most complex of these is Tapas, which is responsible for taking sources from studios and making them playable on the Netflix service. Another high-level service is Sagan, which is used for studio operations like marketing clips or daily production editorial proxies.\nLayering of Cosmos services\nWhen a new title arrives from a production studio, it triggers a Tapas workflow which orchestrates requests to perform inspections, encode video (multiple resolutions, qualities, and video codecs), encode audio (multiple qualities and codecs), generate subtitles (many languages), and package the resulting outputs (multiple player formats). Thus, a single request to Tapas can result in hundreds of requests to other Cosmos services and thousands of Stratum function invocations.\nThe trace below shows an example of how a request at a top level service can trickle down to lower level services, resulting in many different actions. In this case the request took 24 minutes to complete, with hundreds of different actions involving 8 different Cosmos services and 9 different Stratum functions.\nTrace graph of a service request through multiple layers\nWorkflows rule!\nOr should we say workflow rules? Plato is the glue that ties everything together in Cosmos by providing a framework for service developers to define domain logic and orchestrate stateless functions/services. The Optimus API layer has built-in facilities to invoke workflows and examine their state. The Stratum serverless layer generates strongly-typed RPC clients to make invoking a serverless function easy and intuitive.\nPlato is a forward chaining rule engine which lends itself to the asynchronous and compute-intensive nature of our algorithms. Unlike a procedural workflow engine like Netflix’s Conductor, Plato makes it easy to create workflows that are “always on”. For example, as we develop better encoding algorithms, our rules-based workflows automatically manage updating existing videos without us having to trigger and manage new workflows. In addition, any workflow can call another, which enables the layering of services mentioned above.\nPlato is a multi-tenant system (implemented using Apache Karaf), which greatly reduces the operational burden of operating a workflow. Users write and test their rules in their own source code repository and then deploy the workflow by uploading the compiled code to the Plato server.\nDevelopers specify their workflows in a set of rules written in Emirax, a domain specific language built on Groovy. Each rule has 4 sections:\nmatch: Specifies the conditions that must be satisfied for this rule to trigger\naction: Specifies the code to be executed when this rule is triggered; this is where you invoke Stratum functions to process the request.\nreaction: Specifies the code to be executed when the action code completes successfully\nerror: Specifies the code to be executed when an error is encountered.\nIn each of these sections, you typically first record the change in state of the workflow and then perform steps to move the workflow forward, such as executing a Stratum function or returning the results of the execution (For more details, see this presentation).\nLatency-sensitive applications\nCosmos services like Sagan are latency sensitive because they are user-facing. For example, an artist who is working on a social media post doesn’t want to wait a long time when clipping a video from the latest season of Money Heist. For Stratum, latency is a function of the time to perform the work plus the time to get computing resources. When work is very bursty (which is often the case), the “time to get resources” component becomes the significant factor. For illustration, let’s say that one of the things you normally buy when you go shopping is toilet paper. Normally there is no problem putting it in your cart and getting through the checkout line, and the whole process takes you 30 minutes.\nResource scarcity\nThen one day a bad virus thing happens and everyone decides they need more toilet paper at the same time. Your toilet paper latency now goes from 30 minutes to two weeks because the overall demand exceeds the available capacity. Cosmos applications (and Stratum functions in particular) have this same problem in the face of bursty and unpredictable demand. Stratum manages function execution latency in a few ways:\nResource pools. End-users can reserve Stratum computing resources for their own business use case, and resource pools are hierarchical to allow groups of users to share resources.\nWarm capacity. End-users can request compute resources (e.g. containers) in advance of demand to reduce startup latencies in Stratum.\nMicro-batches. Stratum also uses micro-batches, which is a trick found in platforms like Apache Spark to reduce startup latency. The idea is to spread the startup cost across many function invocations. If you invoke your function 10,000 times, it may run one time each on 10,000 containers or it may run 10 times each on 1000 containers.\nPriority. When balancing cost with the desire for low latency, Cosmos services usually land somewhere in the middle: enough resources to handle typical bursts but not enough to handle the largest bursts with the lowest latency. By prioritizing work, applications can still ensure that the most important work is processed with low latency even when resources are scarce. Cosmos service owners can allow end-users to set priority, or set it themselves in the API layer or in the workflow.\nThroughput-sensitive applications\nServices like Tapas are throughput-sensitive because they consume large amounts of computing resources (e.g millions of CPU-hours per day) and are more concerned with the completion of tasks over a period of hours or days rather than the time to complete an individual task. In other words, the service level objectives (SLO) are measured in tasks per day and cost per task rather than tasks per second.\nFor throughput-sensitive workloads, the most important SLOs are those provided by the Stratum serverless layer. Stratum, which is built on top of the Titus container platform, allows throughput sensitive workloads to use “opportunistic” compute resources through flexible resource scheduling. For example, the cost of a serverless function invocation might be lower if it is willing to wait up to an hour to execute.\nThe strangler fig\nWe knew that moving a legacy system as large and complicated as Reloaded was going to be a big leap over a dangerous chasm littered with the shards of failed re-engineering projects, but there was no question that we had to jump. To reduce risk, we adopted the strangler fig pattern which lets the new system grow around the old one and eventually replace it completely.\nStill learning\nWe started building Cosmos in 2018 and have been operating in production since early 2019. Today there are about 40 cosmos services and we expect more growth to come. We are still in mid-journey but we can share a few highlights of what we have learned so far:\nThe Netflix culture played a key role\nThe Netflix engineering culture famously relies on personal judgement rather than top-down control. Software developers have both freedom and responsibility to take risks and make decisions. None of us have the title of Software Architect; all of us play that role. In this context, Cosmos emerged in fits and starts from disparate attempts at local optimization. Optimus, Plato and Stratum were conceived independently and eventually coalesced into the vision of a single platform. The application developers on the team kept everyone focused on user-friendly APIs and developer productivity. It took a strong partnership between infrastructure and media algorithm developers to turn the vision into reality. We couldn’t have done that in a top-down engineering environment.\nMicroservice + Workflow + Serverless\nWe have found that the programming model of “microservices that trigger workflows that orchestrate serverless functions” to be a powerful paradigm. It works well for most of our use cases but some applications are simple enough that the added complexity is not worth the benefits.\nA platform mindset\nMoving from a large distributed application to a “platform plus applications” was a major paradigm shift. Everyone had to change their mindset. Application developers had to give up a certain amount of flexibility in exchange for consistency, reliability, etc. Platform developers had to develop more empathy and prioritize customer service, user productivity, and service levels. There were moments where application developers felt the platform team was not focused appropriately on their needs, and other times when platform teams felt overtaxed by user demands. We got through these tough spots by being open and honest with each other. For example after a recent retrospective, we strengthened our development tracks for crosscutting system qualities such as developer experience, reliability, observability and security.\nPlatform wins\nWe started Cosmos with the goal of enabling developers to work better and faster, spending more time on their business problem and less time dealing with infrastructure. At times the goal has seemed elusive, but we are beginning to see the gains we had hoped for. Some of the system qualities that developers like best in Cosmos are managed delivery, modularity, and observability, and developer support. We are working to make these qualities even better while also working on weaker areas like local development, resilience and testability.\nFuture plans\n2021 will be a big year for Cosmos as we move the majority of work from Reloaded into Cosmos, with more developers and much higher load. We plan to evolve the programming model to accommodate new use cases. Our goals are to make Cosmos easier to use, more resilient, faster and more efficient. Stay tuned to learn more details of how Cosmos works and how we use it.",
      "markdown": "## The Netflix Cosmos Platform\n\n## Orchestrated Functions as a Microservice\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----35c14d9351ad--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----35c14d9351ad--------------------------------)\n\n_by_ [_Frank San Miguel_](https://www.linkedin.com/in/franksanmiguel/) _on behalf of the Cosmos team_\n\n## Introduction\n\nCosmos is a computing platform that combines the best aspects of microservices with asynchronous workflows and serverless functions. Its sweet spot is applications that involve resource-intensive algorithms coordinated via complex, hierarchical workflows that last anywhere from minutes to years. It supports both high throughput services that consume hundreds of thousands of CPUs at a time, and latency-sensitive workloads where humans are waiting for the results of a computation.\n\nA Cosmos service\n\nThis article will explain why we built Cosmos, how it works and share some of the things we have learned along the way.\n\n## Background\n\nThe Media Cloud Engineering and Encoding Technologies teams at Netflix jointly operate a system to process incoming media files from our partners and studios to [make them playable](https://netflixtechblog.com/optimized-shot-based-encodes-for-4k-now-streaming-47b516b10bbb) on all devices. The first generation of this system went live with the streaming launch in 2007. The second generation added scale but was extremely difficult to operate. The third generation, called [Reloaded](https://youtu.be/JouA10QJiNc), has been online for about seven years and has proven to be stable and [massively scalable](https://netflixtechblog.com/creating-your-own-ec2-spot-market-part-2-106e53be9ed7).\n\nWhen Reloaded was designed, we were a small team of developers operating a constrained compute cluster, and focused on one use case: the video/audio processing pipeline. As time passed the number of developers more than tripled, the breadth and depth of our use cases expanded, and our scale increased more than tenfold. The monolithic architecture significantly slowed down the delivery of new features. We could no longer expect everyone to possess the specialized knowledge that was necessary to build and deploy new features. Dealing with production issues became an expensive chore that placed a tax on all developers because infrastructure code was all mixed up with application code. The centralized data model that had served us well when we were a small team became a liability.\n\nOur response was to create Cosmos, a platform for workflow-driven, media-centric microservices. The first-order goals were to preserve our current capabilities while offering:\n\n*   Observability — via built-in logging, tracing, monitoring, alerting and error classification.\n*   Modularity — An opinionated framework for structuring a service and enabling both compile-time and run-time modularity.\n*   Productivity — Local development tools including specialized test runners, code generators, and a command line interface.\n*   Delivery — A fully-managed continuous-delivery system of pipelines, continuous integration jobs, and end to end tests. When you merge your pull request, it makes it to production without manual intervention.\n\nWhile we were at it, we also made improvements to scalability, reliability, security, and other system qualities.\n\n## Overview\n\nA Cosmos service is not a microservice but there are similarities. A typical microservice is an API with stateless business logic which is autoscaled based on request load. The API provides strong contracts with its peers while segregating application data and binary dependencies from other systems.\n\n_A typical microservice_\n\nA Cosmos service retains the strong contracts and segregated data/dependencies of a microservice, but adds multi-step workflows and computationally intensive asynchronous serverless functions. In the diagram below of a typical Cosmos service, clients send requests to a Video encoder service API layer. A set of rules orchestrate workflow steps and a set of serverless functions power domain-specific algorithms. Functions are packaged as Docker images and bring their own media-specific binary dependencies (e.g. debian packages). They are scaled based on queue size, and may run on tens of thousands of different containers. Requests may take hours or days to complete.\n\n_A typical Cosmos service_\n\n## Separation of concerns\n\nCosmos has two axes of separation. On the one hand, logic is divided between API, workflow and serverless functions. On the other hand, logic is separated between application and platform. The platform API provides media-specific abstractions to application developers while hiding the details of distributed computing. For example, a video encoding service is built of components that are scale-agnostic: API, workflow, and functions. They have no special knowledge about the scale at which they run. These domain-specific, scale-agnostic components are built on top of three [scale-aware](https://queue.acm.org/detail.cfm?id=3025012) Cosmos subsystems which handle the details of distributing the work:\n\n*   Optimus, an API layer mapping external requests to internal business models.\n*   [Plato](https://feathercast.apache.org/2019/09/13/serverless-multi-tenant-rule-engine-service-powered-by-apache-karaf-dmitry-vasilyev-saeid-mirzaei-george-ye/), a workflow layer for business rule modeling.\n*   [Stratum](https://plus.streamingtech.se/asset/4c87f560-0d1e-11ea-999c-eb3daca276d7_29C72F), a serverless layer called for running stateless and computational-intensive functions.\n\nThe subsystems all communicate with each other asynchronously via Timestone, a high-scale, low-latency priority queuing system. Each subsystem addresses a different concern of a service and can be deployed independently through a purpose-built managed Continuous Delivery process. This separation of concerns makes it easier to write, test, and operate Cosmos services.\n\n_Separation of Platform and Application_\n\n## A Cosmos service request\n\n_Trace graph of a Cosmos service request_\n\nThe picture above is a screenshot from Nirvana, our observability portal. It shows a typical service request in Cosmos (a video encoder service in this case):\n\n1.  There is one API call to encode, which includes the video source and a recipe\n2.  The video is split into 31 chunks, and the 31 encoding functions run in parallel\n3.  The assemble function is invoked once\n4.  The index function is invoked once\n5.  The workflow is complete after 8 minutes\n\n## Layering of services\n\nCosmos supports decomposition and layering of services. The resulting modular architecture allows teams to concentrate on their area of specialty and control their APIs and release cycles.\n\nFor example, the video service mentioned above is just one of many used to create streams that can be played on devices. These services, which also include inspection, audio, text, and packaging, are orchestrated using higher-level services. The largest and most complex of these is Tapas, which is responsible for taking sources from studios and making them playable on the Netflix service. Another high-level service is Sagan, which is used for studio operations like marketing clips or daily production editorial proxies.\n\nLayering of Cosmos services\n\nWhen a new title arrives from a production studio, it triggers a Tapas workflow which orchestrates requests to perform inspections, encode video (multiple resolutions, qualities, and video codecs), encode audio (multiple qualities and codecs), generate subtitles (many languages), and package the resulting outputs (multiple player formats). Thus, a single request to Tapas can result in hundreds of requests to other Cosmos services and thousands of Stratum function invocations.\n\nThe trace below shows an example of how a request at a top level service can trickle down to lower level services, resulting in many different actions. In this case the request took 24 minutes to complete, with hundreds of different actions involving 8 different Cosmos services and 9 different Stratum functions.\n\n_Trace graph of a service request through multiple layers_\n\n## Workflows rule!\n\nOr should we say _workflow rules_? Plato is the glue that ties everything together in Cosmos by providing a framework for service developers to define domain logic and orchestrate stateless functions/services. The Optimus API layer has built-in facilities to invoke workflows and examine their state. The Stratum serverless layer generates strongly-typed RPC clients to make invoking a serverless function easy and intuitive.\n\nPlato is a forward chaining rule engine which lends itself to the asynchronous and compute-intensive nature of our algorithms. Unlike a procedural workflow engine like [Netflix’s Conductor](https://netflixtechblog.com/evolution-of-netflix-conductor-16600be36bca), Plato makes it easy to create workflows that are “always on”. For example, as we develop better encoding algorithms, our rules-based workflows automatically manage updating existing videos without us having to trigger and manage new workflows. In addition, any workflow can call another, which enables the layering of services mentioned above.\n\nPlato is a multi-tenant system (implemented using [Apache Karaf](https://karaf.apache.org/)), which greatly reduces the operational burden of operating a workflow. Users write and test their rules in their own source code repository and then deploy the workflow by uploading the compiled code to the Plato server.\n\nDevelopers specify their workflows in a set of rules written in Emirax, a domain specific language built on Groovy. Each rule has 4 sections:\n\n*   match: Specifies the conditions that must be satisfied for this rule to trigger\n*   action: Specifies the code to be executed when this rule is triggered; this is where you invoke Stratum functions to process the request.\n*   reaction: Specifies the code to be executed when the action code completes successfully\n*   error: Specifies the code to be executed when an error is encountered.\n\nIn each of these sections, you typically first record the change in state of the workflow and then perform steps to move the workflow forward, such as executing a Stratum function or returning the results of the execution (For more details, see [this presentation](https://feathercast.apache.org/2019/09/13/serverless-multi-tenant-rule-engine-service-powered-by-apache-karaf-dmitry-vasilyev-saeid-mirzaei-george-ye/)).\n\n## Latency-sensitive applications\n\nCosmos services like Sagan are latency sensitive because they are user-facing. For example, an artist who is working on a social media post doesn’t want to wait a long time when clipping a video from the latest season of [Money Heist](https://www.netflix.com/title/80192098). For Stratum, latency is a function of the _time to perform the work_ plus the _time to get computing resources_. When work is very bursty (which is often the case), the “_time to get resources_” component becomes the significant factor. For illustration, let’s say that one of the things you normally buy when you go shopping is toilet paper. Normally there is no problem putting it in your cart and getting through the checkout line, and the whole process takes you 30 minutes.\n\nResource scarcity\n\nThen one day a bad virus thing happens and _everyone_ decides they need more toilet paper at the same time. Your _toilet paper latency_ now goes from 30 minutes to two weeks because the overall demand exceeds the available capacity. Cosmos applications (and Stratum functions in particular) have this same problem in the face of bursty and unpredictable demand. Stratum manages _function execution latency_ in a few ways:\n\n1.  **Resource pools.** End-users can reserve Stratum computing resources for their own business use case, and resource pools are hierarchical to allow groups of users to share resources.\n2.  **Warm capacity**. End-users can request compute resources (e.g. containers) in advance of demand to reduce startup latencies in Stratum.\n3.  **Micro-batches**. Stratum also uses micro-batches, which is a trick found in platforms like Apache Spark to reduce startup latency. The idea is to spread the startup cost across many function invocations. If you invoke your function 10,000 times, it may run one time each on 10,000 containers or it may run 10 times each on 1000 containers.\n4.  **Priority.** When balancing cost with the desire for low latency, Cosmos services usually land somewhere in the middle: enough resources to handle typical bursts but not enough to handle the largest bursts with the lowest latency. By prioritizing work, applications can still ensure that the most important work is processed with low latency even when resources are scarce. Cosmos service owners can allow end-users to set priority, or set it themselves in the API layer or in the workflow.\n\n## Throughput-sensitive applications\n\nServices like Tapas are throughput-sensitive because they consume large amounts of computing resources (e.g millions of CPU-hours per day) and are more concerned with the completion of tasks over a period of hours or days rather than the time to complete an individual task. In other words, the service level objectives (SLO) are measured in _tasks per day_ and _cost per task_ rather than _tasks per second_.\n\nFor throughput-sensitive workloads, the most important SLOs are those provided by the Stratum serverless layer. Stratum, which is built on top of the [Titus container platform](https://netflixtechblog.com/titus-the-netflix-container-management-platform-is-now-open-source-f868c9fb5436), allows throughput sensitive workloads to use “opportunistic” compute resources through flexible resource scheduling. For example, the cost of a serverless function invocation might be lower if it is willing to wait up to an hour to execute.\n\n## The strangler fig\n\nWe knew that moving a legacy system as large and complicated as Reloaded was going to be a big leap over a dangerous chasm littered with the shards of failed re-engineering projects, but there was no question that we had to jump. To reduce risk, we adopted the [strangler fig pattern](https://martinfowler.com/bliki/StranglerFigApplication.html) which lets the new system grow around the old one and eventually replace it completely.\n\n## Still learning\n\nWe started building Cosmos in 2018 and have been operating in production since early 2019. Today there are about 40 cosmos services and we expect more growth to come. We are still in mid-journey but we can share a few highlights of what we have learned so far:\n\n## The [Netflix culture](https://jobs.netflix.com/culture) played a key role\n\nThe Netflix engineering culture famously relies on personal judgement rather than top-down control. Software developers have both freedom and responsibility to take risks and make decisions. None of us have the title of Software Architect; all of us play that role. In this context, Cosmos emerged in fits and starts from disparate attempts at local optimization. Optimus, Plato and Stratum were conceived independently and eventually coalesced into the vision of a single platform. The application developers on the team kept everyone focused on user-friendly APIs and developer productivity. It took a strong partnership between infrastructure and media algorithm developers to turn the vision into reality. We couldn’t have done that in a top-down engineering environment.\n\n## Microservice + Workflow + Serverless\n\nWe have found that the programming model of “_microservices that trigger workflows that orchestrate serverless functions_” to be a powerful paradigm. It works well for most of our use cases but some applications are simple enough that the added complexity is not worth the benefits.\n\n## A platform mindset\n\nMoving from a large distributed application to a “platform plus applications” was a major paradigm shift. Everyone had to change their mindset. Application developers had to give up a certain amount of flexibility in exchange for consistency, reliability, etc. Platform developers had to develop more empathy and prioritize customer service, user productivity, and service levels. There were moments where application developers felt the platform team was not focused appropriately on their needs, and other times when platform teams felt overtaxed by user demands. We got through these tough spots by being open and honest with each other. For example after a recent retrospective, we strengthened our development tracks for crosscutting system qualities such as developer experience, reliability, observability and security.\n\n## Platform wins\n\nWe started Cosmos with the goal of enabling developers to work better and faster, spending more time on their business problem and less time dealing with infrastructure. At times the goal has seemed elusive, but we are beginning to see the gains we had hoped for. Some of the system qualities that developers like best in Cosmos are managed delivery, modularity, and observability, and developer support. We are working to make these qualities even better while also working on weaker areas like local development, resilience and testability.\n\n## Future plans\n\n2021 will be a big year for Cosmos as we move the majority of work from Reloaded into Cosmos, with more developers and much higher load. We plan to evolve the programming model to accommodate new use cases. Our goals are to make Cosmos easier to use, more resilient, faster and more efficient. Stay tuned to learn more details of how Cosmos works and how we use it."
    },
    {
      "url": "https://netflixtechblog.com/?source=post_page-----9340b879176a--------------------------------",
      "crawl": {
        "loadedUrl": "https://medium.com/m/global-identity-2?redirectUrl=https%3A%2F%2Fnetflixtechblog.com%2F%3Fsource%3Dpost_page-----9340b879176a--------------------------------",
        "loadedTime": "2023-12-06T00:05:31.375Z",
        "referrerUrl": "https://netflixtechblog.com/netflix-original-research-mit-code-2023-9340b879176a?source=collection_home---4------0-----------------------",
        "depth": 2,
        "httpStatusCode": 403
      },
      "metadata": {
        "canonicalUrl": "https://medium.com/m/global-identity-2?redirectUrl=https%3A%2F%2Fnetflixtechblog.com%2F%3Fsource%3Dpost_page-----9340b879176a--------------------------------",
        "title": "Just a moment...",
        "description": null,
        "author": null,
        "keywords": null,
        "languageCode": "en-US"
      },
      "screenshotUrl": null,
      "text": "medium.com\nmedium.com needs to review the security of your connection before proceeding.\n<b>Your browser is out of date!</b><br/>Update your browser to view this website correctly. <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://developers.cloudflare.com/fundamentals/get-started/concepts/cloudflare-challenges/#browser-support\">More Information.</a>",
      "markdown": "## ![Icon for medium.com](https://netflixtechblog.com/favicon.ico)medium.com\n\nmedium.com needs to review the security of your connection before proceeding.\n\n<b>Your browser is out of date!</b><br/>Update your browser to view this website correctly. <a target=\"\\_blank\" rel=\"noopener noreferrer\" href=\"https://developers.cloudflare.com/fundamentals/get-started/concepts/cloudflare-challenges/#browser-support\">More Information.</a>"
    },
    {
      "url": "https://netflixtechblog.com/orchestrating-data-ml-workflows-at-scale-with-netflix-maestro-aaa2b41b800c",
      "crawl": {
        "loadedUrl": "https://netflixtechblog.com/orchestrating-data-ml-workflows-at-scale-with-netflix-maestro-aaa2b41b800c?gi=f109f93d2158",
        "loadedTime": "2023-12-06T00:06:48.546Z",
        "referrerUrl": "https://netflixtechblog.com/incremental-processing-using-netflix-maestro-and-apache-iceberg-b8ba072ddeeb?source=collection_home---4------1-----------------------",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://netflixtechblog.com/orchestrating-data-ml-workflows-at-scale-with-netflix-maestro-aaa2b41b800c",
        "title": "Orchestrating Data/ML Workflows at Scale With Netflix Maestro | by Netflix Technology Blog | Netflix TechBlog",
        "description": "At Netflix, Data and Machine Learning (ML) pipelines are widely used and have become central for the business, representing diverse use cases that go beyond recommendations, predictions and data…",
        "author": "Netflix Technology Blog",
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Orchestrating Data/ML Workflows at Scale With Netflix Maestro\nby Jun He, Akash Dwivedi, Natallia Dzenisenka, Snehal Chennuru, Praneeth Yenugutala, Pawan Dixit\nAt Netflix, Data and Machine Learning (ML) pipelines are widely used and have become central for the business, representing diverse use cases that go beyond recommendations, predictions and data transformations. A large number of batch workflows run daily to serve various business needs. These include ETL pipelines, ML model training workflows, batch jobs, etc. As Big data and ML became more prevalent and impactful, the scalability, reliability, and usability of the orchestrating ecosystem have increasingly become more important for our data scientists and the company.\nIn this blog post, we introduce and share learnings on Maestro, a workflow orchestrator that can schedule and manage workflows at a massive scale.\nMotivation\nScalability and usability are essential to enable large-scale workflows and support a wide range of use cases. Our existing orchestrator (Meson) has worked well for several years. It schedules around 70 thousands of workflows and half a million jobs per day. Due to its popularity, the number of workflows managed by the system has grown exponentially. We started seeing signs of scale issues, like:\nSlowness during peak traffic moments like 12 AM UTC, leading to increased operational burden. The scheduler on-call has to closely monitor the system during non-business hours.\nMeson was based on a single leader architecture with high availability. As the usage increased, we had to vertically scale the system to keep up and were approaching AWS instance type limits.\nWith the high growth of workflows in the past few years — increasing at > 100% a year, the need for a scalable data workflow orchestrator has become paramount for Netflix’s business needs. After perusing the current landscape of workflow orchestrators, we decided to develop a next generation system that can scale horizontally to spread the jobs across the cluster consisting of 100’s of nodes. It addresses the key challenges we face with Meson and achieves operational excellence.\nChallenges in Workflow Orchestration\nScalability\nThe orchestrator has to schedule hundreds of thousands of workflows, millions of jobs every day and operate with a strict SLO of less than 1 minute of scheduler introduced delay even when there are spikes in the traffic. At Netflix, the peak traffic load can be a few orders of magnitude higher than the average load. For example, a lot of our workflows are run around midnight UTC. Hence, the system has to withstand bursts in traffic while still maintaining the SLO requirements. Additionally, we would like to have a single scheduler cluster to manage most of user workflows for operational and usability reasons.\nAnother dimension of scalability to consider is the size of the workflow. In the data domain, it is common to have a super large number of jobs within a single workflow. For example, a workflow to backfill hourly data for the past five years can lead to 43800 jobs (24 * 365 * 5), each of which processes data for an hour. Similarly, ML model training workflows usually consist of tens of thousands (or even millions) of training jobs within a single workflow. Those large-scale workflows might create hotspots and overwhelm the orchestrator and downstream systems. Therefore, the orchestrator has to manage a workflow consisting of hundreds of thousands of jobs in a performant way, which is also quite challenging.\nUsability\nNetflix is a data-driven company, where key decisions are driven by data insights, from the pixel color used on the landing page to the renewal of a TV-series. Data scientists, engineers, non-engineers, and even content producers all run their data pipelines to get the necessary insights. Given the diverse backgrounds, usability is a cornerstone of a successful orchestrator at Netflix.\nWe would like our users to focus on their business logic and let the orchestrator solve cross-cutting concerns like scheduling, processing, error handling, security etc. It needs to provide different grains of abstractions for solving similar problems, high-level to cater to non-engineers and low-level for engineers to solve their specific problems. It should also provide all the knobs for configuring their workflows to suit their needs. In addition, it is critical for the system to be debuggable and surface all the errors for users to troubleshoot, as they improve the UX and reduce the operational burden.\nProviding abstractions for the users is also needed to save valuable time on creating workflows and jobs. We want users to rely on shared templates and reuse their workflow definitions across their team, saving time and effort on creating the same functionality. Using job templates across the company also helps with upgrades and fixes: when the change is made in a template it’s automatically updated for all workflows that use it.\nHowever, usability is challenging as it is often opinionated. Different users have different preferences and might ask for different features. Sometimes, the users might ask for the opposite features or ask for some niche cases, which might not necessarily be useful for a broader audience.\nIntroducing Maestro\nMaestro is the next generation Data Workflow Orchestration platform to meet the current and future needs of Netflix. It is a general-purpose workflow orchestrator that provides a fully managed workflow-as-a-service (WAAS) to the data platform at Netflix. It serves thousands of users, including data scientists, data engineers, machine learning engineers, software engineers, content producers, and business analysts, for various use cases.\nMaestro is highly scalable and extensible to support existing and new use cases and offers enhanced usability to end users. Figure 1 shows the high-level architecture.\nFigure 1. Maestro high level architecture\nIn Maestro, a workflow is a DAG (Directed acyclic graph) of individual units of job definition called Steps. Steps can have dependencies, triggers, workflow parameters, metadata, step parameters, configurations, and branches (conditional or unconditional). In this blog, we use step and job interchangeably. A workflow instance is an execution of a workflow, similarly, an execution of a step is called a step instance. Instance data include the evaluated parameters and other information collected at runtime to provide different kinds of execution insights. The system consists of 3 main micro services which we will expand upon in the following sections.\nMaestro ensures the business logic is run in isolation. Maestro launches a unit of work (a.k.a. Steps) in a container and ensures the container is launched with the users/applications identity. Launching with identity ensures the work is launched on-behalf-of the user/application, the identity is later used by the downstream systems to validate if an operation is allowed or not, for an example user/application identity is checked by the data warehouse to validate if a table read/write is allowed or not.\nWorkflow Engine\nWorkflow engine is the core component, which manages workflow definitions, the lifecycle of workflow instances, and step instances. It provides rich features to support:\nAny valid DAG patterns\nPopular data flow constructs like sub workflow, foreach, conditional branching etc.\nMultiple failure modes to handle step failures with different error retry policies\nFlexible concurrency control to throttle the number of executions at workflow/step level\nStep templates for common job patterns like running a Spark query or moving data to Google sheets\nSupport parameter code injection using customized expression language\nWorkflow definition and ownership management.\nTimeline including all state changes and related debug info.\nWe use Netflix open source project Conductor as a library to manage the workflow state machine in Maestro. It ensures to enqueue and dequeue each step defined in a workflow with at least once guarantee.\nTime-Based Scheduling Service\nTime-based scheduling service starts new workflow instances at the scheduled time specified in workflow definitions. Users can define the schedule using cron expression or using periodic schedule templates like hourly, weekly etc;. This service is lightweight and provides an at-least-once scheduling guarantee. Maestro engine service will deduplicate the triggering requests to achieve an exact-once guarantee when scheduling workflows.\nTime-based triggering is popular due to its simplicity and ease of management. But sometimes, it is not efficient. For example, the daily workflow should process the data when the data partition is ready, not always at midnight. Therefore, on top of manual and time-based triggering, we also provide event-driven triggering.\nSignal Service\nMaestro supports event-driven triggering over signals, which are pieces of messages carrying information such as parameter values. Signal triggering is efficient and accurate because we don’t waste resources checking if the workflow is ready to run, instead we only execute the workflow when a condition is met.\nSignals are used in two ways:\nA trigger to start new workflow instances\nA gating function to conditionally start a step (e.g., data partition readiness)\nSignal service goals are to\nCollect and index signals\nRegister and handle workflow trigger subscriptions\nRegister and handle the step gating functions\nCaptures the lineage of workflows triggers and steps unblocked by a signal\nFigure 2. Signal service high level architecture\nThe maestro signal service consumes all the signals from different sources, e.g. all the warehouse table updates, S3 events, a workflow releasing a signal, and then generates the corresponding triggers by correlating a signal with its subscribed workflows. In addition to the transformation between external signals and workflow triggers, this service is also responsible for step dependencies by looking up the received signals in the history. Like the scheduling service, the signal service together with Maestro engine achieves exactly-once triggering guarantees.\nSignal service also provides the signal lineage, which is useful in many cases. For example, a table updated by a workflow could lead to a chain of downstream workflow executions. Most of the time the workflows are owned by different teams, the signal lineage helps the upstream and downstream workflow owners to see who depends on whom.\nOrchestration at Scale\nAll services in the Maestro system are stateless and can be horizontally scaled out. All the requests are processed via distributed queues for message passing. By having a shared nothing architecture, Maestro can horizontally scale to manage the states of millions of workflow and step instances at the same time.\nCockroachDB is used for persisting workflow definitions and instance state. We chose CockroachDB as it is an open-source distributed SQL database that provides strong consistency guarantees that can be scaled horizontally without much operational overhead.\nIt is hard to support super large workflows in general. For example, a workflow definition can explicitly define a DAG consisting of millions of nodes. With that number of nodes in a DAG, UI cannot render it well. We have to enforce some constraints and support valid use cases consisting of hundreds of thousands (or even millions) of step instances in a workflow instance.\nBased on our findings and user feedback, we found that in practice\nUsers don’t want to manually write the definitions for thousands of steps in a single workflow definition, which is hard to manage and navigate over UI. When such a use case exists, it is always feasible to decompose the workflow into smaller sub workflows.\nUsers expect to repeatedly run a certain part of DAG hundreds of thousands (or even millions) times with different parameter settings in a given workflow instance. So at runtime, a workflow instance might include millions of step instances.\nTherefore, we enforce a workflow DAG size limit (e.g. 1K) and we provide a foreach pattern that allows users to define a sub DAG within a foreach block and iterate the sub DAG with a larger limit (e.g. 100K). Note that foreach can be nested by another foreach. So users can run millions or billions of steps in a single workflow instance.\nIn Maestro, foreach itself is a step in the original workflow definition. Foreach is internally treated as another workflow which scales similarly as any other Maestro workflow based on the number of step executions in the foreach loop. The execution of sub DAG within foreach will be delegated to a separate workflow instance. Foreach step will then monitor and collect status of those foreach workflow instances, each of which manages the execution of one iteration.\nFigure 3. Maestro’s scalable foreach design to support super large iterations\nWith this design, foreach pattern supports sequential loop and nested loop with high scalability. It is easy to manage and troubleshoot as users can see the overall loop status at the foreach step or view each iteration separately.\nWorkflow Platform for Everyone\nWe aim to make Maestro user friendly and easy to learn for users with different backgrounds. We made some assumptions about user proficiency in programming languages and they can bring their business logic in multiple ways, including but not limited to, a bash script, a Jupyter notebook, a Java jar, a docker image, a SQL statement, or a few clicks in the UI using parameterized workflow templates.\nUser Interfaces\nMaestro provides multiple domain specific languages (DSLs) including YAML, Python, and Java, for end users to define their workflows, which are decoupled from their business logic. Users can also directly talk to Maestro API to create workflows using the JSON data model. We found that human readable DSL is popular and plays an important role to support different use cases. YAML DSL is the most popular one due to its simplicity and readability.\nHere is an example workflow defined by different DSLs.\nFigure 4. An example workflow defined by YAML, Python, and Java DSLs\nAdditionally, users can also generate certain types of workflows on UI or use other libraries, e.g.\nIn Notebook UI, users can directly schedule to run the chosen notebook periodically.\nIn Maestro UI, users can directly schedule to move data from one source (e.g. a data table or a spreadsheet) to another periodically.\nUsers can use Metaflow library to create workflows in Maestro to execute DAGs consisting of arbitrary Python code.\nParameterized Workflows\nLots of times, users want to define a dynamic workflow to adapt to different scenarios. Based on our experiences, a completely dynamic workflow is less favorable and hard to maintain and troubleshooting. Instead, Maestro provides three features to assist users to define a parameterized workflow\nConditional branching\nSub-workflow\nOutput parameters\nInstead of dynamically changing the workflow DAG at runtime, users can define those changes as sub workflows and then invoke the appropriate sub workflow at runtime because the sub workflow id is a parameter, which is evaluated at runtime. Additionally, using the output parameter, users can produce different results from the upstream job step and then iterate through those within the foreach, pass it to the sub workflow, or use it in the downstream steps.\nHere is an example (using YAML DSL) of backfill workflow with 2 steps. In step1, the step computes the backfill ranges and returns the dates (from 20210101 to 20220101) back. Next, foreach step uses the dates from step1 to create foreach iterations. Finally, each of the backfill jobs gets the date from the foreach and backfills the data based on the date.\nWorkflow:\nid: demo.pipeline\nFROM_DATE: 20210101 #inclusive\nTO_DATE: 20220101 #exclusive\njobs:\n- job:\nid: step1\ntype: NoOp\n'!dates': dateIntsBetween(FROM_DATE, TO_DATE, 1); #SEL expr\n- foreach:\nid: step2\nparams:\ndate: ${dates@step1} #reference upstream step parameter\njobs:\n- job: \nid: backfill\ntype: Notebook\nnotebook:\ninput_path: s3://path/to/notebook.ipynb\narg1: $date #pass the foreach parameter into notebook\nFigure 5. An example of using parameterized workflow to backfill data\nThe parameter system in Maestro is completely dynamic with code injection support. Users can write the code in Java syntax as the parameter definition. We developed our own secured expression language (SEL) to ensure security. It only exposes limited functionality and includes additional checks (e.g. the number of iteration in the loop statement, etc.) in the language parser.\nExecution Abstractions\nMaestro provides multiple levels of execution abstractions. Users can choose to use the provided step type and set its parameters. This helps to encapsulate the business logic of commonly used operations, making it very easy for users to create jobs. For example, for spark step type, all users have to do is just specify needed parameters like spark sql query, memory requirements, etc, and Maestro will do all behind-the-scenes to create the step. If we have to make a change in the business logic of a certain step, we can do so seamlessly for users of that step type.\nIf provided step types are not enough, users can also develop their own business logic in a Jupyter notebook and then pass it to Maestro. Advanced users can develop their own well-tuned docker image and let Maestro handle the scheduling and execution.\nAdditionally, we abstract the common functions or reusable patterns from various use cases and add them to the Maestro in a loosely coupled way by introducing job templates, which are parameterized notebooks. This is different from step types, as templates provide a combination of various steps. Advanced users also leverage this feature to ship common patterns for their own teams. While creating a new template, users can define the list of required/optional parameters with the types and register the template with Maestro. Maestro validates the parameters and types at the push and run time. In the future, we plan to extend this functionality to make it very easy for users to define templates for their teams and for all employees. In some cases, sub-workflows are also used to define common sub DAGs to achieve multi-step functions.\nMoving Forward\nWe are taking Big Data Orchestration to the next level and constantly solving new problems and challenges, please stay tuned. If you are motivated to solve large scale orchestration problems, please join us as we are hiring.\nAcknowledgements\nThanks Andrew Seier, Alexandre Bertails, Jess Hester, Xiao Chen, Liang Tian, Romain Cledat, Yun Li, Olek Gorajek, Anoop Panicker, Aravindan Ramkumar, Andrew Leung, and other stunning colleagues at Netflix for their contributions to the Maestro integration works while developing Maestro. We also thank Eva Tse, Charles Smith, Charles Zhao, and other leaders of Netflix engineering organizations for their constructive feedback and suggestions on the Maestro architecture and design.",
      "markdown": "## Orchestrating Data/ML Workflows at Scale With Netflix Maestro\n\n[\n\n![Netflix Technology Blog](https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg)\n\n\n\n](https://netflixtechblog.medium.com/?source=post_page-----aaa2b41b800c--------------------------------)[\n\n![Netflix TechBlog](https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png)\n\n\n\n](https://netflixtechblog.com/?source=post_page-----aaa2b41b800c--------------------------------)\n\nby [Jun He](https://www.linkedin.com/in/jheua/), [Akash Dwivedi](https://www.linkedin.com/in/akash-dwivedi-b9779317), [Natallia Dzenisenka](https://www.linkedin.com/in/natalliadzenisenka/), [Snehal Chennuru](https://www.linkedin.com/in/snehalchennuru), [Praneeth Yenugutala](https://www.linkedin.com/in/praneethy91), [Pawan Dixit](https://www.linkedin.com/in/pawan-dixit-b4307b2/)\n\nAt Netflix, Data and Machine Learning (ML) pipelines are widely used and have become central for the business, representing diverse use cases that go beyond recommendations, predictions and data transformations. A large number of batch workflows run daily to serve various business needs. These include ETL pipelines, ML model training workflows, batch jobs, etc. As Big data and ML became more prevalent and impactful, the scalability, reliability, and usability of the orchestrating ecosystem have increasingly become more important for our data scientists and the company.\n\nIn this blog post, we introduce and share learnings on Maestro, a workflow orchestrator that can schedule and manage workflows at a massive scale.\n\n## Motivation\n\nScalability and usability are essential to enable large-scale workflows and support a wide range of use cases. Our existing orchestrator (Meson) has worked well for several years. It schedules around 70 thousands of workflows and half a million jobs per day. Due to its popularity, the number of workflows managed by the system has grown exponentially. We started seeing signs of scale issues, like:\n\n*   Slowness during peak traffic moments like 12 AM UTC, leading to increased operational burden. The scheduler on-call has to closely monitor the system during non-business hours.\n*   Meson was based on a single leader architecture with high availability. As the usage increased, we had to vertically scale the system to keep up and were approaching AWS instance type limits.\n\nWith the high growth of workflows in the past few years — increasing at > 100% a year, the need for a scalable data workflow orchestrator has become paramount for Netflix’s business needs. After perusing the current landscape of workflow orchestrators, we decided to develop a next generation system that can scale horizontally to spread the jobs across the cluster consisting of 100’s of nodes. It addresses the key challenges we face with Meson and achieves operational excellence.\n\n## Challenges in Workflow Orchestration\n\n## Scalability\n\nThe orchestrator has to schedule hundreds of thousands of workflows, millions of jobs every day and operate with a strict SLO of less than 1 minute of scheduler introduced delay even when there are spikes in the traffic. At Netflix, the peak traffic load can be a few orders of magnitude higher than the average load. For example, a lot of our workflows are run around midnight UTC. Hence, the system has to withstand bursts in traffic while still maintaining the SLO requirements. Additionally, we would like to have a single scheduler cluster to manage most of user workflows for operational and usability reasons.\n\nAnother dimension of scalability to consider is the size of the workflow. In the data domain, it is common to have a super large number of jobs within a single workflow. For example, a workflow to backfill hourly data for the past five years can lead to 43800 jobs (24 \\* 365 \\* 5), each of which processes data for an hour. Similarly, ML model training workflows usually consist of tens of thousands (or even millions) of training jobs within a single workflow. Those large-scale workflows might create hotspots and overwhelm the orchestrator and downstream systems. Therefore, the orchestrator has to manage a workflow consisting of hundreds of thousands of jobs in a performant way, which is also quite challenging.\n\n## Usability\n\nNetflix is a data-driven company, where key decisions are driven by data insights, from the pixel color used on the landing page to the renewal of a TV-series. Data scientists, engineers, non-engineers, and even content producers all run their data pipelines to get the necessary insights. Given the diverse backgrounds, usability is a cornerstone of a successful orchestrator at Netflix.\n\nWe would like our users to focus on their business logic and let the orchestrator solve cross-cutting concerns like scheduling, processing, error handling, security etc. It needs to provide different grains of abstractions for solving similar problems, high-level to cater to non-engineers and low-level for engineers to solve their specific problems. It should also provide all the knobs for configuring their workflows to suit their needs. In addition, it is critical for the system to be debuggable and surface all the errors for users to troubleshoot, as they improve the UX and reduce the operational burden.\n\nProviding abstractions for the users is also needed to save valuable time on creating workflows and jobs. We want users to rely on shared templates and reuse their workflow definitions across their team, saving time and effort on creating the same functionality. Using job templates across the company also helps with upgrades and fixes: when the change is made in a template it’s automatically updated for all workflows that use it.\n\nHowever, usability is challenging as it is often opinionated. Different users have different preferences and might ask for different features. Sometimes, the users might ask for the opposite features or ask for some niche cases, which might not necessarily be useful for a broader audience.\n\n## Introducing Maestro\n\nMaestro is the next generation Data Workflow Orchestration platform to meet the current and future needs of Netflix. It is a general-purpose workflow orchestrator that provides a fully managed workflow-as-a-service (WAAS) to the data platform at Netflix. It serves thousands of users, including data scientists, data engineers, machine learning engineers, software engineers, content producers, and business analysts, for various use cases.\n\nMaestro is highly scalable and extensible to support existing and new use cases and offers enhanced usability to end users. Figure 1 shows the high-level architecture.\n\nFigure 1. Maestro high level architecture\n\nIn Maestro, a workflow is a [DAG (Directed acyclic graph)](https://en.wikipedia.org/wiki/Directed_acyclic_graph) of individual units of job definition called Steps. Steps can have dependencies, triggers, workflow parameters, metadata, step parameters, configurations, and branches (conditional or unconditional). In this blog, we use step and job interchangeably. A workflow instance is an execution of a workflow, similarly, an execution of a step is called a step instance. Instance data include the evaluated parameters and other information collected at runtime to provide different kinds of execution insights. The system consists of 3 main micro services which we will expand upon in the following sections.\n\nMaestro ensures the business logic is run in isolation. Maestro launches a unit of work (a.k.a. Steps) in a container and ensures the container is launched with the users/applications identity. Launching with identity ensures the work is launched on-behalf-of the user/application, the identity is later used by the downstream systems to validate if an operation is allowed or not, for an example user/application identity is checked by the data warehouse to validate if a table read/write is allowed or not.\n\n## Workflow Engine\n\nWorkflow engine is the core component, which manages workflow definitions, the lifecycle of workflow instances, and step instances. It provides rich features to support:\n\n*   Any valid DAG patterns\n*   Popular data flow constructs like sub workflow, [foreach](#7d0f), conditional branching etc.\n*   Multiple failure modes to handle step failures with different error retry policies\n*   Flexible concurrency control to throttle the number of executions at workflow/step level\n*   Step templates for common job patterns like running a Spark query or moving data to Google sheets\n*   Support parameter code injection using customized expression language\n*   Workflow definition and ownership management.  \n    Timeline including all state changes and related debug info.\n\nWe use [Netflix open source project Conductor](https://conductor.netflix.com/) as a library to manage the workflow state machine in Maestro. It ensures to enqueue and dequeue each step defined in a workflow with at least once guarantee.\n\n## Time-Based Scheduling Service\n\nTime-based scheduling service starts new workflow instances at the scheduled time specified in workflow definitions. Users can define the schedule using cron expression or using periodic schedule templates like hourly, weekly etc;. This service is lightweight and provides an at-least-once scheduling guarantee. Maestro engine service will deduplicate the triggering requests to achieve an exact-once guarantee when scheduling workflows.\n\nTime-based triggering is popular due to its simplicity and ease of management. But sometimes, it is not efficient. For example, the daily workflow should process the data when the data partition is ready, not always at midnight. Therefore, on top of manual and time-based triggering, we also provide event-driven triggering.\n\n## Signal Service\n\nMaestro supports event-driven triggering over signals, which are pieces of messages carrying information such as parameter values. Signal triggering is efficient and accurate because we don’t waste resources checking if the workflow is ready to run, instead we only execute the workflow when a condition is met.\n\nSignals are used in two ways:\n\n*   A trigger to start new workflow instances\n*   A gating function to conditionally start a step (e.g., data partition readiness)\n\nSignal service goals are to\n\n*   Collect and index signals\n*   Register and handle workflow trigger subscriptions\n*   Register and handle the step gating functions\n*   Captures the lineage of workflows triggers and steps unblocked by a signal\n\nFigure 2. Signal service high level architecture\n\nThe maestro signal service consumes all the signals from different sources, e.g. all the warehouse table updates, S3 events, a workflow releasing a signal, and then generates the corresponding triggers by correlating a signal with its subscribed workflows. In addition to the transformation between external signals and workflow triggers, this service is also responsible for step dependencies by looking up the received signals in the history. Like the scheduling service, the signal service together with Maestro engine achieves exactly-once triggering guarantees.\n\nSignal service also provides the signal lineage, which is useful in many cases. For example, a table updated by a workflow could lead to a chain of downstream workflow executions. Most of the time the workflows are owned by different teams, the signal lineage helps the upstream and downstream workflow owners to see who depends on whom.\n\n## Orchestration at Scale\n\nAll services in the Maestro system are stateless and can be horizontally scaled out. All the requests are processed via distributed queues for message passing. By having a shared nothing architecture, Maestro can horizontally scale to manage the states of millions of workflow and step instances at the same time.\n\n[CockroachDB](https://github.com/cockroachdb/cockroach) is used for persisting workflow definitions and instance state. We chose CockroachDB as it is an open-source distributed SQL database that provides strong consistency guarantees that can be scaled horizontally without much operational overhead.\n\nIt is hard to support super large workflows in general. For example, a workflow definition can explicitly define a DAG consisting of millions of nodes. With that number of nodes in a DAG, UI cannot render it well. We have to enforce some constraints and support valid use cases consisting of hundreds of thousands (or even millions) of step instances in a workflow instance.\n\nBased on our findings and user feedback, we found that in practice\n\n*   Users don’t want to manually write the definitions for thousands of steps in a single workflow definition, which is hard to manage and navigate over UI. When such a use case exists, it is always feasible to decompose the workflow into smaller sub workflows.\n*   Users expect to repeatedly run a certain part of DAG hundreds of thousands (or even millions) times with different parameter settings in a given workflow instance. So at runtime, a workflow instance might include millions of step instances.\n\nTherefore, we enforce a workflow DAG size limit (e.g. 1K) and we provide a foreach pattern that allows users to define a sub DAG within a foreach block and iterate the sub DAG with a larger limit (e.g. 100K). Note that foreach can be nested by another foreach. So users can run millions or billions of steps in a single workflow instance.\n\nIn Maestro, foreach itself is a step in the original workflow definition. Foreach is internally treated as another workflow which scales similarly as any other Maestro workflow based on the number of step executions in the foreach loop. The execution of sub DAG within foreach will be delegated to a separate workflow instance. Foreach step will then monitor and collect status of those foreach workflow instances, each of which manages the execution of one iteration.\n\nFigure 3. Maestro’s scalable foreach design to support super large iterations\n\nWith this design, foreach pattern supports sequential loop and nested loop with high scalability. It is easy to manage and troubleshoot as users can see the overall loop status at the foreach step or view each iteration separately.\n\n## Workflow Platform for Everyone\n\nWe aim to make Maestro user friendly and easy to learn for users with different backgrounds. We made some assumptions about user proficiency in programming languages and they can bring their business logic in multiple ways, including but not limited to, a bash script, a [Jupyter notebook](https://jupyter.org/), a Java jar, a docker image, a SQL statement, or a few clicks in the UI using [parameterized workflow templates](#360e).\n\n## User Interfaces\n\nMaestro provides multiple domain specific languages (DSLs) including YAML, Python, and Java, for end users to define their workflows, which are decoupled from their business logic. Users can also directly talk to Maestro API to create workflows using the JSON data model. We found that human readable DSL is popular and plays an important role to support different use cases. YAML DSL is the most popular one due to its simplicity and readability.\n\nHere is an example workflow defined by different DSLs.\n\nFigure 4. An example workflow defined by YAML, Python, and Java DSLs\n\nAdditionally, users can also generate certain types of workflows on UI or use other libraries, e.g.\n\n*   In Notebook UI, users can directly schedule to run the chosen notebook periodically.\n*   In Maestro UI, users can directly schedule to move data from one source (e.g. a data table or a spreadsheet) to another periodically.\n*   Users can use [Metaflow](https://github.com/Netflix/metaflow) library to create workflows in Maestro to execute DAGs consisting of arbitrary Python code.\n\n## Parameterized Workflows\n\nLots of times, users want to define a dynamic workflow to adapt to different scenarios. Based on our experiences, a completely dynamic workflow is less favorable and hard to maintain and troubleshooting. Instead, Maestro provides three features to assist users to define a parameterized workflow\n\n*   Conditional branching\n*   Sub-workflow\n*   Output parameters\n\nInstead of dynamically changing the workflow DAG at runtime, users can define those changes as sub workflows and then invoke the appropriate sub workflow at runtime because the sub workflow id is a parameter, which is evaluated at runtime. Additionally, using the output parameter, users can produce different results from the upstream job step and then iterate through those within the foreach, pass it to the sub workflow, or use it in the downstream steps.\n\nHere is an example (using YAML DSL) of backfill workflow with 2 steps. In step1, the step computes the backfill ranges and returns the dates (from 20210101 to 20220101) back. Next, foreach step uses the dates from step1 to create foreach iterations. Finally, each of the backfill jobs gets the date from the foreach and backfills the data based on the date.\n\nWorkflow:  \n  id: demo.pipeline  \n  FROM\\_DATE: 20210101 #inclusive  \n  TO\\_DATE: 20220101   #exclusive  \n  jobs:  \n    - job:  \n        id: step1  \n        type: NoOp  \n        '!dates': dateIntsBetween(FROM\\_DATE, TO\\_DATE, 1); #[SEL](#0518) expr  \n    - foreach:  \n        id: step2  \n        params:  \n          date: ${dates@step1}  #reference upstream step parameter  \n        jobs:  \n          - job:   \n              id: backfill  \n              type: Notebook  \n              notebook:  \n                input\\_path: s3://path/to/notebook.ipynb  \n              arg1: $date  #pass the foreach parameter into notebook\n\nFigure 5. An example of using parameterized workflow to backfill data\n\nThe parameter system in Maestro is completely dynamic with code injection support. Users can write the code in Java syntax as the parameter definition. We developed our own secured expression language (SEL) to ensure security. It only exposes limited functionality and includes additional checks (e.g. the number of iteration in the loop statement, etc.) in the language parser.\n\n## Execution Abstractions\n\nMaestro provides multiple levels of execution abstractions. Users can choose to use the provided step type and set its parameters. This helps to encapsulate the business logic of commonly used operations, making it very easy for users to create jobs. For example, for spark step type, all users have to do is just specify needed parameters like spark sql query, memory requirements, etc, and Maestro will do all behind-the-scenes to create the step. If we have to make a change in the business logic of a certain step, we can do so seamlessly for users of that step type.\n\nIf provided step types are not enough, users can also develop their own business logic in a Jupyter notebook and then pass it to Maestro. Advanced users can develop their own well-tuned docker image and let Maestro handle the scheduling and execution.\n\nAdditionally, we abstract the common functions or reusable patterns from various use cases and add them to the Maestro in a loosely coupled way by introducing job templates, which are parameterized notebooks. This is different from step types, as templates provide a combination of various steps. Advanced users also leverage this feature to ship common patterns for their own teams. While creating a new template, users can define the list of required/optional parameters with the types and register the template with Maestro. Maestro validates the parameters and types at the push and run time. In the future, we plan to extend this functionality to make it very easy for users to define templates for their teams and for all employees. In some cases, sub-workflows are also used to define common sub DAGs to achieve multi-step functions.\n\n## Moving Forward\n\nWe are taking Big Data Orchestration to the next level and constantly solving new problems and challenges, please stay tuned. If you are motivated to solve large scale orchestration problems, please [join us](https://jobs.netflix.com/search?team=Data+Platform) as we are hiring.\n\n## Acknowledgements\n\nThanks [Andrew Seier](https://www.linkedin.com/in/andrew-seier/), [Alexandre Bertails](https://www.linkedin.com/in/bertails/), [Jess Hester](https://www.linkedin.com/in/hesterjessica), [Xiao Chen](https://www.linkedin.com/in/chenxiao000), [Liang Tian](https://www.linkedin.com/in/liangtian/), [Romain Cledat](https://www.linkedin.com/in/romain-cledat-4a211a5), [Yun Li](https://www.linkedin.com/in/yun-li-66a50719/), [Olek Gorajek](https://www.linkedin.com/in/agorajek/), [Anoop Panicker](https://www.linkedin.com/in/anoop-panicker/), [Aravindan Ramkumar](https://www.linkedin.com/in/aravindanr/), [Andrew Leung](https://www.linkedin.com/in/anwleung), and other stunning colleagues at Netflix for their contributions to the Maestro integration works while developing Maestro. We also thank Eva Tse, Charles Smith, Charles Zhao, and other leaders of Netflix engineering organizations for their constructive feedback and suggestions on the Maestro architecture and design."
    },
    {
      "url": "https://netflixtechblog.com/?source=author_recirc-----b8ba072ddeeb----0---------------------f64bca0b_9af7_47d6_9276_039c5aab85fd-------",
      "crawl": {
        "loadedUrl": "https://medium.com/m/global-identity-2?redirectUrl=https%3A%2F%2Fnetflixtechblog.com%2F%3Fsource%3Dauthor_recirc-----b8ba072ddeeb----0---------------------f64bca0b_9af7_47d6_9276_039c5aab85fd-------",
        "loadedTime": "2023-12-06T00:06:48.783Z",
        "referrerUrl": "https://netflixtechblog.com/incremental-processing-using-netflix-maestro-and-apache-iceberg-b8ba072ddeeb?source=collection_home---4------1-----------------------",
        "depth": 2,
        "httpStatusCode": 403
      },
      "metadata": {
        "canonicalUrl": "https://medium.com/m/global-identity-2?redirectUrl=https%3A%2F%2Fnetflixtechblog.com%2F%3Fsource%3Dauthor_recirc-----b8ba072ddeeb----0---------------------f64bca0b_9af7_47d6_9276_039c5aab85fd-------",
        "title": "Just a moment...",
        "description": null,
        "author": null,
        "keywords": null,
        "languageCode": "en-US"
      },
      "screenshotUrl": null,
      "text": "medium.com\nmedium.com needs to review the security of your connection before proceeding.\n<b>Your browser is out of date!</b><br/>Update your browser to view this website correctly. <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://developers.cloudflare.com/fundamentals/get-started/concepts/cloudflare-challenges/#browser-support\">More Information.</a>",
      "markdown": "## ![Icon for medium.com](https://netflixtechblog.com/favicon.ico)medium.com\n\nmedium.com needs to review the security of your connection before proceeding.\n\n<b>Your browser is out of date!</b><br/>Update your browser to view this website correctly. <a target=\"\\_blank\" rel=\"noopener noreferrer\" href=\"https://developers.cloudflare.com/fundamentals/get-started/concepts/cloudflare-challenges/#browser-support\">More Information.</a>"
    },
    {
      "url": "https://netflixtechblog.com/1-streamlining-membership-data-engineering-at-netflix-with-psyberg-f68830617dd1?source=author_recirc-----260fbe366fe2----3---------------------4c97ffb2_d222_4f6c_bfbe_55fb230cc1f7-------",
      "crawl": {
        "loadedUrl": "https://medium.com/m/global-identity-2?redirectUrl=https%3A%2F%2Fnetflixtechblog.com%2F1-streamlining-membership-data-engineering-at-netflix-with-psyberg-f68830617dd1%3Fsource%3Dauthor_recirc-----260fbe366fe2----3---------------------4c97ffb2_d222_4f6c_bfbe_55fb230cc1f7-------",
        "loadedTime": "2023-12-06T00:08:42.846Z",
        "referrerUrl": "https://netflixtechblog.com/3-psyberg-automated-end-to-end-catch-up-260fbe366fe2?source=collection_home---4------0-----------------------",
        "depth": 2,
        "httpStatusCode": 403
      },
      "metadata": {
        "canonicalUrl": "https://medium.com/m/global-identity-2?redirectUrl=https%3A%2F%2Fnetflixtechblog.com%2F1-streamlining-membership-data-engineering-at-netflix-with-psyberg-f68830617dd1%3Fsource%3Dauthor_recirc-----260fbe366fe2----3---------------------4c97ffb2_d222_4f6c_bfbe_55fb230cc1f7-------",
        "title": "Just a moment...",
        "description": null,
        "author": null,
        "keywords": null,
        "languageCode": "en-US"
      },
      "screenshotUrl": null,
      "text": "medium.com\nmedium.com needs to review the security of your connection before proceeding.\n<b>Your browser is out of date!</b><br/>Update your browser to view this website correctly. <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://developers.cloudflare.com/fundamentals/get-started/concepts/cloudflare-challenges/#browser-support\">More Information.</a>",
      "markdown": "## ![Icon for medium.com](https://netflixtechblog.com/favicon.ico)medium.com\n\nmedium.com needs to review the security of your connection before proceeding.\n\n<b>Your browser is out of date!</b><br/>Update your browser to view this website correctly. <a target=\"\\_blank\" rel=\"noopener noreferrer\" href=\"https://developers.cloudflare.com/fundamentals/get-started/concepts/cloudflare-challenges/#browser-support\">More Information.</a>"
    }
  ],
  "Cloudflare": [
    {
      "url": "https://blog.cloudflare.com/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/",
        "loadedTime": "2023-12-05T02:26:24.897Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 0,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/",
        "title": "The Cloudflare Blog",
        "description": "Get the latest news on how products at Cloudflare are built, technologies used, and join the teams helping to build a better Internet.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Cloudflare Gen 12 Server: Bigger, Better, Cooler in a 2U1N form factor\n12/01/2023\nCloudflare Gen 12 Compute servers are moving to 2U1N form factor to optimize the thermal design to accommodate both high-power CPUs (>350W) and GPUs effectively while maintaining performance and reliability...\nContinue reading » \nCyber Week: Analyzing Internet traffic and e-commerce trends\n11/28/2023\nCloudflare Radar Trends DDoS eCommerce \nHow significant are Cyber Week days on the Internet? Is it a global phenomenon? Does e-commerce interest peak on Black Friday or Cyber Monday, and are attacks increasing during this time? These questions are important to retailers and stakeholders around the world....\nBetter debugging for Cloudflare Workers, now with breakpoints\n11/28/2023\nCloudflare Workers Developers Debugging \nWe provide many tools to help you debug Cloudflare Workers; from your local environment all the way into production. In this post, we highlight some of the tools we currently offer, and do a deep dive into one specific area - breakpoint debugging - a tool we recently added into our workerd runtime...\nSteve Bray: Why I joined Cloudflare\n11/27/2023\nLife @ Cloudflare Careers APJC \nWe're excited to introduce Steve Bray as Cloudflare's new Head of Australia and New Zealand, as we continue to build and grow our customers, partners, and team in the region...\nCloudflare named a leader in Forrester Edge Development Platforms Wave, Q4 2023\n11/27/2023\nForrester \nForrester has recognized Cloudflare as a leader in The Forrester Wave™: Edge Development Platforms, Q4 2023 with the top score in the current offering category...\nDo hackers eat turkey? And other Thanksgiving Internet trends\n11/24/2023\nThanksgiving Cloudflare Radar Internet Traffic Trends DDoS \nOffline for turkey time: Which US states logged off on Thanksgiving Day? Is there a difference between coastal and central states? Do hackers take a Thanksgiving break? Are food delivery services gaining or losing traffic? We answer those questions and more...\nWorkers AI Update: Stable Diffusion, Code Llama + Workers AI in 100 cities\n11/23/2023\nWorkers AI Cloudflare Workers \nWe're thrilled to announce that Stable Diffusion and Code Llama are now available as part of Workers AI, running in over 100 cities across Cloudflare’s global network....\nWorkers AI Update: Hello Mistral 7B\n11/21/2023\nWorkers AI Mistral Cloudflare Workers \nToday we’re excited to announce that we’ve added the Mistral-7B-v0.1-instruct to Workers AI...\n2024, the year of elections\n11/20/2023\nElection Security USA Athenian Project \nWe want to ensure that all groups working to promote democracy around the world have the tools they need to stay secure online...\nHow to execute an object file: Part 4, AArch64 edition\n11/17/2023\nLinux Programming Deep Dive \nThe initial posts are dedicated to the x86 architecture. Since then, the fleet of our working machines has expanded to include a large and growing number of ARM CPUs. This time we’ll repeat this exercise for the aarch64 architecture....\nIntroducing advanced session audit capabilities in Cloudflare One\n11/16/2023\nSASE Cloudflare Zero Trust Product News Cloudflare One Cloudflare Workers KV \nAdministrators can now easily audit all active user sessions and associated data used by their Cloudflare One policies. This enables the best of both worlds: extremely granular controls, while maintaining an improved ability to troubleshoot and diagnose...\nIntroducing hostname and ASN lists to simplify WAF rules creation\n11/15/2023\nWAF Product News \nToday we are expanding Custom Lists by enabling you to create lists of hostnames and ASNs...\nStreaming and longer context lengths for LLMs on Workers AI\n11/14/2023\nWorkers AI Cloudflare Workers Developer Platform JavaScript Serverless \nWorkers AI now supports streaming text responses for the LLM models in our catalog, including Llama-2, using server-sent events...\nPost Mortem on Cloudflare Control Plane and Analytics Outage\n11/04/2023\nOutage Post Mortem \nBeginning on Thursday, November 2, 2023, at 11:43 UTC Cloudflare's control plane and analytics services experienced an outage. Here are the details...\nCloudflare incident on October 30, 2023\n11/01/2023\nPost Mortem \nMultiple Cloudflare services were unavailable for 37 minutes on October 30, 2023, due to the misconfiguration of a deployment tool used by Workers KV....\nIntroducing notifications for HTTP Traffic Anomalies\n10/31/2023\nProduct News Notifications Network Services \nToday we're excited to announce Traffic Anomalies notifications, which proactively alert you when your Internet property is seeing an unexpected change in traffic patterns...\nIntroducing HAR Sanitizer: secure HAR sharing\n10/26/2023\nTools Open Source \nAs a follow-up to the most recent Okta breach, we are making a HAR file sanitizer available to everyone, not just Cloudflare customers, at no cost....\nEmail Routing subdomain support, new APIs and security protocols\n10/26/2023\nEmail Routing Subdomains Email Workers Cloudflare Workers Developer Platform \nIt's been two years since we announced Email Routing, our solution to create custom email addresses for your domains and route incoming emails to your preferred mailbox. Since then, the team has worked hard to evolve the product and add more powerful features to meet our users' expectations....\nDDoS threat report for 2023 Q3\n10/26/2023\nDDoS Attacks Cloudflare Radar DDoS Reports Insights \nIn the past quarter, DDoS attacks surged by 65%. Gaming and Gambling companies were the most attacked and Cloudflare mitigated thousands of hyper-volumetric DDoS attacks. The largest attacks we saw peaked at 201 million rps and 2.6 Tbps....\nQ3 2023 Internet disruption summary\n10/25/2023\nCloudflare Radar Internet Traffic Outage Internet Shutdown Internet Quality \nIn this post, we review selected Internet disruptions observed by Cloudflare during the third quarter of 2023, supported by traffic graphs from Cloudflare Radar and other internal Cloudflare tools, and grouped by associated cause or common geography...\nCache Reserve goes GA: enhanced control to minimize egress costs\n10/25/2023\nCache Reserve General Availability Application Services Performance \nWe're excited to announce the graduation of Cache Reserve from beta to GA, accompanied by the introduction of several exciting new features. These new features include adding Cache Reserve into the analytics shown on the Cache overview section of the Cloudflare dashboard...",
      "markdown": "[\n\n## Cloudflare Gen 12 Server: Bigger, Better, Cooler in a 2U1N form factor\n\n](https://blog.cloudflare.com/cloudflare-gen-12-server-bigger-better-cooler-in-a-2u1n-form-factor/)\n\n12/01/2023\n\nCloudflare Gen 12 Compute servers are moving to 2U1N form factor to optimize the thermal design to accommodate both high-power CPUs (>350W) and GPUs effectively while maintaining performance and reliability...\n\n[Continue reading »](https://blog.cloudflare.com/cloudflare-gen-12-server-bigger-better-cooler-in-a-2u1n-form-factor/)\n\n*   [![JQ Lau](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/_tmp_mini_magick20221017-42-17izwqk.jpg)](https://blog.cloudflare.com/author/jq/)\n*   [![Syona Sarma](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/12/1516264359757.jpg)](https://blog.cloudflare.com/author/syona/)\n\n![](https://blog.cloudflare.com/content/images/2023/12/image5-1.png)\n\n[\n\n## Cyber Week: Analyzing Internet traffic and e-commerce trends\n\n](https://blog.cloudflare.com/cyber-week-analyzing-internet-traffic-and-e-commerce-trends/)\n\n11/28/2023\n\n[Cloudflare Radar](https://blog.cloudflare.com/tag/cloudflare-radar/) [Trends](https://blog.cloudflare.com/tag/trends/) [DDoS](https://blog.cloudflare.com/tag/ddos/) [eCommerce](https://blog.cloudflare.com/tag/ecommerce/)\n\nHow significant are Cyber Week days on the Internet? Is it a global phenomenon? Does e-commerce interest peak on Black Friday or Cyber Monday, and are attacks increasing during this time? These questions are important to retailers and stakeholders around the world....\n\n*   [![João Tomé](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/V0x3WKfJ_400x400-1.jpeg)](https://blog.cloudflare.com/author/joao-tome/)\n\n[\n\n## Better debugging for Cloudflare Workers, now with breakpoints\n\n](https://blog.cloudflare.com/debugging-cloudflare-workers/)\n\n11/28/2023\n\n[Cloudflare Workers](https://blog.cloudflare.com/tag/workers/) [Developers](https://blog.cloudflare.com/tag/developers/) [Debugging](https://blog.cloudflare.com/tag/debugging/)\n\nWe provide many tools to help you debug Cloudflare Workers; from your local environment all the way into production. In this post, we highlight some of the tools we currently offer, and do a deep dive into one specific area - breakpoint debugging - a tool we recently added into our workerd runtime...\n\n*   [![Adam Murray](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/04/adam_headshot-1.jpg)](https://blog.cloudflare.com/author/adam-murray/)\n*   [![Brendan Coll](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/11/brendan-coll.png)](https://blog.cloudflare.com/author/brendan-coll/)\n\n[\n\n## Steve Bray: Why I joined Cloudflare\n\n](https://blog.cloudflare.com/steve-bray-why-i-joined-cloudflare/)\n\n11/27/2023\n\n[Life @ Cloudflare](https://blog.cloudflare.com/tag/life-at-cloudflare/) [Careers](https://blog.cloudflare.com/tag/careers/) [APJC](https://blog.cloudflare.com/tag/apjc/)\n\nWe're excited to introduce Steve Bray as Cloudflare's new Head of Australia and New Zealand, as we continue to build and grow our customers, partners, and team in the region...\n\n*   [![Steve Bray](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/11/-Steve-Bray.jpeg)](https://blog.cloudflare.com/author/steve-bray/)\n\n[\n\n## Cloudflare named a leader in Forrester Edge Development Platforms Wave, Q4 2023\n\n](https://blog.cloudflare.com/forrester-wave-edge-development-2023/)\n\n11/27/2023\n\n[Forrester](https://blog.cloudflare.com/tag/forrester/)\n\nForrester has recognized Cloudflare as a leader in The Forrester Wave™: Edge Development Platforms, Q4 2023 with the top score in the current offering category...\n\n*   [![Brendan Irvine-Broque](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/09/IMG_2312.JPG)](https://blog.cloudflare.com/author/brendan-irvine-broque/)\n*   [![Dawn Parzych](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/04/Untitled---1-of-1.jpeg)](https://blog.cloudflare.com/author/dawn/)\n\n[\n\n## Do hackers eat turkey? And other Thanksgiving Internet trends\n\n](https://blog.cloudflare.com/do-hackers-eat-turkey-and-other-thanksgiving-internet-trends/)\n\n11/24/2023\n\n[Thanksgiving](https://blog.cloudflare.com/tag/thanksgiving/) [Cloudflare Radar](https://blog.cloudflare.com/tag/cloudflare-radar/) [Internet Traffic](https://blog.cloudflare.com/tag/internet-traffic/) [Trends](https://blog.cloudflare.com/tag/trends/) [DDoS](https://blog.cloudflare.com/tag/ddos/)\n\nOffline for turkey time: Which US states logged off on Thanksgiving Day? Is there a difference between coastal and central states? Do hackers take a Thanksgiving break? Are food delivery services gaining or losing traffic? We answer those questions and more...\n\n*   [![João Tomé](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/V0x3WKfJ_400x400-1.jpeg)](https://blog.cloudflare.com/author/joao-tome/)\n\n[\n\n## Workers AI Update: Stable Diffusion, Code Llama + Workers AI in 100 cities\n\n](https://blog.cloudflare.com/workers-ai-update-stable-diffusion-code-llama-workers-ai-in-100-cities/)\n\n11/23/2023\n\n[Workers AI](https://blog.cloudflare.com/tag/workers-ai/) [Cloudflare Workers](https://blog.cloudflare.com/tag/workers/)\n\nWe're thrilled to announce that Stable Diffusion and Code Llama are now available as part of Workers AI, running in over 100 cities across Cloudflare’s global network....\n\n*   [![Phil Wittig](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/09/phil.jpeg)](https://blog.cloudflare.com/author/phil/)\n\n[\n\n## Workers AI Update: Hello Mistral 7B\n\n](https://blog.cloudflare.com/workers-ai-update-hello-mistral-7b/)\n\n11/21/2023\n\n[Workers AI](https://blog.cloudflare.com/tag/workers-ai/) [Mistral](https://blog.cloudflare.com/tag/mistral/) [Cloudflare Workers](https://blog.cloudflare.com/tag/workers/)\n\nToday we’re excited to announce that we’ve added the Mistral-7B-v0.1-instruct to Workers AI...\n\n*   [![Jesse Kipp](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2020/12/jesse-headshot-square.jpg)](https://blog.cloudflare.com/author/jesse/)\n*   [![Isaac Rehg](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/11/Isaac-Rehg.jpeg)](https://blog.cloudflare.com/author/isaac-rehg/)\n\n[\n\n## 2024, the year of elections\n\n](https://blog.cloudflare.com/2024-the-year-of-elections/)\n\n11/20/2023\n\n[Election Security](https://blog.cloudflare.com/tag/election-security/) [USA](https://blog.cloudflare.com/tag/usa/) [Athenian Project](https://blog.cloudflare.com/tag/athenian-project/)\n\nWe want to ensure that all groups working to promote democracy around the world have the tools they need to stay secure online...\n\n*   [![Jocelyn Woolbright](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2019/12/0-1.jpg)](https://blog.cloudflare.com/author/jocelyn/)\n\n[\n\n## How to execute an object file: Part 4, AArch64 edition\n\n](https://blog.cloudflare.com/how-to-execute-an-object-file-part-4/)\n\n11/17/2023\n\n[Linux](https://blog.cloudflare.com/tag/linux/) [Programming](https://blog.cloudflare.com/tag/programming/) [Deep Dive](https://blog.cloudflare.com/tag/deep-dive/)\n\nThe initial posts are dedicated to the x86 architecture. Since then, the fleet of our working machines has expanded to include a large and growing number of ARM CPUs. This time we’ll repeat this exercise for the aarch64 architecture....\n\n*   [![Oxana Kharitonova](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/05/Oxana-Kharitonova.png)](https://blog.cloudflare.com/author/oxana/)\n\n[\n\n## Introducing advanced session audit capabilities in Cloudflare One\n\n](https://blog.cloudflare.com/introducing-advanced-session-audit-capabilities-in-cloudflare-one/)\n\n11/16/2023\n\n[SASE](https://blog.cloudflare.com/tag/sase/) [Cloudflare Zero Trust](https://blog.cloudflare.com/tag/cloudflare-zero-trust/) [Product News](https://blog.cloudflare.com/tag/product-news/) [Cloudflare One](https://blog.cloudflare.com/tag/cloudflare-one/) [Cloudflare Workers KV](https://blog.cloudflare.com/tag/cloudflare-workers-kv/)\n\nAdministrators can now easily audit all active user sessions and associated data used by their Cloudflare One policies. This enables the best of both worlds: extremely granular controls, while maintaining an improved ability to troubleshoot and diagnose...\n\n*   [![Kenny Johnson](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2020/12/0DA903DE-E315-43D5-B0DE-39870448303D.jpeg)](https://blog.cloudflare.com/author/kenny/)\n\n[\n\n## Introducing hostname and ASN lists to simplify WAF rules creation\n\n](https://blog.cloudflare.com/hostname-asn-lists/)\n\n11/15/2023\n\n[WAF](https://blog.cloudflare.com/tag/waf/) [Product News](https://blog.cloudflare.com/tag/product-news/)\n\nToday we are expanding Custom Lists by enabling you to create lists of hostnames and ASNs...\n\n*   [![Daniele Molteni](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2020/10/meProfessional-1-flipped.jpg)](https://blog.cloudflare.com/author/daniele/)\n\n[\n\n## Streaming and longer context lengths for LLMs on Workers AI\n\n](https://blog.cloudflare.com/workers-ai-streaming/)\n\n11/14/2023\n\n[Workers AI](https://blog.cloudflare.com/tag/workers-ai/) [Cloudflare Workers](https://blog.cloudflare.com/tag/workers/) [Developer Platform](https://blog.cloudflare.com/tag/developer-platform/) [JavaScript](https://blog.cloudflare.com/tag/javascript/) [Serverless](https://blog.cloudflare.com/tag/serverless/)\n\nWorkers AI now supports streaming text responses for the LLM models in our catalog, including Llama-2, using server-sent events...\n\n*   [![Jesse Kipp](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2020/12/jesse-headshot-square.jpg)](https://blog.cloudflare.com/author/jesse/)\n*   [![Celso Martinho](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/Celso-Martinho.png)](https://blog.cloudflare.com/author/celso/)\n\n[\n\n## Post Mortem on Cloudflare Control Plane and Analytics Outage\n\n](https://blog.cloudflare.com/post-mortem-on-cloudflare-control-plane-and-analytics-outage/)\n\n11/04/2023\n\n[Outage](https://blog.cloudflare.com/tag/outage/) [Post Mortem](https://blog.cloudflare.com/tag/post-mortem/)\n\nBeginning on Thursday, November 2, 2023, at 11:43 UTC Cloudflare's control plane and analytics services experienced an outage. Here are the details...\n\n*   [![Matthew Prince](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/06/Matthew-Prince-3.jpeg)](https://blog.cloudflare.com/author/matthew-prince/)\n\n[\n\n## Cloudflare incident on October 30, 2023\n\n](https://blog.cloudflare.com/cloudflare-incident-on-october-30-2023/)\n\n11/01/2023\n\n[Post Mortem](https://blog.cloudflare.com/tag/post-mortem/)\n\nMultiple Cloudflare services were unavailable for 37 minutes on October 30, 2023, due to the misconfiguration of a deployment tool used by Workers KV....\n\n*   [![Matt Silverlock](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/08/profile-1500px-square.jpeg)](https://blog.cloudflare.com/author/silverlock/)\n*   [![Kris Evans](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/08/1687236408752.jpeg)](https://blog.cloudflare.com/author/kris-evans/)\n\n[\n\n## Introducing notifications for HTTP Traffic Anomalies\n\n](https://blog.cloudflare.com/introducing-http-traffic-anomalies-notifications/)\n\n10/31/2023\n\n[Product News](https://blog.cloudflare.com/tag/product-news/) [Notifications](https://blog.cloudflare.com/tag/notifications/) [Network Services](https://blog.cloudflare.com/tag/network-services/)\n\nToday we're excited to announce Traffic Anomalies notifications, which proactively alert you when your Internet property is seeing an unexpected change in traffic patterns...\n\n*   [![Cathy Chi](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/1610954482110.jpg)](https://blog.cloudflare.com/author/cathy-chi/)\n*   [![Natasha Wissmann](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/05/IMG-1568.jpg)](https://blog.cloudflare.com/author/natasha/)\n\n[\n\n## Introducing HAR Sanitizer: secure HAR sharing\n\n](https://blog.cloudflare.com/introducing-har-sanitizer-secure-har-sharing/)\n\n10/26/2023\n\n[Tools](https://blog.cloudflare.com/tag/tools/) [Open Source](https://blog.cloudflare.com/tag/open-source/)\n\nAs a follow-up to the most recent Okta breach, we are making a HAR file sanitizer available to everyone, not just Cloudflare customers, at no cost....\n\n*   [![Kenny Johnson](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2020/12/0DA903DE-E315-43D5-B0DE-39870448303D.jpeg)](https://blog.cloudflare.com/author/kenny/)\n\n[\n\n## Email Routing subdomain support, new APIs and security protocols\n\n](https://blog.cloudflare.com/email-routing-subdomains/)\n\n10/26/2023\n\n[Email Routing](https://blog.cloudflare.com/tag/email-routing/) [Subdomains](https://blog.cloudflare.com/tag/subdomains/) [Email Workers](https://blog.cloudflare.com/tag/email-workers/) [Cloudflare Workers](https://blog.cloudflare.com/tag/workers/) [Developer Platform](https://blog.cloudflare.com/tag/developer-platform/)\n\nIt's been two years since we announced Email Routing, our solution to create custom email addresses for your domains and route incoming emails to your preferred mailbox. Since then, the team has worked hard to evolve the product and add more powerful features to meet our users' expectations....\n\n*   [![Celso Martinho](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/Celso-Martinho.png)](https://blog.cloudflare.com/author/celso/)\n*   [![André Cruz](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/02/andre2.jpg)](https://blog.cloudflare.com/author/andre-cruz/)\n*   [![Nelson Duarte](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/10/me_1x1.jpg)](https://blog.cloudflare.com/author/nelson-duarte/)\n\n[\n\n## DDoS threat report for 2023 Q3\n\n](https://blog.cloudflare.com/ddos-threat-report-2023-q3/)\n\n10/26/2023\n\n[DDoS](https://blog.cloudflare.com/tag/ddos/) [Attacks](https://blog.cloudflare.com/tag/attacks/) [Cloudflare Radar](https://blog.cloudflare.com/tag/cloudflare-radar/) [DDoS Reports](https://blog.cloudflare.com/tag/ddos-reports/) [Insights](https://blog.cloudflare.com/tag/insights/)\n\nIn the past quarter, DDoS attacks surged by 65%. Gaming and Gambling companies were the most attacked and Cloudflare mitigated thousands of hyper-volumetric DDoS attacks. The largest attacks we saw peaked at 201 million rps and 2.6 Tbps....\n\n*   [![Omer Yoachimik](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/06/Screenshot-2023-06-02-at-9.38.13-AM.png)](https://blog.cloudflare.com/author/omer/)\n*   [![Jorge Pacheco](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/04/CV_Profile.jpeg)](https://blog.cloudflare.com/author/jorge/)\n\n[\n\n## Q3 2023 Internet disruption summary\n\n](https://blog.cloudflare.com/q3-2023-internet-disruption-summary/)\n\n10/25/2023\n\n[Cloudflare Radar](https://blog.cloudflare.com/tag/cloudflare-radar/) [Internet Traffic](https://blog.cloudflare.com/tag/internet-traffic/) [Outage](https://blog.cloudflare.com/tag/outage/) [Internet Shutdown](https://blog.cloudflare.com/tag/internet-shutdown/) [Internet Quality](https://blog.cloudflare.com/tag/internet-quality/)\n\nIn this post, we review selected Internet disruptions observed by Cloudflare during the third quarter of 2023, supported by traffic graphs from Cloudflare Radar and other internal Cloudflare tools, and grouped by associated cause or common geography...\n\n*   [![David Belson](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/11/David-Belson.jpeg)](https://blog.cloudflare.com/author/david-belson/)\n\n[\n\n## Cache Reserve goes GA: enhanced control to minimize egress costs\n\n](https://blog.cloudflare.com/cache-reserve-goes-ga/)\n\n10/25/2023\n\n[Cache Reserve](https://blog.cloudflare.com/tag/cache-reserve/) [General Availability](https://blog.cloudflare.com/tag/general-availability/) [Application Services](https://blog.cloudflare.com/tag/application-services/) [Performance](https://blog.cloudflare.com/tag/performance/)\n\nWe're excited to announce the graduation of Cache Reserve from beta to GA, accompanied by the introduction of several exciting new features. These new features include adding Cache Reserve into the analytics shown on the Cache overview section of the Cloudflare dashboard...\n\n*   [![Alex Krivit](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2018/08/RE0BQ6EF_400x400.jpg)](https://blog.cloudflare.com/author/alex/)"
    },
    {
      "url": "https://blog.cloudflare.com/cloudflare-gen-12-server-bigger-better-cooler-in-a-2u1n-form-factor/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/cloudflare-gen-12-server-bigger-better-cooler-in-a-2u1n-form-factor/",
        "loadedTime": "2023-12-05T02:26:42.139Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/cloudflare-gen-12-server-bigger-better-cooler-in-a-2u1n-form-factor/",
        "title": "Cloudflare Gen 12 Server: Bigger, Better, Cooler in a 2U1N form factor",
        "description": "Cloudflare Gen 12 Compute servers are moving to 2U1N form factor to optimize the thermal design to accommodate both high-power CPUs (>350W) and GPUs effectively while maintaining performance and reliability",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "12/01/2023\n6 min read\nTwo years ago, Cloudflare undertook a significant upgrade to our compute server hardware as we deployed our cutting-edge 11th Generation server fleet, based on AMD EPYC Milan x86 processors. It's nearly time for another refresh to our x86 infrastructure, with deployment planned for 2024. This involves upgrading not only the processor itself, but many of the server's components. It must be able to accommodate the GPUs that drive inference on Workers AI, and leverage the latest advances in memory, storage, and security. Every aspect of the server is rigorously evaluated — including the server form factor itself.\nOne crucial variable always in consideration is temperature. The latest generations of x86 processors have yielded significant leaps forward in performance, with the tradeoff of higher power draw and heat output. In this post we will explore this trend, and how it informed our decision to adopt a new physical footprint for our next-generation fleet of servers. \nIn preparation for the upcoming refresh, we conducted an extensive survey of the x86 CPU landscape. AMD recently introduced its latest offerings: Genoa, Bergamo, and Genoa-X, featuring the power of their innovative Zen 4 architecture. At the same time, Intel unveiled Sapphire Rapids as part of its 4th Generation Intel Xeon Scalable Processor Platform, code-named “Eagle Stream”, showcasing their own advancements. These options offer valuable choices as we consider how to shape the future of Cloudflare's server technology to match the needs of our customers.\nA continuing challenge we face across x86 CPU vendors, including the new Intel and AMD chipsets, is the rapidly increasing CPU Thermal Design Point (TDP) generation over generation. TDP is defined to be the maximum heat dissipated by the CPU under load that a cooling system should be designed to cool; TDP also describes the maximum power consumption of the CPU socket. This plot shows the CPU TDP trend of each hardware server generation since 2014: \nAt Cloudflare, our Gen 9 server was based on Intel Skylake 6162 with a TDP of 150W, our Gen 10 server was based on AMD Rome 7642 at 240W, and our Gen 11 server was based on AMD Milan 7713 at 240W. Today, AMD EPYC 9004 Series SKU Stack default TDP goes up to 360W and is configurable up to 400W. Intel Sapphire Rapid SKU stack default TDP goes up to 350W. This trend will continue, with the next generation of x86 CPU offerings from AMD and Intel specified for up to 500W TDP. \nDesigning multi-generational cooling solutions\nCloudflare Gen 10 servers and Gen 11 servers were designed in a 1U1N form factor, with air cooling to maximize rack density (1U means the server form factor is 1 Rack Unit, which is 1.75” in height or thickness; 1N means there is one server node per chassis). However, to cool more than 350 Watt TDP CPU with air in a 1U1N form factor requires fans to be spinning at 100% duty cycle (running all the time, at max speed). A single fan running at full speed consumes about 40W, and a typical server configuration of 7–8 dual rotor fans per server can hit 280–320 W to power the fans alone. At peak loads, the total system power consumed, including the cooling fans, processor, and other components, can eclipse 750 Watt per server. \nThe 1U form factor can fit a maximum of eight 40mm dual rotor fans, which sets an upper bound on the temperature range it can support. We first take into account ambient room temperature, which we assume to be 40° C (the maximum expected temperature under normal conditions). Under these conditions we determined that air-cooled servers, with all eight fans running at 100% duty cycle, can support CPUs with a maximum TDP of 400W. \nThis poses a challenge, because the next generation of CPUs, while being socket compatible with the current gen, rise up to 500W TDP. In order to future-proof, and re-use as much of Gen 12 design as possible for future generations, we will need a scalable thermal solution. Moreover, many co-location facilities where Cloudflare deploys servers have a rack power limit. With total system power consumption at north of 750 Watt per node, and after accounting for space utilized by networking gear, we would have been underutilizing rack space by as much as 50%.\nWe have a problem!\nWe do have a variety of SKU options available to use on each CPU generation, and if power is the primary constraint, we could choose to limit the TDP and use a lower core count, low-power SKU. To evaluate this, the hardware team ran a synthetic workload benchmark in the lab across several CPU SKUs. We found that Cloudflare services continue to scale with cores effectively up to 128 cores or 256 hardware threads, resulting in significant performance gain, and Total Cost of Ownership (TCO) benefit, at and above 360W TDP.\nHowever, while the performance metric and TCO metric look good on a per-server basis, this is only part of the story: servers go into a server rack when they are deployed, and server racks come with constraints and limitations that have to be taken into design consideration. The two limiting factors are rack power budget and rack height. Taking these two rack-level constraints into account, how does the combined Total Cost of Ownership (TCO) benefit scale with TDP? We ran a performance sweep across the configurable TDP range of the highest core count CPUs and noticed that rack-level TCO benefit stagnates when CPU TDP rises above roughly 340W. \nTCO advantage stagnates because we hit our rack power budget limit — the incremental performance gain per server, coinciding with an incremental increase of CPU TDP above 340W, is negated by the reduction in the number of servers that can be installed in a rack to remain within the rack’s power budget. Even with CPU TDP power capped at 340W, we are still underutilizing the rack, with 30% of the space still available.\nThankfully, there is an alternative to power capping and compromising on possible performance gain, by increasing the chassis height to a 2U form factor (from 1.75” in height to 3.5” in height). The benefits from doing this include: \nLarger fans (up to 80mm) that can move more air\nAllowing for a taller and larger heatsink that can dissipate heat more effectively\nLess air impedance within the chassis since the majority of components are 1U height\nProviding sufficient room to add PCIe attached accelerators / GPUs, including dual-slot form factor options\nDeprecated 1U designNew 2U design\n\t\t\nClick images to enlarge \n2U chassis design is nothing new, and is actually very common in the industry for various reasons, one of which is better airflow to dissipate more heat, but it does come with the tradeoff of taking up more space and limiting the number of servers than can be installed in a rack. Since we are power constrained instead of space constrained, the tradeoff did not negatively impact our design. \nThermal simulations provided by Cloudflare vendors showed that 4x 60mm fans or 4x 80mm fans at less than 40 Watt per fan is sufficient to cool the system. That is a theoretical savings of at least 150 Watt compared to 8x 40mm fans in a 1U design, which would result in significant Operational Expenditure (OPEX) savings and a boost to TCO improvement. Switching to a 2U form factor also gives us the benefit of fully utilizing our rack power budget and our rack space, and provides ample room for the addition of PCIe attached accelerators / GPUs, including dual-slot form factor options.\nConclusion\nIt might seem counter-intuitive, but our observations indicate that growing the server chassis, and utilizing more space per node actually increases rack density and improves overall TCO benefit over previous generation deployments, since it allows for a better thermal design. We are very happy with the result of this technical readiness investigation, and are actively working on validating our Gen 12 Compute servers and launching them into production soon. Stay tuned for more details on our Gen 12 designs. \nIf you are excited about helping build a better Internet, come join us, we are hiring!\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nAMD Hardware Cloudflare Network",
      "markdown": "12/01/2023\n\n*   [![JQ Lau](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/_tmp_mini_magick20221017-42-17izwqk.jpg)](https://blog.cloudflare.com/author/jq/)\n*   [![Syona Sarma](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/12/1516264359757.jpg)](https://blog.cloudflare.com/author/syona/)\n\n6 min read\n\n![](https://blog.cloudflare.com/content/images/2023/12/image5.png)\n\nTwo years ago, Cloudflare undertook a significant upgrade to our compute server hardware as we deployed our cutting-edge [11th Generation server fleet](https://blog.cloudflare.com/the-epyc-journey-continues-to-milan-in-cloudflares-11th-generation-edge-server/), based on AMD EPYC Milan x86 processors. It's nearly time for another refresh to our x86 infrastructure, with deployment planned for 2024. This involves upgrading not only the processor itself, but many of the server's components. It must be able to accommodate the GPUs that drive inference on [Workers AI](https://blog.cloudflare.com/workers-ai/), and leverage the latest advances in memory, storage, and security. Every aspect of the server is rigorously evaluated — including the server form factor itself.\n\nOne crucial variable always in consideration is temperature. The latest generations of x86 processors have yielded significant leaps forward in performance, with the tradeoff of higher power draw and heat output. In this post we will explore this trend, and how it informed our decision to adopt a new physical footprint for our next-generation fleet of servers.\n\nIn preparation for the upcoming refresh, we conducted an extensive survey of the x86 CPU landscape. AMD recently introduced its latest offerings: Genoa, Bergamo, and Genoa-X, featuring the power of their innovative Zen 4 architecture. At the same time, Intel unveiled Sapphire Rapids as part of its 4th Generation Intel Xeon Scalable Processor Platform, code-named “Eagle Stream”, showcasing their own advancements. These options offer valuable choices as we consider how to shape the future of Cloudflare's server technology to match the needs of our customers.\n\nA continuing challenge we face across x86 CPU vendors, including the new Intel and AMD chipsets, is the rapidly increasing CPU Thermal Design Point (TDP) generation over generation. TDP is defined to be the maximum heat dissipated by the CPU under load that a cooling system should be designed to cool; TDP also describes the maximum power consumption of the CPU socket. This plot shows the CPU TDP trend of each hardware server generation since 2014:\n\n![](https://blog.cloudflare.com/content/images/2023/12/image4.png \"Chart\")\n\nAt Cloudflare, our [Gen 9 server](https://blog.cloudflare.com/a-tour-inside-cloudflares-g9-servers/) was based on Intel Skylake 6162 with a TDP of 150W, our [Gen 10 server](https://blog.cloudflare.com/cloudflares-gen-x-servers-for-an-accelerated-future/) was based on AMD Rome 7642 at 240W, and our [Gen 11 server](https://blog.cloudflare.com/the-epyc-journey-continues-to-milan-in-cloudflares-11th-generation-edge-server/) was based on AMD Milan 7713 at 240W. Today, [AMD EPYC 9004 Series SKU Stack](https://www.amd.com/system/files/documents/epyc-9004-series-processors-data-sheet.pdf) default TDP goes up to 360W and is configurable up to 400W. [Intel Sapphire Rapid SKU stack](https://ark.intel.com/content/www/us/en/ark/products/codename/126212/products-formerly-sapphire-rapids.html#@Server) default TDP goes up to 350W. This trend will continue, with the next generation of x86 CPU offerings from AMD and Intel specified for up to 500W TDP.\n\n## Designing multi-generational cooling solutions\n\nCloudflare Gen 10 servers and Gen 11 servers were designed in a 1U1N form factor, with air cooling to maximize rack density (1U means the server form factor is 1 Rack Unit, which is 1.75” in height or thickness; 1N means there is one server node per chassis). However, to cool more than 350 Watt TDP CPU with air in a 1U1N form factor requires fans to be spinning at 100% duty cycle (running all the time, at max speed). A single fan running at full speed consumes about 40W, and a typical server configuration of 7–8 dual rotor fans per server can hit 280–320 W to power the fans alone. At peak loads, the total system power consumed, including the cooling fans, processor, and other components, can eclipse 750 Watt per server.\n\nThe 1U form factor can fit a maximum of eight 40mm dual rotor fans, which sets an upper bound on the temperature range it can support. We first take into account ambient room temperature, which we assume to be 40° C (the maximum expected temperature under normal conditions). Under these conditions we determined that air-cooled servers, with all eight fans running at 100% duty cycle, can support CPUs with a maximum TDP of 400W.\n\nThis poses a challenge, because the next generation of CPUs, while being socket compatible with the current gen, rise up to 500W TDP. In order to future-proof, and re-use as much of Gen 12 design as possible for future generations, we will need a scalable thermal solution. Moreover, many co-location facilities where Cloudflare deploys servers have a rack power limit. With total system power consumption at north of 750 Watt per node, and after accounting for space utilized by networking gear, we would have been underutilizing rack space by as much as 50%.\n\n### We have a problem!\n\nWe do have a variety of SKU options available to use on each CPU generation, and if power is the primary constraint, we could choose to limit the TDP and use a lower core count, low-power SKU. To evaluate this, the hardware team ran a synthetic workload benchmark in the lab across several CPU SKUs. We found that Cloudflare services continue to scale with cores effectively up to 128 cores or 256 hardware threads, resulting in significant performance gain, and Total Cost of Ownership (TCO) benefit, at and above 360W TDP.\n\nHowever, while the performance metric and TCO metric look good on a per-server basis, this is only part of the story: servers go into a server rack when they are deployed, and server racks come with constraints and limitations that have to be taken into design consideration. The two limiting factors are rack power budget and rack height. Taking these two rack-level constraints into account, how does the combined Total Cost of Ownership (TCO) benefit scale with TDP? We ran a performance sweep across the configurable TDP range of the highest core count CPUs and noticed that rack-level TCO benefit stagnates when CPU TDP rises above roughly 340W.\n\nTCO advantage stagnates because we hit our rack power budget limit — the incremental performance gain per server, coinciding with an incremental increase of CPU TDP above 340W, is negated by the reduction in the number of servers that can be installed in a rack to remain within the rack’s power budget. Even with CPU TDP power capped at 340W, we are still underutilizing the rack, with 30% of the space still available.\n\nThankfully, there is an alternative to power capping and compromising on possible performance gain, by increasing the chassis height to a 2U form factor (from 1.75” in height to 3.5” in height). The benefits from doing this include:\n\n*   Larger fans (up to 80mm) that can move more air\n*   Allowing for a taller and larger heatsink that can dissipate heat more effectively\n*   Less air impedance within the chassis since the majority of components are 1U height\n*   Providing sufficient room to add PCIe attached accelerators / GPUs, including dual-slot form factor options\n\n| Deprecated 1U design | New 2U design |\n| --- | --- |\n| [![click to enlarge](https://blog.cloudflare.com/content/images/2023/12/old_server_2.jpg)](https://blog.cloudflare.com/content/images/2023/12/old_server_2.jpg) | [![click to enlarge](https://blog.cloudflare.com/content/images/2023/12/server_large_2.jpg)](https://blog.cloudflare.com/content/images/2023/12/server_large_2.jpg) |\n\n_Click images to enlarge_\n\n2U chassis design is nothing new, and is actually very common in the industry for various reasons, one of which is better airflow to dissipate more heat, but it does come with the tradeoff of taking up more space and limiting the number of servers than can be installed in a rack. Since we are power constrained instead of space constrained, the tradeoff did not negatively impact our design.\n\nThermal simulations provided by Cloudflare vendors showed that 4x 60mm fans or 4x 80mm fans at less than 40 Watt per fan is sufficient to cool the system. That is a theoretical savings of at least 150 Watt compared to 8x 40mm fans in a 1U design, which would result in significant Operational Expenditure (OPEX) savings and a boost to TCO improvement. Switching to a 2U form factor also gives us the benefit of fully utilizing our rack power budget and our rack space, and provides ample room for the addition of PCIe attached accelerators / GPUs, including dual-slot form factor options.\n\n## Conclusion\n\nIt might seem counter-intuitive, but our observations indicate that growing the server chassis, and utilizing more space per node actually increases rack density and improves overall TCO benefit over previous generation deployments, since it allows for a better thermal design. We are very happy with the result of this technical readiness investigation, and are actively working on validating our Gen 12 Compute servers and launching them into production soon. Stay tuned for more details on our Gen 12 designs.\n\nIf you are excited about helping build  a better Internet, come join us, [we are hiring](https://www.cloudflare.com/careers/jobs/)!\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[AMD](https://blog.cloudflare.com/tag/amd/) [Hardware](https://blog.cloudflare.com/tag/hardware/) [Cloudflare Network](https://blog.cloudflare.com/tag/cloudflare-network/)"
    },
    {
      "url": "https://blog.cloudflare.com/page/2/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/page/2/",
        "loadedTime": "2023-12-05T02:26:38.146Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/page/2/",
        "title": "The Cloudflare Blog (Page 2)",
        "description": null,
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "October 25, 2023 2:00PM \nCache Reserve goes GA: enhanced control to minimize egress costs\nCache Reserve General Availability Application Services Performance \nWe're excited to announce the graduation of Cache Reserve from beta to GA, accompanied by the introduction of several exciting new features. These new features include adding Cache Reserve into the analytics shown on the Cache overview section of the Cloudflare dashboard...\nOctober 24, 2023 2:00PM \nCache Rules are now GA: precision control over every part of your cache\nGeneral Availability Cache Rules Product News Application Services Performance Cache \nToday, we're thrilled to share that Cache Rules, along with several other Rules products, are generally available (GA). But that’s not all — we're also introducing new configuration options for Cache Rules...\nOctober 23, 2023 2:32PM \nCyber attacks in the Israel-Hamas war\nDDoS Attacks Israel Cloudflare Radar Insights Trends \nSince the October 7 Hamas attack, DDoS attackers have been targeting Israeli newspaper and media websites as well as software companies and financial institutions....\nOctober 20, 2023 10:39PM \nHow Cloudflare mitigated yet another Okta compromise\nOkta Post Mortem 1.1.1.1 \nOn Wednesday, October 18, 2023, we discovered attacks on our system that we were able to trace back to Okta. We have verified that no Cloudflare customer information or systems were impacted by this event because of our rapid response....\nOctober 20, 2023 2:41PM \nEmpowering our partners with the new Tenant Platform dashboard\nPartners Product News Cloudflare One \nWe are proud to announce the general availability of our first dashboard for our Tenant Platform, providing an intuitive user interface for agencies and partners to manage their client accounts...\nOctober 18, 2023 2:00PM \nNetwork flow monitoring is GA, providing end-to-end traffic visibility\nMagic Network Monitoring Network Services Magic Transit Magic WAN Product News \nNetwork engineers often need better visibility into their network’s traffic when analyzing DDoS attacks or troubleshooting other traffic anomalies. To solve this problem, Cloudflare offers a network flow monitoring product that gives customers end-to-end traffic visibility across their network....\nOctober 16, 2023 6:53PM \nIntroducing the Project Argus Datacenter-ready Secure Control Module design specification\nHardware Security \nThe DC-SCM (Datacenter-ready Secure Control Module) decouples server management from the server motherboard. It provides flexibility to implement multiple server management and security solutions with the same server motherboard design...\nOctober 14, 2023 1:00AM \nMalicious “RedAlert - Rocket Alerts” Application Targets Israeli Phone Calls, SMS, and User Information\nCloudforce One Vulnerabilities Internet Traffic Malware Threat Intelligence \nOn October 13, 2023, Cloudflare’s Cloudforce One Threat Operations Team became aware of a malicious Google Android application impersonating the real-time rocket alert app, Red Alert, which provides real-time rocket alerts for Israeli citizens...\nOctober 12, 2023 2:00PM \nHow Prisma saved 98% on distribution costs with Cloudflare R2\nPartners R2 Storage Developers \nCloudflare products provide much of the underlying infrastructure for Prisma Accelerate and Prisma Pulse, empowering user-focused product development. This ongoing collaboration extends to enhancing the Prisma ORM...\nOctober 10, 2023 1:02PM \nHTTP/2 Rapid Reset: deconstructing the record-breaking attack\nDDoS Vulnerabilities Trends Attacks Security \nThis post dives into the details of the HTTP/2 protocol, the feature that attackers exploited to generate the massive Rapid Reset attacks, and the mitigation strategies we took to ensure all our customers are protected...\nOctober 10, 2023 1:02PM \nHTTP/2 Zero-Day vulnerability results in record-breaking DDoS attacks\nSecurity Vulnerabilities Attacks DDoS \nThe “HTTP/2 Rapid Reset” attack exploits a weakness in the HTTP/2 protocol to generate enormous, hyper-volumetric DDoS attacks. Cloudflare has mitigated a barrage of these attacks in recent months, including an attack three times larger than any previous attack we’ve observed...\nOctober 09, 2023 9:05PM \nInternet traffic patterns in Israel and Palestine following the October 2023 attacks\nCloudflare Radar Trends Internet Traffic Outage \nOn Saturday, October 7, 2023, attacks from the Palestinian group Hamas launched from the Gaza Strip against the south of Israel started a new conflict in the region. Cloudflare's data shows that Internet traffic was impacted in different ways...\nOctober 06, 2023 2:05PM \nVirtual networking 101: Bridging the gap to understanding TAP\nDeep Dive \nTap devices were historically used for VPN clients. Using them for virtual machines is essentially reversing their original purpose - from traffic sinks to traffic sources. In the article I explore the intricacies of tap devices, covering topics like offloads, segmentation, and multi-queue....\nOctober 05, 2023 4:00PM \nUncovering the Hidden WebP vulnerability: a tale of a CVE with much bigger implications than it originally seemed\nVulnerabilities Chrome WebP Security Swift \nRecently, Google announced a security issue in Google Chrome, titled \"Heap buffer overflow in WebP in Google Chrome.\" Initially, it seemed like just another bug in the popular web browser. However, what we discovered was far more significant and had implications that extended well beyond Chrome...\nOctober 05, 2023 2:00PM \nCloudflare's a Top 100 Most Loved Workplace for the second consecutive year in 2023\nLife @ Cloudflare Careers People Employee Resource Groups \nWe are proud to share that Cloudflare has been certified and recognized as one of the Top 100 Most Loved Workplaces in 2023 by Newsweek and the Best Practice Institute (BPI) for the second consecutive year....\nOctober 04, 2023 8:40PM \n1.1.1.1 lookup failures on October 4th, 2023\n1.1.1.1 Post Mortem Outage \nOn 4 October 2023, Cloudflare experienced DNS resolution problems. Some users may have received SERVFAIL DNS responses to valid queries. In this blog, we’re going to talk about what the failure was, why it occurred, and what we’re doing to make sure this doesn’t happen again...\nOctober 04, 2023 5:03PM \nAll Cloudflare Customers Protected from Atlassian Confluence CVE-2023-22515\nAtlassian CVE WAF \nOn 2023-10-04 at 13:00 UTC, Atlassian released details of the zero-day vulnerability described as “Privilege Escalation Vulnerability in Confluence Data Center and Server” (CVE-2023-22515), a zero-day vulnerability impacting Confluence Server and Data Center products...\nOctober 04, 2023 2:00PM \nWaiting Room adds multi-host and path coverage, unlocking broader protection and multilingual setups\nWaiting Room Always Online Traffic \nToday, we are thrilled to announce that Waiting Room now supports coverage of multiple hostname and path combinations with a single waiting room, giving customers more flexibility and offering broader site coverage without interruptions to end-user flows...\nOctober 03, 2023 1:55PM \nAnnouncing General Availability for the Magic WAN Connector: the easiest way to jumpstart SASE transformation for your network\nProduct News Security Magic WAN Magic WAN Connector SASE Zero Trust \nWe’re announcing the general availability of the Magic WAN Connector, which serves as the glue between your existing network hardware and Cloudflare’s networ...\nOctober 02, 2023 2:00PM \nBirthday Week recap: everything we announced — plus an AI-powered opportunity for startups\nBirthday Week Product News AI Turnstile CAPTCHA Research Machine Learning Security Connectivity Cloud R2 Storage D1 Beta \nNeed a recap or refresher on all the big Birthday Week news this week? This recap has you covered...",
      "markdown": "October 25, 2023 2:00PM\n\n[\n\n## Cache Reserve goes GA: enhanced control to minimize egress costs\n\n](https://blog.cloudflare.com/cache-reserve-goes-ga/)[Cache Reserve](https://blog.cloudflare.com/tag/cache-reserve/) [General Availability](https://blog.cloudflare.com/tag/general-availability/) [Application Services](https://blog.cloudflare.com/tag/application-services/) [Performance](https://blog.cloudflare.com/tag/performance/)\n\nWe're excited to announce the graduation of Cache Reserve from beta to GA, accompanied by the introduction of several exciting new features. These new features include adding Cache Reserve into the analytics shown on the Cache overview section of the Cloudflare dashboard...\n\n*   [![Alex Krivit](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2018/08/RE0BQ6EF_400x400.jpg)](https://blog.cloudflare.com/author/alex/)\n\nOctober 24, 2023 2:00PM\n\n[\n\n## Cache Rules are now GA: precision control over every part of your cache\n\n](https://blog.cloudflare.com/cache-rules-go-ga/)[General Availability](https://blog.cloudflare.com/tag/general-availability/) [Cache Rules](https://blog.cloudflare.com/tag/cache-rules/) [Product News](https://blog.cloudflare.com/tag/product-news/) [Application Services](https://blog.cloudflare.com/tag/application-services/) [Performance](https://blog.cloudflare.com/tag/performance/) [Cache](https://blog.cloudflare.com/tag/cache/)\n\nToday, we're thrilled to share that Cache Rules, along with several other Rules products, are generally available (GA). But that’s not all — we're also introducing new configuration options for Cache Rules...\n\n*   [![Alex Krivit](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2018/08/RE0BQ6EF_400x400.jpg)](https://blog.cloudflare.com/author/alex/)\n\nOctober 23, 2023 2:32PM\n\n[\n\n## Cyber attacks in the Israel-Hamas war\n\n](https://blog.cloudflare.com/cyber-attacks-in-the-israel-hamas-war/)[DDoS](https://blog.cloudflare.com/tag/ddos/) [Attacks](https://blog.cloudflare.com/tag/attacks/) [Israel](https://blog.cloudflare.com/tag/israel/) [Cloudflare Radar](https://blog.cloudflare.com/tag/cloudflare-radar/) [Insights](https://blog.cloudflare.com/tag/insights/) [Trends](https://blog.cloudflare.com/tag/trends/)\n\nSince the October 7 Hamas attack, DDoS attackers have been targeting Israeli newspaper and media websites as well as software companies and financial institutions....\n\n*   [![Omer Yoachimik](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/06/Screenshot-2023-06-02-at-9.38.13-AM.png)](https://blog.cloudflare.com/author/omer/)\n*   [![Jorge Pacheco](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/04/CV_Profile.jpeg)](https://blog.cloudflare.com/author/jorge/)\n\nOctober 20, 2023 10:39PM\n\n[\n\n## How Cloudflare mitigated yet another Okta compromise\n\n](https://blog.cloudflare.com/how-cloudflare-mitigated-yet-another-okta-compromise/)[Okta](https://blog.cloudflare.com/tag/okta/) [Post Mortem](https://blog.cloudflare.com/tag/post-mortem/) [1.1.1.1](https://blog.cloudflare.com/tag/1-1-1-1/)\n\nOn Wednesday, October 18, 2023, we discovered attacks on our system that we were able to trace back to Okta. We have verified that no Cloudflare customer information or systems were impacted by this event because of our rapid response....\n\n*   [![Sourov Zaman](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/05/Screen-Shot-2022-06-03-at-1.49.26-AM.png)](https://blog.cloudflare.com/author/sourov/)\n*   [![Lucas Ferreira](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/02/IMG_20210718_203048.jpeg)](https://blog.cloudflare.com/author/lucas-ferreira/)\n*   [![Kimberly Hall](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/_tmp_mini_magick20221017-43-3fd60e.jpg)](https://blog.cloudflare.com/author/kimberly/)\n*   [![Grant Bourzikas](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/04/Headshot---GB_LinkedIn.jpg)](https://blog.cloudflare.com/author/grant/)\n\nOctober 20, 2023 2:41PM\n\n[\n\n## Empowering our partners with the new Tenant Platform dashboard\n\n](https://blog.cloudflare.com/tenant-platform-ui-ga/)[Partners](https://blog.cloudflare.com/tag/partners/) [Product News](https://blog.cloudflare.com/tag/product-news/) [Cloudflare One](https://blog.cloudflare.com/tag/cloudflare-one/)\n\nWe are proud to announce the general availability of our first dashboard for our Tenant Platform, providing an intuitive user interface for agencies and partners to manage their client accounts...\n\n*   [![Dan Hollinger](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2019/06/Tim-King-Session---1977.jpg)](https://blog.cloudflare.com/author/dan/)\n\nOctober 18, 2023 2:00PM\n\n[\n\n## Network flow monitoring is GA, providing end-to-end traffic visibility\n\n](https://blog.cloudflare.com/network-flow-monitoring-generally-available/)[Magic Network Monitoring](https://blog.cloudflare.com/tag/magic-network-monitoring/) [Network Services](https://blog.cloudflare.com/tag/network-services/) [Magic Transit](https://blog.cloudflare.com/tag/magic-transit/) [Magic WAN](https://blog.cloudflare.com/tag/magic-wan/) [Product News](https://blog.cloudflare.com/tag/product-news/)\n\nNetwork engineers often need better visibility into their network’s traffic when analyzing DDoS attacks or troubleshooting other traffic anomalies. To solve this problem, Cloudflare offers a network flow monitoring product that gives customers end-to-end traffic visibility across their network....\n\n*   [![Chris Draper](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/06/blog_headshot.jpg)](https://blog.cloudflare.com/author/chris-draper/)\n*   [![Chris J Arges](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/12/chris.jpg)](https://blog.cloudflare.com/author/arges/)\n*   [![Ana Oliveira](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/_tmp_mini_magick20221018-42-1ufmnay.jpg)](https://blog.cloudflare.com/author/ana/)\n*   [![João Santos](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/_tmp_mini_magick20221017-42-2823yd.jpg)](https://blog.cloudflare.com/author/joao-santos/)\n*   [![Luís Franco](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/Profile-Picture.PNG)](https://blog.cloudflare.com/author/luis/)\n*   [![Nadin El-Yabroudi](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2019/11/Screen-Shot-2019-11-21-at-12.26.44-PM.png)](https://blog.cloudflare.com/author/nadin/)\n*   [![Dan Geraghty](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/_tmp_uploaded20220909-4-1fgoh1l.jpg)](https://blog.cloudflare.com/author/dan-geraghty/)\n\nOctober 16, 2023 6:53PM\n\n[\n\n## Introducing the Project Argus Datacenter-ready Secure Control Module design specification\n\n](https://blog.cloudflare.com/introducing-the-project-argus-datacenter-ready-secure-control-module-design-specification/)[Hardware](https://blog.cloudflare.com/tag/hardware/) [Security](https://blog.cloudflare.com/tag/security/)\n\nThe DC-SCM (Datacenter-ready Secure Control Module) decouples server management from the server motherboard. It provides flexibility to implement multiple server management and security solutions with the same server motherboard design...\n\n*   [![Xiaomin Shen](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/04/mmexport1632277733244_mr1632497208886--1-.jpg)](https://blog.cloudflare.com/author/xiaomin/)\n*   [![JQ Lau](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/_tmp_mini_magick20221017-42-17izwqk.jpg)](https://blog.cloudflare.com/author/jq/)\n\nOctober 14, 2023 1:00AM\n\n[\n\n## Malicious “RedAlert - Rocket Alerts” Application Targets Israeli Phone Calls, SMS, and User Information\n\n](https://blog.cloudflare.com/malicious-redalert-rocket-alerts-application-targets-israeli-phone-calls-sms-and-user-information/)[Cloudforce One](https://blog.cloudflare.com/tag/cloudforce-one/) [Vulnerabilities](https://blog.cloudflare.com/tag/vulnerabilities/) [Internet Traffic](https://blog.cloudflare.com/tag/internet-traffic/) [Malware](https://blog.cloudflare.com/tag/malware/) [Threat Intelligence](https://blog.cloudflare.com/tag/threat-intelligence/)\n\nOn October 13, 2023, Cloudflare’s Cloudforce One Threat Operations Team became aware of a malicious Google Android application impersonating the real-time rocket alert app, Red Alert, which provides real-time rocket alerts for Israeli citizens...\n\n*   [![Blake Darché](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/06/30DC176C-CDE6-4271-AD8C-CC27B2A9E730.jpeg)](https://blog.cloudflare.com/author/blake/)\n*   [![Armen Boursalian](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/Screen-Shot-2022-08-18-at-11.51.06-AM-1.png)](https://blog.cloudflare.com/author/armen/)\n*   [![Javier Castro](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/javier-castro-photo.png)](https://blog.cloudflare.com/author/javier/)\n\nOctober 12, 2023 2:00PM\n\n[\n\n## How Prisma saved 98% on distribution costs with Cloudflare R2\n\n](https://blog.cloudflare.com/how-prisma-saved-98-percent-on-distribution-costs-with-cloudflare-r2/)[Partners](https://blog.cloudflare.com/tag/partners/) [R2 Storage](https://blog.cloudflare.com/tag/cloudflare-r2/) [Developers](https://blog.cloudflare.com/tag/developers/)\n\nCloudflare products provide much of the underlying infrastructure for Prisma Accelerate and Prisma Pulse, empowering user-focused product development. This ongoing collaboration extends to enhancing the Prisma ORM...\n\n*   [![Pierre-Antoine Mills (Guest Author)](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/09/Pierre-Antoine-Mills.jpeg)](https://blog.cloudflare.com/author/pierre-antoine-mills/)\n*   [![Miguel Fernández (Guest Author)](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/09/Miguel-Ferna-ndez.jpeg)](https://blog.cloudflare.com/author/miguel-fernandez-guest-author/)\n*   [![Petra Donka (Guest Author)](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/09/Petra-Donka.jpeg)](https://blog.cloudflare.com/author/petra-donka/)\n\nOctober 10, 2023 1:02PM\n\n[\n\n## HTTP/2 Rapid Reset: deconstructing the record-breaking attack\n\n](https://blog.cloudflare.com/technical-breakdown-http2-rapid-reset-ddos-attack/)[DDoS](https://blog.cloudflare.com/tag/ddos/) [Vulnerabilities](https://blog.cloudflare.com/tag/vulnerabilities/) [Trends](https://blog.cloudflare.com/tag/trends/) [Attacks](https://blog.cloudflare.com/tag/attacks/) [Security](https://blog.cloudflare.com/tag/security/)\n\nThis post dives into the details of the HTTP/2 protocol, the feature that attackers exploited to generate the massive Rapid Reset attacks, and the mitigation strategies we took to ensure all our customers are protected...\n\n*   [![Lucas Pardue](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2018/12/Lucas---KIN01111.jpg)](https://blog.cloudflare.com/author/lucas/)\n*   [![Julien Desgats](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2017/06/profile-square-small.jpg)](https://blog.cloudflare.com/author/julien-desgats/)\n\nOctober 10, 2023 1:02PM\n\n[\n\n## HTTP/2 Zero-Day vulnerability results in record-breaking DDoS attacks\n\n](https://blog.cloudflare.com/zero-day-rapid-reset-http2-record-breaking-ddos-attack/)[Security](https://blog.cloudflare.com/tag/security/) [Vulnerabilities](https://blog.cloudflare.com/tag/vulnerabilities/) [Attacks](https://blog.cloudflare.com/tag/attacks/) [DDoS](https://blog.cloudflare.com/tag/ddos/)\n\nThe “HTTP/2 Rapid Reset” attack exploits a weakness in the HTTP/2 protocol to generate enormous, hyper-volumetric DDoS attacks. Cloudflare has mitigated a barrage of these attacks in recent months, including an attack three times larger than any previous attack we’ve observed...\n\n*   [![Grant Bourzikas](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/04/Headshot---GB_LinkedIn.jpg)](https://blog.cloudflare.com/author/grant/)\n\nOctober 09, 2023 9:05PM\n\n[\n\n## Internet traffic patterns in Israel and Palestine following the October 2023 attacks\n\n](https://blog.cloudflare.com/internet-traffic-patterns-in-israel-and-palestine-following-the-october-2023-attacks/)[Cloudflare Radar](https://blog.cloudflare.com/tag/cloudflare-radar/) [Trends](https://blog.cloudflare.com/tag/trends/) [Internet Traffic](https://blog.cloudflare.com/tag/internet-traffic/) [Outage](https://blog.cloudflare.com/tag/outage/)\n\nOn Saturday, October 7, 2023, attacks from the Palestinian group Hamas launched from the Gaza Strip against the south of Israel started a new conflict in the region. Cloudflare's data shows that Internet traffic was impacted in different ways...\n\n*   [![João Tomé](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/V0x3WKfJ_400x400-1.jpeg)](https://blog.cloudflare.com/author/joao-tome/)\n\nOctober 06, 2023 2:05PM\n\n[\n\n## Virtual networking 101: Bridging the gap to understanding TAP\n\n](https://blog.cloudflare.com/virtual-networking-101-understanding-tap/)[Deep Dive](https://blog.cloudflare.com/tag/deep-dive/)\n\nTap devices were historically used for VPN clients. Using them for virtual machines is essentially reversing their original purpose - from traffic sinks to traffic sources. In the article I explore the intricacies of tap devices, covering topics like offloads, segmentation, and multi-queue....\n\n*   [![Marek Majkowski](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2017/03/b5967d6c687939594adb6992723d0529.jpeg)](https://blog.cloudflare.com/author/marek-majkowski/)\n\nOctober 05, 2023 4:00PM\n\n[\n\n## Uncovering the Hidden WebP vulnerability: a tale of a CVE with much bigger implications than it originally seemed\n\n](https://blog.cloudflare.com/uncovering-the-hidden-webp-vulnerability-cve-2023-4863/)[Vulnerabilities](https://blog.cloudflare.com/tag/vulnerabilities/) [Chrome](https://blog.cloudflare.com/tag/chrome/) [WebP](https://blog.cloudflare.com/tag/webp/) [Security](https://blog.cloudflare.com/tag/security/) [Swift](https://blog.cloudflare.com/tag/swift/)\n\nRecently, Google announced a security issue in Google Chrome, titled \"Heap buffer overflow in WebP in Google Chrome.\" Initially, it seemed like just another bug in the popular web browser. However, what we discovered was far more significant and had implications that extended well beyond Chrome...\n\n*   [![Willi Geiger](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/09/selfie-small.jpg)](https://blog.cloudflare.com/author/willi/)\n*   [![Kornel Lesiński](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/09/3a534a3c391d25cb34cb4078cb0c1148.jpg)](https://blog.cloudflare.com/author/kornel/)\n\nOctober 05, 2023 2:00PM\n\n[\n\n## Cloudflare's a Top 100 Most Loved Workplace for the second consecutive year in 2023\n\n](https://blog.cloudflare.com/cloudflares-a-top-100-most-loved-workplace-for-2023/)[Life @ Cloudflare](https://blog.cloudflare.com/tag/life-at-cloudflare/) [Careers](https://blog.cloudflare.com/tag/careers/) [People](https://blog.cloudflare.com/tag/people/) [Employee Resource Groups](https://blog.cloudflare.com/tag/employee-resource-groups/)\n\nWe are proud to share that Cloudflare has been certified and recognized as one of the Top 100 Most Loved Workplaces in 2023 by Newsweek and the Best Practice Institute (BPI) for the second consecutive year....\n\n*   [![Scott Tomtania](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/11/Pictures.png)](https://blog.cloudflare.com/author/scott/)\n\nOctober 04, 2023 8:40PM\n\n[\n\n## 1.1.1.1 lookup failures on October 4th, 2023\n\n](https://blog.cloudflare.com/1-1-1-1-lookup-failures-on-october-4th-2023/)[1.1.1.1](https://blog.cloudflare.com/tag/1-1-1-1/) [Post Mortem](https://blog.cloudflare.com/tag/post-mortem/) [Outage](https://blog.cloudflare.com/tag/outage/)\n\nOn 4 October 2023, Cloudflare experienced DNS resolution problems. Some users may have received SERVFAIL DNS responses to valid queries. In this blog, we’re going to talk about what the failure was, why it occurred, and what we’re doing to make sure this doesn’t happen again...\n\n*   [![Ólafur Guðmundsson](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/_tmp_mini_magick20230112-42-p4sd2l.jpg)](https://blog.cloudflare.com/author/olafur-gudmundsson/)\n\nOctober 04, 2023 5:03PM\n\n[\n\n## All Cloudflare Customers Protected from Atlassian Confluence CVE-2023-22515\n\n](https://blog.cloudflare.com/all-cloudflare-customers-protected-atlassian-cve-2023-22515/)[Atlassian](https://blog.cloudflare.com/tag/atlassian/) [CVE](https://blog.cloudflare.com/tag/cve/) [WAF](https://blog.cloudflare.com/tag/waf/)\n\nOn 2023-10-04 at 13:00 UTC, Atlassian released details of the zero-day vulnerability described as “Privilege Escalation Vulnerability in Confluence Data Center and Server” (CVE-2023-22515), a zero-day vulnerability impacting Confluence Server and Data Center products...\n\n*   [![Himanshu Anand](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/03/unnamed-4.png)](https://blog.cloudflare.com/author/himanshu/)\n*   [![Daniele Molteni](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2020/10/meProfessional-1-flipped.jpg)](https://blog.cloudflare.com/author/daniele/)\n*   [![Sourov Zaman](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/05/Screen-Shot-2022-06-03-at-1.49.26-AM.png)](https://blog.cloudflare.com/author/sourov/)\n*   [![Vaibhav Singhal](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/06/VS_Photo-Final1.png)](https://blog.cloudflare.com/author/vaibhav/)\n*   [![Ary Widdes](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/_tmp_mini_magick20230419-40-19b08mj.jpg)](https://blog.cloudflare.com/author/ary/)\n*   [![Myles Robinson](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/_tmp_mini_magick20230729-2-1pc5jly.jpg)](https://blog.cloudflare.com/author/myles/)\n\nOctober 04, 2023 2:00PM\n\n[\n\n## Waiting Room adds multi-host and path coverage, unlocking broader protection and multilingual setups\n\n](https://blog.cloudflare.com/multihost-waiting-room/)[Waiting Room](https://blog.cloudflare.com/tag/waiting-room/) [Always Online](https://blog.cloudflare.com/tag/always-online/) [Traffic](https://blog.cloudflare.com/tag/traffic/)\n\nToday, we are thrilled to announce that Waiting Room now supports coverage of multiple hostname and path combinations with a single waiting room, giving customers more flexibility and offering broader site coverage without interruptions to end-user flows...\n\n*   [![Arielle Olache](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/03/20181111-144427-Original-1-.jpg)](https://blog.cloudflare.com/author/arielle/)\n*   [![Yawar Jamal](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/dp.jpg)](https://blog.cloudflare.com/author/yawar/)\n\nOctober 03, 2023 1:55PM\n\n[\n\n## Announcing General Availability for the Magic WAN Connector: the easiest way to jumpstart SASE transformation for your network\n\n](https://blog.cloudflare.com/magic-wan-connector-general-availability/)[Product News](https://blog.cloudflare.com/tag/product-news/) [Security](https://blog.cloudflare.com/tag/security/) [Magic WAN](https://blog.cloudflare.com/tag/magic-wan/) [Magic WAN Connector](https://blog.cloudflare.com/tag/magic-wan-connector/) [SASE](https://blog.cloudflare.com/tag/sase/) [Zero Trust](https://blog.cloudflare.com/tag/zero-trust/)\n\nWe’re announcing the general availability of the Magic WAN Connector, which serves as the glue between your existing network hardware and Cloudflare’s networ...\n\n*   [![Annika Garbers](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2020/04/annika.png)](https://blog.cloudflare.com/author/annika/)\n\nOctober 02, 2023 2:00PM\n\n[\n\n## Birthday Week recap: everything we announced — plus an AI-powered opportunity for startups\n\n](https://blog.cloudflare.com/birthday-week-2023-wrap-up/)[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [Product News](https://blog.cloudflare.com/tag/product-news/) [AI](https://blog.cloudflare.com/tag/ai/) [Turnstile](https://blog.cloudflare.com/tag/turnstile/) [CAPTCHA](https://blog.cloudflare.com/tag/captcha/) [Research](https://blog.cloudflare.com/tag/research/) [Machine Learning](https://blog.cloudflare.com/tag/machine-learning/) [Security](https://blog.cloudflare.com/tag/security/) [Connectivity Cloud](https://blog.cloudflare.com/tag/connectivity-cloud/) [R2 Storage](https://blog.cloudflare.com/tag/cloudflare-r2/) [D1](https://blog.cloudflare.com/tag/d1/) [Beta](https://blog.cloudflare.com/tag/beta/)\n\nNeed a recap or refresher on all the big Birthday Week news this week? This recap has you covered...\n\n*   [![Dina Kozlov](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2019/06/headshot.jpg)](https://blog.cloudflare.com/author/dina/)\n*   [![Mia Wang](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/09/MW-headshot.JPG)](https://blog.cloudflare.com/author/mia-wang/)"
    },
    {
      "url": "https://blog.cloudflare.com/cyber-week-analyzing-internet-traffic-and-e-commerce-trends/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/cyber-week-analyzing-internet-traffic-and-e-commerce-trends/",
        "loadedTime": "2023-12-05T02:26:46.334Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/cyber-week-analyzing-internet-traffic-and-e-commerce-trends/",
        "title": "Cyber Week: Analyzing Internet traffic and e-commerce trends",
        "description": "How significant are Cyber Week days on the Internet? Is it a global phenomenon? Does e-commerce interest peak on Black Friday or Cyber Monday, and are attacks increasing during this time? These questions are important to retailers and stakeholders around the world.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/28/2023\n8 min read\nThroughout the year, special events lead to changes in Internet traffic. We observed this with Thanksgiving in the US last week, where traffic dipped, and during periods like Black Friday (November 24, 2023) and Cyber Monday (November 27, 2023), where traffic spiked. \nBut how significant are these Cyber Week days on the Internet? Is it a global phenomenon? Does e-commerce interest peak on Black Friday or Cyber Monday, and are attacks increasing during this time? These questions are important to retailers and stakeholders around the world. At Cloudflare, we manage substantial traffic for our customers, which gives us a unique vantage from which to analyze traffic and attack patterns across large swaths of the Internet.\nAs we'll explore next, we observed varying trends. From a global perspective, there was a clear Internet traffic winner: Cyber Monday was the highest overall traffic day of 2023 (as it was for 2022), followed by Black Friday, and then Monday, November 21 from the same week. But zooming in, this pattern didn’t hold in some countries.\nFor this analysis, we examined anonymized samples of HTTP requests crossing our network, as well as DNS queries. Cloudflare's global data shows that peak request traffic occurred on Cyber Monday, and that recent weeks have generally been the year’s busiest. Here are some notable figures:\nCloudflare processed a peak of 80 million HTTP requests per second at 16:10 UTC on November 27.\nThe peak hour of 16:00 UTC saw more than 230 billion hourly requests.\nCloudflare powered around 4 trillion daily requests on Cyber Monday (with blocked attacks comprising around 5% of all traffic), a figure only approached by Black Friday, which saw 3.86 trillion requests.\nThere was a 27% increase in HTTP requests on Cyber Monday 2023 (November 27) as compared to Cyber Monday 2022 (November 28).\nWhat about DNS queries?\nAggregated from our 1.1.1.1 resolver showed that Cyber Monday 2023 experienced a peak of 1.68 trillion queries per day, with 22 million queries per second around 15:00 UTC. Of these DNS queries, 15% were encrypted (HTTPS and TLS). Back in August, the peak was at 1.35 trillion queries per day, marking a 24% increase.\nTraffic to our authoritative DNS servers also peaked on Cyber Monday, with 811 billion daily queries and a peak of 9.4 million queries per second around 15:00 UTC.\nSo, during Cyber Monday, we saw a combined peak of over 100 million requests and queries per second across all Cloudflare services at around 16:00 UTC (November 27).\nBlack Friday week Internet traffic daily ranking\nThese numbers and trends are consistent with what we observed in 2022 and previous years, where traffic peaks in late November but usually drops in December. Here's a snapshot of global human Internet traffic this year (bot traffic shows a similar pattern).\nWorldwide. Most popular Internet traffic days\nCyber Monday, November 27\nBlack Friday, November 24\nMonday, November 21\nFrom the US perspective, the ranking is similar, with Saturday, November 25, the day after Black Friday, ranking as the third busiest day for Internet traffic.\nUS. Most popular Internet traffic days\nCyber Monday, November 27\nBlack Friday, November 24\nSaturday, November 25\nAdditionally, most U.S. states show a similar trend, with Cyber Monday experiencing the most traffic, followed by Black Friday. However, Alaska is a notable exception, where the days with the highest Internet traffic were November 13 and 14, coinciding with a snow emergency that closed schools and roads.\nStates like Colorado, Hawaii, Idaho, New Mexico, and California also had Saturday, November 25, as their second busiest day, but Cyber Monday also “won” there.\nDoes the Black Friday week impact other countries?\nInternationally, a trend of peak Internet traffic in November is observed in most countries, as highlighted in our previous 2022 Year in Review (stay tuned for our 2023 edition in the next few weeks). This trend is likely linked to colder weather in the Northern Hemisphere, where approximately 87% of the world's population resided in 2023, as well as holidays and shopping periods, among other factors.\nHere's a table summarizing the November days with the most traffic, where the Black Friday week plays a significant role.\nMost popular Internet traffic days\nUK. \n#1. Black Friday, November 24\n#2. Cyber Monday, November 27\n#3. Sunday, November 20\n\t\nCanada. \n#1. Black Friday, November 24\n#2. Cyber Monday, November 27\n#3. Thursday, November 23\n\t\nGermany. \n#1. Black Friday, November 24\n#2. Sunday, November 26\n#3. Cyber Monday, November 27\n\t\nMexico. \n#1. Monday, November 21\n#2. Friday, November 17 (one week before Black Friday)\n#3. Black Friday, November 24\n\t\nFrance. \n#1. Cyber Monday, November 27\n#2. Sunday, November 26\n#3. Black Friday, November 24\n\t\nBrazil. \n#1. Tuesday, November 22 \n#2. Black Friday, November 24\n#3. Monday, November 21\n\t\nSpain. \n#1. Cyber Monday, November 27\n#2. Sunday, November 20\n#3. Monday, November 21\n\t\nAustralia. \n#1. Black Friday, November 24\n#2. Thursday, November 23\n#3. Sunday, November 20\n\t\nEgypt. \n#1. Saturday, November 25\n#2. Sunday, November 26\n#3. Black Friday, November 24\n\t\nSingapore. \n#1. Cyber Monday, November 27\n#2. Black Friday, November 24\n#3. Thursday, November 23\n\t\nTurkey. \n#1. Sunday, November 26 (Black Friday weekend)\n#2. Saturday, November 25\n#3. Singles Day, November 11\n\t\nPhilippines. \n#1. Cyber Monday, November 27\n#2. Wednesday, November 22\n#3. Sunday, November 20\n\t\nCountries like India, Japan, South Korea, Thailand, and Indonesia, though they show increased traffic during October and November compared to other months, do not exhibit an obvious increase in traffic during Black Friday week.\nSingles' Day (November 11), a popular Asian shopping event, only features in the top three traffic days in Turkey. In China, October saw bigger traffic peaks than November. However, in November, both Black Friday and the following day (November 25) showed clear increases in traffic, similar to Singles' Day. In South Africa, Singles' Day and Black Friday were the busiest traffic days in November, even though October also had higher peaks.\nBlack Friday goes mobile, Cyber Monday goes desktop\nWe observed last week that during Thanksgiving Day, mobile use in US Internet traffic was higher than in the previous week. This trend was intensified on Black Friday, peaking at 55.3% of all traffic, surpassing the typical weekend, which usually sees a higher mobile usage percentage. However, on Cyber Monday, desktop use took the lead, with the percentage of mobile device traffic dropping to 47.6%, lower than the previous Monday.\nThis trend seems to suggest that Black Friday shopping might involve more offline activities, with people in the US using their mobile devices more for Internet access on that day.\nE-commerce DNS trends\nUsing aggregated data from our 1.1.1.1 resolver, we have a more focused, category-specific view of the DNS traffic growth to e-commerce sites. There's a general rising trend throughout November, very similar to what we observed in the Internet traffic section.\nLooking more closely at the US aggregated e-commerce sites, it's evident that Cyber Monday and Black Friday, in that order, were the days with the most DNS traffic, with Saturday, November 25, ranking third on the podium — exactly mirroring the HTTP traffic pattern discussed earlier.\nThe peak hours of DNS traffic on Black Friday were around 16:00 and 17:00 UTC, which correspond to 12:00 and 13:00 EST and 09:00 and 10:00 PST. The same pattern was observed on Cyber Monday.\nDuring Cyber Week (November 20 to 27), there was a 15% increase in DNS traffic compared with the previous week. A consistently high level of DNS traffic was maintained throughout Black Friday week, starting on Monday, November 20, with the sole exception being a noticeable drop on Thanksgiving Day — DNS traffic to e-commerce sites was 6% lower than the previous week on that day.\nA glimpse into Europe’s e-commerce trends\nThe UK shows a very similar trend to the US in terms of Black Friday and Cyber Monday interest. However, in 2023, Black Friday and Cyber Monday are tied for the top spot, followed by Tuesday, November 21.\nIn Australia, Cyber Monday ranked as the most popular day for e-commerce DNS traffic, followed by Black Friday. Canada showed a similar pattern, with Black Friday being the most trafficked day, followed by Cyber Monday.\nIn Germany, Black Friday indisputably led in e-commerce DNS traffic, followed by the previous Friday, November 17, and then the Black Friday weekend. Cyber Monday did not make it to the top three in Germany.\nIn France, Black Friday was the most popular e-commerce day, followed by Saturday, November 18.\nElectronics, fast fashion, and second-hand trends\nFocusing on the US only again, electronics e-commerce sites experienced more DNS traffic on Black Friday than on Cyber Monday.\nThis trend was mirrored in the fast fashion category, with Black Friday clearly in the lead.\nIt's perhaps unsurprising that second-hand shopping sites in the US gained more momentum and DNS traffic in the preceding week (November 12-18) leading up to Black Friday. However, these sites then reached their peak on Cyber Monday.\nHow about cyber threats?\nRegarding cyber threats, let's focus on DDoS (distributed denial-of-service) attacks, a popular method for disrupting Internet properties. Data from November 2023 shows that on Thanksgiving, DDoS attacks accounted for the lowest daily fraction of traffic volume observed in the month of November across the US. There were higher percentages of DDoS attacks in late August and September, associated with the HTTP/2 Zero-Day vulnerability, which led to record-breaking attacks.\nThe Black Friday week was not a peak period for DDoS attacks. The highest activity in November occurred earlier, mainly in the week of November 6-13.\nThis pattern is consistent with 2022, where a higher percentage of DDoS attacks was observed before November 21.\nGoing back to 2023, in terms of potential blocked attacks targeting the \"Shopping & General Merchandise\" industry, a similar pre-Black Friday week trend is evident. Here we’re including both DDoS and attacks blocked by the Managed Ruleset enforced by Cloudflare’s Web Application Firewall (WAF), and it’s a global perspective. The peak of 7.3 billion daily HTTP requests blocked by our WAF occurred during the weekend before Thanksgiving (November 18), coinciding with early Black Friday promotions.\nConclusion\nThe trends in Internet traffic during events like Black Friday and Cyber Monday highlight a complex pattern of behavior globally and regionally. Cyber Monday leads the way in Internet traffic, closely followed by Black Friday. The trends in the US and UK are similar, but other countries like Germany and France show distinct patterns. The period before Black Friday also gained traction in terms of Internet and e-commerce activity in some countries.\nThe shift towards mobile usage on Black Friday and desktop dominance on Cyber Monday (in the US) suggests different consumer behaviors, with e-commerce sites experiencing significant DNS traffic increase during these peak shopping periods.\nIn terms of cybersecurity, while attacks are constant, we observed a lower incidence of DDoS attacks during the Black Friday week in 2023, but there was a clear increase in the two weeks leading up to the most shopping-intense period.\nAnd finally — don't forget, you can check Cloudflare Radar to track global and country-specific Internet traffic trends.\n​​Happy Holidays from everyone at Cloudflare!\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nCloudflare Radar Trends DDoS eCommerce",
      "markdown": "11/28/2023\n\n*   [![João Tomé](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/V0x3WKfJ_400x400-1.jpeg)](https://blog.cloudflare.com/author/joao-tome/)\n\n8 min read\n\n![](https://blog.cloudflare.com/content/images/2023/11/Hero.png)\n\nThroughout the year, special events lead to changes in Internet traffic. We observed this with [Thanksgiving](https://blog.cloudflare.com/do-hackers-eat-turkey-and-other-thanksgiving-internet-trends/) in the US last week, where traffic dipped, and during periods like Black Friday (November 24, 2023) and Cyber Monday (November 27, 2023), where traffic spiked.\n\nBut how significant are these Cyber Week days on the Internet? Is it a global phenomenon? Does e-commerce interest peak on Black Friday or Cyber Monday, and are attacks increasing during this time? These questions are important to retailers and stakeholders around the world. At Cloudflare, we manage substantial traffic for our customers, which gives us a unique vantage from which to analyze traffic and attack patterns across large swaths of the Internet.\n\nAs we'll explore next, we observed varying trends. From a global perspective, there was a clear Internet traffic winner: Cyber Monday was the highest overall traffic day of 2023 (as it was for 2022), followed by Black Friday, and then Monday, November 21 from the same week. But zooming in, this pattern didn’t hold in some countries.\n\nFor this analysis, we examined anonymized samples of HTTP requests crossing our network, as well as DNS queries. Cloudflare's global data shows that peak request traffic occurred on Cyber Monday, and that recent weeks have generally been the year’s busiest. Here are some notable figures:\n\n*   Cloudflare processed a peak of 80 million HTTP requests per second at 16:10 UTC on November 27.\n*   The peak hour of 16:00 UTC saw more than 230 billion hourly requests.\n*   Cloudflare powered around 4 trillion daily requests on Cyber Monday (with blocked attacks comprising around 5% of all traffic), a figure only approached by Black Friday, which saw 3.86 trillion requests.\n*   There was a 27% increase in HTTP requests on Cyber Monday 2023 (November 27) as compared to Cyber Monday 2022 (November 28).\n\nWhat about DNS queries?\n\n*   Aggregated from our [1.1.1.1](https://1.1.1.1/) resolver showed that Cyber Monday 2023 experienced a peak of 1.68 trillion queries per day, with 22 million queries per second around 15:00 UTC. Of these DNS queries, 15% were encrypted (HTTPS and TLS). Back in August, the peak was at 1.35 trillion queries per day, marking a 24% increase.\n*   Traffic to our authoritative DNS servers also peaked on Cyber Monday, with 811 billion daily queries and a peak of 9.4 million queries per second around 15:00 UTC.\n*   So, during Cyber Monday, we saw a combined peak of over 100 million requests and queries per second across all Cloudflare services at around 16:00 UTC (November 27).\n\n## Black Friday week Internet traffic daily ranking\n\nThese numbers and trends are consistent with what we observed in 2022 and previous years, where traffic peaks in late November but usually drops in December. Here's a snapshot of global human Internet traffic this year (bot traffic shows a similar pattern).\n\n**Worldwide. Most popular Internet traffic days**\n\n1.  Cyber Monday, November 27\n2.  Black Friday, November 24\n3.  Monday, November 21\n\n![](https://blog.cloudflare.com/content/images/2023/11/Daily-HTTP-requests-worldwide--2023-.png)\n\nFrom the US perspective, the ranking is similar, with Saturday, November 25, the day after Black Friday, ranking as the third busiest day for Internet traffic.\n\n**US. Most popular Internet traffic days**\n\n1.  Cyber Monday, November 27\n2.  Black Friday, November 24\n3.  Saturday, November 25\n\n![](https://blog.cloudflare.com/content/images/2023/11/Daily-HTTP-requests-in-the-US--2023-.png)\n\nAdditionally, most U.S. states show a similar trend, with Cyber Monday experiencing the most traffic, followed by Black Friday. However, Alaska is a notable exception, where the days with the highest Internet traffic were November 13 and 14, coinciding with a [snow emergency that closed schools and roads](https://www.adn.com/alaska-news/weather/2023/11/13/another-heavy-snowfall-buries-anchorage-closing-schools-and-clogging-already-bad-roads/).\n\nStates like Colorado, Hawaii, Idaho, New Mexico, and California also had Saturday, November 25, as their second busiest day, but Cyber Monday also “won” there.\n\n## Does the Black Friday week impact other countries?\n\nInternationally, a trend of peak Internet traffic in November is observed in most countries, as highlighted in our previous [2022 Year in Review](https://radar.cloudflare.com/year-in-review/2022) (stay tuned for our 2023 edition in the next few weeks). This trend is likely linked to colder weather in the Northern Hemisphere, where approximately 87% of the world's population resided in 2023, as well as holidays and shopping periods, among other factors.\n\nHere's a table summarizing the November days with the most traffic, where the Black Friday week plays a significant role.\n\n**Most popular Internet traffic days**\n\n|     |     |\n| --- | --- |\n| UK. <br><br>#1. Black Friday, November 24<br><br>#2. Cyber Monday, November 27<br><br>#3. Sunday, November 20 | Canada. <br><br>#1. Black Friday, November 24<br><br>#2. Cyber Monday, November 27<br><br>#3. Thursday, November 23 |\n| Germany. <br><br>#1. Black Friday, November 24<br><br>#2. Sunday, November 26<br><br>#3. Cyber Monday, November 27 | Mexico. <br><br>#1. Monday, November 21<br><br>#2. Friday, November 17 (one week before Black Friday)<br><br>#3. Black Friday, November 24 |\n| France. <br><br>#1. Cyber Monday, November 27<br><br>#2. Sunday, November 26<br><br>#3. Black Friday, November 24 | Brazil. <br><br>#1. Tuesday, November 22 <br><br>#2. Black Friday, November 24<br><br>#3. Monday, November 21 |\n| Spain. <br><br>#1. Cyber Monday, November 27<br><br>#2. Sunday, November 20<br><br>#3. Monday, November 21 | Australia. <br><br>#1. Black Friday, November 24<br><br>#2. Thursday, November 23<br><br>#3. Sunday, November 20 |\n| Egypt. <br><br>#1. Saturday, November 25<br><br>#2. Sunday, November 26<br><br>#3. Black Friday, November 24 | Singapore. <br><br>#1. Cyber Monday, November 27<br><br>#2. Black Friday, November 24<br><br>#3. Thursday, November 23 |\n| Turkey. <br><br>#1. Sunday, November 26 (Black Friday weekend)<br><br>#2. Saturday, November 25<br><br>#3. Singles Day, November 11 | Philippines. <br><br>#1. Cyber Monday, November 27<br><br>#2. Wednesday, November 22<br><br>#3. Sunday, November 20 |\n\nCountries like India, Japan, South Korea, Thailand, and Indonesia, though they show increased traffic during October and November compared to other months, do not exhibit an obvious increase in traffic during Black Friday week.\n\nSingles' Day (November 11), a popular Asian shopping event, only features in the top three traffic days in Turkey. In China, October saw bigger traffic peaks than November. However, in November, both Black Friday and the following day (November 25) showed clear increases in traffic, similar to Singles' Day. In South Africa, Singles' Day and Black Friday were the busiest traffic days in November, even though October also had higher peaks.\n\n## Black Friday goes mobile, Cyber Monday goes desktop\n\nWe observed [last week](https://blog.cloudflare.com/do-hackers-eat-turkey-and-other-thanksgiving-internet-trends/) that during Thanksgiving Day, mobile use in US Internet traffic was higher than in the previous week. This trend was intensified on Black Friday, peaking at 55.3% of all traffic, surpassing the typical weekend, which usually sees a higher mobile usage percentage. However, on Cyber Monday, desktop use took the lead, with the percentage of mobile device traffic dropping to 47.6%, lower than the previous Monday.\n\n![](https://blog.cloudflare.com/content/images/2023/11/Daily-aggregated-requests-in-US--by-platform-.png)\n\nThis trend seems to suggest that Black Friday shopping might involve more offline activities, with people in the US using their mobile devices more for Internet access on that day.\n\n## E-commerce DNS trends\n\nUsing aggregated data from our [1.1.1.1](https://1.1.1.1/) resolver, we have a more focused, category-specific view of the DNS traffic growth to e-commerce sites. There's a general rising trend throughout November, very similar to what we observed in the Internet traffic section.\n\nLooking more closely at the US aggregated e-commerce sites, it's evident that Cyber Monday and Black Friday, in that order, were the days with the most DNS traffic, with Saturday, November 25, ranking third on the podium — exactly mirroring the HTTP traffic pattern discussed earlier.\n\n![](https://blog.cloudflare.com/content/images/2023/11/DNS-traffic-to-E-commerce-domains-by-country.png)\n\nThe peak hours of DNS traffic on Black Friday were around 16:00 and 17:00 UTC, which correspond to 12:00 and 13:00 EST and 09:00 and 10:00 PST. The same pattern was observed on Cyber Monday.\n\nDuring Cyber Week (November 20 to 27), there was a 15% increase in DNS traffic compared with the previous week. A consistently high level of DNS traffic was maintained throughout Black Friday week, starting on Monday, November 20, with the sole exception being a noticeable drop on Thanksgiving Day — DNS traffic to e-commerce sites was 6% lower than the previous week on that day.\n\n## A glimpse into Europe’s e-commerce trends\n\nThe UK shows a very similar trend to the US in terms of Black Friday and Cyber Monday interest. However, in 2023, Black Friday and Cyber Monday are tied for the top spot, followed by Tuesday, November 21.\n\n![](https://blog.cloudflare.com/content/images/2023/11/DNS-traffic-to-E-commerce-domains-by-country-NOERROR.png)\n\nIn Australia, Cyber Monday ranked as the most popular day for e-commerce DNS traffic, followed by Black Friday. Canada showed a similar pattern, with Black Friday being the most trafficked day, followed by Cyber Monday.\n\nIn Germany, Black Friday indisputably led in e-commerce DNS traffic, followed by the previous Friday, November 17, and then the Black Friday weekend. Cyber Monday did not make it to the top three in Germany.\n\nIn France, Black Friday was the most popular e-commerce day, followed by Saturday, November 18.\n\n## Electronics, fast fashion, and second-hand trends\n\nFocusing on the US only again, electronics e-commerce sites experienced more DNS traffic on Black Friday than on Cyber Monday.\n\n![](https://blog.cloudflare.com/content/images/2023/11/DNS-traffic-to-Electronics-e-commerce-domains-by-country.png)\n\nThis trend was mirrored in the fast fashion category, with Black Friday clearly in the lead.\n\n![](https://blog.cloudflare.com/content/images/2023/11/DNS-traffic-to-Fast-fashion-domains-by-country.png)\n\nIt's perhaps unsurprising that second-hand shopping sites in the US gained more momentum and DNS traffic in the preceding week (November 12-18) leading up to Black Friday. However, these sites then reached their peak on Cyber Monday.\n\n![](https://blog.cloudflare.com/content/images/2023/11/DNS-traffic-to-Second-hand-shopping-domains-by-country.png)\n\n## How about cyber threats?\n\nRegarding cyber threats, let's focus on DDoS (distributed denial-of-service) attacks, a popular method for disrupting Internet properties. Data from November 2023 shows that on [Thanksgiving](https://blog.cloudflare.com/do-hackers-eat-turkey-and-other-thanksgiving-internet-trends/), DDoS attacks accounted for the lowest daily fraction of traffic volume observed in the month of November across the US. There were higher percentages of DDoS attacks in late August and September, associated with the HTTP/2 Zero-Day vulnerability, which led to [record-breaking attacks](https://blog.cloudflare.com/zero-day-rapid-reset-http2-record-breaking-ddos-attack/).\n\nThe Black Friday week was not a peak period for DDoS attacks. The highest activity in November occurred earlier, mainly in the week of November 6-13.\n\n![](https://blog.cloudflare.com/content/images/2023/11/Percentage-of-traffic-that-was-DDoS-mitigated-in-the-US.png)\n\nThis pattern is consistent with 2022, where a higher percentage of DDoS attacks was observed before November 21.\n\n![](https://blog.cloudflare.com/content/images/2023/11/Percentage-of-traffic-that-was-DDoS-mitigated-in-the-US--2022-data-.png)\n\nGoing back to 2023, in terms of potential blocked attacks targeting the \"Shopping & General Merchandise\" industry, a similar pre-Black Friday week trend is evident. Here we’re including both DDoS and attacks blocked by the Managed Ruleset enforced by Cloudflare’s Web Application Firewall (WAF), and it’s a global perspective. The peak of 7.3 billion daily HTTP requests blocked by our WAF occurred during the weekend before Thanksgiving (November 18), coinciding with early Black Friday promotions.\n\n![](https://blog.cloudflare.com/content/images/2023/11/Daily-aggregations-of-mitigations-to-Shopping---General-Merchandise-domains.png)\n\n## Conclusion\n\nThe trends in Internet traffic during events like Black Friday and Cyber Monday highlight a complex pattern of behavior globally and regionally. Cyber Monday leads the way in Internet traffic, closely followed by Black Friday. The trends in the US and UK are similar, but other countries like Germany and France show distinct patterns. The period before Black Friday also gained traction in terms of Internet and e-commerce activity in some countries.\n\nThe shift towards mobile usage on Black Friday and desktop dominance on Cyber Monday (in the US) suggests different consumer behaviors, with e-commerce sites experiencing significant DNS traffic increase during these peak shopping periods.\n\nIn terms of cybersecurity, while attacks are constant, we observed a lower incidence of DDoS attacks during the Black Friday week in 2023, but there was a clear increase in the two weeks leading up to the most shopping-intense period.\n\nAnd finally — don't forget, you can check [Cloudflare Radar](https://radar.cloudflare.com/) to track global and country-specific Internet traffic trends.\n\n​​Happy Holidays from everyone at Cloudflare!\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Cloudflare Radar](https://blog.cloudflare.com/tag/cloudflare-radar/) [Trends](https://blog.cloudflare.com/tag/trends/) [DDoS](https://blog.cloudflare.com/tag/ddos/) [eCommerce](https://blog.cloudflare.com/tag/ecommerce/)"
    },
    {
      "url": "https://blog.cloudflare.com/search/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/search/",
        "loadedTime": "2023-12-05T02:26:54.459Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/search/",
        "title": "The Cloudflare Blog",
        "description": null,
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Subscribe to receive notifications of new posts:\nSubscription confirmed. Thank you for subscribing!",
      "markdown": "Subscribe to receive notifications of new posts:\n\nSubscription confirmed. Thank you for subscribing!"
    },
    {
      "url": "https://blog.cloudflare.com/forrester-wave-edge-development-2023/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/forrester-wave-edge-development-2023/",
        "loadedTime": "2023-12-05T02:27:04.834Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/forrester-wave-edge-development-2023/",
        "title": "Cloudflare named a leader in Forrester Edge Development Platforms Wave, Q4 2023",
        "description": "Forrester has recognized Cloudflare as a leader in The Forrester Wave™: Edge Development Platforms, Q4 2023 with the top score in the current offering category.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/27/2023\n4 min read\nThis post is also available in 简体中文, 繁體中文, 日本語, 한국어, Deutsch, Français, Español.\nForrester has recognized Cloudflare as a leader in The Forrester Wave™: Edge Development Platforms, Q4 2023 with the top score in the current offering category. \nAccording to the report by Principal Analyst, Devin Dickerson, “Cloudflare’s edge development platform provides the building blocks enterprises need to create full stack distributed applications and enables developers to take advantage of a globally distributed network of compute, storage and programmable security without being experts on CAP theorem.“\nOver one million developers are building applications using the Developer Platform products including Workers, Pages, R2, KV, Queues, Durable Objects, D1, Stream, Images, and more. Developers can easily deploy highly distributed, full-stack applications using Cloudflare’s full suite of compute, storage, and developer services.\nWorkers make Cloudflare’s network programmable\n“ A key strength of the platform is the interoperability with Cloudflare’s programmable global CDN combined with a deployment model that leverages intelligent workload placement.” \n– The Forrester Wave™: Edge Development Platforms, Q4 2023\nWorkers run across Cloudflare’s global network, provide APIs to read from and write directly to the local cache, and expose context from Cloudflare’s CDN directly on the request object that a Worker receives.\nThis close integration with Cloudflare’s network allows developers to build, protect, and connect globally distributed applications, without deploying to specific regions. Smart Placement optimizes Workers to run in the location that yields the fastest overall performance, whether it's the location closest to the data, or closest to the user. Hyperdrive automatically pools database connections, allowing Workers running all over the world to reuse them when querying PostgreSQL databases, avoiding the scaling challenges that make it hard to use traditional databases with a serverless architecture. Cron Triggers allow for up to 15 minutes of CPU time, allowing for compute intensive background work.\nCloudflare is beyond edge computing — it’s everywhere computing. We use our network to make your application perform best, shaped by real-world data and tailored to access patterns and programming paradigms.\nDeploy distributed systems, without being a distributed systems expert\n“ Reference customers consistently call out the ease of onboarding, which sees developers with no prior background delivering workloads across the globe in minutes, and production quality applications within a week. “ \n– The Forrester Wave™: Edge Development Platforms, Q4 2023\nWorkers empower any developer to deploy globally distributed applications, without needing to become distributed systems experts or experts in configuring cloud infrastructure.\nWhen you deploy a Worker, behind the scenes Cloudflare distributes it across the globe. But to you, it’s a single application that you can run and test locally, using the same open-source JavaScript runtime that your Workers run on in production.\nWhen you deploy a Durable Object to coordinate real-time state, you’ve built a distributed application, but instead of having to learn RPC protocols and scale infrastructure, you’ve programmed the whole thing in JavaScript using web standard APIs that front-end developers know and rely on daily.\nEnqueuing and processing batches of messages with Cloudflare Queues takes adding just a few more lines of JavaScript to an existing Worker.\nWhen you create a web application with Cloudflare Pages, you’ve set up a complete continuous build and deployment pipeline with preview URLs, just by connecting to a GitHub repository.\nDevelopers who previously only wrote front-end code are able to build the back end, and make their app real-time and reactive. Teams stuck waiting for infrastructure experts to provision resources are able to start prototyping today rather than next week. Writing and deploying a Worker is familiar and accessible, and this lets engineering teams move faster, with less overhead.\nWhy are teams able to get started so quickly?\nWorkers use web standard APIs that front-end developers and anyone building web applications already use every day. Cloudflare was a founding member of the Web-interoperable Runtimes Community Group (WinterCG) and is committed to interoperability across runtimes.\nThe tools developers already use every day are native to our platform. We publish TypeScript types for all APIs, and support compiling TypeScript when authoring and deploying via the Wrangler CLI or via the code editor in the Cloudflare dashboard — which itself is powered by the popular VSCode editor.\nThe open-source frameworks that developers prefer to build with are supported. A growing set of APIs from Node.js are available natively in the Workers runtime, allowing existing open source libraries to work on Workers. And increasingly, new open source projects that developers depend on are designed from day one to work across all WinterCG runtimes. Every day, more of the JavaScript ecosystem works on Workers.\nExpanding into AI with GPUs, LLMs, and more\n“Its superior vision refuses to limit the future footprint to the edge, and their purposeful approach to building out capabilities on the roadmap suggests that it will be increasingly well positioned to take on public cloud hyperscalers for workloads. “ \n– The Forrester Wave™: Edge Development Platforms, Q4 2023\nWe are building a complete compute platform for production applications at scale. And as every company and every developer is now building or experimenting with AI, Cloudflare has made GPUs an integrated part of our developer platform. We’ve made it just as easy to get started with AI as we have to deliver a global workload. In mid-November, we hit our goal to have Workers AI Inference running in over 100 cities around the world, and by the end of the 2024 Workers AI will be running in nearly every city Cloudflare has a presence in.\nWorkers AI allows developers to build applications using the latest open-source AI models, without provisioning any infrastructure or paying for costly unused capacity. We’re extending this to support deploying models directly from Hugging Face to Workers AI, for an even wider set of AI models. And unlike provisioning a VM with a GPU in a specific data center, we’re building this such that we can treat our whole network as one giant compute resource, running models in the right place at the right time to serve developers’ needs.\nBeyond model inference, we’re doubling down on supporting web standard APIs and making the WebGPU API available from within the Workers platform. While we’re proud to be recognized as a leading edge platform, we’re not just that —we are a platform for developing full-stack applications, even those that require compute power that just one year ago very few used or needed.\nWe’re excited to show you what’s next, including a new way to manage secrets across Cloudflare products, improved observability, and better tools for releasing changes. Every day we see more advanced applications built on our platform, and we’re committed to matching that with tools to serve the most mission-critical workloads — the same ones we use ourselves to build our products on our own platform.\nDownload the report here.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nForrester",
      "markdown": "11/27/2023\n\n*   [![Brendan Irvine-Broque](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/09/IMG_2312.JPG)](https://blog.cloudflare.com/author/brendan-irvine-broque/)\n*   [![Dawn Parzych](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/04/Untitled---1-of-1.jpeg)](https://blog.cloudflare.com/author/dawn/)\n\n4 min read\n\nThis post is also available in [简体中文](https://blog.cloudflare.com/zh-cn/forrester-wave-edge-development-2023-zh-cn/), [繁體中文](https://blog.cloudflare.com/zh-tw/forrester-wave-edge-development-2023-zh-tw/), [日本語](https://blog.cloudflare.com/ja-jp/forrester-wave-edge-development-2023-ja-jp/), [한국어](https://blog.cloudflare.com/ko-kr/forrester-wave-edge-development-2023-ko-kr/), [Deutsch](https://blog.cloudflare.com/de-de/SLUG-de-de/), [Français](https://blog.cloudflare.com/fr-fr/forrester-wave-edge-development-2023-fr-fr/), [Español](https://blog.cloudflare.com/es-es/forrester-wave-edge-development-2023-es-es/).\n\nForrester has recognized Cloudflare as a leader in The Forrester Wave™: Edge Development Platforms, Q4 2023 with the top score in the current offering category.\n\nAccording to [the report](https://www.cloudflare.com/lp/forrester-wave-edge-development-q4-2023/) by Principal Analyst, Devin Dickerson, “Cloudflare’s edge development platform provides the building blocks enterprises need to create full stack distributed applications and enables developers to take advantage of a globally distributed network of compute, storage and programmable security without being experts on CAP theorem.“\n\nOver one million developers are building applications using the Developer Platform products including [Workers](https://workers.cloudflare.com/), [Pages](https://pages.cloudflare.com/), [R2](https://developers.cloudflare.com/r2/), [KV](https://developers.cloudflare.com/kv/), [Queues](https://developers.cloudflare.com/queues/), [Durable Objects](https://developers.cloudflare.com/durable-objects/), [D1](https://developers.cloudflare.com/d1/), [Stream](https://developers.cloudflare.com/stream/), [Images](https://developers.cloudflare.com/images/), and more. Developers can easily deploy highly distributed, full-stack applications using Cloudflare’s full suite of compute, storage, and developer services.\n\n### Workers make Cloudflare’s network programmable\n\n> _**“ A key strength of the platform is the interoperability with Cloudflare’s programmable global CDN combined with a deployment model that leverages intelligent workload placement.”**_  \n> _**–**_ **The Forrester Wave™: Edge Development Platforms, Q4 2023**\n\nWorkers run across [Cloudflare’s global network](https://www.cloudflare.com/network/), provide [APIs](https://developers.cloudflare.com/workers/runtime-apis/cache/) to read from and write directly to the local cache, and expose context from Cloudflare’s CDN directly on the [request object](https://developers.cloudflare.com/workers/runtime-apis/request/#incomingrequestcfproperties) that a Worker receives.\n\nThis close integration with Cloudflare’s network allows developers to build, protect, and connect globally distributed applications, without deploying to specific regions. [Smart Placement](https://developers.cloudflare.com/workers/configuration/smart-placement/) optimizes Workers to run in the location that yields the fastest overall performance, whether it's the location closest to the data, or closest to the user. [Hyperdrive](https://developers.cloudflare.com/hyperdrive/learning/how-hyperdrive-works/) automatically pools database connections, allowing Workers running all over the world to reuse them when querying PostgreSQL databases, avoiding the scaling challenges that make it hard to use traditional databases with a serverless architecture. [Cron Triggers](https://developers.cloudflare.com/workers/configuration/cron-triggers/) allow for up to 15 minutes of CPU time, allowing for compute intensive background work.\n\nCloudflare is beyond edge computing — it’s everywhere computing. We use our network to make your application perform best, shaped by real-world data and tailored to access patterns and programming paradigms.\n\n### Deploy distributed systems, without being a distributed systems expert\n\n> _**“ Reference customers consistently call out the ease of onboarding, which sees developers with no prior background delivering workloads across the globe in minutes, and production quality applications within a week. “**_  \n> _**–**_ **The Forrester Wave™: Edge Development Platforms, Q4 2023**\n\nWorkers empower any developer to deploy globally distributed applications, without needing to become distributed systems experts or experts in configuring cloud infrastructure.\n\n*   When you deploy a Worker, behind the scenes Cloudflare distributes it across the globe. But to you, it’s a single application that you can [run and test locally](https://developers.cloudflare.com/workers/observability/local-development-and-testing/), using the same [open-source JavaScript runtime](https://github.com/cloudflare/workerd) that your Workers run on in production.\n*   When you deploy a [Durable Object](https://developers.cloudflare.com/durable-objects/) to coordinate real-time state, you’ve built a distributed application, but instead of having to learn RPC protocols and scale infrastructure, you’ve programmed the whole thing in JavaScript using web standard APIs that front-end developers know and rely on daily.\n*   Enqueuing and processing batches of messages with [Cloudflare Queues](https://developers.cloudflare.com/queues/) takes adding just a few more lines of JavaScript to an existing Worker.\n*   When you create a web application with [Cloudflare Pages](https://pages.cloudflare.com/), you’ve set up a complete continuous build and deployment pipeline with preview URLs, just by connecting to a GitHub repository.\n\nDevelopers who previously only wrote front-end code are able to build the back end, and make their app real-time and reactive. Teams stuck waiting for infrastructure experts to provision resources are able to start prototyping today rather than next week. Writing and deploying a Worker is familiar and accessible, and this lets engineering teams move faster, with less overhead.\n\nWhy are teams able to get started so quickly?\n\n1.  Workers use [web standard APIs](https://developers.cloudflare.com/workers/runtime-apis/) that front-end developers and anyone building web applications already use every day. Cloudflare was a founding member of the Web-interoperable Runtimes Community Group ([WinterCG](https://wintercg.org/)) and is committed to [interoperability across runtimes](https://blog.cloudflare.com/socket-api-works-javascript-runtimes-wintercg-polyfill-connect/).\n2.  The tools developers already use every day are native to our platform. We publish [TypeScript](https://www.npmjs.com/package/@cloudflare/workers-types) types for all APIs, and support compiling TypeScript when authoring and deploying via the Wrangler CLI or via the code editor in the Cloudflare dashboard — which itself is [powered by](https://blog.cloudflare.com/improved-quick-edit/) the popular VSCode editor.\n3.  The open-source [frameworks](https://developers.cloudflare.com/pages/framework-guides/) that developers prefer to build with are supported. A growing set of APIs from Node.js are [available natively](https://developers.cloudflare.com/workers/runtime-apis/nodejs/) in the Workers runtime, allowing existing open source libraries to work on Workers. And increasingly, new open source projects that developers depend on are designed from day one to work across all WinterCG runtimes. Every day, more of the JavaScript ecosystem works on Workers.\n\n### Expanding into AI with GPUs, LLMs, and more\n\n> _**“Its superior vision refuses to limit the future footprint to the edge, and their purposeful approach to building out capabilities on the roadmap suggests that it will be increasingly well positioned to take on public cloud hyperscalers for workloads. “**_  \n> _**–**_ **The Forrester Wave™: Edge Development Platforms, Q4 2023**\n\nWe are building a complete compute platform for production applications at scale. And as every company and every developer is now building or experimenting with AI, Cloudflare has [made GPUs an integrated part of our developer platform](https://blog.cloudflare.com/workers-ai). We’ve made it just as easy to get started with AI as we have to deliver a global workload. In mid-November, we hit our goal to have Workers AI Inference running in [over 100 cities](https://blog.cloudflare.com/workers-ai-update-stable-diffusion-code-llama-workers-ai-in-100-cities/) around the world, and by the end of the 2024 Workers AI will be running in nearly every city Cloudflare has a presence in.\n\n[Workers AI](https://developers.cloudflare.com/workers-ai/) allows developers to build applications using the latest open-source AI models, without provisioning any infrastructure or paying for costly unused capacity. We’re extending this to support [deploying models directly from Hugging Face to Workers AI](https://blog.cloudflare.com/partnering-with-hugging-face-deploying-ai-easier-affordable/), for an even wider set of AI models. And unlike provisioning a VM with a GPU in a specific data center, we’re building this such that we can treat our whole network as one giant compute resource, running models in the right place at the right time to serve developers’ needs.\n\nBeyond model inference, we’re doubling down on supporting web standard APIs and making the [WebGPU](https://blog.cloudflare.com/webgpu-in-workers/) API available from within the Workers platform. While we’re proud to be recognized as a leading edge platform, we’re not just that —we are a platform for developing full-stack applications, even those that require compute power that just one year ago very few used or needed.\n\nWe’re excited to show you what’s next, including [a new way to manage secrets](https://blog.cloudflare.com/secrets-store/) across Cloudflare products, improved observability, and better tools for releasing changes. Every day we see more advanced applications built on our platform, and we’re committed to matching that with tools to serve the most mission-critical workloads — the same ones we use ourselves to build our products on our own platform.\n\nDownload the report [here](https://www.cloudflare.com/lp/forrester-wave-edge-development-q4-2023/).\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Forrester](https://blog.cloudflare.com/tag/forrester/)"
    },
    {
      "url": "https://blog.cloudflare.com/debugging-cloudflare-workers/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/debugging-cloudflare-workers/",
        "loadedTime": "2023-12-05T02:27:10.188Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/debugging-cloudflare-workers/",
        "title": "Better debugging for Cloudflare Workers, now with breakpoints",
        "description": "We provide many tools to help you debug Cloudflare Workers; from your local environment all the way into production. In this post, we highlight some of the tools we currently offer, and do a deep dive into one specific area - breakpoint debugging - a tool we recently added into our workerd runtime.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/28/2023\n9 min read\nAs developers, we’ve all experienced times when our code doesn’t work like we expect it to. Whatever the root cause is, being able to quickly dive in, diagnose the problem, and ship a fix is invaluable. \nIf you’re developing with Cloudflare Workers, we provide many tools to help you debug your applications; from your local environment all the way into production. This additional insight helps save you time and resources and provides visibility into how your application actually works — which can help you optimize and refactor code even before it goes live.\nIn this post, we’ll explore some of the tools we currently offer, and do a deep dive into one specific area — breakpoint debugging — looking at not only how to use it, but how we recently implemented it in our runtime, workerd.\nLogs\nconsole.log. It might be the simplest tool for a developer to debug, but don’t underestimate it. Built into the Cloudflare runtime is node-like logging, which provides detailed, color-coded logs. Locally, you can view these logs in a terminal window, and they will look like this:\nOutside local development, once your Worker is deployed, console.log statements are visible via the Real-time Logs interface in the Cloudflare Dashboard or via the Workers CLI tool, Wrangler, using the wrangler tail command. Each log that comes through wrangler tail is structured JSON, and the command has options to filter and search incoming logs to make results as relevant as possible.\nIf you’d like to send these logs to third-parties for processing and storage, you can leverage Workers Trace Events Logpush which supports a variety of destinations.\nDevTools\nIn addition to logging, you can also leverage our implementation of Chrome’s DevTools to do things like view and debug network requests, take memory heap snapshots, and monitor CPU usage.\nThis interactive tool provides even further insight and information about your Cloudflare Workers, and can be started from within Wrangler by running wrangler dev and pressing [d] once the dev server is spun up. It can also be accessed by the editor that is built into the Cloudflare Dashboard or the Workers Playground.\nBreakpoints\nBreakpoints allow developers to stop code execution at specific points (lines) to evaluate what is happening. This is great for situations where you might have a race condition or times when you don’t know exactly what is happening, and your code isn’t behaving as expected. Breakpoints allow you to walk through your code line by line to see how it behaves.\nYou can get started with breakpoint debugging from within the Wrangler CLI by running wrangler dev and pressing [d] to open up a DevTools debugger session. If you prefer to debug via your IDE, we support VSCode and WebStorm.\nSetting up VSCode\nTo set up VSCode to debug Cloudflare Workers with breakpoints, you’ll need to create a new .vscode/launch.json file with the following content:\n{ \"configurations\": [ { \"name\": \"Wrangler\", \"type\": \"node\", \"request\": \"attach\", \"port\": 9229, \"cwd\": \"/\", \"resolveSourceMapLocations\": null, \"attachExistingChildren\": false, \"autoAttachChildProcesses\": false } ] } \nOnce you’ve created this configuration in launch.json, open your project in VSCode. Open a new terminal window from VSCode, and run npx wrangler dev to start a local dev server.\nAt the top of the Run & Debug panel, you should see an option to select a configuration. Choose Wrangler, and select the play icon. You should see Wrangler: Remote Process [0] show up in the Call Stack panel on the left. Go back to a .js or .ts file in your project and add at least one breakpoint.\nOpen your browser and go to the Worker’s local URL (default http://127.0.0.1:8787). The breakpoint should be hit, and you should see details about your code at the specified line.\nSetting up WebStorm\nTo set up WebStorm with breakpoint debugging, create a new “Attach to Node.js/Chrome” Debug Configuration, setting the port to 9229:\nRun npx wrangler dev to start a local dev server, then start the Debug Configuration:\nAdd a breakpoint, then open your browser and go to the Worker’s local URL (default http://127.0.0.1:8787). The breakpoint should be hit, and you should see details about your code at the specified line.\nHow we enabled breakpoint debugging via workerd\nBoth workerd and Cloudflare Workers embed V8 to run workers code written in JavaScript and WASM. V8 is a component of the world’s most widely used web browser today, Google Chrome, and it is also widely used embedded into open source projects like Node.js.\nThe Google Chrome team has created a set of web developer tools, Chrome DevTools, that are built directly into the browser. These provide a wide range of features for inspecting, debugging, editing, and optimizing web pages. Chrome DevTools are exposed through a UI in Chrome that talks to the components of the browser, such as V8, using the Chrome DevTools Protocol (CDP). The protocol uses JSON-RPC transmitted over a websocket to exchange messages and notifications between clients, like the DevTools UI, and the components of Chrome. Within Chrome Devtools protocols are domains (DOM, Debugger, Media) that group related commands by functionality that can be implemented by different components in Chrome.\nV8 supports the following CDP domains:\nRuntime\nDebugger\nProfiler\nHeapProfiler\nThese domains are available to all projects that embed V8, including workerd, so long as the embedding application is able to route messages between a DevTools client and V8. DevTools clients use the Debugger domain to implement debugging functionality. The Debugger domain exposes all the commands to debug an application, such as setting breakpoints. It also sends debugger events, like hitting a breakpoint, up to DevTools clients, so they can present the state of the script in a debugger UI.\nWhile workerd has supported CDP since its first release, support for the Debugger domain is new. The Debugger domain differs from the other domains exposed by V8 because it requires the ability to suspend the execution of a script whilst it is being debugged. This presents a complication for introducing breakpoint debugging in workerd, because workerd runs each Worker in a V8 isolate in which there is just a single thread that receives incoming requests and runs the scripts associated with them.\nWhy is this a problem? Workerd uses an event-driven programming model and its single thread is responsible for both responding to incoming requests and for running JavaScript / WASM code. In practice, this is implemented via an event loop that sits at the bottom of the call stack that sends and receives network messages and calls event handlers that run JavaScript code. The thread needs to fall back into the event loop after running event handlers to be able to process network messages. However, the V8 API for handling breakpoints expects execution to be suspended within a method implemented by the embedder that is called from V8 when a breakpoint is hit. This method is called from the event handler that is running JavaScript in V8. Unfortunately, this prevents the workerd thread from falling back into the event loop and processing any incoming network events, including all CDP commands relating to debugging. So if a client asks to resume execution by sending a CDP command, it cannot be relayed to the executing thread because it is unable to fall into the eventloop whilst in a breakpoint.\nWe solved this event processing problem by adding an I/O thread to workerd. The I/O thread handles sending and receiving CDP messages, because the thread executing JavaScript can be suspended due to hitting a breakpoint or a JavaScript `debugger` statement. The I/O thread wakes the JavaScript thread when CDP commands arrive and also handles sending responses back to the CDP client. Conceptually, this was not difficult, but it required some careful synchronization to avoid dropped messages.\nUse the Source\nWhen debugging, JavaScript developers expect to see their source code in the debugger. For this to work, the embedded V8 needs to be able to locate sources. It is common for JavaScript code to be generated either by combining and minifying multiple JavaScript sources, or by transpiling to JavaScript from another language, such as TypeScript, Dart, CoffeeScript, or Elm. To render the source code in the debugger in its original form, the embedded V8 needs to know 1) where the source code came from and 2) how any given line of JavaScript visible to V8 maps back to the original sources before any transformation of the original sources was applied. The standard solution to this problem is to embed a source map into the JavaScript code that JavaScript engine runs. The embedding is performed through a special comment in the JavaScript running in the JavaScript engine:\n//# sourceMappingURL=generated.js.map\nThis source map’s URL is resolved relative to the source URL. This can be set when instantiating a source file with the V8 API, or via another special comment:\n//# sourceURL=file:///absolute/path/to/generated.js\nAn example source map looks something like this:\n{ \"version\": 3, \"sources\": [\"../src/index.ts\"], \"sourcesContent\": [\"interface Env { ... }\\n\\nexport default ...\"], \"mappings\": \";AAIA,IAAO,mBAA8B;AAAA,EACjC,MAAM,MAAM,SAAS,KAAK,KAAK;...\", \"names\": [] } \nEach of the relative paths in sources are resolved relative to the source map’s fully-qualified URL. When DevTools connects to V8 and enables the Debugger domain, V8 will send information on all parsed scripts including the source map’s fully-qualified URL. In our example, this would be file:///absolute/path/to/generated.js.map. DevTools needs to fetch this URL along with source URLs to perform source mapping. Unfortunately, our patched version of DevTools is hosted at https://devtools.devprod.cloudflare.dev/, and browsers prohibit fetching file:// URLs from non-file:// origins for security reasons. However, we need to use file:// URLs so IDEs like Visual Studio Code can match up source files from source maps to files on disk. To get around this, we used Wrangler's inspector proxy to rewrite the CDP script parsed messages sent by V8 to use a different protocol if the User-Agent of the inspector WebSocket handshake is a browser.\nNow that we can set breakpoints and fetch source maps, DevTools works as normal. When a user tries to set a breakpoint in an original source file, DevTools will use the map’s mappings to find the location in the generated JavaScript file and set a breakpoint there. This is the opposite problem to source mapping error stack traces. When V8 hits this JavaScript breakpoint, DevTools will pause on the location in the original source file. Stepping through the source file requires mapping the stepped-over segment to generated code, sending the step-over command to V8, then mapping back the new paused location in generated code to the original source file.\n(sequence diagram showing process of setting, hitting, and stepping over breakpoints) (mermaid URL)\nFuture Work\nBoth the Visual Studio Code and WebStorm configurations for breakpoint debugging require attaching to an existing dev server. It would be great if your IDE could launch the dev server too, and automatically attach to it.\nWhen you debug a Node.js program in Visual Studio Code or WebStorm, an additional --require hook is added to the NODE_OPTIONS environment variable. This hook registers the process’s inspector URL with the editor over a well-known socket. This means if your Node.js process spawns another Node.js child process, your editor will debug that child process too. This is how Visual Studio Code’s JavaScript Debug Terminal works, and is how editors can debug Node.js processes started by npm scripts.\nOur plan is to detect this --require hook, and register workerd child processes started by Wrangler and Miniflare. This will mean you can debug npm launch tasks, without having to worry about starting the dev server and then attaching to it.\nStart debugging!\nAll the debugging tools listed above are ready to be used today. Logs and DevTools can be accessed either by logging into the Cloudflare dashboard or by downloading Wrangler, the command-line tool for the Cloudflare Developer Platform. Breakpoint debugging and Node-style logging is built into the latest version of Wrangler, and can be accessed by running npx wrangler@latest dev in a terminal window. Let us know what you think in the #wrangler channel on the Cloudflare Developers Discord, and please open a GitHub issue if you hit any unexpected behavior.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nCloudflare Workers Developers Debugging",
      "markdown": "11/28/2023\n\n*   [![Adam Murray](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/04/adam_headshot-1.jpg)](https://blog.cloudflare.com/author/adam-murray/)\n*   [![Brendan Coll](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/11/brendan-coll.png)](https://blog.cloudflare.com/author/brendan-coll/)\n\n9 min read\n\n![Better debugging for Cloudflare Workers, now with breakpoints](https://blog.cloudflare.com/content/images/2023/11/Debugging-1.png)\n\nAs developers, we’ve all experienced times when our code doesn’t work like we expect it to. Whatever the root cause is, being able to quickly dive in, diagnose the problem, and ship a fix is invaluable.\n\nIf you’re developing with Cloudflare Workers, we provide many tools to help you debug your applications; from your local environment all the way into production. This additional insight helps save you time and resources and provides visibility into how your application actually works — which can help you optimize and refactor code even before it goes live.\n\nIn this post, we’ll explore some of the tools we currently offer, and do a deep dive into one specific area — breakpoint debugging — looking at not only how to use it, but how we recently implemented it in our runtime, [workerd](https://github.com/cloudflare/workerd).\n\n### Logs\n\n`console.log`. It might be the simplest tool for a developer to debug, but don’t underestimate it. Built into the Cloudflare runtime is node-like logging, which provides detailed, color-coded logs. Locally, you can view these logs in a terminal window, and they will look like this:\n\n![](https://blog.cloudflare.com/content/images/2023/11/image5-2.png)\n\nOutside local development, once your Worker is deployed, `console.log` statements are visible via the Real-time Logs interface in the Cloudflare Dashboard or via the Workers CLI tool, [Wrangler](https://developers.cloudflare.com/workers/wrangler/install-and-update/), using the [`wrangler tail`](https://developers.cloudflare.com/workers/wrangler/commands/#tail) command. Each log that comes through `wrangler tail` is structured JSON, and the command has options to filter and search incoming logs to make results as relevant as possible.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image2-4.png)\n\nIf you’d like to send these logs to third-parties for processing and storage, you can leverage [Workers Trace Events Logpush](https://developers.cloudflare.com/workers/observability/logpush/) which supports a variety of [destinations](https://developers.cloudflare.com/logs/get-started/enable-destinations/).\n\n### DevTools\n\nIn addition to logging, you can also leverage [our implementation](https://github.com/cloudflare/workers-sdk/tree/main/packages/wrangler-devtools) of [Chrome’s DevTools](https://developer.chrome.com/docs/devtools/overview/) to do things like view and debug network requests, take memory heap snapshots, and monitor CPU usage.\n\nThis interactive tool provides even further insight and information about your Cloudflare Workers, and can be started from within Wrangler by running [`wrangler dev`](https://developers.cloudflare.com/workers/wrangler/commands/#dev) and pressing **\\[d\\]** once the dev server is spun up. It can also be accessed by the editor that is built into the [Cloudflare Dashboard](https://dash.cloudflare.com/login?redirect_uri=https%3A%2F%2Fdash.cloudflare.com%2F%3Faccount%3Dworkers) or the [Workers Playground](https://workers.new/).\n\n![](https://blog.cloudflare.com/content/images/2023/11/image6-1.png)\n\n### Breakpoints\n\nBreakpoints allow developers to stop code execution at specific points (lines) to evaluate what is happening. This is great for situations where you might have a race condition or times when you don’t know exactly what is happening, and your code isn’t behaving as expected. Breakpoints allow you to walk through your code line by line to see how it behaves.\n\nYou can get started with breakpoint debugging from within the Wrangler CLI by running [`wrangler dev`](https://developers.cloudflare.com/workers/wrangler/commands/#dev) and pressing **\\[d\\]** to open up a DevTools debugger session. If you prefer to debug via your IDE, we support VSCode and WebStorm.\n\n**Setting up VSCode**  \nTo set up VSCode to debug Cloudflare Workers with breakpoints, you’ll need to create a new `.vscode/launch.json` file with the following content:\n\n```\n{\n  \"configurations\": [\n    {\n  \"name\": \"Wrangler\",\n  \"type\": \"node\",\n  \"request\": \"attach\",\n  \"port\": 9229,\n  \"cwd\": \"/\",\n  \"resolveSourceMapLocations\": null,\n  \"attachExistingChildren\": false,\n  \"autoAttachChildProcesses\": false\n    }\n  ]\n}\n```\n\nOnce you’ve created this configuration in `launch.json`, open your project in VSCode. Open a new terminal window from VSCode, and run `npx wrangler dev` to start a local dev server.\n\nAt the top of the **Run & Debug** panel, you should see an option to select a configuration. Choose **Wrangler**, and select the play icon. You should see **Wrangler: Remote Process \\[0\\]** show up in the Call Stack panel on the left. Go back to a **.js** or **.ts** file in your project and add at least one breakpoint.\n\nOpen your browser and go to the Worker’s local URL (default http://127.0.0.1:8787). The breakpoint should be hit, and you should see details about your code at the specified line.\n\n**Setting up WebStorm**  \nTo set up WebStorm with breakpoint debugging, create a new “Attach to Node.js/Chrome” Debug Configuration, setting the port to `9229`:\n\n![](https://blog.cloudflare.com/content/images/2023/11/image4-2.png)\n\nRun `npx wrangler dev` to start a local dev server, then start the Debug Configuration:\n\n![](https://blog.cloudflare.com/content/images/2023/11/Screenshot-2023-11-28-at-10.07.57.png)\n\nAdd a breakpoint, then open your browser and go to the Worker’s local URL (default http://127.0.0.1:8787). The breakpoint should be hit, and you should see details about your code at the specified line.\n\n## How we enabled breakpoint debugging via workerd\n\nBoth [workerd](https://blog.cloudflare.com/workerd-open-source-workers-runtime/) and Cloudflare Workers embed [V8](https://v8.dev/) to run workers code written in JavaScript and WASM. V8 is a component of the world’s most widely used web browser today, [Google Chrome](https://www.google.com/chrome/), and it is also widely used embedded into open source projects like [Node.js](https://nodejs.org/).\n\nThe Google Chrome team has created a set of web developer tools, [Chrome DevTools](https://developer.chrome.com/docs/devtools/), that are built directly into the browser. These provide a wide range of features for inspecting, debugging, editing, and optimizing web pages. Chrome DevTools are exposed through a UI in Chrome that talks to the components of the browser, such as V8, using the [Chrome DevTools Protocol](https://chromedevtools.github.io/devtools-protocol/) (CDP). The protocol uses JSON-RPC transmitted over a websocket to exchange messages and notifications between clients, like the DevTools UI, and the components of Chrome. Within Chrome Devtools protocols are domains (DOM, Debugger, Media) that group related commands by functionality that can be implemented by different components in Chrome.\n\nV8 supports the following CDP domains:\n\n*   Runtime\n*   Debugger\n*   Profiler\n*   HeapProfiler\n\nThese domains are available to all projects that embed V8, including workerd, so long as the embedding application is able to route messages between a DevTools client and V8. DevTools clients use the Debugger domain to implement debugging functionality. The Debugger domain exposes all the commands to debug an application, such as setting breakpoints. It also sends debugger events, like hitting a breakpoint, up to DevTools clients, so they can present the state of the script in a debugger UI.\n\nWhile workerd has supported CDP since its first release, support for the Debugger domain is new. The Debugger domain differs from the other domains exposed by V8 because it requires the ability to suspend the execution of a script whilst it is being debugged. This presents a complication for introducing breakpoint debugging in workerd, because workerd runs each Worker in a V8 isolate in which there is just a single thread that receives incoming requests and runs the scripts associated with them.\n\nWhy is this a problem? Workerd uses an event-driven programming model and its single thread is responsible for both responding to incoming requests and for running JavaScript / WASM code. In practice, this is implemented via an event loop that sits at the bottom of the call stack that sends and receives network messages and calls event handlers that run JavaScript code. The thread needs to fall back into the event loop after running event handlers to be able to process network messages. However, the V8 API for handling breakpoints expects execution to be suspended within a method implemented by the embedder that is called from V8 when a breakpoint is hit. This method is called from the event handler that is running JavaScript in V8. Unfortunately, this prevents the workerd thread from falling back into the event loop and processing any incoming network events, including all CDP commands relating to debugging. So if a client asks to resume execution by sending a CDP command, it cannot be relayed to the executing thread because it is unable to fall into the eventloop whilst in a breakpoint.\n\nWe solved this event processing problem by adding an I/O thread to workerd. The I/O thread handles sending and receiving CDP messages, because the thread executing JavaScript can be suspended due to hitting a breakpoint or a JavaScript \\`debugger\\` statement. The I/O thread wakes the JavaScript thread when CDP commands arrive and also handles sending responses back to the CDP client. Conceptually, this was not difficult, but it required some careful synchronization to avoid dropped messages.\n\n## Use the Source\n\nWhen debugging, JavaScript developers expect to see their source code in the debugger. For this to work, the embedded V8 needs to be able to locate sources. It is common for JavaScript code to be generated either by combining and minifying multiple JavaScript sources, or by transpiling to JavaScript from another language, such as [TypeScript](https://www.typescriptlang.org/), [Dart](https://dart.dev/), [CoffeeScript](https://coffeescript.org/), or [Elm](https://elm-lang.org/). To render the source code in the debugger in its original form, the embedded V8 needs to know 1) where the source code came from and 2) how any given line of JavaScript visible to V8 maps back to the original sources before any transformation of the original sources was applied. The standard solution to this problem is to embed a [source map](https://firefox-source-docs.mozilla.org/devtools-user/debugger/how_to/use_a_source_map/index.html) into the JavaScript code that JavaScript engine runs. The embedding is performed through a special comment in the JavaScript running in the JavaScript engine:\n\n`//# sourceMappingURL=generated.js.map`\n\nThis source map’s URL is resolved relative to the source URL. This can be set when instantiating a source file with the V8 API, or via another special comment:\n\n`//# sourceURL=file:///absolute/path/to/generated.js`\n\nAn example source map looks something like this:\n\n```\n{\n  \"version\": 3,\n  \"sources\": [\"../src/index.ts\"],\n  \"sourcesContent\": [\"interface Env { ... }\\n\\nexport default ...\"],\n  \"mappings\": \";AAIA,IAAO,mBAA8B;AAAA,EACjC,MAAM,MAAM,SAAS,KAAK,KAAK;...\",\n  \"names\": []\n}\n```\n\nEach of the relative paths in `sources` are resolved relative to the source map’s fully-qualified URL. When DevTools connects to V8 and enables the Debugger domain, V8 will send information on all parsed scripts including the source map’s fully-qualified URL. In our example, this would be `file:///absolute/path/to/generated.js.map`. DevTools needs to fetch this URL along with source URLs to perform source mapping. Unfortunately, our patched version of DevTools is hosted at [https://devtools.devprod.cloudflare.dev/](https://devtools.devprod.cloudflare.dev/), and browsers prohibit fetching `file://` URLs from non-`file://` origins for security reasons. However, we need to use `file://` URLs so IDEs like Visual Studio Code can match up source files from source maps to files on disk. To get around this, we used Wrangler's inspector proxy to rewrite the CDP script parsed messages sent by V8 to use a different protocol if the `User-Agent` of the inspector WebSocket handshake is a browser.\n\nNow that we can set breakpoints and fetch source maps, DevTools works as normal. When a user tries to set a breakpoint in an original source file, DevTools will use the map’s `mappings` to find the location in the generated JavaScript file and set a breakpoint there. This is the opposite problem to source mapping error stack traces. When V8 hits this JavaScript breakpoint, DevTools will pause on the location in the original source file. Stepping through the source file requires mapping the stepped-over segment to generated code, sending the step-over command to V8, then mapping back the new paused location in generated code to the original source file.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image3-5.png)\n\n_(sequence diagram showing process of setting, hitting, and stepping over breakpoints) ([mermaid URL](https://mermaid.live/edit#pako:eNq1VUtvEzEQ_iuWTyClG1DUA6sqh6pCQmpLlRJxqHNw1pONidde_AitVvnvjOPdNgmbEgTsyfbMfN88dxpaGAE0pw6-B9AFXEleWl4xTfDjhTeWTB3YdK-59bKQNdeeXMH6izHK_Sr5arkuVZ_ND2NXYEUSdABn43FnkePjPJQl2Aw0nytImp0YNVuEnCRJez3rx3CFlbW_49aBuJjbcUOcCbaAG15PJ9eIQRdSQT4cZlk2lFrAY_bNZRWvGR0QfCObA36k6bz-I5rkZkt0EYIU4wOK3mTcgo-WmTJctOcJJOzEE6w6DX4ngy8BNMTDo48ADWOMrsE6aXQ85qOtNaOdfWyBPdt78OTSAl_VRmofvXmTEuhd_uHtKQV24F8ALp-mVu3F1FOZGJOSGm5DNY9Q56NjMR7pkl0_GjJ_Zv8UNen7_HyUv8t7iV_NYxJp44FYWS49MYttwmKWtHBkEkfL-VM6tubhuYkKrtRHHEVAigd0ZzYgUbCUO3mLotc9n3Wu4yzLNUcf94awv7UP6oeyFM_d1r1jLeGhJp_X3dz_pvqoHHXbeVnJ-lo67MUHvHj8Y8QC7deabNpqz04veD-3BReqLgwB_yIxk13Ivyjx_yrWsSiZpgNaga24FLgEmvjKqF9CBfgXwKOABQ_KM8r0BlV58Ob-SRc09zbAgIZaIGa7M7pHEBLXxk3aK9v1svkJ1EsiBw))_\n\n## Future Work\n\nBoth the Visual Studio Code and WebStorm configurations for breakpoint debugging require attaching to an _existing_ dev server. It would be great if your IDE could _launch_ the dev server too, and automatically attach to it.\n\nWhen you debug a Node.js program in Visual Studio Code or WebStorm, an additional [`--require`](https://nodejs.org/api/cli.html#-r---require-module) hook is added to the [`NODE_OPTIONS`](https://nodejs.org/api/cli.html#node_optionsoptions) environment variable. This hook registers the process’s inspector URL with the editor over a well-known socket. This means if your Node.js process spawns another Node.js child process, your editor will debug that child process too. This is how Visual Studio Code’s [JavaScript Debug Terminal](https://code.visualstudio.com/docs/nodejs/nodejs-debugging#_javascript-debug-terminal) works, and is how editors can debug Node.js processes started by npm scripts.\n\nOur plan is to detect this `--require` hook, and register `workerd` child processes started by Wrangler and Miniflare. This will mean you can debug `npm` launch tasks, without having to worry about starting the dev server and then attaching to it.\n\n## Start debugging!\n\nAll the debugging tools listed above are ready to be used today. Logs and DevTools can be accessed either by logging into the Cloudflare dashboard or by downloading [Wrangler](https://www.npmjs.com/package/wrangler), the command-line tool for the Cloudflare Developer Platform. Breakpoint debugging and Node-style logging is built into the latest version of Wrangler, and can be accessed by running `npx wrangler@latest dev` in a terminal window. Let us know what you think in the #wrangler channel on the [Cloudflare Developers Discord](https://discord.gg/cloudflaredev), and please [open a GitHub issue](https://github.com/cloudflare/workers-sdk/issues/new/choose) if you hit any unexpected behavior.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Cloudflare Workers](https://blog.cloudflare.com/tag/workers/) [Developers](https://blog.cloudflare.com/tag/developers/) [Debugging](https://blog.cloudflare.com/tag/debugging/)"
    },
    {
      "url": "https://blog.cloudflare.com/steve-bray-why-i-joined-cloudflare/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/steve-bray-why-i-joined-cloudflare/",
        "loadedTime": "2023-12-05T02:27:11.663Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/steve-bray-why-i-joined-cloudflare/",
        "title": "Steve Bray: Why I joined Cloudflare",
        "description": "We're excited to introduce Steve Bray as Cloudflare's new Head of Australia and New Zealand, as we continue to build and grow our customers, partners, and team in the region.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/27/2023\n2 min read\nI am excited to announce that I joined Cloudflare last month as Head of Australia & New Zealand, to continue to build on Cloudflare’s success in the region through extending our valuable relationships with our customers and partners.\nMy journey to Cloudflare\nI’ve been fortunate over my 25-year career in the IT industry to have worked for some of the most recognised and innovative organisations such as Oracle, Salesforce, and Zendesk. It’s been exciting to be inside these businesses as they’ve taken new ideas about how software can be developed and delivered to solve real world problems for any organisation’s customers. I’ve learned a lot by being a part of the industry, but probably more importantly, I’ve learned the most from the smart, experienced, diverse groups of talented people that I’ve had the pleasure to work with in ANZ and across Asia Pacific. I have always been interested in the problems that organisations are trying to solve through technology — for example, responding to strategic challenges, reducing cost, improving revenue, reducing risk — and joining Cloudflare is an opportunity to stay focussed on addressing those critical issues with our customers and partners using Cloudflare’s innovative solutions.\nSo why Cloudflare?\nCloudflare’s mission is to help build a better Internet. So the question I asked myself is, “why is that so important?” In truth, the Internet has become such an integral part of our everyday lives that we take it for granted and forget that we are using it today in ways for which it was not originally designed or architected. It doesn’t have the security, performance, or reliability that is required to ensure the integrity that the modern world expects. Legacy solutions to solve these problems aren’t scalable or cost-effective. This became especially true once the cloud happened.\nThe solution has been a massive shift at the network layer as “as-a-service” delivery architecture evolves. Cloudflare is leading this transition through its extensive global network, providing security and enhancing the performance of business critical applications, while at the same time eliminating the cost and complexity of managing network hardware within a global cloud platform.\nOn this basis I’m convinced that Cloudflare, with its distributed network, continued focus on innovation, as well as our commitment to helping our customers, is in the best position to respond to the challenges being presented.\nA little about me\nI live in Melbourne and enjoy all the wonderful things that the city has to offer, from beautiful parks for some exercise, to its vibrant restaurant and bar scene, to the exciting array of major and global sporting events such as the AFL, Australian Open Tennis and Formula One Grand Prix. I love spending time with family and friends.\nI’m looking forward to working with our customers and partners on their projects to deliver genuine business value. I believe that Cloudflare’s products and solutions will make a real difference to the way organisations in every part of ANZ’s economy can successfully respond to today’s security challenges.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nLife @ Cloudflare Careers APJC \nRelated Posts\nJanuary 30, 2014 2:00PM\nStories from our recent global data center upgrade\nEach day at CloudFlare is full of surprises. As it turns out, it takes a lot of work to stop massive attacks and to help make the web faster. Over the past six months, our entire team has contributed in every way imaginable to more than double the capacity of our global network....\nBy \nAugust 19, 2014 5:32AM\nCloudFlare hiring Go programmers in London and San Francisco\nAre you familiar with the Go programming language and looking for a job in San Francisco or London? Then think about applying to CloudFlare. We're looking for people with experience writing Go in both locations....\nBy \nJuly 30, 2021 2:00PM\nBuilding a sustainable workforce, through communities\nAt Cloudflare, we place a lot of value on the importance of diversity, equity and inclusion. Diversity, equity, and inclusion lead to better outcomes through improved decision-making, more innovative teams, stronger financial returns and simply a better place to work for everyone....\nBy \nOctober 24, 2019 2:00PM\nWho DDoS'd Austin?\nIt was a scorching Monday on July 22 as temperatures soared above 37°C (99°F) in Austin, TX, the live music capital of the world. Only hours earlier, the last crowds dispersed from the historic East 6th Street entertainment district....\nBy",
      "markdown": "11/27/2023\n\n*   [![Steve Bray](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/11/-Steve-Bray.jpeg)](https://blog.cloudflare.com/author/steve-bray/)\n\n2 min read\n\n![Steve Bray: Why I joined Cloudflare.](https://blog.cloudflare.com/content/images/2023/11/image1-8.png)\n\nI am excited to announce that I joined Cloudflare last month as Head of Australia & New Zealand, to continue to build on Cloudflare’s success in the region through extending our valuable relationships with our customers and partners.\n\n### My journey to Cloudflare\n\nI’ve been fortunate over my 25-year career in the IT industry to have worked for some of the most recognised and innovative organisations such as Oracle, Salesforce, and Zendesk. It’s been exciting to be inside these businesses as they’ve taken new ideas about how software can be developed and delivered to solve real world problems for any organisation’s customers. I’ve learned a lot by being a part of the industry, but probably more importantly, I’ve learned the most from the smart, experienced, diverse groups of talented people that I’ve had the pleasure to work with in ANZ and across Asia Pacific. I have always been interested in the problems that organisations are trying to solve through technology — for example, responding to strategic challenges, reducing cost, improving revenue, reducing risk — and joining Cloudflare is an opportunity to stay focussed on addressing those critical issues with our customers and partners using Cloudflare’s innovative solutions.\n\n### So why Cloudflare?\n\nCloudflare’s mission is to help build a better Internet. So the question I asked myself is, “why is that so important?” In truth, the Internet has become such an integral part of our everyday lives that we take it for granted and forget that we are using it today in ways for which it was not originally designed or architected. It doesn’t have the security, performance, or reliability that is required to ensure the integrity that the modern world expects. Legacy solutions to solve these problems aren’t scalable or cost-effective. This became especially true once the cloud happened.\n\nThe solution has been a massive shift at the network layer as “as-a-service” delivery architecture evolves. Cloudflare is leading this transition through its extensive global network, providing security and enhancing the performance of business critical applications, while at the same time eliminating the cost and complexity of managing network hardware within a global cloud platform.\n\nOn this basis I’m convinced that Cloudflare, with its distributed network, continued focus on innovation, as well as our commitment to helping our customers, is in the best position to respond to the challenges being presented.\n\n### A little about me\n\nI live in Melbourne and enjoy all the wonderful things that the city has to offer, from beautiful parks for some exercise, to its vibrant restaurant and bar scene, to the exciting array of major and global sporting events such as the AFL, Australian Open Tennis and Formula One Grand Prix. I love spending time with family and friends.\n\nI’m looking forward to working with our customers and partners on their projects to deliver genuine business value. I believe that Cloudflare’s products and solutions will make a real difference to the way organisations in every part of ANZ’s economy can successfully respond to today’s security challenges.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Life @ Cloudflare](https://blog.cloudflare.com/tag/life-at-cloudflare/) [Careers](https://blog.cloudflare.com/tag/careers/) [APJC](https://blog.cloudflare.com/tag/apjc/)\n\nRelated Posts\n\nJanuary 30, 2014 2:00PM\n\n[\n\n## Stories from our recent global data center upgrade\n\n](https://blog.cloudflare.com/stories-from-our-recent-global-data-center-upgrade/)\n\nEach day at CloudFlare is full of surprises. As it turns out, it takes a lot of work to stop massive attacks and to help make the web faster. Over the past six months, our entire team has contributed in every way imaginable to more than double the capacity of our global network....\n\nBy \n\nAugust 19, 2014 5:32AM\n\n[\n\n## CloudFlare hiring Go programmers in London and San Francisco\n\n](https://blog.cloudflare.com/cloudflare-needs-go-programmers-in-london-and-san-francisco/)\n\nAre you familiar with the Go programming language and looking for a job in San Francisco or London? Then think about applying to CloudFlare. We're looking for people with experience writing Go in both locations....\n\nBy \n\nJuly 30, 2021 2:00PM\n\n[\n\n## Building a sustainable workforce, through communities\n\n](https://blog.cloudflare.com/building-a-sustainable-workforce-through-communities/)\n\nAt Cloudflare, we place a lot of value on the importance of diversity, equity and inclusion. Diversity, equity, and inclusion lead to better outcomes through improved decision-making, more innovative teams, stronger financial returns and simply a better place to work for everyone....\n\nBy \n\nOctober 24, 2019 2:00PM\n\n[\n\n## Who DDoS'd Austin?\n\n](https://blog.cloudflare.com/who-ddosd-austin/)\n\nIt was a scorching Monday on July 22 as temperatures soared above 37°C (99°F) in Austin, TX, the live music capital of the world. Only hours earlier, the last crowds dispersed from the historic East 6th Street entertainment district....\n\nBy"
    },
    {
      "url": "https://blog.cloudflare.com/do-hackers-eat-turkey-and-other-thanksgiving-internet-trends/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/do-hackers-eat-turkey-and-other-thanksgiving-internet-trends/",
        "loadedTime": "2023-12-05T02:27:19.529Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/do-hackers-eat-turkey-and-other-thanksgiving-internet-trends/",
        "title": "Do hackers eat turkey? And other Thanksgiving Internet trends",
        "description": "Offline for turkey time: Which US states logged off on Thanksgiving Day? Is there a difference between coastal and central states? Do hackers take a Thanksgiving break? Are food delivery services gaining or losing traffic? We answer those questions and more.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/24/2023\n8 min read\nThanksgiving is a tradition celebrated by millions of Americans across six time zones and 50 states, usually involving travel and bringing families together. This year, it was celebrated yesterday, on November 23, 2023. With the Internet so deeply enmeshed into our daily lives, anything that changes how so many people behave is going to also have an impact on online traffic. But how big an impact, exactly?\nAt a high level: a 10% daily decrease in Internet traffic in the US (compared to the previous week). That happens to be the exact same percentage decrease we observed in 2022. So, Thanksgiving in the US, at least in the realm of Internet traffic, seems consistent with last year.\nLet’s dig into more details about how people deal with cooking (or online ordering!) and whether family gatherings are less online, according to our Cloudflare Radar data. We’ll also touch on whether hackers stop for turkey, too.\nThe Thanksgiving hour: around 15:00 (local time)\nWhile we can see a 10% overall daily drop in US traffic due to Thanksgiving, the drop is even more noticeable when examining traffic on an hour-by-hour basis. Internet activity began to decrease significantly after 12:00 EST, persisting until 19:00 EST (during those times, it was at least 15% lower compared to the previous week).\nThe peak drop for the entire country occurred around 21:00 UTC, which is 16:00 EST and 13:00 PST. That drop represented 22% less traffic than the previous week at the same hour. That’s also the same time and percentage of drop we’ve seen in 2022. \nIf we continue the country-wide comparison with the previous week, we also see how traffic really begins to pick up again during early Black Friday morning in the US (as much as 18% higher than in the previous week).\nHowever, it’s also interesting to do an analysis of state by state looking at local time. One question we were curious about: from an Internet perspective, what time best represents the Thanksgiving hour? This would be the time when traffic dropped the most in each state.\nWe find that across states, it’s not exactly 4pm, as The Atlantic has made a case for!, but rather, most states experience the largest drop the hour before — 15:00 local time. But that’s not the only interesting trend! We observe that:\nCentral US states such as Kansas, Iowa, Alabama, or Mississippi apparently had an earlier Thanksgiving — given the biggest drop in traffic was at 13:00.\nCoastal US states like Washington, California, Florida, Maryland, or Delaware had a later Thanksgiving, around 17:00. There’s also Hawaii, which had the latest of all — experiencing the biggest drop in traffic around 18:00 local time.\nWhat surprised us the most when looking at these trends was how the “Thanksgiving time” was the same from our 2022 data in almost all the states, but also the hourly and daily drop in traffic across the US was mostly the same. It appears that when it comes to Thanksgiving, we are indeed creatures of habit.\nThe Thanksgiving effect: US states where traffic drop the most\nTo consider when traffic drops the most, we look between the local time of 13:00-18:00 and compare that to the week before.\nThis method allows us to observe clear differences between states, with more central US states showing larger drops in traffic compared to the previous week, while coastal states are not as significantly impacted. The exception along the US coast is Massachusetts, which experienced a 31% drop in traffic. East coast states also show a bigger drop in traffic compared to the West coast.\nHere’s the ranking of the 50 states (plus DC or the District of Columbia), ordered by the biggest drops in traffic, for those who want to explore our data better:\nU.S. State Drop in traffic % Peak Internet traffic drop (local time) \nNorth Dakota\t-36%\t15:00 (CST)\t\nSouth Dakota\t-35%\t14:00 (CST)\t\nMississippi\t-33%\t13:00 (CST)\t\nDistrict of Columbia\t-32%\t16:00 (EST)\t\nOklahoma\t-32%\t14:00 (CST)\t\nMassachusetts\t-31%\t16:00 (EST)\t\nArkansas\t-30%\t14:00 (CST)\t\nRhode Island\t-30%\t16:00 (EST)\t\nKansas\t-28%\t13:00 (CST)\t\nConnecticut\t-27%\t16:00 (EST)\t\nIdaho\t-27%\t16:00 (MST)\t\nNew Hampshire\t-27%\t14:00 (EST)\t\nColorado\t-26%\t16:00 (MST)\t\nLouisiana\t-25%\t14:00 (CST)\t\nMaine\t-25%\t15:00 (EST)\t\nNew Mexico\t-25%\t14:00 (MST)\t\nPennsylvania\t-25%\t16:00 (EST)\t\nUtah\t-25%\t15:00 (MST)\t\nArizona\t-24%\t16:00 (MST)\t\nMissouri\t-24%\t15:00 (CST)\t\nMaryland\t-23%\t17:00 (EST)\t\nGeorgia\t-22%\t16:00 (EST)\t\nTennessee\t-22%\t14:00 (CST)\t\nVermont\t-22%\t15:00 (EST)\t\nDelaware\t-21%\t17:00 (EST)\t\nIndiana\t-21%\t15:00 (EST)\t\nMinnesota\t-21%\t15:00 (CST)\t\nNew York\t-21%\t16:00 (EST)\t\nAlaska\t-20%\t16:00 (AKST)\t\nFlorida\t-20%\t17:00 (EST)\t\nIowa\t-20%\t13:00 (CST)\t\nKentucky\t-20%\t14:00 (EST)\t\nMichigan\t-20%\t16:00 (EST)\t\nNorth Carolina\t-20%\t16:00 (EST)\t\nTexas\t-20%\t15:00 (CST)\t\nWisconsin\t-20%\t15:00 (CST)\t\nAlabama\t-19%\t13:00 (CST)\t\nOhio\t-18%\t16:00 (EST)\t\nSouth Carolina\t-18%\t15:00 (EST)\t\nNew Jersey\t-17%\t16:00 (EST)\t\nWest Virginia\t-17%\t16:00 (EST)\t\nIllinois\t-16%\t16:00 (CST)\t\nNebraska\t-16%\t15:00 (CST)\t\nMontana\t-15%\t16:00 (MST)\t\nWashington\t-15%\t17:00 (PST)\t\nCalifornia\t-14%\t17:00 (PST)\t\nNevada\t-12%\t17:00 (PST)\t\nOregon\t-12%\t15:00 (PST)\t\nWyoming\t-10%\t16:00 (MST)\t\nHawaii\t-9%\t18:00 (HST)\t\nVirginia\t-9%\t16:00 (EST)\t\nMobile traffic percentage goes up\nAnother, perhaps unsurprising, trend is the rise of mobile devices over the Thanksgiving week in the US. Yesterday, on November 23, mobile traffic accounted for 54.5% of the Internet traffic in the US (the graph below rounds the percentages). It followed a similar trend in 2021 — we published a blog about it — and in 2022, although last year it was at 53.8%.\nLooking at the past few weeks, the growth in mobile use in US Internet traffic is more evident. The average percentage of mobile traffic during the first week of November was 47% in the US; during this Thanksgiving week, it reached 51%, with the previously mentioned 54.5% peak on Thanksgiving Day (even higher than the typical weekend, which usually demonstrates more mobile usage).\nIt’s not just mobile usage that’s going up, though. Over the next few days, we’re expecting to see a surge in traffic to make up for the Thanksgiving lull.\nThe following chart presents the 2022 perspective on HTTP requests in the US, illustrating how the peak traffic of the year was reached on November 28, Cyber Monday. It's also notable how Christmas Eve and Christmas Day, followed by January 1, 2023, exhibit the most significant drops in traffic in the US.\nFood delivery and online groceries trends\nNow, let’s explore whether there was an increase in late food delivery or online grocery shopping related to Thanksgiving. Traditionally, this is a time for cooking with family, but not everyone enjoys cooking. DNS traffic (from our 1.1.1.1 resolver) to food delivery sites was higher than the previous week on Tuesday and Wednesday, November 21 and 22, 2023, respectively, but notably dropped in the early morning on Thanksgiving Day.\nDaily DNS traffic to food delivery services indicates a gradual increase throughout this month leading up to Thanksgiving Day, followed by a clear drop on the day itself, as much as 12%.\nHow about online grocery shopping services, catering to those last minute ingredients? DNS traffic to those sites was noticeably higher than the previous week on Tuesday but decreased on Wednesday, experiencing a distinct drop on Thanksgiving Day.\nAnd do hackers stop for turkey, too?\nTo answer that, let’s examine DDoS (distributed denial-of-service) attacks, which remain one of the most common methods to disrupt or take down Internet properties. Our data indicates that in November 2023, Thanksgiving had the lowest percentage of traffic classified as DDoS attacks targeting the US.\nEmail messages slow down\nCloudflare Area 1 also enables us to analyze email messages sent from the US perspective. Unsurprisingly, our data reveals a 43% drop in email messages sent on Thanksgiving Day compared to the previous week. However, the spam percentage of all emails originating from the US increased to 4%, significantly higher than the 2% recorded on the same day of the previous week.\nOn the flip side, messages considered malicious stayed consistent in their percentage of all messages.\nConclusion\n\"The more you practice the art of thankfulness, the more you have to be thankful for.\" — Norman Vincent Peale, American author\nThanksgiving Day in the United States still holds as a strong tradition in 2023, celebrating family, togetherness, and feasting that go beyond state borders and screens. Yet, notable differences exist among states, especially between the coastal and the central areas of the country.\nOur data also hints at a slowdown in food deliveries and cyber threats during this time. Perhaps hackers are taking a day off. But, just wait for the story to change on Black Friday and Cyber Monday. We'll keep an eye out.\nThanksgiving 2023 was also the day we announced that Stable Diffusion and Code Llama AI models are now available as part of Workers AI, running in over 100 cities across Cloudflare’s global network. If you’re looking to tinker with some new technology over this holiday weekend, we think you’ll enjoy these!\nAnd finally — don't forget, you can check Cloudflare Radar to track global and country-specific Internet traffic trends.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nThanksgiving Cloudflare Radar Internet Traffic Trends DDoS",
      "markdown": "11/24/2023\n\n*   [![João Tomé](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/V0x3WKfJ_400x400-1.jpeg)](https://blog.cloudflare.com/author/joao-tome/)\n\n8 min read\n\n![Do hackers eat turkey? And other Thanksgiving Internet trends](https://blog.cloudflare.com/content/images/2023/11/image3-4.png)\n\nThanksgiving is a tradition celebrated by millions of Americans across six time zones and 50 states, usually involving travel and bringing families together. This year, it was celebrated yesterday, on November 23, 2023. With the Internet so deeply enmeshed into our daily lives, anything that changes how so many people behave is going to also have an impact on online traffic. But how big an impact, exactly?\n\nAt a high level: a 10% daily decrease in Internet traffic in the US (compared to the previous week). That happens to be the exact same percentage decrease we observed in 2022. So, Thanksgiving in the US, at least in the realm of Internet traffic, seems consistent with last year.\n\nLet’s dig into more details about how people deal with cooking (or online ordering!) and whether family gatherings are less online, according to our [Cloudflare Radar](https://radar.cloudflare.com/us) data. We’ll also touch on whether hackers stop for turkey, too.\n\n### The Thanksgiving hour: around 15:00 (local time)\n\nWhile we can see a 10% overall daily drop in US traffic due to Thanksgiving, the drop is even more noticeable when examining traffic on an hour-by-hour basis. Internet activity began to decrease significantly after 12:00 EST, persisting until 19:00 EST (during those times, it was at least 15% lower compared to the previous week).\n\nThe peak drop for the entire country occurred around 21:00 UTC, which is 16:00 EST and 13:00 PST. That drop represented 22% less traffic than the previous week at the same hour. That’s also the same time and percentage of drop we’ve seen in 2022.\n\n![](https://blog.cloudflare.com/content/images/2023/11/Untitled.png)\n\nIf we continue the country-wide comparison with the previous week, we also see how traffic really begins to pick up again during early Black Friday morning in the US (as much as 18% higher than in the previous week).\n\nHowever, it’s also interesting to do an analysis of state by state looking at local time. One question we were curious about: from an Internet perspective, what time best represents the Thanksgiving hour? This would be the time when traffic dropped the most in each state.\n\nWe find that across states, it’s not exactly 4pm, as [The Atlantic](https://www.theatlantic.com/family/archive/2018/11/when-thanksgiving-dinner/576274/) has made a case for!, but rather, most states experience the largest drop the hour before — 15:00 local time. But that’s not the only interesting trend! We observe that:\n\n*   Central US states such as Kansas, Iowa, Alabama, or Mississippi apparently had _an earlier Thanksgiving_ — given the biggest drop in traffic was at 13:00.\n*   Coastal US states like Washington, California, Florida, Maryland, or Delaware had _a later Thanksgiving_, around 17:00. There’s also Hawaii, which had the latest of all — experiencing the biggest drop in traffic around 18:00 local time.\n\nWhat surprised us the most when looking at these trends was how the “Thanksgiving time” was the same from our 2022 data in almost all the states, but also the hourly and daily drop in traffic across the US was mostly the same. It appears that when it comes to Thanksgiving, we are indeed creatures of habit.\n\n### The Thanksgiving effect: US states where traffic drop the most\n\nTo consider when traffic drops the most, we look between the local time of 13:00-18:00 and compare that to the week before.\n\n![](https://blog.cloudflare.com/content/images/2023/11/Untitled--1-.png)\n\nThis method allows us to observe clear differences between states, with more central US states showing larger drops in traffic compared to the previous week, while coastal states are not as significantly impacted. The exception along the US coast is Massachusetts, which experienced a 31% drop in traffic. East coast states also show a bigger drop in traffic compared to the West coast.\n\nHere’s the ranking of the 50 states (plus DC or the District of Columbia), ordered by the biggest drops in traffic, for those who want to explore our data better:\n\n| U.S. State | Drop in traffic % | Peak Internet traffic drop (local time) |\n| --- | --- | --- |\n| North Dakota | \\-36% | 15:00 (CST) |\n| South Dakota | \\-35% | 14:00 (CST) |\n| Mississippi | \\-33% | 13:00 (CST) |\n| District of Columbia | \\-32% | 16:00 (EST) |\n| Oklahoma | \\-32% | 14:00 (CST) |\n| Massachusetts | \\-31% | 16:00 (EST) |\n| Arkansas | \\-30% | 14:00 (CST) |\n| Rhode Island | \\-30% | 16:00 (EST) |\n| Kansas | \\-28% | 13:00 (CST) |\n| Connecticut | \\-27% | 16:00 (EST) |\n| Idaho | \\-27% | 16:00 (MST) |\n| New Hampshire | \\-27% | 14:00 (EST) |\n| Colorado | \\-26% | 16:00 (MST) |\n| Louisiana | \\-25% | 14:00 (CST) |\n| Maine | \\-25% | 15:00 (EST) |\n| New Mexico | \\-25% | 14:00 (MST) |\n| Pennsylvania | \\-25% | 16:00 (EST) |\n| Utah | \\-25% | 15:00 (MST) |\n| Arizona | \\-24% | 16:00 (MST) |\n| Missouri | \\-24% | 15:00 (CST) |\n| Maryland | \\-23% | 17:00 (EST) |\n| Georgia | \\-22% | 16:00 (EST) |\n| Tennessee | \\-22% | 14:00 (CST) |\n| Vermont | \\-22% | 15:00 (EST) |\n| Delaware | \\-21% | 17:00 (EST) |\n| Indiana | \\-21% | 15:00 (EST) |\n| Minnesota | \\-21% | 15:00 (CST) |\n| New York | \\-21% | 16:00 (EST) |\n| Alaska | \\-20% | 16:00 (AKST) |\n| Florida | \\-20% | 17:00 (EST) |\n| Iowa | \\-20% | 13:00 (CST) |\n| Kentucky | \\-20% | 14:00 (EST) |\n| Michigan | \\-20% | 16:00 (EST) |\n| North Carolina | \\-20% | 16:00 (EST) |\n| Texas | \\-20% | 15:00 (CST) |\n| Wisconsin | \\-20% | 15:00 (CST) |\n| Alabama | \\-19% | 13:00 (CST) |\n| Ohio | \\-18% | 16:00 (EST) |\n| South Carolina | \\-18% | 15:00 (EST) |\n| New Jersey | \\-17% | 16:00 (EST) |\n| West Virginia | \\-17% | 16:00 (EST) |\n| Illinois | \\-16% | 16:00 (CST) |\n| Nebraska | \\-16% | 15:00 (CST) |\n| Montana | \\-15% | 16:00 (MST) |\n| Washington | \\-15% | 17:00 (PST) |\n| California | \\-14% | 17:00 (PST) |\n| Nevada | \\-12% | 17:00 (PST) |\n| Oregon | \\-12% | 15:00 (PST) |\n| Wyoming | \\-10% | 16:00 (MST) |\n| Hawaii | \\-9% | 18:00 (HST) |\n| Virginia | \\-9% | 16:00 (EST) |\n\n### Mobile traffic percentage goes up\n\nAnother, perhaps unsurprising, trend is the rise of mobile devices over the Thanksgiving week in the US. Yesterday, on November 23, mobile traffic accounted for 54.5% of the Internet traffic in the US (the graph below rounds the percentages). It followed a similar trend in 2021 — we [published](https://blog.cloudflare.com/how-the-us-paused-shopping-and-browsing-for-thanksgiving/) a blog about it — and in 2022, although last year it was at 53.8%.\n\n![](https://blog.cloudflare.com/content/images/2023/11/Screenshot-2023-11-24-at-15.02.16.png)\n\nLooking at the past few weeks, the growth in mobile use in US Internet traffic is more evident. The average percentage of mobile traffic during the first week of November was 47% in the US; during this Thanksgiving week, it reached 51%, with the previously mentioned 54.5% peak on Thanksgiving Day (even higher than the typical weekend, which usually demonstrates more mobile usage).\n\n![](https://blog.cloudflare.com/content/images/2023/11/Untitled--2-.png)\n\nIt’s not just mobile usage that’s going up, though. Over the next few days, we’re expecting to see a surge in traffic to make up for the Thanksgiving lull.\n\nThe following chart presents the 2022 perspective on HTTP requests in the US, illustrating how the peak traffic of the year was reached on November 28, Cyber Monday. It's also notable how Christmas Eve and Christmas Day, followed by January 1, 2023, exhibit the most significant drops in traffic in the US.\n\n![](https://blog.cloudflare.com/content/images/2023/11/Untitled--3-.png)\n\n### Food delivery and online groceries trends\n\nNow, let’s explore whether there was an increase in late food delivery or online grocery shopping related to Thanksgiving. Traditionally, this is a time for cooking with family, but not everyone enjoys cooking. DNS traffic (from our [1.1.1.1](https://1.1.1.1/) resolver) to food delivery sites was higher than the previous week on Tuesday and Wednesday, November 21 and 22, 2023, respectively, but notably dropped in the early morning on Thanksgiving Day.\n\n![](https://blog.cloudflare.com/content/images/2023/11/Untitled--4-.png)\n\nDaily DNS traffic to food delivery services indicates a gradual increase throughout this month leading up to Thanksgiving Day, followed by a clear drop on the day itself, as much as 12%.\n\n![](https://blog.cloudflare.com/content/images/2023/11/pasted-image-0-1.png)\n\nHow about online grocery shopping services, catering to those last minute ingredients? DNS traffic to those sites was noticeably higher than the previous week on Tuesday but decreased on Wednesday, experiencing a distinct drop on Thanksgiving Day.\n\n![](https://blog.cloudflare.com/content/images/2023/11/Untitled--5-.png)\n\n### And do hackers stop for turkey, too?\n\nTo answer that, let’s examine [DDoS](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/) (distributed denial-of-service) attacks, which remain one of the most common methods to disrupt or take down Internet properties. Our data indicates that in November 2023, Thanksgiving had the lowest percentage of traffic classified as DDoS attacks targeting the US.\n\n![](https://blog.cloudflare.com/content/images/2023/11/pasted-image-0--1--1.png)\n\n### Email messages slow down\n\n[Cloudflare Area 1](https://www.cloudflare.com/zero-trust/products/email-security/) also enables us to analyze email messages sent from the US perspective. Unsurprisingly, our data reveals a 43% drop in email messages sent on Thanksgiving Day compared to the previous week. However, the spam percentage of all emails originating from the US increased to 4%, significantly higher than the 2% recorded on the same day of the previous week.\n\n![](https://blog.cloudflare.com/content/images/2023/11/Untitled--6-.png)\n\nOn the flip side, messages considered malicious stayed consistent in their percentage of all messages.\n\n### Conclusion\n\n> _\"The more you practice the art of thankfulness, the more you have to be thankful for.\" — Norman Vincent Peale, American author_\n\nThanksgiving Day in the United States still holds as a strong tradition in 2023, celebrating family, togetherness, and feasting that go beyond state borders and screens. Yet, notable differences exist among states, especially between the coastal and the central areas of the country.\n\nOur data also hints at a slowdown in food deliveries and cyber threats during this time. Perhaps hackers are taking a day off. But, just wait for the story to change on Black Friday and Cyber Monday. We'll keep an eye out.\n\nThanksgiving 2023 was also the day we [announced](https://blog.cloudflare.com/workers-ai-update-stable-diffusion-code-llama-workers-ai-in-100-cities/) that Stable Diffusion and Code Llama AI models are now available as part of Workers AI, running in over 100 cities across Cloudflare’s global network. If you’re looking to tinker with some new technology over this holiday weekend, we think you’ll enjoy these!\n\nAnd finally — don't forget, you can check [Cloudflare Radar](https://radar.cloudflare.com/) to track global and country-specific Internet traffic trends.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Thanksgiving](https://blog.cloudflare.com/tag/thanksgiving/) [Cloudflare Radar](https://blog.cloudflare.com/tag/cloudflare-radar/) [Internet Traffic](https://blog.cloudflare.com/tag/internet-traffic/) [Trends](https://blog.cloudflare.com/tag/trends/) [DDoS](https://blog.cloudflare.com/tag/ddos/)"
    },
    {
      "url": "https://blog.cloudflare.com/workers-ai-update-stable-diffusion-code-llama-workers-ai-in-100-cities/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/workers-ai-update-stable-diffusion-code-llama-workers-ai-in-100-cities/",
        "loadedTime": "2023-12-05T02:27:26.265Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/workers-ai-update-stable-diffusion-code-llama-workers-ai-in-100-cities/",
        "title": "Workers AI Update: Stable Diffusion, Code Llama + Workers AI in 100 cities",
        "description": "we're thrilled to announce that Stable Diffusion and Code Llama are now available as part of Workers AI, running in over 100 cities across Cloudflare’s global network.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/23/2023\n5 min read\nThanksgiving might be a US holiday (and one of our favorites — we have many things to be thankful for!). Many people get excited about the food or deals, but for me as a developer, it’s also always been a nice quiet holiday to hack around and play with new tech. So in that spirit, we're thrilled to announce that Stable Diffusion and Code Llama are now available as part of Workers AI, running in over 100 cities across Cloudflare’s global network.\nAs many AI fans are aware, Stable Diffusion is the groundbreaking image-generation model that can conjure images based on text input. Code Llama is a powerful language model optimized for generating programming code.\nFor more of the fun details, read on, or head over to the developer docs to get started!\nGenerated by Stable Diffusion - “Happy llama in an orange cloud celebrating thanksgiving”\nGenerating images with Stable Diffusion\nStability AI launched Stable Diffusion XL 1.0 (SDXL) this past summer. You can read more about it here, but we’ll briefly mention some really cool aspects.\nFirst off, “Distinct images can be prompted without having any particular ‘feel’ imparted by the model, ensuring absolute freedom of style”. This is great as it gives you a blank canvas as a developer, or should I say artist.\nAdditionally, it’s “particularly well-tuned for vibrant and accurate colors, with better contrast, lighting, and shadows than its predecessor, all in native 1024x1024 resolution.” With the advancements in today's cameras (or phone cameras), quality images are table stakes, and it’s nice to see these models keeping up.\nGetting started with Workers AI + SDXL (via API) couldn’t be easier. Check out the example below:\ncurl -X POST \\ \"https://api.cloudflare.com/client/v4/accounts/{account-id}/ai/run/@cf/stabilityai/stable-diffusion-xl-base-1.0\" \\ -H \"Authorization: Bearer {api-token}\" \\ -H \"Content-Type:application/json\" \\ -d '{ \"prompt\": \"A happy llama running through an orange cloud\" }' \\ -o 'happy-llama.png' \nAnd here is our happy llama:\nYou can also do this in a Worker:\nimport { Ai } from '@cloudflare/ai'; export default { async fetch(request, env) { const ai = new Ai(env.AI); const response = await ai.run('@cf/stabilityai/stable-diffusion-xl-base-1.0', { prompt: 'A happy llama running through an orange cloud' }); return new Response(response, { headers: { \"content-type\": \"image/png\", }, }); } } \nGenerate code with Code Llama\nIf you’re not into generating art, then maybe you can have some fun with code. Code Llama, which was also released this last summer by Meta, is built on top of Llama 2, but optimized to understand and generate code in many popular languages (Python, C++, Java, PHP, Typescript / Javascript, C#, and Bash).\nYou can use it to help you generate code for a tough problem you're faced with, or you can also use it to help you understand code — perfect if you are picking up an existing, unknown codebase.\nAnd just like all the other models, generating code with Workers AI is really easy.\nFrom a Worker:\nimport { Ai } from '@cloudflare/ai'; // Enable env.AI for your worker by adding the ai binding to your wrangler.toml file: // [ai] // binding = \"AI\" export default { async fetch(request, env) { const ai = new Ai(env.AI); const response = await ai.run('@hf/thebloke/codellama-7b-instruct-awq', { prompt: 'In JavaScript, define a priority queue class. The constructor must take a function that is called on each object to determine its priority.' }); return Response.json(response); } } \nUsing curl:\ncurl -X POST \\ \"https://api.cloudflare.com/client/v4/accounts/{account-id}/ai/run/@hf/thebloke/codellama-7b-instruct-awq\" \\ -H \"Authorization: Bearer {api-token}\" \\-H \"Content-Type: application/json\" \\ -d '{ \"prompt\": \"In JavaScript, define a priority queue class. The constructor must take a function that is called on each object to determine its priority.\" } \nUsing python:\n#!/usr/bin/env python3 import json import os import requests ACCOUNT_ID=os.environ[\"ACCOUNT_ID\"] API_TOKEN=os.environ[\"API_TOKEN\"] MODEL=\"@hf/thebloke/codellama-7b-instruct-awq\" prompt=\"\"\"In JavaScript, define a priority queue class. The constructor must take a function that is called on each object to determine its priority.\"\"\" url = f\"https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai/run/{MODEL}\" headers = { \"Authorization\": f\"Bearer {API_TOKEN}\" } payload = json.dumps({ \"prompt\": prompt }) print(url) r = requests.post(url, data=payload, headers=headers) j = r.json() if \"result\" in j and \"response\" in j[\"result\"]: print(r.json()[\"result\"][\"response\"]) else: print(json.dumps(j, indent=2)) \nWorkers AI inference now available in 100 cities\nWhen we first released Workers AI back in September we launched with inference running in seven cities, but set an ambitious target to support Workers AI inference in 100 cities by the end of the year, and nearly everywhere by the end of 2024. We’re proud to say that we’re ahead of schedule and now support Workers AI Inference in 100 cities thanks to some awesome, hard-working folks across multiple teams. For developers this means that your inference tasks are more likely to run near your users, and it will only continue to improve over the next 18 months.\nMistral, in case you missed it\nLastly, in case you didn’t see our other update earlier this week, we also launched Mistral 7B, a super capable and powerful language model that packs a punch for its size. You can read more about it here, or start building with it here.\nGo forth and build something fun\nToday we gave you images (art), code, and Workers AI inference running in more cities. Please go have fun, build something cool, and if you need help, want to give feedback, or want to share what you’re building just pop into our Developer Discord!\nHappy Thanksgiving!\nAdditionally, if you’re just getting started with AI, we’ll be offering a series of developer workshops ranging from understanding the basics such as embeddings, models and vector databases, getting started with LLMs on Workers AI and more. We encourage you to sign up here.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nWorkers AI Cloudflare Workers",
      "markdown": "11/23/2023\n\n*   [![Phil Wittig](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/09/phil.jpeg)](https://blog.cloudflare.com/author/phil/)\n\n5 min read\n\n![Workers AI Update: Stable Diffusion, Code Llama + Workers AI in 100 cities](https://blog.cloudflare.com/content/images/2023/11/image3-1.png)\n\nThanksgiving might be a US holiday (and one of our favorites — we have many things to be thankful for!). Many people get excited about the food or deals, but for me as a developer, it’s also always been a nice quiet holiday to hack around and play with new tech. So in that spirit, we're thrilled to announce that **Stable Diffusion** and **Code Llama** are now available as part of Workers AI, running in over 100 cities across Cloudflare’s global network.\n\nAs many AI fans are aware, Stable Diffusion is the groundbreaking image-generation model that can conjure images based on text input. Code Llama is a powerful language model optimized for generating programming code.\n\nFor more of the fun details, read on, or head over to the [developer docs](https://developers.cloudflare.com/workers-ai/models/) to get started!\n\n![](https://blog.cloudflare.com/content/images/2023/11/image2-3.png)\n\n_Generated by Stable Diffusion - “Happy llama in an orange cloud celebrating thanksgiving”_\n\n### Generating images with Stable Diffusion\n\nStability AI launched Stable Diffusion XL 1.0 (SDXL) this past summer. You can read more about it [here](https://stability.ai/news/stable-diffusion-public-release), but we’ll briefly mention some really cool aspects.\n\nFirst off, “Distinct images can be prompted without having any particular ‘feel’ imparted by the model, ensuring absolute freedom of style”. This is great as it gives you a blank canvas as a developer, or should I say artist.\n\nAdditionally, it’s “particularly well-tuned for vibrant and accurate colors, with better contrast, lighting, and shadows than its predecessor, all in native 1024x1024 resolution.” With the advancements in today's cameras (or phone cameras), quality images are table stakes, and it’s nice to see these models keeping up.\n\nGetting started with Workers AI + SDXL (via [API](https://developers.cloudflare.com/workers-ai/models/text-to-image/)) couldn’t be easier. Check out the example below:\n\n```\ncurl -X POST \\\n\"https://api.cloudflare.com/client/v4/accounts/{account-id}/ai/run/@cf/stabilityai/stable-diffusion-xl-base-1.0\" \\\n-H \"Authorization: Bearer {api-token}\" \\\n-H \"Content-Type:application/json\" \\\n-d '{ \"prompt\": \"A happy llama running through an orange cloud\" }' \\\n-o 'happy-llama.png'\n```\n\nAnd here is our happy llama:\n\n![](https://blog.cloudflare.com/content/images/2023/11/image1-9.png)\n\nYou can also do this in a [Worker](https://developers.cloudflare.com/workers-ai/models/text-to-image/):\n\n```\nimport { Ai } from '@cloudflare/ai';\nexport default {\n  async fetch(request, env) {\n    const ai = new Ai(env.AI);\n    const response = await ai.run('@cf/stabilityai/stable-diffusion-xl-base-1.0', {\n      prompt: 'A happy llama running through an orange cloud'\n    });\n    return new Response(response, {\n      headers: {\n          \"content-type\": \"image/png\",\n      },\n  });\n  }\n}\n```\n\n### Generate code with Code Llama\n\nIf you’re not into generating art, then maybe you can have some fun with code. Code Llama, which was also released this last summer by Meta, is built on top of Llama 2, but optimized to understand and generate code in many popular languages (Python, C++, Java, PHP, Typescript / Javascript, C#, and Bash).\n\nYou can use it to help you generate code for a tough problem you're faced with, or you can also use it to help you understand code — perfect if you are picking up an existing, unknown codebase.\n\nAnd just like all the other models, generating code with Workers AI is really easy.\n\nFrom a Worker:\n\n```\nimport { Ai } from '@cloudflare/ai';\n\n// Enable env.AI for your worker by adding the ai binding to your wrangler.toml file:\n// [ai]\n// binding = \"AI\"\n\nexport default {\n  async fetch(request, env) {\n    const ai = new Ai(env.AI);\n    const response = await ai.run('@hf/thebloke/codellama-7b-instruct-awq', {\n      prompt: 'In JavaScript, define a priority queue class. The constructor must take a function that is called on each object to determine its priority.'\n    });\n    return Response.json(response);\n  }\n}\n```\n\nUsing curl:\n\n```\ncurl -X POST \\\n\"https://api.cloudflare.com/client/v4/accounts/{account-id}/ai/run/@hf/thebloke/codellama-7b-instruct-awq\" \\\n\n-H \"Authorization: Bearer {api-token}\" \\-H \"Content-Type: application/json\" \\\n-d '{ \"prompt\": \"In JavaScript, define a priority queue class. The constructor must take a function that is called on each object to determine its priority.\" }\n```\n\nUsing python:\n\n```\n#!/usr/bin/env python3\n\nimport json\nimport os\nimport requests\n\nACCOUNT_ID=os.environ[\"ACCOUNT_ID\"]\nAPI_TOKEN=os.environ[\"API_TOKEN\"]\nMODEL=\"@hf/thebloke/codellama-7b-instruct-awq\"\n\nprompt=\"\"\"In JavaScript, define a priority queue class. The constructor must take a function that is called on each object to determine its priority.\"\"\"\nurl = f\"https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai/run/{MODEL}\"\nheaders = {\n  \"Authorization\": f\"Bearer {API_TOKEN}\"\n}\npayload = json.dumps({\n  \"prompt\": prompt\n})\n\nprint(url)\nr = requests.post(url, data=payload, headers=headers)\n\nj = r.json()\nif \"result\" in j and \"response\" in j[\"result\"]:\n   print(r.json()[\"result\"][\"response\"])\nelse:\n   print(json.dumps(j, indent=2))\n```\n\n### Workers AI inference now available in 100 cities\n\nWhen we [first released Workers AI](https://blog.cloudflare.com/workers-ai/) back in September we launched with inference running in seven cities, but set an ambitious target to support Workers AI inference in 100 cities by the end of the year, and nearly everywhere by the end of 2024. We’re proud to say that we’re ahead of schedule and now support Workers AI Inference in 100 cities thanks to some awesome, hard-working folks across multiple teams. For developers this means that your inference tasks are more likely to run near your users, and it will only continue to improve over the next 18 months.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image4-1.png)\n\n### Mistral, in case you missed it\n\nLastly, in case you didn’t see our other update earlier this week, we also launched Mistral 7B, a super capable and powerful language model that packs a punch for its size. You can read more about it [here](https://blog.cloudflare.com/workers-ai-update-hello-mistral-7b/), or start building with it [here](https://developers.cloudflare.com/workers-ai/models/text-generation/).\n\n### Go forth and build something fun\n\nToday we gave you images (art), code, and Workers AI inference running in more cities. Please go have fun, build something cool, and if you need help, want to give feedback, or want to share what you’re building just pop into our [Developer Discord](https://discord.com/invite/cloudflaredev)!\n\n### Happy Thanksgiving!\n\nAdditionally, if you’re just getting started with AI, we’ll be offering a series of developer workshops ranging from understanding the basics such as embeddings, models and vector databases, getting started with LLMs on Workers AI and more. We encourage you to [sign up here](https://www.cloudflare.com/lp/ai-developer-workshop/).\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Workers AI](https://blog.cloudflare.com/tag/workers-ai/) [Cloudflare Workers](https://blog.cloudflare.com/tag/workers/)"
    },
    {
      "url": "https://blog.cloudflare.com/workers-ai-update-hello-mistral-7b/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/workers-ai-update-hello-mistral-7b/",
        "loadedTime": "2023-12-05T02:27:37.356Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/workers-ai-update-hello-mistral-7b/",
        "title": "Workers AI Update: Hello Mistral 7B",
        "description": "Today we’re excited to announce that we’ve added the Mistral-7B-v0.1-instruct to Workers AI.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/21/2023\n7 min read\nThis post is also available in Deutsch, Français and Nederlands.\nToday we’re excited to announce that we’ve added the Mistral-7B-v0.1-instruct to Workers AI. Mistral 7B is a 7.3 billion parameter language model with a number of unique advantages. With some help from the founders of Mistral AI, we’ll look at some of the highlights of the Mistral 7B model, and use the opportunity to dive deeper into “attention” and its variations such as multi-query attention and grouped-query attention.\nMistral 7B tl;dr:\nMistral 7B is a 7.3 billion parameter model that puts up impressive numbers on benchmarks. The model:\nOutperforms Llama 2 13B on all benchmarks\nOutperforms Llama 1 34B on many benchmarks,\nApproaches CodeLlama 7B performance on code, while remaining good at English tasks, and\nThe chat fine-tuned version we’ve deployed outperforms Llama 2 13B chat in the benchmarks provided by Mistral.\nHere’s an example of using streaming with the REST API:\ncurl -X POST \\ “https://api.cloudflare.com/client/v4/accounts/{account-id}/ai/run/@cf/mistral/mistral-7b-instruct-v0.1” \\ -H “Authorization: Bearer {api-token}” \\ -H “Content-Type:application/json” \\ -d '{ “prompt”: “What is grouped query attention”, “stream”: true }' API Response: { response: “Grouped query attention is a technique used in natural language processing (NLP) and machine learning to improve the performance of models…” } \nAnd here’s an example using a Worker script:\nimport { Ai } from '@cloudflare/ai'; export default { async fetch(request, env) { const ai = new Ai(env.AI); const stream = await ai.run('@cf/mistral/mistral-7b-instruct-v0.1', { prompt: 'What is grouped query attention', stream: true }); return Response.json(stream, { headers: { “content-type”: “text/event-stream” } }); } } \nMistral takes advantage of grouped-query attention for faster inference. This recently-developed technique improves the speed of inference without compromising output quality. For 7 billion parameter models, we can generate close to 4x as many tokens per second with Mistral as we can with Llama, thanks to Grouped-Query attention.\nYou don’t need any information beyond this to start using Mistral-7B, you can test it out today ai.cloudflare.com. To learn more about attention and Grouped-Query attention, read on!\nSo what is “attention” anyway?\nThe basic mechanism of attention, specifically “Scaled Dot-Product Attention” as introduced in the landmark paper Attention Is All You Need, is fairly simple:\nWe call our particular attention “Scale Dot-Product Attention”. The input consists of query and keys of dimension d_k, and values of dimension d_v. We compute the dot products of the query with all the keys, divide each by sqrt(d_k) and apply a softmax function to obtain the weights on the values.\nMore concretely, this looks like this:\nsource\nIn simpler terms, this allows models to focus on important parts of the input. Imagine you are reading a sentence and trying to understand it. Scaled dot product attention enables you to pay more attention to certain words based on their relevance. It works by calculating the similarity between each word (K) in the sentence and a query (Q). Then, it scales the similarity scores by dividing them by the square root of the dimension of the query. This scaling helps to avoid very small or very large values. Finally, using these scaled similarity scores, we can determine how much attention or importance each word should receive. This attention mechanism helps models identify crucial information (V) and improve their understanding and translation capabilities.\nEasy, right? To get from this simple mechanism to an AI that can write a “Seinfeld episode in which Jerry learns the bubble sort algorithm,” we’ll need to make it more complex. In fact, everything we’ve just covered doesn’t even have any learned parameters — constant values learned during model training that customize the output of the attention block!\nAttention blocks in the style of Attention is All You Need add mainly three types of complexity:\nLearned parameters\nLearned parameters refer to values or weights that are adjusted during the training process of a model to improve its performance. These parameters are used to control the flow of information or attention within the model, allowing it to focus on the most relevant parts of the input data. In simpler terms, learned parameters are like adjustable knobs on a machine that can be turned to optimize its operation.\nVertical stacking - layered attention blocks\nVertical layered stacking is a way to stack multiple attention mechanisms on top of each other, with each layer building on the output of the previous layer. This allows the model to focus on different parts of the input data at different levels of abstraction, which can lead to better performance on certain tasks.\nHorizontal stacking - aka Multi-Head Attention\nThe figure from the paper displays the full multi-head attention module. Multiple attention operations are carried out in parallel, with the Q-K-V input for each generated by a unique linear projection of the same input data (defined by a unique set of learned parameters). These parallel attention blocks are referred to as “attention heads”. The weighted-sum outputs of all attention heads are concatenated into a single vector and passed through another parameterized linear transformation to get the final output.\nsource\nThis mechanism allows a model to focus on different parts of the input data concurrently. Imagine you are trying to understand a complex piece of information, like a sentence or a paragraph. In order to understand it, you need to pay attention to different parts of it at the same time. For example, you might need to pay attention to the subject of the sentence, the verb, and the object, all simultaneously, in order to understand the meaning of the sentence. Multi-headed attention works similarly. It allows a model to pay attention to different parts of the input data at the same time, by using multiple \"heads\" of attention. Each head of attention focuses on a different aspect of the input data, and the outputs of all the heads are combined to produce the final output of the model.\nStyles of attention\nThere are three common arrangements of attention blocks used by large language models developed in recent years: multi-head attention, grouped-query attention and multi-query attention. They differ in the number of K and V vectors relative to the number of query vectors. Multi-head attention uses the same number of K and V vectors as Q vectors, denoted by “N” in the table below. Multi-query attention uses only a single K and V vector. Grouped-query attention, the type used in the Mistral 7B model, divides the Q vectors evenly into groups containing “G” vectors each, then uses a single K and V vector for each group for a total of N divided by G sets of K and V vectors. This summarizes the differences, and we’ll dive into the implications of these below.\n\t\nNumber of Key/Value Blocks\n\t\nQuality\n\t\nMemory Usage\n\t\nMulti-head attention (MHA)\n\t\nN\n\t\nBest\n\t\nMost\n\t\nGrouped-query attention (GQA)\n\t\nN / G\n\t\nBetter\n\t\nLess\n\t\nMulti-query attention (MQA)\n\t\n1\n\t\nGood\n\t\nLeast\n\t\nSummary of attention styles\nAnd this diagram helps illustrate the difference between the three styles:\nsource\nMulti-Query Attention\nMulti-query attention was described in 2019 in the paper from Google: Fast Transformer Decoding: One Write-Head is All You Need. The idea is that instead of creating separate K and V entries for every Q vector in the attention mechanism, as in multi-head attention above, only a single K and V vector is used for the entire set of Q vectors. Thus the name, multiple queries combined into a single attention mechanism. In the paper, this was benchmarked on a translation task and showed performance equal to multi-head attention on the benchmark task.\nOriginally the idea was to reduce the total size of memory that is accessed when performing inference for the model. Since then, as generalized models have emerged and grown in number of parameters, the GPU memory needed is often the bottleneck which is the strength of multi-query attention, as it requires the least accelerator memory of the three types of attention. However, as models grew in size and generality, performance of multi-query attention fell relative to multi-head attention.\nGrouped-Query Attention\nThe newest of the bunch — and the one used by Mistral — is grouped-query attention, as described in the paper GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints that was published on arxiv.org in May 2023. Grouped-query attention combines the best of both worlds: the quality of multi-headed attention with the speed and low memory usage of multi-query attention. Instead of either a single set of K and V vectors or one set for every Q vector, a fixed ratio of 1 set of K and V vectors for every Q vector is used, reducing memory usage but retaining high performance on many tasks.\nOften choosing a model for a production task is not just about picking the best model available because we must consider tradeoffs between performance, memory usage, batch size, and available hardware (or cloud costs). Understanding these three styles of attention can help guide those decisions and understand when we might choose a particular model given our circumstances.\nEnter Mistral — try it today\nBeing one of the first large language models to leverage grouped-query attention and combining it with sliding window attention, Mistral seems to have hit the goldilocks zone — it’s low latency, high-throughput, and it performs really well on benchmarks even when compared to bigger models (13B). All this to say is that it packs a punch for its size, and we couldn't be more excited to make it available to all developers today, via Workers AI.\nHead over to our developer docs to get started, and if you need help, want to give feedback, or want to share what you’re building just pop into our Developer Discord!\nThe Workers AI team is also expanding and hiring; check our jobs page for open roles if you’re passionate about AI engineering and want to help us build and evolve our global, serverless GPU-powered inference platform.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nWorkers AI Mistral Cloudflare Workers",
      "markdown": "11/21/2023\n\n*   [![Jesse Kipp](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2020/12/jesse-headshot-square.jpg)](https://blog.cloudflare.com/author/jesse/)\n*   [![Isaac Rehg](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/11/Isaac-Rehg.jpeg)](https://blog.cloudflare.com/author/isaac-rehg/)\n\n7 min read\n\nThis post is also available in [Deutsch](https://blog.cloudflare.com/de-de/workers-ai-update-hello-mistral-7b-de-de/), [Français](https://blog.cloudflare.com/fr-fr/workers-ai-update-hello-mistral-7b-fr-fr/) and [Nederlands](https://blog.cloudflare.com/nl-nl/workers-ai-update-hello-mistral-7b-nl-nl/).\n\n![Workers AI Update: Hello Mistral 7B](https://blog.cloudflare.com/content/images/2023/11/Mistral-1.png)\n\nToday we’re excited to announce that we’ve added the Mistral-7B-v0.1-instruct to Workers AI. Mistral 7B is a 7.3 billion parameter language model with a number of unique advantages. With some help from the founders of Mistral AI, we’ll look at some of the highlights of the Mistral 7B model, and use the opportunity to dive deeper into “attention” and its variations such as multi-query attention and grouped-query attention.\n\n## Mistral 7B tl;dr:\n\nMistral 7B is a 7.3 billion parameter model that puts up [impressive numbers on benchmarks](https://mistral.ai/news/announcing-mistral-7b/). The model:\n\n*   Outperforms Llama 2 13B on all benchmarks\n*   Outperforms Llama 1 34B on many benchmarks,\n*   Approaches CodeLlama 7B performance on code, while remaining good at English tasks, and\n*   The chat fine-tuned version we’ve deployed outperforms Llama 2 13B chat in the benchmarks provided by Mistral.\n\nHere’s an example of using streaming with the [REST API](https://developers.cloudflare.com/workers-ai/get-started/rest-api/):\n\n```\ncurl -X POST \\\n“https://api.cloudflare.com/client/v4/accounts/{account-id}/ai/run/@cf/mistral/mistral-7b-instruct-v0.1” \\\n-H “Authorization: Bearer {api-token}” \\\n-H “Content-Type:application/json” \\\n-d '{ “prompt”: “What is grouped query attention”, “stream”: true }'\n\nAPI Response: { response: “Grouped query attention is a technique used in natural language processing  (NLP) and machine learning to improve the performance of models…” }\n```\n\nAnd here’s an example using a Worker script:\n\n```\nimport { Ai } from '@cloudflare/ai';\nexport default {\n    async fetch(request, env) {\n        const ai = new Ai(env.AI);\n        const stream = await ai.run('@cf/mistral/mistral-7b-instruct-v0.1', {\n            prompt: 'What is grouped query attention',\n            stream: true\n        });\n        return Response.json(stream, { headers: { “content-type”: “text/event-stream” } });\n    }\n}\n```\n\nMistral takes advantage of [grouped-query attention](https://arxiv.org/abs/2305.13245) for faster inference. This recently-developed technique improves the speed of inference without compromising output quality. For 7 billion parameter models, we can generate close to 4x as many tokens per second with Mistral as we can with Llama, thanks to Grouped-Query attention.\n\nYou don’t need any information beyond this to start using Mistral-7B, you can test it out today [ai.cloudflare.com](https://ai.cloudflare.com/). To learn more about attention and Grouped-Query attention, read on!\n\n## So what is “attention” anyway?\n\nThe basic mechanism of attention, specifically “Scaled Dot-Product Attention” as introduced in the landmark paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762), is fairly simple:\n\n> We call our particular attention “Scale Dot-Product Attention”. The input consists of query and keys of dimension d\\_k, and values of dimension d\\_v. We compute the dot products of the query with all the keys, divide each by sqrt(d\\_k) and apply a softmax function to obtain the weights on the values.\n\nMore concretely, this looks like this:\n\n![](https://blog.cloudflare.com/content/images/2023/11/Screenshot-2023-11-21-at-09.12.30.png)\n\n[source](https://arxiv.org/abs/1706.03762)\n\nIn simpler terms, this allows models to focus on important parts of the input. Imagine you are reading a sentence and trying to understand it. Scaled dot product attention enables you to pay more attention to certain words based on their relevance. It works by calculating the similarity between each word (K) in the sentence and a query (Q). Then, it scales the similarity scores by dividing them by the square root of the dimension of the query. This scaling helps to avoid very small or very large values. Finally, using these scaled similarity scores, we can determine how much attention or importance each word should receive. This attention mechanism helps models identify crucial information (V) and improve their understanding and translation capabilities.\n\nEasy, right? To get from this simple mechanism to an AI that can write a “Seinfeld episode in which Jerry learns the bubble sort algorithm,” we’ll need to make it more complex. In fact, everything we’ve just covered doesn’t even have any learned parameters — constant values learned during model training that customize the output of the attention block!  \nAttention blocks in the style of _Attention is All You Need_ add mainly three types of complexity:\n\n### Learned parameters\n\nLearned parameters refer to values or weights that are adjusted during the training process of a model to improve its performance. These parameters are used to control the flow of information or attention within the model, allowing it to focus on the most relevant parts of the input data. In simpler terms, learned parameters are like adjustable knobs on a machine that can be turned to optimize its operation.\n\n### Vertical stacking - layered attention blocks\n\nVertical layered stacking is a way to stack multiple attention mechanisms on top of each other, with each layer building on the output of the previous layer. This allows the model to focus on different parts of the input data at different levels of abstraction, which can lead to better performance on certain tasks.\n\n### Horizontal stacking - aka Multi-Head Attention\n\nThe figure from the paper displays the full multi-head attention module. Multiple attention operations are carried out in parallel, with the Q-K-V input for each generated by a unique linear projection of the same input data (defined by a unique set of learned parameters). These parallel attention blocks are referred to as “attention heads”. The weighted-sum outputs of all attention heads are concatenated into a single vector and passed through another parameterized linear transformation to get the final output.\n\n![](https://blog.cloudflare.com/content/images/2023/11/Screenshot-2023-11-21-at-09.13.49.png)\n\n[source](https://arxiv.org/abs/1706.03762)\n\nThis mechanism allows a model to focus on different parts of the input data concurrently. Imagine you are trying to understand a complex piece of information, like a sentence or a paragraph. In order to understand it, you need to pay attention to different parts of it at the same time. For example, you might need to pay attention to the subject of the sentence, the verb, and the object, all simultaneously, in order to understand the meaning of the sentence. Multi-headed attention works similarly. It allows a model to pay attention to different parts of the input data at the same time, by using multiple \"heads\" of attention. Each head of attention focuses on a different aspect of the input data, and the outputs of all the heads are combined to produce the final output of the model.\n\n## Styles of attention\n\nThere are three common arrangements of attention blocks used by large language models developed in recent years: multi-head attention, grouped-query attention and multi-query attention. They differ in the number of K and V vectors relative to the number of query vectors. **Multi-head attention** uses the same number of K and V vectors as Q vectors, denoted by “N” in the table below. **Multi-query attention** uses only a single K and V vector. **Grouped-query attention**, the type used in the Mistral 7B model, divides the Q vectors evenly into groups containing “G” vectors each, then uses a single K and V vector for each group for a total of N divided by G sets of K and V vectors. This summarizes the differences, and we’ll dive into the implications of these below.\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|     | **Number of Key/Value Blocks** | **Quality** | **Memory Usage** |\n| **Multi-head attention (MHA)** | N   | Best | Most |\n| **Grouped-query attention (GQA)** | N / G | Better | Less |\n| **Multi-query attention (MQA)** | 1   | Good | Least |\n\nSummary of attention styles\n\nAnd this diagram helps illustrate the difference between the three styles:\n\n![](https://blog.cloudflare.com/content/images/2023/11/image1-6.png)\n\n[source](https://arxiv.org/pdf/2305.13245.pdf)\n\n### Multi-Query Attention\n\nMulti-query attention was described in 2019 in the paper from Google: [Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/abs/1911.02150). The idea is that instead of creating separate K and V entries for every Q vector in the attention mechanism, as in multi-head attention above, only a single K and V vector is used for the entire set of Q vectors. Thus the name, multiple queries combined into a single attention mechanism. In the paper, this was benchmarked on a translation task and showed performance equal to multi-head attention on the benchmark task.\n\nOriginally the idea was to reduce the total size of memory that is accessed when performing inference for the model. Since then, as generalized models have emerged and grown in number of parameters, the GPU memory needed is often the bottleneck which is the strength of multi-query attention, as it requires the least accelerator memory of the three types of attention. However, as models grew in size and generality, performance of multi-query attention fell relative to multi-head attention.\n\n### Grouped-Query Attention\n\nThe newest of the bunch — and the one used by Mistral — is grouped-query attention, as described in the paper [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/abs/2305.13245) that was published on arxiv.org in May 2023. Grouped-query attention combines the best of both worlds: the quality of multi-headed attention with the speed and low memory usage of multi-query attention. Instead of either a single set of K and V vectors or one set for every Q vector, a fixed ratio of 1 set of K and V vectors for every Q vector is used, reducing memory usage but retaining high performance on many tasks.\n\nOften choosing a model for a production task is not just about picking the best model available because we must consider tradeoffs between performance, memory usage, batch size, and available hardware (or cloud costs). Understanding these three styles of attention can help guide those decisions and understand when we might choose a particular model given our circumstances.\n\n## Enter Mistral — try it today\n\nBeing one of the first large language models to leverage grouped-query attention and combining it with sliding window attention, Mistral seems to have hit the goldilocks zone — it’s low latency, high-throughput, and it performs really well on benchmarks even when compared to bigger models (13B). All this to say is that it packs a punch for its size, and we couldn't be more excited to make it available to all developers today, via Workers AI.\n\nHead over to our [developer docs](https://developers.cloudflare.com/workers-ai/models/text-generation/) to get started, and if you need help, want to give feedback, or want to share what you’re building just pop into our [Developer Discord](https://discord.com/invite/cloudflaredev)!\n\nThe Workers AI team is also expanding and hiring; check our [jobs page](https://www.cloudflare.com/careers/jobs/) for open roles if you’re passionate about AI engineering and want to help us build and evolve our global, serverless GPU-powered inference platform.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Workers AI](https://blog.cloudflare.com/tag/workers-ai/) [Mistral](https://blog.cloudflare.com/tag/mistral/) [Cloudflare Workers](https://blog.cloudflare.com/tag/workers/)"
    },
    {
      "url": "https://blog.cloudflare.com/2024-the-year-of-elections/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/2024-the-year-of-elections/",
        "loadedTime": "2023-12-05T02:27:46.965Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/2024-the-year-of-elections/",
        "title": "2024, the year of elections",
        "description": "We want to ensure that all groups working to promote democracy around the world have the tools they need to stay secure online.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/20/2023\n8 min read\n2024 is a year of elections, with more than 70 elections scheduled in 40 countries around the world. One of the key pillars of democracy is trust. To that end, ensuring that the Internet is trusted, secure, reliable, and accessible for the public and those working in the election space is critical to any free and fair election.\nCloudflare has considerable experience in gearing up for elections and identifying how our cyber security tools can be used to help vulnerable groups in the election space. In December 2022, we expanded our product set to include Zero Trust products to assist these groups against new and emerging threats. Over the last few years, we’ve reported on our work in protecting a range of election entities and as we prepare for the 2024 elections, we want to provide insight into attack trends we’ve seen against these groups to understand what to expect in the next year.\nFor this blog post, we identified cyber attack trends for a variety of groups in the elections space based in the United States, as many of our Cloudflare Impact projects provide services to these groups. These include U.S. state and local government websites protected under the Athenian Project, as well as U.S. nonprofit organizations that work in voting rights and promoting democracy under Project Galileo, and political campaigns and parties under Cloudflare for Campaigns.\nOur main findings:\nFrom November 1, 2022, to August 31, 2023, Cloudflare mitigated 234,740,000 threats to U.S elections groups surveyed.\nInternet traffic to these websites has steadily increased, up nearly 25% between January 2023 and August 2023.\nWe observed an increase in traffic to political campaign websites during elections, then steadily decreasing traffic until elections in the following year, as shown with the traffic spikes we see during the analyzed time period.\nHTTP Anomaly remained the top layer 7 attack vector mitigated by the Web Application Firewall, followed by SQL Injection.\nSupporting state and local governments that run elections with the Athenian Project\nUnder the Athenian Project, Cloudflare provides our highest level of protection to state and local governments in the United States that run elections. As of November 2023, 390 state and local governments in 31 states are protected under the project. Across this cohort, Cloudflare mitigated 213.78 million threats to government election sites between November 1, 2022, and August 31, 2023, an average of 703,223 threats per day.\nOn Election Day, November 7, 2022, we saw traffic to state and local government sites increase by more than 500%. Analysis shows that 80% of this traffic was classified as coming from human users, which is expected, as we tend to see an increase in traffic during election time as constituents view their local county board of election sites to identify polling locations and election results.\nWe’ve also seen an increase in state and local governments onboarding .gov domains to Cloudflare. In September 2022, The U.S Department of Homeland Security’s Cybersecurity and Infrastructure Security Agency (CISA) announced the agency would launch a new .gov registrar with the intent of making it easier for government organizations to set up a .gov website, while also making the domain more secure. We observed that 65% of traffic to Athenian domains is to .gov domains.\nWhen we look at traffic that was mitigated by Cloudflare’s Web Application Firewall (WAF), specifically Cloudflare managed rulesets, we see an oscillating traffic pattern identified as HTTP anomalies until a sudden (and seemingly permanent) drop after mid-April 2023. Managed rulesets are pre-configured firewall rules that provide immediate protection against common vulnerabilities. These managed rulesets are created by the Cloudflare security team, provide fast and effective protection for customer applications, and are updated frequently to cover new vulnerabilities and reduce false positives.\nThe managed rules are a great feature, especially for organizations with limited security resources, as they are easy to enable and protect against common vulnerabilities that Cloudflare has identified that have hit thousands of websites. Within the WAF Managed Rules, the top category that we see for mitigations is HTTP Anomaly. HTTP anomalies include such things as malformed method names, null byte characters in headers, non-standard ports, or content length of zero with a POST request.\nWe found 76% of traffic that was mitigated by the WAF was HTTP anomalies, followed by SQL Injection (SQLi) at only 8%. There is another pattern seen in XSS (Cross-Site-Scripting) attempts that are observed every 23rd day of the month. Given this very \"strict\" pattern, this could be due to an automated attack of some sort.\nSupporting political campaigns and state parties with Cloudflare for Campaigns\nCloudflare launched Cloudflare for Campaigns in January 2020, in partnership with the nonprofit, nonpartisan organization Defending Digital Campaigns. Under the partnership, we protect 70 political campaigns and 20 political parties in the United States. Between November 1, 2022, and August 31, 2023,Cloudflare mitigated 1.83 million threats to political campaign sites, which is an average of 6,019 threats per day.\nWhen we look at traffic trends for these domains, we see a spike in November 2022 during the midterm elections in the United States, but significantly lower traffic after this time. Overall, interest in these campaign websites appears to be limited only to election times and some months prior.\nWhen we identify traffic that was blocked by Cloudflare, a majority (79%) was blocked by WAF rules. However, this wasn’t all from malicious sources, as some of the rules have been configured by the campaigns themselves to block other types of unwanted traffic. For example, some campaigns block traffic from outside of the United States from accessing the website, which would be classified as a blocked request. As we’ve worked with many campaigns in the past on how to get the most out of Cloudflare security tools, we think it is a sign of progress that campaigns are setting specific rules that help them mitigate or challenge traffic that they may not want to access the site.\nIn addition to the customer-configured rules, these campaign sites are also protected by WAF managed rules (run by Cloudflare), with 47% of mitigated traffic identified as HTTP Anomaly and 30% SQLi.\nSupporting organizations that promote free and fair elections with Project Galileo\nAs part of our analysis we also identified 69 organizations in the United States that are protected under Project Galileo that work on a range of topics related to voting rights and promoting free and fair elections. For those organizations, Cloudflare mitigated 19.13 million threats between November 1, 2022, and August 31, 2023, an average of 62,927 threats per day.\nWe saw a spike in traffic during election time in November 2022 and another slight increase in April 2023. During this time, the largest number of blocked requests was mitigated by Cloudflare’s Security Level. Cloudflare’s Security Level is a security tool that ranks requests based on IP reputation to decide whether to present a Managed Challenge page. A managed challenge helps determine whether the request is considered malicious or legitimate. If the visitor passes the challenge, their request is allowed. If they fail, the request will be blocked. Many of these challenges are issued as a result of domains enabling Under Attack Mode, which enforces an elevated Security Level to help mitigate layer 7 DDoS attacks.\nFor traffic that was mitigated by the WAF, we found the top mitigation categories to be HTTP Anomalies at 48% and SQLi at 25%. Overall, we saw more requests mitigated by Cloudflare’s WAF than traffic that was considered DDoS.\nTaking our elections expertise global\nIn 2021, we announced our partnership with the International Foundation for Electoral Systems (IFES) to provide our highest level of protection for free to election management bodies (EMBs) around the world. An EMB is an institution responsible for organizing and overseeing elections in a particular jurisdiction with a primary role of ensuring that the electoral process is conducted fairly and transparently. Since beginning our partnership, we’ve provided protection or expertise to 7 election management bodies to support their work in promoting free and fair elections. As part of this, we’ve worked with election commissions in Kosovo and North Macedonia to protect their election infrastructure.\n“Security is the cornerstone of any democratic process, and free and fair elections are no exception. Security products like those from Cloudflare become even more critical in an increasingly digital world. With Cloudflare, we have effectively mitigated numerous cyber threats, ensuring citizens uninterrupted access to electoral information in Kosovo. This has significantly fostered trust and transparency in our electoral processes.”\n- Kreshnik Spahiu\nDirector of the Information Technology Department, Central Election Commission of Kosovo\nAs we approach 2024 with many elections in newly emerging democracies, we are excited to continue our work with IFES to provide our services and share our expertise to help election groups stay secure online.\nLooking toward 2024…\nIf 2024 is anything like 2023, we should continue to expect irregularities regarding Internet access during elections. We’ve seen this in areas such as Cambodia, where ahead of the 2023 elections, Cambodian officials ordered internet service providers to block website access to three news outlets reporting on the election as a way to control the independent media. In Zimbabwe, a new law known as the Patriotic Bill was passed before the general election, encompassing a wide range of provisions that make it illegal to engage in speech deemed to pose a threat to the nation's sovereignty or vital national interests.\nThe last few years contain many examples of how governments have undermined and controlled the flow of information through Internet shutdowns, restricted social media sites during elections, and imposed blocking of websites that report on results. If current trends continue, 2024 will be a pivotal year for online freedoms.\nIn light of this, we want to ensure that all groups working to promote democracy around the world have the tools they need to stay secure online. If you work in the election space and need our help, please apply at https://www.cloudflare.com/election-security.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nElection Security USA Athenian Project",
      "markdown": "11/20/2023\n\n*   [![Jocelyn Woolbright](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2019/12/0-1.jpg)](https://blog.cloudflare.com/author/jocelyn/)\n\n8 min read\n\n![2024, the year of elections](https://blog.cloudflare.com/content/images/2023/11/image1-4.png)\n\n2024 is a year of elections, with more than 70 elections scheduled in 40 countries around the world. One of the key pillars of democracy is trust. To that end, ensuring that the Internet is trusted, secure, reliable, and accessible for the public and those working in the election space is critical to any free and fair election.\n\nCloudflare has considerable experience in gearing up for elections and identifying how our cyber security tools can be used to help vulnerable groups in the election space. In December 2022, we [expanded our product](https://blog.cloudflare.com/cloudflare-zero-trust-for-galileo-and-athenian/) set to include Zero Trust products to assist these groups against new and emerging threats. Over the last few years, [we’ve reported on our work](https://www.cloudflare.com/election-security/) in protecting a range of election entities and as we prepare for the 2024 elections, we want to provide insight into attack trends we’ve seen against these groups to understand what to expect in the next year.\n\nFor this blog post, we identified cyber attack trends for a variety of groups in the elections space based in the United States, as many of our [Cloudflare Impact](https://www.cloudflare.com/impact/) projects provide services to these groups. These include U.S. state and local government websites protected under the [Athenian Project](https://www.cloudflare.com/athenian/), as well as U.S. nonprofit organizations that work in voting rights and promoting democracy under [Project Galileo](https://www.cloudflare.com/galileo/), and political campaigns and parties under [Cloudflare for Campaigns](https://www.cloudflare.com/campaigns/usa/).\n\nOur main findings:\n\n*   From November 1, 2022, to August 31, 2023, Cloudflare mitigated 234,740,000 threats to U.S elections groups surveyed.\n*   Internet traffic to these websites has steadily increased, up nearly 25% between January 2023 and August 2023.\n*   We observed an increase in traffic to political campaign websites during elections, then steadily decreasing traffic until elections in the following year, as shown with the traffic spikes we see during the analyzed time period.\n*   HTTP Anomaly remained the top layer 7 attack vector mitigated by the Web Application Firewall, followed by [SQL Injection](https://www.cloudflare.com/learning/security/threats/how-to-prevent-sql-injection/).\n\n![](https://blog.cloudflare.com/content/images/2023/11/pasted-image-0.png)\n\n### Supporting state and local governments that run elections with the Athenian Project\n\nUnder the [Athenian Project](https://www.cloudflare.com/athenian/), Cloudflare provides our highest level of protection to state and local governments in the United States that run elections. As of November 2023, 390 state and local governments in 31 states are protected under the project. Across this cohort, Cloudflare mitigated 213.78 million threats to government election sites between November 1, 2022, and August 31, 2023, an average of 703,223 threats per day.\n\nOn Election Day, November 7, 2022, we saw traffic to state and local government sites increase by more than 500%. Analysis shows that 80% of this traffic was classified as coming from human users, which is expected, as we tend to see an increase in traffic during election time as constituents view their local county board of election sites to identify polling locations and election results.\n\n![](https://blog.cloudflare.com/content/images/2023/11/pasted-image-0--1-.png)\n\nWe’ve also seen an increase in state and local governments onboarding .gov domains to Cloudflare. In September 2022, The U.S Department of Homeland Security’s Cybersecurity and Infrastructure Security Agency (CISA) [announced](https://www.meritalk.com/articles/cisa-making-big-changes-to-gov-registration-management/) the agency would launch a new .gov registrar with the intent of making it easier for government organizations to set up a .gov website, while also making the domain more secure. We observed that 65% of traffic to Athenian domains is to .gov domains.\n\nWhen we look at traffic that was mitigated by Cloudflare’s [Web Application Firewall](https://www.cloudflare.com/application-services/products/waf/) (WAF), specifically [Cloudflare managed rulesets](https://developers.cloudflare.com/waf/managed-rules/reference/cloudflare-managed-ruleset/), we see an oscillating traffic pattern identified as HTTP anomalies until a sudden (and seemingly permanent) drop after mid-April 2023. Managed rulesets are pre-configured firewall rules that provide immediate protection against common vulnerabilities. These managed rulesets are created by the Cloudflare security team, provide fast and effective protection for customer applications, and are updated frequently to cover new vulnerabilities and reduce false positives.\n\n![](https://blog.cloudflare.com/content/images/2023/11/pasted-image-0--2-.png)\n\nThe managed rules are a great feature, especially for organizations with limited security resources, as they are easy to enable and protect against common vulnerabilities that Cloudflare has identified that have hit thousands of websites. Within the WAF Managed Rules, the top category that we see for mitigations is HTTP Anomaly. HTTP anomalies include such things as malformed method names, null byte characters in headers, non-standard ports, or content length of zero with a POST request.\n\nWe found 76% of traffic that was mitigated by the WAF was HTTP anomalies, followed by [SQL Injection (SQLi)](https://www.cloudflare.com/learning/security/threats/sql-injection/) at only 8%. There is another pattern seen in [XSS (Cross-Site-Scripting)](https://www.cloudflare.com/learning/security/threats/cross-site-scripting/) attempts that are observed every 23rd day of the month. Given this very \"strict\" pattern, this could be due to an automated attack of some sort.\n\n![](https://blog.cloudflare.com/content/images/2023/11/pasted-image-0--3-.png)\n\n### Supporting political campaigns and state parties with Cloudflare for Campaigns\n\nCloudflare [launched](https://blog.cloudflare.com/introducing-cloudflare-for-campaigns/) Cloudflare for Campaigns in January 2020, in partnership with the nonprofit, nonpartisan organization [Defending Digital Campaigns](https://defendcampaigns.org/). Under the partnership, we protect 70 political campaigns and 20 political parties in the United States. Between November 1, 2022, and August 31, 2023,Cloudflare mitigated 1.83 million threats to political campaign sites, which is an average of 6,019 threats per day.\n\nWhen we look at traffic trends for these domains, we see a spike in November 2022 during the midterm elections in the United States, but significantly lower traffic after this time. Overall, interest in these campaign websites appears to be limited only to election times and some months prior.\n\n![](https://blog.cloudflare.com/content/images/2023/11/pasted-image-0--4-.png)\n\nWhen we identify traffic that was blocked by Cloudflare, a majority (79%) was blocked by WAF rules. However, this wasn’t all from malicious sources, as some of the rules have been configured by the campaigns themselves to block other types of unwanted traffic. For example, some campaigns block traffic from outside of the United States from accessing the website, which would be classified as a blocked request. As we’ve worked with many campaigns in the past on how to get the most out of [Cloudflare security tools](https://www.cloudflare.com/security/), we think it is a sign of progress that campaigns are setting specific rules that help them mitigate or challenge traffic that they may not want to access the site.\n\n![](https://blog.cloudflare.com/content/images/2023/11/pasted-image-0--5-.png)\n\nIn addition to the customer-configured rules, these campaign sites are also protected by WAF managed rules (run by Cloudflare), with 47% of mitigated traffic identified as HTTP Anomaly and 30% SQLi.\n\n![](https://blog.cloudflare.com/content/images/2023/11/pasted-image-0--6-.png)\n\n### Supporting organizations that promote free and fair elections with Project Galileo\n\nAs part of our analysis we also identified 69 organizations in the United States that are protected under Project Galileo that work on a range of topics related to voting rights and promoting free and fair elections. For those organizations, Cloudflare mitigated 19.13 million threats between November 1, 2022, and August 31, 2023, an average of 62,927 threats per day.\n\nWe saw a spike in traffic during election time in November 2022 and another slight increase in April 2023. During this time, the largest number of blocked requests was mitigated by Cloudflare’s [Security Level](https://developers.cloudflare.com/waf/tools/security-level/). Cloudflare’s Security Level is a security tool that ranks requests based on IP reputation to decide whether to present a [Managed Challenge](https://developers.cloudflare.com/firewall/cf-firewall-rules/cloudflare-challenges/#managed-challenge-recommended) page. A managed challenge helps determine whether the request is considered malicious or legitimate. If the visitor passes the challenge, their request is allowed. If they fail, the request will be blocked. Many of these challenges are issued as a result of domains enabling [Under Attack Mode](https://developers.cloudflare.com/fundamentals/reference/under-attack-mode/), which enforces an elevated Security Level to help mitigate [layer 7](https://www.cloudflare.com/learning/ddos/what-is-layer-7/) DDoS attacks.\n\n![](https://blog.cloudflare.com/content/images/2023/11/pasted-image-0--7-.png)\n\nFor traffic that was mitigated by the WAF, we found the top mitigation categories to be HTTP Anomalies at 48% and SQLi at 25%. Overall, we saw more requests mitigated by Cloudflare’s WAF than traffic that was considered [DDoS](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/).\n\n![](https://blog.cloudflare.com/content/images/2023/11/pasted-image-0--8-.png)\n\n### Taking our elections expertise global\n\nIn 2021, [we announced our partnership](https://blog.cloudflare.com/cloudflares-athenian-project-expands-internationally/) with the [International Foundation for Electoral Systems](https://www.ifes.org/) (IFES) to provide our highest level of protection for free to election management bodies (EMBs) around the world. An EMB is an institution responsible for organizing and overseeing elections in a particular jurisdiction with a primary role of ensuring that the electoral process is conducted fairly and transparently. Since beginning our partnership, we’ve provided protection or expertise to 7 election management bodies to support their work in promoting free and fair elections. As part of this, we’ve worked with election commissions in Kosovo and North Macedonia to protect their election infrastructure.\n\n> “Security is the cornerstone of any democratic process, and free and fair elections are no exception. Security products like those from Cloudflare become even more critical in an increasingly digital world. With Cloudflare, we have effectively mitigated numerous cyber threats, ensuring citizens uninterrupted access to electoral information in Kosovo. This has significantly fostered trust and transparency in our electoral processes.”  \n> \\- **Kreshnik Spahiu**  \n> Director of the Information Technology Department, Central Election Commission of Kosovo\n\nAs we approach 2024 with many elections in newly emerging democracies, we are excited to continue our work with IFES to provide our services and share our expertise to help election groups stay secure online.\n\n![](https://blog.cloudflare.com/content/images/2023/11/Screenshot-2023-11-20-at-10.31.41.png)\n\n### Looking toward 2024…\n\nIf 2024 is anything like 2023, we should continue to expect irregularities regarding Internet access during elections. We’ve seen this in areas such as Cambodia, where ahead of the 2023 elections, [Cambodian officials ordered internet service providers](https://www.voanews.com/a/cambodian-government-blocks-news-sites-before-unopposed-election-/7185151.html) to block website access to three news outlets reporting on the election as a way to control the independent media. In Zimbabwe, a new law known as the [Patriotic Bill was passed before the general election](https://www.amnesty.org/en/latest/news/2023/06/zimbabwe-parliaments-passing-of-patriotic-bill-is-a-grave-assault-on-the-human-rights/), encompassing a wide range of provisions that make it illegal to engage in speech deemed to pose a threat to the nation's sovereignty or vital national interests.\n\nThe last few years contain many examples of how governments have undermined and controlled the flow of information through Internet shutdowns, restricted social media sites during elections, and imposed blocking of websites that report on results. If current trends continue, 2024 will be a pivotal year for online freedoms.\n\nIn light of this, we want to ensure that all groups working to promote democracy around the world have the tools they need to stay secure online. If you work in the election space and need our help, please apply at [https://www.cloudflare.com/election-security](https://www.cloudflare.com/election-security/).\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Election Security](https://blog.cloudflare.com/tag/election-security/) [USA](https://blog.cloudflare.com/tag/usa/) [Athenian Project](https://blog.cloudflare.com/tag/athenian-project/)"
    },
    {
      "url": "https://blog.cloudflare.com/introducing-advanced-session-audit-capabilities-in-cloudflare-one/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/introducing-advanced-session-audit-capabilities-in-cloudflare-one/",
        "loadedTime": "2023-12-05T02:27:56.046Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/introducing-advanced-session-audit-capabilities-in-cloudflare-one/",
        "title": "Introducing advanced session audit capabilities in Cloudflare One",
        "description": "Administrators can now easily audit all active user sessions and associated data used by their Cloudflare One policies. This enables the best of both worlds: extremely granular controls, while maintaining an improved ability to troubleshoot and diagnose",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/16/2023\n6 min read\nThis post is also available in 简体中文, 繁體中文, 日本語, 한국어, Français, Español and Deutsch.\nThe basis of Zero Trust is defining granular controls and authorization policies per application, user, and device. Having a system with a sufficient level of granularity to do this is crucial to meet both regulatory and security requirements. But there is a potential downside to so many controls: in order to troubleshoot user issues, an administrator has to consider a complex combination of variables across applications, user identity, and device information, which may require painstakingly sifting through logs.\nWe think there’s a better way — which is why, starting today, administrators can easily audit all active user sessions and associated data used by their Cloudflare One policies. This enables the best of both worlds: extremely granular controls, while maintaining an improved ability to troubleshoot and diagnose Zero Trust deployments in a single, simple control panel. Information that previously lived in a user’s browser or changed dynamically is now available to administrators without the need to bother an end user or dig into logs.\nA quick primer on application authentication and authorization\nAuthentication and Authorization are the two components that a Zero Trust policy evaluates before allowing a user access to a resource.\nAuthentication is the process of verifying the identity of a user, device, or system. Common methods of authentication include entering usernames and passwords, presenting a digital certificate, or even biometrics like a fingerprint or face scan. Multi-factor authentication (MFA) requires two or more separate methods of authentication for enhanced security, like a hardware key in combination with a password.\nAuthorization is the process of granting or denying access to specific resources or permissions once an entity has been successfully authenticated. It defines what the authenticated entity can and cannot do within the system.\nApplication authentication/authorization mechanisms\nWeb applications, which we'll focus on, generally use HTTP cookies to handle both authentication and authorization.\nAuthentication:\nLogin: When a user logs into a web application by entering their username and password, the application verifies these credentials against its database or in an Identity Provider (IdP). Additional forms of authentication may also be applied to achieve multiple factors of authentication. If they match, the server or external security service (e.g., Cloudflare Access) considers the user authenticated.\nCookie/Token Creation: The server then creates a session for the user in the form of a cookie or JSON Web Token. The cookie is valid for a period of time until the user has to reauthenticate.\nSending and Storing Cookies: The server sends a response back to the user's browser which includes the session ID and other identifying information about the user in the cookie. The browser then stores this cookie. This cookie is used to recognize the user in their subsequent requests.\nAuthorization:\nSubsequent Requests: For all subsequent requests to the web application, the user's browser automatically includes the cookie (with the session ID and other identifying information) in the request.\nServer-side Verification: The server gets the user data from the cookie and checks if the session is valid. If it's valid, the server also retrieves the user's details and their access permissions associated with that session ID.\nAuthorization Decision: Based on the user's access permissions, the server decides whether the user is authorized to perform the requested operation or access the requested resource.\nThis way, the user stays authenticated (and their authorization can be checked) for all subsequent requests after logging in, until the session expires, or they log out.\nIn modern web applications, this session state is most commonly stored in the form of a JSON Web Token (JWT).\nDebugging JWT based authentication\nJWTs are used in many modern web applications, and Zero Trust Network Access (ZTNA) solutions like Cloudflare Access, for authentication and authorization. A JWT includes a payload that encodes information about the user and possibly other data, and it's signed by the server to prevent tampering. JWTs are often used in a stateless manner, meaning the server doesn't keep a copy of each JWT—it simply verifies and decodes them as they come in with requests. The stateless nature of JWTs means that you do not have to rely on a central system to handle user session management which avoids creating scalability issues as the number of users accessing a system increases.\nHowever, this stateless nature of JWTs makes debugging JWT-based authentication tricky without getting the specific JWT from a user. Here's why:\n1. Token Specificity: Each JWT is specific to a user and a session. It contains information (claims) about the user, the issuing authority, the token's issuing time, expiration time, and possibly other data. Therefore, to debug a problem, you often need the exact JWT that's causing the issue.\n2. No Server-side Records: Since JWTs are stateless, the server does not store sessions by default. It can't look up past tokens or their associated state, unless it's been specifically designed to log them, which is usually not the case due to privacy and data minimization considerations.\n3. Transient Issues: Problems with JWTs can be transient—they might relate to the specific moment the token was used. For instance, if a token was expired when a user tried to use it, you'd need that specific token to debug the issue.\n4. Privacy and Security: JWTs can contain sensitive information, so they should be handled with care. Getting a JWT from a user might expose their personal information or security credentials to whoever is debugging the issue. In addition, if a user sends their JWT through an insecure channel to a developer or an IT help desk, it could be intercepted (Cloudflare recently released a free HAR Sanitizer to help mitigate this concern).\nThese factors make it difficult to troubleshoot issues with JWT based authentication without having the specific token in question.\nA better way to debug identity issues\nWe set out to build a better way to debug issues related to a user’s identity in Cloudflare Zero Trust without sharing JWTs or HAR files back and forth. Administrators can now view a user’s Registry Identity (used for Gateway policies) and all active Access sessions. \nThis session information includes the full identity evaluated by Zero Trust including IdP claims, device posture information, network context and more. We were able to build this feature without any additional load on Access’ authentication logic by leveraging Cloudflare Workers KV. At the time a user authenticates with Access, their associated identity is immediately saved into a Key/Value pair in Workers KV. This all occurs within the context of the user’s authentication event which means there is minimal latency impact or reliance on an external service.\nThis feature is available to all customers across all Zero Trust plans. If you would like to get started with Cloudflare Zero Trust, sign up for a free account for up to 50 users, today! Or, collaborate with Cloudflare experts to discuss SSE or SASE for your organization and tackle your Zero Trust use cases one step at a time.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nSASE Cloudflare Zero Trust Product News Cloudflare One Cloudflare Workers KV",
      "markdown": "11/16/2023\n\n*   [![Kenny Johnson](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2020/12/0DA903DE-E315-43D5-B0DE-39870448303D.jpeg)](https://blog.cloudflare.com/author/kenny/)\n\n6 min read\n\nThis post is also available in [简体中文](https://blog.cloudflare.com/zh-cn/introducing-advanced-session-audit-capabilities-in-cloudflare-one-zh-cn/), [繁體中文](https://blog.cloudflare.com/zh-tw/introducing-advanced-session-audit-capabilities-in-cloudflare-one-zh-tw/), [日本語](https://blog.cloudflare.com/ja-jp/introducing-advanced-session-audit-capabilities-in-cloudflare-one-ja-jp/), [한국어](https://blog.cloudflare.com/ko-kr/introducing-advanced-session-audit-capabilities-in-cloudflare-one-ko-kr/), [Français](https://blog.cloudflare.com/fr-fr/introducing-advanced-session-audit-capabilities-in-cloudflare-one-fr-fr/), [Español](https://blog.cloudflare.com/es-es/introducing-advanced-session-audit-capabilities-in-cloudflare-one-es-es/) and [Deutsch](https://blog.cloudflare.com/de-de/introducing-advanced-session-audit-capabilities-in-cloudflare-one-de-de/).\n\n![](https://blog.cloudflare.com/content/images/2023/11/image5.png)\n\nThe basis of Zero Trust is defining granular controls and authorization policies per application, user, and device. Having a system with a sufficient level of granularity to do this is crucial to meet both regulatory and security requirements. But there is a potential downside to so many controls: in order to troubleshoot user issues, an administrator has to consider a complex combination of variables across applications, user identity, and device information, which may require painstakingly sifting through logs.\n\nWe think there’s a better way — which is why, starting today, administrators can easily audit all active user sessions and associated data used by their Cloudflare One policies. This enables the best of both worlds: extremely granular controls, while maintaining an improved ability to troubleshoot and diagnose [Zero Trust](https://www.cloudflare.com/learning/security/glossary/what-is-zero-trust/) deployments in a single, simple control panel. Information that previously lived in a user’s browser or changed dynamically is now available to administrators without the need to bother an end user or dig into logs.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image4.png)\n\n### **A quick primer on application authentication and authorization**\n\n_Authentication_ and _Authorization_ are the two components that a Zero Trust policy evaluates before allowing a user access to a resource.\n\n**Authentication** is the process of verifying the identity of a user, device, or system. Common methods of [authentication](https://www.cloudflare.com/learning/access-management/what-is-authentication/) include entering usernames and passwords, presenting a digital certificate, or even biometrics like a fingerprint or face scan. [Multi-factor authentication (MFA)](https://www.cloudflare.com/learning/access-management/what-is-multi-factor-authentication/) requires two or more separate methods of authentication for enhanced security, like a hardware key in combination with a password.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image6.png)\n\n**Authorization** is the process of granting or denying access to specific resources or permissions once an entity has been successfully authenticated. It defines what the authenticated entity can and cannot do within the system.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image1-3.png)\n\n### **Application authentication/authorization mechanisms**\n\nWeb applications, which we'll focus on, generally use HTTP cookies to handle both authentication and authorization.\n\n**Authentication:**\n\n1.  **Login**: When a user logs into a web application by entering their username and password, the application verifies these credentials against its database or in an [Identity Provider (IdP)](https://www.cloudflare.com/learning/access-management/what-is-an-identity-provider/). Additional forms of authentication may also be applied to achieve multiple factors of authentication. If they match, the server or external security service (e.g., Cloudflare Access) considers the user authenticated.\n2.  **Cookie/Token Creation**: The server then creates a session for the user in the form of a cookie or JSON Web Token. The cookie is valid for a period of time until the user has to reauthenticate.\n3.  **Sending and Storing Cookies**: The server sends a response back to the user's browser which includes the session ID and other identifying information about the user in the cookie. The browser then stores this cookie. This cookie is used to recognize the user in their subsequent requests.\n\n**Authorization:**\n\n1.  **Subsequent Requests**: For all subsequent requests to the web application, the user's browser automatically includes the cookie (with the session ID and other identifying information) in the request.\n2.  **Server-side Verification**: The server gets the user data from the cookie and checks if the session is valid. If it's valid, the server also retrieves the user's details and their access permissions associated with that session ID.\n3.  **Authorization Decision**: Based on the user's access permissions, the server decides whether the user is authorized to perform the requested operation or access the requested resource.\n\nThis way, the user stays authenticated (and their authorization can be checked) for all subsequent requests after logging in, until the session expires, or they log out.\n\nIn modern web applications, this session state is most commonly stored in the form of a JSON Web Token (JWT).\n\n![](https://blog.cloudflare.com/content/images/2023/11/image8.png)\n\n### **Debugging JWT based authentication**\n\nJWTs are used in many modern web applications, and [Zero Trust Network Access (ZTNA)](https://www.cloudflare.com/learning/access-management/what-is-ztna/) solutions like Cloudflare Access, for authentication and authorization. A JWT includes a payload that encodes information about the user and possibly other data, and it's signed by the server to prevent tampering. JWTs are often used in a stateless manner, meaning the server doesn't keep a copy of each JWT—it simply verifies and decodes them as they come in with requests. The stateless nature of JWTs means that you do not have to rely on a central system to handle user session management which avoids creating scalability issues as the number of users accessing a system increases.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image2-2.png)\n\nHowever, this stateless nature of JWTs makes debugging JWT-based authentication tricky without getting the specific JWT from a user. Here's why:\n\n**1\\. Token Specificity**: Each JWT is specific to a user and a session. It contains information (claims) about the user, the issuing authority, the token's issuing time, expiration time, and possibly other data. Therefore, to debug a problem, you often need the exact JWT that's causing the issue.\n\n**2\\. No Server-side Records**: Since JWTs are stateless, the server does not store sessions by default. It can't look up past tokens or their associated state, unless it's been specifically designed to log them, which is usually not the case due to privacy and data minimization considerations.\n\n**3\\. Transient Issues**: Problems with JWTs can be transient—they might relate to the specific moment the token was used. For instance, if a token was expired when a user tried to use it, you'd need that specific token to debug the issue.\n\n**4\\. Privacy and Security**: JWTs can contain sensitive information, so they should be handled with care. Getting a JWT from a user might expose their personal information or security credentials to whoever is debugging the issue. In addition, if a user sends their JWT through an insecure channel to a developer or an IT help desk, it could be intercepted (Cloudflare recently released a free [HAR Sanitizer](https://blog.cloudflare.com/introducing-har-sanitizer-secure-har-sharing/) to help mitigate this concern).\n\nThese factors make it difficult to troubleshoot issues with JWT based authentication without having the specific token in question.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image3.png)\n\n### **A better way to debug identity issues**\n\nWe set out to build a better way to debug issues related to a user’s identity in Cloudflare Zero Trust without sharing JWTs or HAR files back and forth. Administrators can now view a user’s Registry Identity (used for Gateway policies) and all active Access sessions.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image7.png)\n\nThis session information includes the full identity evaluated by Zero Trust including IdP claims, device posture information, network context and more. We were able to build this feature without any additional load on Access’ authentication logic by leveraging Cloudflare Workers KV. At the time a user authenticates with Access, their associated identity is immediately saved into a Key/Value pair in Workers KV. This all occurs within the context of the user’s authentication event which means there is minimal latency impact or reliance on an external service.\n\nThis feature is available to all customers across all Zero Trust plans. If you would like to get started with Cloudflare Zero Trust, [sign up for a free account](https://dash.cloudflare.com/sign-up/teams) for up to 50 users, today! Or, [collaborate with Cloudflare experts](https://www.cloudflare.com/products/zero-trust/plans/enterprise/) to discuss SSE or SASE for your organization and tackle your Zero Trust use cases one step at a time.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[SASE](https://blog.cloudflare.com/tag/sase/) [Cloudflare Zero Trust](https://blog.cloudflare.com/tag/cloudflare-zero-trust/) [Product News](https://blog.cloudflare.com/tag/product-news/) [Cloudflare One](https://blog.cloudflare.com/tag/cloudflare-one/) [Cloudflare Workers KV](https://blog.cloudflare.com/tag/cloudflare-workers-kv/)"
    },
    {
      "url": "https://blog.cloudflare.com/how-to-execute-an-object-file-part-4/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/how-to-execute-an-object-file-part-4/",
        "loadedTime": "2023-12-05T02:27:55.542Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/how-to-execute-an-object-file-part-4/",
        "title": "How to execute an object file: Part 4, AArch64 edition",
        "description": "The initial posts are dedicated to the x86 architecture. Since then, the fleet of our working machines has expanded to include a large and growing number of ARM CPUs. This time we’ll repeat this exercise for the aarch64 architecture.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/17/2023\n19 min read\nTranslating source code written in a high-level programming language into an executable binary typically involves a series of steps, namely compiling and assembling the code into object files, and then linking those object files into the final executable. However, there are certain scenarios where it can be useful to apply an alternate approach that involves executing object files directly, bypassing the linker. For example, we might use it for malware analysis or when part of the code requires an incompatible compiler. We’ll be focusing on the latter scenario: when one of our libraries needed to be compiled differently from the rest of the code. Learning how to execute an object file directly will give you a much better sense of how code is compiled and linked together.\nTo demonstrate how this was done, we have previously published a series of posts on executing an object file:\nHow to execute an object file: Part 1\nHow to execute an object file: Part 2\nHow to execute an object file: Part 3\nThe initial posts are dedicated to the x86 architecture. Since then the fleet of our working machines has expanded to include a large and growing number of ARM CPUs. This time we’ll repeat this exercise for the aarch64 architecture. You can pause here to read the previous blog posts before proceeding with this one, or read through the brief summary below and reference the earlier posts for more detail. We might reiterate some theory as working with ELF files can be daunting, if it’s not your day-to-day routine. Also, please be mindful that for simplicity, these examples omit bounds and integrity checks. Let the journey begin!\nIntroduction\nIn order to obtain an object file or an executable binary from a high-level compiled programming language the code needs to be processed by three components: compiler, assembler and linker. The compiler generates an assembly listing. This assembly listing is picked up by the assembler and translated into an object file. All source files, if a program contains multiple, go through these two steps generating an object file for each source file. At the final step the linker unites all object files into one binary, additionally resolving references to the shared libraries (i.e. we don’t implement the printf function each time, rather we take it from a system library). Even though the approach is platform independent, the compiler output varies by platform as the assembly listing is closely tied to the CPU architecture.\nGCC (GNU Compiler Collection) can run each step: compiler, assembler and linker separately for us:\nmain.c:\n#include <stdio.h> int main(void) { puts(\"Hello, world!\"); return 0; } \nCompiler (output main.s - assembly listing):\n$ gcc -S main.c $ ls main.c main.s \nAssembler (output main.o - an object file):\n$ gcc -c main.s -o main.o $ ls main.c main.o main.s \nLinker (main - an object file):\n$ gcc main.o -o main $ ls main main.c main.o main.s $ ./main Hello, world! \nAll the examples assume gcc is running on a native aarch64 architecture or include a cross compilation flag for those who want to reproduce and have no aarch64.\nWe have two object files in the output above: main.o and main. Object files are files encoded with the ELF (Executable and Linkable Format) standard. Although, main.o is an ELF file, it doesn’t contain all the information to be fully executable.\n$ file main.o main.o: ELF 64-bit LSB relocatable, ARM aarch64, version 1 (SYSV), not stripped $ file main main: ELF 64-bit LSB pie executable, ARM aarch64, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-aarch64.so.1, BuildID[sha1]=d3ecd2f8ac3b2dec11ed4cc424f15b3e1f130dd4, for GNU/Linux 3.7.0, not stripped \nThe ELF File\nThe central idea of this series of blog posts is to understand how to resolve dependencies from object files without directly involving the linker. For illustrative purposes we generated an object file based on some C-code and used it as a library for our main program. Before switching to the code, we need to understand the basics of the ELF structure.\nEach ELF file is made up of one ELF header, followed by file data. The data can include: a program header table, a section header table, and the data which is referred to by the program or section header tables.\nThe ELF header provides some basic information about the file: what architecture the file is compiled for, the program entry point and the references to other tables.\nThe ELF Header:\n$ readelf -h main ELF Header: Magic: 7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00 Class: ELF64 Data: 2's complement, little endian Version: 1 (current) OS/ABI: UNIX - System V ABI Version: 0 Type: DYN (Position-Independent Executable file) Machine: AArch64 Version: 0x1 Entry point address: 0x640 Start of program headers: 64 (bytes into file) Start of section headers: 68576 (bytes into file) Flags: 0x0 Size of this header: 64 (bytes) Size of program headers: 56 (bytes) Number of program headers: 9 Size of section headers: 64 (bytes) Number of section headers: 29 Section header string table index: 28 \nThe execution process of almost every program starts from an auxiliary program, called loader, which arranges the memory and calls the program’s entry point. In the following output the loader is marked with a line “Requesting program interpreter: /lib/ld-linux-aarch64.so.1”. The whole program memory is split into different segments with associated size, permissions and type (which instructs the loader on how to interpret this block of memory). Because the execution process should be performed in the shortest possible time, the sections with the same characteristics and located nearby are grouped into bigger blocks — segments — and placed in the program header. We can say that the program header summarizes the types of data that appear in the section header.\nThe ELF Program Header:\n$ readelf -Wl main Elf file type is DYN (Position-Independent Executable file) Entry point 0x640 There are 9 program headers, starting at offset 64 Program Headers: Type Offset VirtAddr PhysAddr FileSiz MemSiz Flg Align PHDR 0x000040 0x0000000000000040 0x0000000000000040 0x0001f8 0x0001f8 R 0x8 INTERP 0x000238 0x0000000000000238 0x0000000000000238 0x00001b 0x00001b R 0x1 [Requesting program interpreter: /lib/ld-linux-aarch64.so.1] LOAD 0x000000 0x0000000000000000 0x0000000000000000 0x00088c 0x00088c R E 0x10000 LOAD 0x00fdc8 0x000000000001fdc8 0x000000000001fdc8 0x000270 0x000278 RW 0x10000 DYNAMIC 0x00fdd8 0x000000000001fdd8 0x000000000001fdd8 0x0001e0 0x0001e0 RW 0x8 NOTE 0x000254 0x0000000000000254 0x0000000000000254 0x000044 0x000044 R 0x4 GNU_EH_FRAME 0x0007a0 0x00000000000007a0 0x00000000000007a0 0x00003c 0x00003c R 0x4 GNU_STACK 0x000000 0x0000000000000000 0x0000000000000000 0x000000 0x000000 RW 0x10 GNU_RELRO 0x00fdc8 0x000000000001fdc8 0x000000000001fdc8 0x000238 0x000238 R 0x1 Section to Segment mapping: Segment Sections... 00 01 .interp 02 .interp .note.gnu.build-id .note.ABI-tag .gnu.hash .dynsym .dynstr .gnu.version .gnu.version_r .rela.dyn .rela.plt .init .plt .text .fini .rodata .eh_frame_hdr .eh_frame 03 .init_array .fini_array .dynamic .got .got.plt .data .bss 04 .dynamic 05 .note.gnu.build-id .note.ABI-tag 06 .eh_frame_hdr 07 08 .init_array .fini_array .dynamic .got \nIn the source code of high-level languages, variables, functions, and constants are mixed together. However, in assembly you might see that the data and instructions are separated into different blocks. The ELF file content is divided in an even more granular way. For example, variables with initial values are placed into different sections than the uninitialized ones. This approach optimizes for space, otherwise the values for uninitialized variables would be filled with zeros. Along with the space efficiency, there are security reasons for stratification — executable instructions can’t have writable permissions, while memory containing variables can't be executable. The section header describes each of these sections.\nThe ELF Section Header:\n$ readelf -SW main There are 29 section headers, starting at offset 0x10be0: Section Headers: [Nr] Name Type Address Off Size ES Flg Lk Inf Al [ 0] NULL 0000000000000000 000000 000000 00 0 0 0 [ 1] .interp PROGBITS 0000000000000238 000238 00001b 00 A 0 0 1 [ 2] .note.gnu.build-id NOTE 0000000000000254 000254 000024 00 A 0 0 4 [ 3] .note.ABI-tag NOTE 0000000000000278 000278 000020 00 A 0 0 4 [ 4] .gnu.hash GNU_HASH 0000000000000298 000298 00001c 00 A 5 0 8 [ 5] .dynsym DYNSYM 00000000000002b8 0002b8 0000f0 18 A 6 3 8 [ 6] .dynstr STRTAB 00000000000003a8 0003a8 000092 00 A 0 0 1 [ 7] .gnu.version VERSYM 000000000000043a 00043a 000014 02 A 5 0 2 [ 8] .gnu.version_r VERNEED 0000000000000450 000450 000030 00 A 6 1 8 [ 9] .rela.dyn RELA 0000000000000480 000480 0000c0 18 A 5 0 8 [10] .rela.plt RELA 0000000000000540 000540 000078 18 AI 5 22 8 [11] .init PROGBITS 00000000000005b8 0005b8 000018 00 AX 0 0 4 [12] .plt PROGBITS 00000000000005d0 0005d0 000070 00 AX 0 0 16 [13] .text PROGBITS 0000000000000640 000640 000134 00 AX 0 0 64 [14] .fini PROGBITS 0000000000000774 000774 000014 00 AX 0 0 4 [15] .rodata PROGBITS 0000000000000788 000788 000016 00 A 0 0 8 [16] .eh_frame_hdr PROGBITS 00000000000007a0 0007a0 00003c 00 A 0 0 4 [17] .eh_frame PROGBITS 00000000000007e0 0007e0 0000ac 00 A 0 0 8 [18] .init_array INIT_ARRAY 000000000001fdc8 00fdc8 000008 08 WA 0 0 8 [19] .fini_array FINI_ARRAY 000000000001fdd0 00fdd0 000008 08 WA 0 0 8 [20] .dynamic DYNAMIC 000000000001fdd8 00fdd8 0001e0 10 WA 6 0 8 [21] .got PROGBITS 000000000001ffb8 00ffb8 000030 08 WA 0 0 8 [22] .got.plt PROGBITS 000000000001ffe8 00ffe8 000040 08 WA 0 0 8 [23] .data PROGBITS 0000000000020028 010028 000010 00 WA 0 0 8 [24] .bss NOBITS 0000000000020038 010038 000008 00 WA 0 0 1 [25] .comment PROGBITS 0000000000000000 010038 00001f 01 MS 0 0 1 [26] .symtab SYMTAB 0000000000000000 010058 000858 18 27 66 8 [27] .strtab STRTAB 0000000000000000 0108b0 00022c 00 0 0 1 [28] .shstrtab STRTAB 0000000000000000 010adc 000103 00 0 0 1 Key to Flags: W (write), A (alloc), X (execute), M (merge), S (strings), I (info), L (link order), O (extra OS processing required), G (group), T (TLS), C (compressed), x (unknown), o (OS specific), E (exclude), D (mbind), p (processor specific) \nExecuting example from Part 1 on aarch64\nActually, our initial code from Part 1 works on aarch64 as is!\nLet’s have a quick summary about what was done in the code:\nWe need to find the code of two functions (add5 and add10) in the .text section of our object file (obj.o)\nLoad the functions in the executable memory\nReturn the memory locations of the functions to the main program\nThere is one nuance: even though all the sections are in the section header, neither of them have a string name. Without the names we can’t identify them. However, having an additional character field for each section in the ELF structure would be inefficient for the space — it must be limited by some maximum length and those names which are shorter would leave the space unfilled. Instead, ELF provides an additional section, .shstrtab. This string table concatenates all the names where each name ends with a null terminated byte. We can iterate over the names and match with an offset held by other sections to reference their name. But how do we find .shstrtab itself if we don’t have a name? To solve this chicken and egg problem, the ELF program header provides a direct pointer to .shstrtab. The similar approach is applied to two other sections: .symtab and .strtab. Where .symtab contains all information about the symbols and .strtab holds the list of symbol names. In the code we work with these tables to resolve all their dependencies and find our functions.\nExecuting example from Part 2 on aarch64\nAt the beginning of the second blog post we made the function add10 depend on add5 instead of being self-contained. This is the first time when we faced relocations. Relocations is the process of loading symbols defined outside the current scope. The relocated symbols can present global or thread-local variables, constant, functions, etc. We’ll start from checking assembly instructions which trigger relocations and uncovering how the ELF format handles them in a more general way.\nAfter making add10 depend on add5 our aarch64 version stopped working as well, similarly to the x86. Let’s take a look at assembly listing:\n$ objdump --disassemble --section=.text obj.o obj.o: file format elf64-littleaarch64 Disassembly of section .text: 0000000000000000 <add5>: 0: d10043ff sub sp, sp, #0x10 4: b9000fe0 str w0, [sp, #12] 8: b9400fe0 ldr w0, [sp, #12] c: 11001400 add w0, w0, #0x5 10: 910043ff add sp, sp, #0x10 14: d65f03c0 ret 0000000000000018 <add10>: 18: a9be7bfd stp x29, x30, [sp, #-32]! 1c: 910003fd mov x29, sp 20: b9001fe0 str w0, [sp, #28] 24: b9401fe0 ldr w0, [sp, #28] 28: 94000000 bl 0 <add5> 2c: b9001fe0 str w0, [sp, #28] 30: b9401fe0 ldr w0, [sp, #28] 34: 94000000 bl 0 <add5> 38: a8c27bfd ldp x29, x30, [sp], #32 3c: d65f03c0 ret \nHave you noticed that all the hex values in the second column are exactly the same length, in contrast with the instructions lengths seen for x86 in Part 2 of our series? This is because all Armv8-A instructions are presented in 32 bits. Since it is impossible to encode every immediate value into less than 32 bits, some operations require more than one instruction, as we’ll see later. For now, we’re interested in one instruction - bl (branch with link) on rows 28 and 34. The bl is a “jump” instruction, but before the jump it preserves the next instruction after the current one in the link register (lr). When the callee finishes execution the caller address is recovered from lr. Usually, the aarch64 instructions reserve the last 6 bits [31:26] for opcode and some auxiliary fields such as running architecture (32 or 64 bits), condition flag and others. Remaining bits are shared between arguments like source register, destination register and immediate value. Since the bl instruction does not require a source or destination register, the full 26 bits can be used to encode the immediate offset instead. However, 26 bits can only encode a small range (+/-32 MB), but because the jump can only target a beginning of an instruction, it must always be aligned to 4 bytes, which increases the effective range of the encoded immediate fourfold, to +/-128 MB.\nSimilarly to what we did in Part 2 we’re going to resolve our relocations - first by manually calculating the correct addresses and then by using an approach similar to what the linker does. The current value of our bl instruction is 94000000 or in binary representation 10010100000000000000000000000000. All 26 bits are zeros, so we don't jump anywhere. The address is calculated by an offset from the current program counter (pc), which can be positive or negative. In our case we expect it to be -0x28 and -0x34. As described above, it should be divided by 4 and taken as two's complements: -0x28 / 4 = -0xA == 0xFFFFFFF6 and -0x34 / 4 = -0xD == 0xFFFFFFF3. From these values we need to take the lower 26 bits and concatenate them with the initial 6 bits to get the final instruction. So, the final ones will be: 10010111111111111111111111110110 == 0x97FFFFF6 and 10010111111111111111111111110011 == 0x97FFFFF3. Have you noticed that all the distance calculations are done relative to the bl (or current pc), not the next instruction as in x86?\nLet’s add to the code and execute:\n... static void parse_obj(void) { ... /* copy the contents of `.text` section from the ELF file */ memcpy(text_runtime_base, obj.base + text_hdr->sh_offset, text_hdr->sh_size); *((uint32_t *)(text_runtime_base + 0x28)) = 0x97FFFFF6; *((uint32_t *)(text_runtime_base + 0x34)) = 0x97FFFFF3; /* make the `.text` copy readonly and executable */ if (mprotect(text_runtime_base, page_align(text_hdr->sh_size), PROT_READ | PROT_EXEC)) { ... \nCompile and run:\n$ gcc -o loader loader.c $ ./loader Executing add5... add5(42) = 47 Executing add10... add10(42) = 52 \nIt works! But this is not how the linker handles the relocations. The linker resolves relocation based on the type and formula assigned to this type. In our Part 2 we investigated it quite well. Here again we need to find the type and check the formula for this type: \n$ readelf --relocs obj.o Relocation section '.rela.text' at offset 0x228 contains 2 entries: Offset Info Type Sym. Value Sym. Name + Addend 000000000028 000a0000011b R_AARCH64_CALL26 0000000000000000 add5 + 0 000000000034 000a0000011b R_AARCH64_CALL26 0000000000000000 add5 + 0 Relocation section '.rela.eh_frame' at offset 0x258 contains 2 entries: Offset Info Type Sym. Value Sym. Name + Addend 00000000001c 000200000105 R_AARCH64_PREL32 0000000000000000 .text + 0 000000000034 000200000105 R_AARCH64_PREL32 0000000000000000 .text + 18 \nOur Type is R_AARCH64_CALL26 and the formula for it is:\nELF64 Code\n\t\nName\n\t\nOperation\n\t\n283\n\t\nR_<CLS>_CALL26\n\t\nS + A - P\n\t\nwhere:\nS (when used on its own) is the address of the symbol\nA is the addend for the relocation\nP is the address of the place being relocated (derived from r_offset)\nHere are the relevant changes to loader.c:\n/* Replace `#define R_X86_64_PLT32 4` with our Type */ #define R_AARCH64_CALL26 283 ... static void do_text_relocations(void) { ... uint32_t val; switch (type) { case R_AARCH64_CALL26: /* The mask separates opcode (6 bits) and the immediate value */ uint32_t mask_bl = (0xffffffff << 26); /* S+A-P, divided by 4 */ val = (symbol_address + relocations[i].r_addend - patch_offset) >> 2; /* Concatenate opcode and value to get final instruction */ *((uint32_t *)patch_offset) &= mask_bl; val &= ~mask_bl; *((uint32_t *)patch_offset) |= val; break; } ... } \nCompile and run:\n$ gcc -o loader loader.c $ ./loader Calculated relocation: 0x97fffff6 Calculated relocation: 0x97fffff3 Executing add5... add5(42) = 47 Executing add10... add10(42) = 52 \nSo far so good. The next challenge is to add constant data and global variables to our object file and check relocations again:\n$ readelf --relocs --wide obj.o Relocation section '.rela.text' at offset 0x388 contains 8 entries: Offset Info Type Symbol's Value Symbol's Name + Addend 0000000000000000 0000000500000113 R_AARCH64_ADR_PREL_PG_HI21 0000000000000000 .rodata + 0 0000000000000004 0000000500000115 R_AARCH64_ADD_ABS_LO12_NC 0000000000000000 .rodata + 0 000000000000000c 0000000300000113 R_AARCH64_ADR_PREL_PG_HI21 0000000000000000 .data + 0 0000000000000010 0000000300000115 R_AARCH64_ADD_ABS_LO12_NC 0000000000000000 .data + 0 0000000000000024 0000000300000113 R_AARCH64_ADR_PREL_PG_HI21 0000000000000000 .data + 0 0000000000000028 0000000300000115 R_AARCH64_ADD_ABS_LO12_NC 0000000000000000 .data + 0 0000000000000068 000000110000011b R_AARCH64_CALL26 0000000000000040 add5 + 0 0000000000000074 000000110000011b R_AARCH64_CALL26 0000000000000040 add5 + 0 ... \nWe have even two new relocations: R_AARCH64_ADD_ABS_LO12_NC and R_AARCH64_ADR_PREL_PG_HI21. Their formulas are:\nELF64 Code\n\t\nName\n\t\nOperation\n\t\n275\n\t\nR_<CLS>_ ADR_PREL_PG_HI21\n\t\nPage(S+A) - Page(P)\n\t\n277\n\t\nR_<CLS>_ ADD_ABS_LO12_NC\n\t\nS + A\n\t\nwhere:\nPage(expr) is the page address of the expression expr, defined as (expr & ~0xFFF). (This applies even if the machine page size supported by the platform has a different value.)\nIt’s a bit unclear why we have two new types, while in x86 we had only one. Let’s investigate the assembly code:\n$ objdump --disassemble --section=.text obj.o obj.o: file format elf64-littleaarch64 Disassembly of section .text: 0000000000000000 <get_hello>: 0: 90000000 adrp x0, 0 <get_hello> 4: 91000000 add x0, x0, #0x0 8: d65f03c0 ret 000000000000000c <get_var>: c: 90000000 adrp x0, 0 <get_hello> 10: 91000000 add x0, x0, #0x0 14: b9400000 ldr w0, [x0] 18: d65f03c0 ret 000000000000001c <set_var>: 1c: d10043ff sub sp, sp, #0x10 20: b9000fe0 str w0, [sp, #12] 24: 90000000 adrp x0, 0 <get_hello> 28: 91000000 add x0, x0, #0x0 2c: b9400fe1 ldr w1, [sp, #12] 30: b9000001 str w1, [x0] 34: d503201f nop 38: 910043ff add sp, sp, #0x10 3c: d65f03c0 ret \nWe see that all adrp instructions are followed by add instructions. The add instruction adds an immediate value to the source register and writes the result to the destination register. The source and destination registers can be the same, the immediate value is 12 bits. The adrp instruction generates a pc-relative (program counter) address and writes the result to the destination register. It takes pc of the instruction itself and adds a 21-bit immediate value shifted left by 12 bits. If the immediate value weren’t shifted it would lie in a range of +/-1 MB memory, which isn’t enough. The left shift increases the range up to +/-1 GB. However, because the 12 bits are masked out with the shift, we need to store them somewhere and restore later. That’s why we see add instruction following adrp and two types instead of one. Also, it’s a bit tricky to encode adrp: 2 low bits of immediate value are placed in the position 30:29 and the rest in the position 23:5. Due to size limitations, the aarch64 instructions try to make the most out of 32 bits.\nIn the code we are going to use the formulas to calculate the values and description of adrp and add instructions to obtain the final opcode:\n#define R_AARCH64_CALL26 283 #define R_AARCH64_ADD_ABS_LO12_NC 277 #define R_AARCH64_ADR_PREL_PG_HI21 275 ... { case R_AARCH64_CALL26: /* The mask separates opcode (6 bits) and the immediate value */ uint32_t mask_bl = (0xffffffff << 26); /* S+A-P, divided by 4 */ val = (symbol_address + relocations[i].r_addend - patch_offset) >> 2; /* Concatenate opcode and value to get final instruction */ *((uint32_t *)patch_offset) &= mask_bl; val &= ~mask_bl; *((uint32_t *)patch_offset) |= val; break; case R_AARCH64_ADD_ABS_LO12_NC: /* The mask of `add` instruction to separate * opcode, registers and calculated value */ uint32_t mask_add = 0b11111111110000000000001111111111; /* S + A */ uint32_t val = *(symbol_address + relocations[i].r_addend); val &= ~mask_add; *((uint32_t *)patch_offset) &= mask_add; /* Final instruction */ *((uint32_t *)patch_offset) |= val; case R_AARCH64_ADR_PREL_PG_HI21: /* Page(S+A)-Page(P), Page(expr) is defined as (expr & ~0xFFF) */ val = (((uint64_t)(symbol_address + relocations[i].r_addend)) & ~0xFFF) - (((uint64_t)patch_offset) & ~0xFFF); /* Shift right the calculated value by 12 bits. * During decoding it will be shifted left as described above, * so we do the opposite. */ val >>= 12; /* Separate the lower and upper bits to place them in different positions */ uint32_t immlo = (val & (0xf >> 2)) << 29 ; uint32_t immhi = (val & ((0xffffff >> 13) << 2)) << 22; *((uint32_t *)patch_offset) |= immlo; *((uint32_t *)patch_offset) |= immhi; break; } \nCompile and run:\n$ gcc -o loader loader.c $ ./loader Executing add5... add5(42) = 47 Executing add10... add10(42) = 52 Executing get_hello... get_hello() = Hello, world! Executing get_var... get_var() = 5 Executing set_var(42)... Executing get_var again... get_var() = 42 \nIt works! The final code is here.\nExecuting example from Part 3 on aarch64\nOur Part 3 is about resolving external dependencies. When we write code we don’t think much about how to allocate memory or print debug information to the console. Instead, we involve functions from the system libraries. But the code of system libraries needs to be passed through to our programs somehow. Additionally, for optimization purposes, it would be nice if this code would be stored in one place and shared between all programs. And another wish — we don’t want to resolve all the functions and global variables from the libraries, only those which we need and at those times when we need them. To solve these problems, ELF introduced two sections: PLT (the procedure linkage table) and GOT (the global offset table). The dynamic loader creates a list which contains all external functions and variables from the shared library, but doesn’t resolve them immediately; instead they are placed in the PLT section. Each external symbol is presented by a small function, a stub, e.g. puts@plt. When an external symbol is requested, the stub checks if it was resolved previously. If not, the stub searches for an absolute address of the symbol, returns to the requester and writes it in the GOT table. The next time, the address returns directly from the GOT table.\nIn Part 3 we implemented a simplified PLT/GOT resolution. Firstly, we added a new function say_hello in the obj.c, which calls unresolved system library function puts. Further we added an optional wrapper my_puts in the loader.c. The wrapper isn’t required, we could’ve resolved directly to a standard function, but it's a good example of how the implementation of some functions can be overwritten with custom code. In the next steps we added our PLT/GOT resolution:\nPLT section we replaced with a jumptable\nGOT we replaced with assembly instructions\nBasically, we created a small stub with assembly code (our jumptable) to resolve the global address of our my_puts wrapper and jump to it.\nThe approach for aarch64 is the same. But the jumptable is very different as it consists of different assembly instructions.\nThe big difference here compared to the other parts is that we need to work with a 64-bit address for the GOT resolution. Our custom PLT or jumptable is placed close to the main code of obj.c and can operate with the relative addresses as before. For the GOT or referencing my_puts wrapper we’ll use different branch instructions — br or blr. These instructions branch to the register, where the aarch64 registers can hold 64-bit values.\nWe can check how it resolves with the native PLT/GOT in our loader assembly code:\n$ objdump --disassemble --section=.text loader ... 1d2c: 97fffb45 bl a40 <puts@plt> 1d30: f94017e0 ldr x0, [sp, #40] 1d34: d63f0000 blr x0 ... \nThe first instruction is bl jump to puts@plt stub. The next ldr instruction tells us that some value was loaded into the register x0 from the stack. Each function has its own stack frame to hold the local variables. The last blr instruction makes a jump to the address stored in x0 register. There is an agreement in the register naming: if the stored value is 64-bits then the register is called x0-x30; if only 32-bits are used then it’s called w0-w30 (the value will be stored in the lower 32-bits and upper 32-bits will be zeroed).\nWe need to do something similar — place the absolute address of our my_puts wrapper in some register and call br on this register. We don’t need to store the link before branching, the call will be returned to say_hello from obj.c, which is why a plain br will be enough. Let’s check an assembly of simple C-function:\nhello.c:\n#include <stdint.h> void say_hello(void) { uint64_t reg = 0x555555550c14; } \n$ gcc -c hello.c $ objdump --disassemble --section=.text hello.o hello.o: file format elf64-littleaarch64 Disassembly of section .text: 0000000000000000 <say_hello>: 0: d10043ff sub sp, sp, #0x10 4: d2818280 mov x0, #0xc14 // #3092 8: f2aaaaa0 movk x0, #0x5555, lsl #16 c: f2caaaa0 movk x0, #0x5555, lsl #32 10: f90007e0 str x0, [sp, #8] 14: d503201f nop 18: 910043ff add sp, sp, #0x10 1c: d65f03c0 ret \nThe number 0x555555550c14 is the address returned by lookup_ext_function. We’ve printed it out to use as an example, but any 48-bits hex value can be used.\nIn our output we see that the value was split in three sections and written in x0 register with three instructions: one mov and two movk. The documentation says that there are only 16 bits for the immediate value, but a shift can be applied (in our case left shift lsl).\nHowever, we can’t use x0 in our context. By convention the registers x0-x7 are caller-saved and used to pass function parameters between calls to other functions. Let’s use x9 then.\nWe need to modify our loader. Firstly let’s change the jumptable structure.\nloader.c:\n... struct ext_jump { uint32_t instr[4]; }; ... \nAs we saw above, we need four instructions: mov, movk, movk, br. We don’t need a stack frame as we aren’t preserving any local variables. We just want to load the address into the register and branch to it. But we can’t write human-readable code\ne.g. mov x0, #0xc14 into instructions, we need machine binary or hex representation, e.g. d2818280. \nLet’s write a simple assembly code to get it:\nhw.s:\n.global _start _start: mov x9, #0xc14 movk x9, #0x5555, lsl #16 movk x9, #0x5555, lsl #32 br x9 \n$ as -o hw.o hw.s $ objdump --disassemble --section=.text hw.o hw.o: file format elf64-littleaarch64 Disassembly of section .text: 0000000000000000 <_start>: 0: d2818289 mov x9, #0xc14 // #3092 4: f2aaaaa9 movk x9, #0x5555, lsl #16 8: f2caaaa9 movk x9, #0x5555, lsl #32 c: d61f0120 br x9 \nAlmost done! But there’s one more thing to consider. Even if the value 0x555555550c14 is a real my_puts wrapper address, it will be different on each run if ASLR(Address space layout randomization) is enabled. We need to patch these instructions to put the value which will be returned by lookup_ext_function on each run. We’ll split the obtained value in three parts, 16-bits each, and replace them in our mov and movk instructions according to the documentation, similar to what we did before for our second part.\nif (symbols[symbol_idx].st_shndx == SHN_UNDEF) { static int curr_jmp_idx = 0; uint64_t addr = lookup_ext_function(strtab + symbols[symbol_idx].st_name); uint32_t mov = 0b11010010100000000000000000001001 | ((addr << 48) >> 43); uint32_t movk1 = 0b11110010101000000000000000001001 | (((addr >> 16) << 48) >> 43); uint32_t movk2 = 0b11110010110000000000000000001001 | (((addr >> 32) << 48) >> 43); jumptable[curr_jmp_idx].instr[0] = mov; // mov x9, #0x0c14 jumptable[curr_jmp_idx].instr[1] = movk1; // movk x9, #0x5555, lsl #16 jumptable[curr_jmp_idx].instr[2] = movk2; // movk x9, #0x5555, lsl #32 jumptable[curr_jmp_idx].instr[3] = 0xd61f0120; // br x9 symbol_address = (uint8_t *)(&jumptable[curr_jmp_idx].instr[0]); curr_jmp_idx++; } else { symbol_address = section_runtime_base(&sections[symbols[symbol_idx].st_shndx]) + symbols[symbol_idx].st_value; } uint32_t val; switch (type) { case R_AARCH64_CALL26: /* The mask separates opcode (6 bits) and the immediate value */ uint32_t mask_bl = (0xffffffff << 26); /* S+A-P, divided by 4 */ val = (symbol_address + relocations[i].r_addend - patch_offset) >> 2; /* Concatenate opcode and value to get final instruction */ *((uint32_t *)patch_offset) &= mask_bl; val &= ~mask_bl; *((uint32_t *)patch_offset) |= val; break; ... \nIn the code we took the address of the first instruction &jumptable[curr_jmp_idx].instr[0] and wrote it in the symbol_address, further because the type is still R_AARCH64_CALL26 it will be put into bl - jump to the relative address. Where our relative address is the first mov instruction. The whole jumptable code will be executed and finished with the blr instruction.\nThe final run:\n$ gcc -o loader loader.c $ ./loader Executing add5... add5(42) = 47 Executing add10... add10(42) = 52 Executing get_hello... get_hello() = Hello, world! Executing get_var... get_var() = 5 Executing set_var(42)... Executing get_var again... get_var() = 42 Executing say_hello... my_puts executed Hello, world! \nThe final code is here.\nSummary\nThere are several things we covered in this blog post. We gave a brief introduction on how the binary got executed on Linux and how all components are linked together. We saw a big difference between x86 and aarch64 assembly. We learned how we can hook into the code and change its behavior. But just as it was said in the first blog post of this series, the most important thing is to remember to always think about security first. Processing external inputs should always be done with great care. Bounds and integrity checks have been omitted for the purposes of keeping the examples short, so readers should be aware that the code is not production ready and is designed for educational purposes only.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nLinux Programming Deep Dive",
      "markdown": "11/17/2023\n\n*   [![Oxana Kharitonova](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/05/Oxana-Kharitonova.png)](https://blog.cloudflare.com/author/oxana/)\n\n19 min read\n\n![How to execute an object file: Part 4, AArch64 edition](https://blog.cloudflare.com/content/images/2023/11/image1-1.png)\n\nTranslating source code written in a high-level programming language into an executable binary typically involves a series of steps, namely compiling and assembling the code into object files, and then linking those object files into the final executable. However, there are certain scenarios where it can be useful to apply an alternate approach that involves executing object files directly, bypassing the linker. For example, we might use it for malware analysis or when part of the code requires an incompatible compiler. We’ll be focusing on the latter scenario: when one of our libraries needed to be compiled differently from the rest of the code. Learning how to execute an object file directly will give you a much better sense of how code is compiled and linked together.\n\nTo demonstrate how this was done, we have previously published a series of posts on executing an object file:\n\n*   [How to execute an object file: Part 1](https://blog.cloudflare.com/how-to-execute-an-object-file-part-1/)\n*   [How to execute an object file: Part 2](https://blog.cloudflare.com/how-to-execute-an-object-file-part-2/)\n*   [How to execute an object file: Part 3](https://blog.cloudflare.com/how-to-execute-an-object-file-part-3/)\n\nThe initial posts are dedicated to the x86 architecture. Since then the fleet of our working machines has expanded to include a large and growing number of ARM CPUs. This time we’ll repeat this exercise for the aarch64 architecture. You can pause here to read the previous blog posts before proceeding with this one, or read through the brief summary below and reference the earlier posts for more detail. We might reiterate some theory as working with ELF files can be daunting, if it’s not your day-to-day routine. Also, please be mindful that for simplicity, these examples omit bounds and integrity checks. Let the journey begin!\n\n## Introduction\n\nIn order to obtain an object file or an executable binary from a high-level compiled programming language the code needs to be processed by three components: compiler, assembler and linker. The compiler generates an assembly listing. This assembly listing is picked up by the assembler and translated into an object file. All source files, if a program contains multiple, go through these two steps generating an object file for each source file. At the final step the linker unites all object files into one binary, additionally resolving references to the shared libraries (i.e. we don’t implement the `printf` function each time, rather we take it from a system library). Even though the approach is platform independent, the compiler output varies by platform as the assembly listing is closely tied to the CPU architecture.\n\nGCC (GNU Compiler Collection) can run each step: compiler, assembler and linker separately for us:\n\nmain.c:\n\n```\n#include <stdio.h>\n\nint main(void)\n{\n\tputs(\"Hello, world!\");\n\treturn 0;\n}\n```\n\nCompiler (output `main.s` - assembly listing):\n\n```\n$ gcc -S main.c\n$ ls\nmain.c  main.s\n```\n\nAssembler (output `main.o` - an object file):\n\n```\n$ gcc -c main.s -o main.o\n$ ls\nmain.c  main.o  main.s\n```\n\nLinker (`main` - an object file):\n\n```\n$ gcc main.o -o main\n$ ls\nmain  main.c  main.o  main.s\n$ ./main\nHello, world!\n```\n\nAll the examples assume gcc is running on a native aarch64 architecture or include a cross compilation flag for those who want to reproduce and have no aarch64.\n\nWe have two object files in the output above: `main.o` and `main`. Object files are files encoded with the [ELF (Executable and Linkable Format)](https://en.wikipedia.org/wiki/Executable_and_Linkable_Format) standard. Although, `main.o` is an ELF file, it doesn’t contain all the information to be fully executable.\n\n```\n$ file main.o\nmain.o: ELF 64-bit LSB relocatable, ARM aarch64, version 1 (SYSV), not stripped\n\n$ file main\nmain: ELF 64-bit LSB pie executable, ARM aarch64, version 1 (SYSV), dynamically\nlinked, interpreter /lib/ld-linux-aarch64.so.1,\nBuildID[sha1]=d3ecd2f8ac3b2dec11ed4cc424f15b3e1f130dd4, for GNU/Linux 3.7.0, not stripped\n```\n\n## The ELF File\n\nThe central idea of this series of blog posts is to understand how to resolve dependencies from object files without directly involving the linker. For illustrative purposes we generated an object file based on some C-code and used it as a library for our main program. Before switching to the code, we need to understand the basics of the ELF structure.\n\nEach ELF file is made up of one _ELF header_, followed by file data. The data can include: a _program header_ table, a _section header_ table, and the data which is referred to by the program or section header tables.\n\nThe ELF header provides some basic information about the file: what architecture the file is compiled for, the program entry point and the references to other tables.\n\nThe ELF Header:\n\n```\n$ readelf -h main\nELF Header:\n  Magic:   7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00 \n  Class:                             ELF64\n  Data:                              2's complement, little endian\n  Version:                           1 (current)\n  OS/ABI:                            UNIX - System V\n  ABI Version:                       0\n  Type:                              DYN (Position-Independent Executable file)\n  Machine:                           AArch64\n  Version:                           0x1\n  Entry point address:               0x640\n  Start of program headers:          64 (bytes into file)\n  Start of section headers:          68576 (bytes into file)\n  Flags:                             0x0\n  Size of this header:               64 (bytes)\n  Size of program headers:           56 (bytes)\n  Number of program headers:         9\n  Size of section headers:           64 (bytes)\n  Number of section headers:         29\n  Section header string table index: 28\n```\n\nThe execution process of almost every program starts from an auxiliary program, called loader, which arranges the memory and calls the program’s entry point. In the following output the loader is marked with a line `“Requesting program interpreter: /lib/ld-linux-aarch64.so.1”`. The whole program memory is split into different segments with associated size, permissions and type (which instructs the loader on how to interpret this block of memory). Because the execution process should be performed in the shortest possible time, the _sections_ with the same characteristics and located nearby are grouped into bigger blocks — _segments_ — and placed in the _program header_. We can say that the _program header_ summarizes the types of data that appear in the _section header_.\n\nThe ELF Program Header:\n\n```\n$ readelf -Wl main\n\nElf file type is DYN (Position-Independent Executable file)\nEntry point 0x640\nThere are 9 program headers, starting at offset 64\n\nProgram Headers:\n  Type           Offset   VirtAddr           PhysAddr           FileSiz  MemSiz   Flg Align\n  PHDR           0x000040 0x0000000000000040 0x0000000000000040 0x0001f8 0x0001f8 R   0x8\n  INTERP         0x000238 0x0000000000000238 0x0000000000000238 0x00001b 0x00001b R   0x1\n      [Requesting program interpreter: /lib/ld-linux-aarch64.so.1]\n  LOAD           0x000000 0x0000000000000000 0x0000000000000000 0x00088c 0x00088c R E 0x10000\n  LOAD           0x00fdc8 0x000000000001fdc8 0x000000000001fdc8 0x000270 0x000278 RW  0x10000\n  DYNAMIC        0x00fdd8 0x000000000001fdd8 0x000000000001fdd8 0x0001e0 0x0001e0 RW  0x8\n  NOTE           0x000254 0x0000000000000254 0x0000000000000254 0x000044 0x000044 R   0x4\n  GNU_EH_FRAME   0x0007a0 0x00000000000007a0 0x00000000000007a0 0x00003c 0x00003c R   0x4\n  GNU_STACK      0x000000 0x0000000000000000 0x0000000000000000 0x000000 0x000000 RW  0x10\n  GNU_RELRO      0x00fdc8 0x000000000001fdc8 0x000000000001fdc8 0x000238 0x000238 R   0x1\n\n Section to Segment mapping:\n  Segment Sections...\n   00     \n   01     .interp \n   02     .interp .note.gnu.build-id .note.ABI-tag .gnu.hash .dynsym .dynstr .gnu.version .gnu.version_r .rela.dyn .rela.plt .init .plt .text .fini .rodata .eh_frame_hdr .eh_frame \n   03     .init_array .fini_array .dynamic .got .got.plt .data .bss \n   04     .dynamic \n   05     .note.gnu.build-id .note.ABI-tag \n   06     .eh_frame_hdr \n   07     \n   08     .init_array .fini_array .dynamic .got \n```\n\nIn the source code of high-level languages, variables, functions, and constants are mixed together. However, in assembly you might see that the data and instructions are separated into different blocks. The ELF file content is divided in an even more granular way. For example, variables with initial values are placed into different sections than the uninitialized ones. This approach optimizes for space, otherwise the values for uninitialized variables would be filled with zeros. Along with the space efficiency, there are security reasons for stratification — executable instructions can’t have writable permissions, while memory containing variables can't be executable. The section header describes each of these sections.  \n\nThe ELF Section Header:\n\n```\n$ readelf -SW main\nThere are 29 section headers, starting at offset 0x10be0:\n\nSection Headers:\n  [Nr] Name              Type            Address          Off    Size   ES Flg Lk Inf Al\n  [ 0]                   NULL            0000000000000000 000000 000000 00      0   0  0\n  [ 1] .interp           PROGBITS        0000000000000238 000238 00001b 00   A  0   0  1\n  [ 2] .note.gnu.build-id NOTE            0000000000000254 000254 000024 00   A  0   0  4\n  [ 3] .note.ABI-tag     NOTE            0000000000000278 000278 000020 00   A  0   0  4\n  [ 4] .gnu.hash         GNU_HASH        0000000000000298 000298 00001c 00   A  5   0  8\n  [ 5] .dynsym           DYNSYM          00000000000002b8 0002b8 0000f0 18   A  6   3  8\n  [ 6] .dynstr           STRTAB          00000000000003a8 0003a8 000092 00   A  0   0  1\n  [ 7] .gnu.version      VERSYM          000000000000043a 00043a 000014 02   A  5   0  2\n  [ 8] .gnu.version_r    VERNEED         0000000000000450 000450 000030 00   A  6   1  8\n  [ 9] .rela.dyn         RELA            0000000000000480 000480 0000c0 18   A  5   0  8\n  [10] .rela.plt         RELA            0000000000000540 000540 000078 18  AI  5  22  8\n  [11] .init             PROGBITS        00000000000005b8 0005b8 000018 00  AX  0   0  4\n  [12] .plt              PROGBITS        00000000000005d0 0005d0 000070 00  AX  0   0 16\n  [13] .text             PROGBITS        0000000000000640 000640 000134 00  AX  0   0 64\n  [14] .fini             PROGBITS        0000000000000774 000774 000014 00  AX  0   0  4\n  [15] .rodata           PROGBITS        0000000000000788 000788 000016 00   A  0   0  8\n  [16] .eh_frame_hdr     PROGBITS        00000000000007a0 0007a0 00003c 00   A  0   0  4\n  [17] .eh_frame         PROGBITS        00000000000007e0 0007e0 0000ac 00   A  0   0  8\n  [18] .init_array       INIT_ARRAY      000000000001fdc8 00fdc8 000008 08  WA  0   0  8\n  [19] .fini_array       FINI_ARRAY      000000000001fdd0 00fdd0 000008 08  WA  0   0  8\n  [20] .dynamic          DYNAMIC         000000000001fdd8 00fdd8 0001e0 10  WA  6   0  8\n  [21] .got              PROGBITS        000000000001ffb8 00ffb8 000030 08  WA  0   0  8\n  [22] .got.plt          PROGBITS        000000000001ffe8 00ffe8 000040 08  WA  0   0  8\n  [23] .data             PROGBITS        0000000000020028 010028 000010 00  WA  0   0  8\n  [24] .bss              NOBITS          0000000000020038 010038 000008 00  WA  0   0  1\n  [25] .comment          PROGBITS        0000000000000000 010038 00001f 01  MS  0   0  1\n  [26] .symtab           SYMTAB          0000000000000000 010058 000858 18     27  66  8\n  [27] .strtab           STRTAB          0000000000000000 0108b0 00022c 00      0   0  1\n  [28] .shstrtab         STRTAB          0000000000000000 010adc 000103 00      0   0  1\nKey to Flags:\n  W (write), A (alloc), X (execute), M (merge), S (strings), I (info),\n  L (link order), O (extra OS processing required), G (group), T (TLS),\n  C (compressed), x (unknown), o (OS specific), E (exclude),\n  D (mbind), p (processor specific)\n```\n\n## Executing example from Part 1 on aarch64\n\nActually, our [initial code](https://github.com/cloudflare/cloudflare-blog/tree/master/2021-03-obj-file/1) from [Part 1](https://blog.cloudflare.com/how-to-execute-an-object-file-part-1/) works on aarch64 as is!\n\nLet’s have a quick summary about what was done in the code:\n\n1.  We need to find the code of two functions (`add5` and `add10`) in the `.text` section of our object file (`obj.o`)\n2.  Load the functions in the executable memory\n3.  Return the memory locations of the functions to the main program\n\nThere is one nuance: even though all the sections are in the section header, neither of them have a string name. Without the names we can’t identify them. However, having an additional character field for each section in the ELF structure would be inefficient for the space — it must be limited by some maximum length and those names which are shorter would leave the space unfilled. Instead, ELF provides an additional section, `.shstrtab`. This string table concatenates all the names where each name ends with a null terminated byte. We can iterate over the names and match with an offset held by other sections to reference their name. But how do we find `.shstrtab` itself if we don’t have a name? To solve this chicken and egg problem, the ELF program header provides a direct pointer to `.shstrtab`. The similar approach is applied to two other sections: `.symtab` and `.strtab`. Where `.symtab` contains all information about the symbols and `.strtab` holds the list of symbol names. In the code we work with these tables to resolve all their dependencies and find our functions.\n\n## Executing example from Part 2 on aarch64\n\nAt the beginning of the [second blog post](https://blog.cloudflare.com/how-to-execute-an-object-file-part-2/) we made the function `add10` depend on `add5` instead of being self-contained. This is the first time when we faced relocations. _Relocations_ is the process of loading symbols defined outside the current scope. The relocated symbols can present global or thread-local variables, constant, functions, etc. We’ll start from checking assembly instructions which trigger relocations and uncovering how the ELF format handles them in a more general way.\n\nAfter making `add10` depend on `add5` our aarch64 version stopped working as well, similarly to the x86. Let’s take a look at assembly listing:\n\n```\n$ objdump --disassemble --section=.text obj.o\n\nobj.o:     file format elf64-littleaarch64\n\n\nDisassembly of section .text:\n\n0000000000000000 <add5>:\n   0:\td10043ff \tsub\tsp, sp, #0x10\n   4:\tb9000fe0 \tstr\tw0, [sp, #12]\n   8:\tb9400fe0 \tldr\tw0, [sp, #12]\n   c:\t11001400 \tadd\tw0, w0, #0x5\n  10:\t910043ff \tadd\tsp, sp, #0x10\n  14:\td65f03c0 \tret\n\n0000000000000018 <add10>:\n  18:\ta9be7bfd \tstp\tx29, x30, [sp, #-32]!\n  1c:\t910003fd \tmov\tx29, sp\n  20:\tb9001fe0 \tstr\tw0, [sp, #28]\n  24:\tb9401fe0 \tldr\tw0, [sp, #28]\n  28:\t94000000 \tbl\t0 <add5>\n  2c:\tb9001fe0 \tstr\tw0, [sp, #28]\n  30:\tb9401fe0 \tldr\tw0, [sp, #28]\n  34:\t94000000 \tbl\t0 <add5>\n  38:\ta8c27bfd \tldp\tx29, x30, [sp], #32\n  3c:\td65f03c0 \tret\n```\n\nHave you noticed that all the hex values in the second column are exactly the same length, in contrast with the instructions lengths seen for x86 in Part 2 of our series? This is because all Armv8-A instructions are presented in 32 bits. Since it is impossible to encode every immediate value into less than 32 bits, some operations require more than one instruction, as we’ll see later. For now, we’re interested in one instruction `- bl` ([branch with link](https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/BL--Branch-with-Link-?lang=en)) on rows `28` and `34`. The `bl` is a “jump” instruction, but before the jump it preserves the next instruction after the current one in the link register (`lr`). When the callee finishes execution the caller address is recovered from `lr`. Usually, the aarch64 instructions reserve the last 6 bits \\[31:26\\] for opcode and some auxiliary fields such as running architecture (32 or 64 bits), condition flag and others. Remaining bits are shared between arguments like source register, destination register and immediate value. Since the `bl` instruction does not require a source or destination register, the full 26 bits can be used to encode the immediate offset instead. However, 26 bits can only encode a small range (+/-32 MB), but because the jump can only target a beginning of an instruction, it must always be aligned to 4 bytes, which increases the effective range of the encoded immediate fourfold, to +/-128 MB.\n\nSimilarly to what we did in [Part 2](https://blog.cloudflare.com/how-to-execute-an-object-file-part-2/) we’re going to resolve our relocations - first by manually calculating the correct addresses and then by using an approach similar to what the linker does. The current value of our `bl` instruction is `94000000` or in binary representation `100101**00000000000000000000000000**`. All 26 bits are zeros, so we don't jump anywhere. The address is calculated by an offset from the current _program counter_ (`pc`), which can be positive or negative. In our case we expect it to be `-0x28` and `-0x34`. As described above, it should be divided by 4 and taken as [two's complements](https://en.wikipedia.org/wiki/Two%27s_complement): `-0x28 / 4 = -0xA == 0xFFFFFFF6` and `-0x34 / 4 = -0xD == 0xFFFFFFF3`. From these values we need to take the lower 26 bits and concatenate them with the initial 6 bits to get the final instruction. So, the final ones will be: `100101**11111111111111111111110110** == 0x97FFFFF6` and `100101**11111111111111111111110011** == 0x97FFFFF3`. Have you noticed that all the distance calculations are done relative to the `bl` (or current `pc`), not the next instruction as in x86?\n\nLet’s add to the code and execute:\n\n```\n... \n\nstatic void parse_obj(void)\n{\n\t...\n\t/* copy the contents of `.text` section from the ELF file */\n\tmemcpy(text_runtime_base, obj.base + text_hdr->sh_offset, text_hdr->sh_size);\n\n\t*((uint32_t *)(text_runtime_base + 0x28)) = 0x97FFFFF6;\n\t*((uint32_t *)(text_runtime_base + 0x34)) = 0x97FFFFF3;\n\n\t/* make the `.text` copy readonly and executable */\n\tif (mprotect(text_runtime_base, page_align(text_hdr->sh_size), PROT_READ | PROT_EXEC)) {\n\t...\n```\n\nCompile and run:\n\n```\n$ gcc -o loader loader.c\n$ ./loader\nExecuting add5...\nadd5(42) = 47\nExecuting add10...\nadd10(42) = 52\n```\n\nIt works! But this is not how the linker handles the relocations. The linker resolves relocation based on the type and formula assigned to this type. In our [Part 2](https://blog.cloudflare.com/how-to-execute-an-object-file-part-2/) we investigated it quite well. Here again we need to find the type and check the formula for this type:\n\n```\n$ readelf --relocs obj.o\n\nRelocation section '.rela.text' at offset 0x228 contains 2 entries:\n  Offset          Info           Type           Sym. Value    Sym. Name + Addend\n000000000028  000a0000011b R_AARCH64_CALL26  0000000000000000 add5 + 0\n000000000034  000a0000011b R_AARCH64_CALL26  0000000000000000 add5 + 0\n\nRelocation section '.rela.eh_frame' at offset 0x258 contains 2 entries:\n  Offset          Info           Type           Sym. Value    Sym. Name + Addend\n00000000001c  000200000105 R_AARCH64_PREL32  0000000000000000 .text + 0\n000000000034  000200000105 R_AARCH64_PREL32  0000000000000000 .text + 18\n```\n\nOur Type is R\\_AARCH64\\_CALL26 and the [formula](https://github.com/ARM-software/abi-aa/blob/main/aaelf64/aaelf64.rst#5733relocation-operations) for it is:\n\n|     |     |     |\n| --- | --- | --- |\n| **ELF64 Code** | **Name** | **Operation** |\n| 283 | R\\_<CLS>\\_CALL26 | S + A - P |\n\nwhere:\n\n*   `S` (when used on its own) is the address of the symbol\n*   `A` is the addend for the relocation\n*   `P` is the address of the place being relocated (derived from `r_offset`)\n\nHere are the relevant changes to loader.c:\n\n```\n/* Replace `#define R_X86_64_PLT32 4` with our Type */\n#define R_AARCH64_CALL26 283\n...\n\nstatic void do_text_relocations(void)\n{\n\t...\n\tuint32_t val;\n\n\tswitch (type)\n\t{\n\tcase R_AARCH64_CALL26:\n\t\t/* The mask separates opcode (6 bits) and the immediate value */\n\t\tuint32_t mask_bl = (0xffffffff << 26);\n\t\t/* S+A-P, divided by 4 */\n\t\tval = (symbol_address + relocations[i].r_addend - patch_offset) >> 2;\n\t\t/* Concatenate opcode and value to get final instruction */\n\t\t*((uint32_t *)patch_offset) &= mask_bl;\n\t\tval &= ~mask_bl;\n\t\t*((uint32_t *)patch_offset) |= val;\n\t\tbreak;\n\t}\n\t...\n}\n```\n\nCompile and run:\n\n```\n$ gcc -o loader loader.c \n$ ./loader\nCalculated relocation: 0x97fffff6\nCalculated relocation: 0x97fffff3\nExecuting add5...\nadd5(42) = 47\nExecuting add10...\nadd10(42) = 52\n```\n\nSo far so good. The next challenge is to [add constant data and global variables](https://github.com/cloudflare/cloudflare-blog/blob/master/2021-03-obj-file/2/obj.c#L16-L31) to our object file and check relocations again:\n\n```\n$ readelf --relocs --wide obj.o\n\nRelocation section '.rela.text' at offset 0x388 contains 8 entries:\n    Offset             Info             Type               Symbol's Value  Symbol's Name + Addend\n0000000000000000  0000000500000113 R_AARCH64_ADR_PREL_PG_HI21 0000000000000000 .rodata + 0\n0000000000000004  0000000500000115 R_AARCH64_ADD_ABS_LO12_NC 0000000000000000 .rodata + 0\n000000000000000c  0000000300000113 R_AARCH64_ADR_PREL_PG_HI21 0000000000000000 .data + 0\n0000000000000010  0000000300000115 R_AARCH64_ADD_ABS_LO12_NC 0000000000000000 .data + 0\n0000000000000024  0000000300000113 R_AARCH64_ADR_PREL_PG_HI21 0000000000000000 .data + 0\n0000000000000028  0000000300000115 R_AARCH64_ADD_ABS_LO12_NC 0000000000000000 .data + 0\n0000000000000068  000000110000011b R_AARCH64_CALL26       0000000000000040 add5 + 0\n0000000000000074  000000110000011b R_AARCH64_CALL26       0000000000000040 add5 + 0\n...\n```\n\nWe have even two new relocations: `R_AARCH64_ADD_ABS_LO12_NC` and `R_AARCH64_ADR_PREL_PG_HI21`. Their formulas are:\n\n|     |     |     |\n| --- | --- | --- |\n| **ELF64 Code** | **Name** | **Operation** |\n| 275 | R\\_<CLS>\\_ ADR\\_PREL\\_PG\\_HI21 | Page(S+A) - Page(P) |\n| 277 | R\\_<CLS>\\_ ADD\\_ABS\\_LO12\\_NC | S + A |\n\nwhere:\n\n`Page(expr)` is the page address of the expression expr, defined as `(expr & ~0xFFF)`. (This applies even if the machine page size supported by the platform has a different value.)\n\nIt’s a bit unclear why we have two new types, while in x86 we had only one. Let’s investigate the assembly code:\n\n```\n$ objdump --disassemble --section=.text obj.o\n\nobj.o:     file format elf64-littleaarch64\n\n\nDisassembly of section .text:\n\n0000000000000000 <get_hello>:\n   0:\t90000000 \tadrp\tx0, 0 <get_hello>\n   4:\t91000000 \tadd\tx0, x0, #0x0\n   8:\td65f03c0 \tret\n\n000000000000000c <get_var>:\n   c:\t90000000 \tadrp\tx0, 0 <get_hello>\n  10:\t91000000 \tadd\tx0, x0, #0x0\n  14:\tb9400000 \tldr\tw0, [x0]\n  18:\td65f03c0 \tret\n\n000000000000001c <set_var>:\n  1c:\td10043ff \tsub\tsp, sp, #0x10\n  20:\tb9000fe0 \tstr\tw0, [sp, #12]\n  24:\t90000000 \tadrp\tx0, 0 <get_hello>\n  28:\t91000000 \tadd\tx0, x0, #0x0\n  2c:\tb9400fe1 \tldr\tw1, [sp, #12]\n  30:\tb9000001 \tstr\tw1, [x0]\n  34:\td503201f \tnop\n  38:\t910043ff \tadd\tsp, sp, #0x10\n  3c:\td65f03c0 \tret\n```\n\nWe see that all `adrp` instructions are followed by `add` instructions. The `[add](https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/ADD--immediate---Add--immediate--?lang=en)` instruction adds an immediate value to the source register and writes the result to the destination register. The source and destination registers can be the same, the immediate value is 12 bits. The `[adrp](https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/ADRP--Form-PC-relative-address-to-4KB-page-?lang=en)` instruction generates a `pc`\\-relative (program counter) address and writes the result to the destination register. It takes `pc` of the instruction itself and adds a 21-bit immediate value shifted left by 12 bits. If the immediate value weren’t shifted it would lie in a range of +/-1 MB memory, which isn’t enough. The left shift increases the range up to +/-1 GB. However, because the 12 bits are masked out with the shift, we need to store them somewhere and restore later. That’s why we see add instruction following `adrp` and two types instead of one. Also, it’s a bit tricky to encode [`adrp`](https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/ADRP--Form-PC-relative-address-to-4KB-page-?lang=en): 2 low bits of immediate value are placed in the position 30:29 and the rest in the position 23:5. Due to size limitations, the aarch64 instructions try to make the most out of 32 bits.\n\nIn the code we are going to use the formulas to calculate the values and description of `adrp` and `add` instructions to obtain the final opcode:\n\n```\n#define R_AARCH64_CALL26 283\n#define R_AARCH64_ADD_ABS_LO12_NC 277\n#define R_AARCH64_ADR_PREL_PG_HI21 275\n...\n\n{\ncase R_AARCH64_CALL26:\n\t/* The mask separates opcode (6 bits) and the immediate value */\n\tuint32_t mask_bl = (0xffffffff << 26);\n\t/* S+A-P, divided by 4 */\n\tval = (symbol_address + relocations[i].r_addend - patch_offset) >> 2;\n\t/* Concatenate opcode and value to get final instruction */\n\t*((uint32_t *)patch_offset) &= mask_bl;\n\tval &= ~mask_bl;\n\t*((uint32_t *)patch_offset) |= val;\n\tbreak;\ncase R_AARCH64_ADD_ABS_LO12_NC:\n\t/* The mask of `add` instruction to separate \n\t* opcode, registers and calculated value \n\t*/\n\tuint32_t mask_add = 0b11111111110000000000001111111111;\n\t/* S + A */\n\tuint32_t val = *(symbol_address + relocations[i].r_addend);\n\tval &= ~mask_add;\n\t*((uint32_t *)patch_offset) &= mask_add;\n\t/* Final instruction */\n\t*((uint32_t *)patch_offset) |= val;\ncase R_AARCH64_ADR_PREL_PG_HI21:\n\t/* Page(S+A)-Page(P), Page(expr) is defined as (expr & ~0xFFF) */\n\tval = (((uint64_t)(symbol_address + relocations[i].r_addend)) & ~0xFFF) - (((uint64_t)patch_offset) & ~0xFFF);\n\t/* Shift right the calculated value by 12 bits.\n\t * During decoding it will be shifted left as described above, \n\t * so we do the opposite.\n\t*/\n\tval >>= 12;\n\t/* Separate the lower and upper bits to place them in different positions */ \n\tuint32_t immlo = (val & (0xf >> 2)) << 29 ;\n\tuint32_t immhi = (val & ((0xffffff >> 13) << 2)) << 22;\n\t*((uint32_t *)patch_offset) |= immlo;\n\t*((uint32_t *)patch_offset) |= immhi;\n\tbreak;\n}\n```\n\nCompile and run:\n\n```\n$ gcc -o loader loader.c \n$ ./loader\nExecuting add5...\nadd5(42) = 47\nExecuting add10...\nadd10(42) = 52\nExecuting get_hello...\nget_hello() = Hello, world!\nExecuting get_var...\nget_var() = 5\nExecuting set_var(42)...\nExecuting get_var again...\nget_var() = 42\n```\n\nIt works! The final code is [here](https://github.com/cloudflare/cloudflare-blog/tree/master/2021-03-obj-file/4/2).\n\n## Executing example from Part 3 on aarch64\n\nOur [Part 3](https://blog.cloudflare.com/how-to-execute-an-object-file-part-3/) is about resolving external dependencies. When we write code we don’t think much about how to allocate memory or print debug information to the console. Instead, we involve functions from the system libraries. But the code of system libraries needs to be passed through to our programs somehow. Additionally, for optimization purposes, it would be nice if this code would be stored in one place and shared between all programs. And another wish — we don’t want to resolve all the functions and global variables from the libraries, only those which we need and at those times when we need them. To solve these problems, ELF introduced two sections: PLT (the procedure linkage table) and GOT (the global offset table). The dynamic loader creates a list which contains all external functions and variables from the shared library, but doesn’t resolve them immediately; instead they are placed in the PLT section. Each external symbol is presented by a small function, a stub, e.g. `puts@plt`. When an external symbol is requested, the stub checks if it was resolved previously. If not, the stub searches for an absolute address of the symbol, returns to the requester and writes it in the GOT table. The next time, the address returns directly from the GOT table.\n\nIn [Part 3](https://blog.cloudflare.com/how-to-execute-an-object-file-part-3/) we implemented a simplified PLT/GOT resolution. Firstly, we added a new function [`say_hello`](https://github.com/cloudflare/cloudflare-blog/blob/master/2021-03-obj-file/3/obj.c#L35) in the `obj.c`, which calls unresolved system library function `puts`. Further we added an optional wrapper [`my_puts`](https://github.com/cloudflare/cloudflare-blog/blob/master/2021-03-obj-file/3/loader.c#L73) in the `loader.c`. The wrapper isn’t required, we could’ve resolved directly to a standard function, but it's a good example of how the implementation of some functions can be overwritten with custom code. In the next steps we added our PLT/GOT resolution:\n\n*   PLT section we replaced with a [jumptable](https://github.com/cloudflare/cloudflare-blog/blob/master/2021-03-obj-file/3/loader.c#L340)\n*   GOT we replaced with [assembly instructions](https://github.com/cloudflare/cloudflare-blog/blob/master/2021-03-obj-file/3/loader.c#L238-L248)\n\nBasically, we created a small stub with assembly code (our `jumptable`) to resolve the global address of our `my_puts` wrapper and jump to it.\n\nThe approach for aarch64 is the same. But the `jumptable` is very different as it consists of different assembly instructions.\n\nThe big difference here compared to the other parts is that we need to work with a 64-bit address for the GOT resolution. Our custom PLT or `jumptable` is placed close to the main code of `obj.c` and can operate with the relative addresses as before. For the GOT or referencing `my_puts` wrapper we’ll use different branch instructions — [`br`](https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/BR--Branch-to-Register-) or [`blr`](https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/BLR--Branch-with-Link-to-Register-). These instructions branch to the register, where the aarch64 registers can hold 64-bit values.\n\nWe can check how it resolves with the native PLT/GOT in our loader assembly code:\n\n```\n$ objdump --disassemble --section=.text loader\n...\n1d2c:\t97fffb45 \tbl\ta40 <puts@plt>\n1d30:\tf94017e0 \tldr\tx0, [sp, #40]\n1d34:\td63f0000 \tblr\tx0\n...\n```\n\nThe first instruction is `bl` jump to `puts@plt` stub. The next [`ldr`](https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/LDR--immediate---Load-Register--immediate--) instruction tells us that some value was loaded into the register `x0` from the stack. Each function has its own [stack frame](https://en.wikipedia.org/wiki/Call_stack#Stack_and_frame_pointers) to hold the local variables. The last `blr` instruction makes a jump to the address stored in `x0` register. There is an agreement in the register naming: if the stored value is 64-bits then the register is called `x0-x30`; if only 32-bits are used then it’s called `w0-w30` (the value will be stored in the lower 32-bits and upper 32-bits will be zeroed).\n\nWe need to do something similar — place the absolute address of our `my_puts` wrapper in some register and call `br` on this register. We don’t need to store the link before branching, the call will be returned to `say_hello` from `obj.c`, which is why a plain `br` will be enough. Let’s check an assembly of simple C-function:\n\nhello.c:\n\n```\n#include <stdint.h>\n\nvoid say_hello(void)\n{\n    uint64_t reg = 0x555555550c14;\n}\n```\n\n```\n$ gcc -c hello.c\n$ objdump --disassemble --section=.text hello.o\n\nhello.o:     file format elf64-littleaarch64\n\n\nDisassembly of section .text:\n\n0000000000000000 <say_hello>:\n   0:\td10043ff \tsub\tsp, sp, #0x10\n   4:\td2818280 \tmov\tx0, #0xc14                 \t// #3092\n   8:\tf2aaaaa0 \tmovk\tx0, #0x5555, lsl #16\n   c:\tf2caaaa0 \tmovk\tx0, #0x5555, lsl #32\n  10:\tf90007e0 \tstr\tx0, [sp, #8]\n  14:\td503201f \tnop\n  18:\t910043ff \tadd\tsp, sp, #0x10\n  1c:\td65f03c0 \tret\n```\n\nThe number `0x555555550c14` is the address returned by [`lookup_ext_function`](https://github.com/cloudflare/cloudflare-blog/blob/master/2021-03-obj-file/3/loader.c#L238). We’ve printed it out to use as an example, but any [48-bits](https://www.kernel.org/doc/html/latest/arch/arm64/memory.html) hex value can be used.\n\nIn our output we see that the value was split in three sections and written in `x0` register with three instructions: one [`mov`](https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/MOV--inverted-wide-immediate---Move--inverted-wide-immediate---an-alias-of-MOVN-) and two [`movk`](https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/MOVK--Move-wide-with-keep-?lang=en). The documentation says that there are only 16 bits for the immediate value, but a shift can be applied (in our case left shift `lsl`).\n\nHowever, we can’t use `x0` in our context. By [convention](https://developer.arm.com/documentation/den0024/a/The-ABI-for-ARM-64-bit-Architecture/Register-use-in-the-AArch64-Procedure-Call-Standard/Parameters-in-general-purpose-registers) the registers `x0-x7` are caller-saved and used to pass function parameters between calls to other functions. Let’s use `x9` then.\n\nWe need to modify our loader. Firstly let’s change the jumptable structure.\n\nloader.c:\n\n```\n...\nstruct ext_jump {\n\tuint32_t instr[4];\n};\n...\n```\n\nAs we saw above, we need four instructions: `mov`, `movk`, `movk`, `br`. We don’t need a stack frame as we aren’t preserving any local variables. We just want to load the address into the register and branch to it. But we can’t write human-readable code\n\ne.g. `mov  x0, #0xc14` into instructions, we need machine binary or hex representation, e.g. `d2818280`.\n\nLet’s write a simple assembly code to get it:\n\nhw.s:\n\n```\n.global _start\n\n_start: mov     x9, #0xc14 \n        movk    x9, #0x5555, lsl #16\n        movk    x9, #0x5555, lsl #32\n        br      x9\n```\n\n```\n$ as -o hw.o hw.s\n$ objdump --disassemble --section=.text hw.o\n\nhw.o:     file format elf64-littleaarch64\n\n\nDisassembly of section .text:\n\n0000000000000000 <_start>:\n   0:\td2818289 \tmov\tx9, #0xc14                 \t// #3092\n   4:\tf2aaaaa9 \tmovk\tx9, #0x5555, lsl #16\n   8:\tf2caaaa9 \tmovk\tx9, #0x5555, lsl #32\n   c:\td61f0120 \tbr\tx9\n```\n\nAlmost done! But there’s one more thing to consider. Even if the value `0x555555550c14` is a real `my_puts` wrapper address, it will be different on each run if [ASLR(Address space layout randomization)](https://en.wikipedia.org/wiki/Address_space_layout_randomization) is enabled. We need to patch these instructions to put the value which will be returned by [`lookup_ext_function`](https://github.com/cloudflare/cloudflare-blog/blob/master/2021-03-obj-file/3/loader.c#L238) on each run. We’ll split the obtained value in three parts, 16-bits each, and replace them in our [`mov`](https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/MOV--inverted-wide-immediate---Move--inverted-wide-immediate---an-alias-of-MOVN-) and [`movk`](https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/MOVK--Move-wide-with-keep-?lang=en) instructions according to the documentation, similar to what we did before for our second part.\n\n```\nif (symbols[symbol_idx].st_shndx == SHN_UNDEF) {\n\tstatic int curr_jmp_idx = 0;\n\n\tuint64_t addr = lookup_ext_function(strtab +  symbols[symbol_idx].st_name);\n\tuint32_t mov = 0b11010010100000000000000000001001 | ((addr << 48) >> 43);\n\tuint32_t movk1 = 0b11110010101000000000000000001001 | (((addr >> 16) << 48) >> 43);\n\tuint32_t movk2 = 0b11110010110000000000000000001001 | (((addr >> 32) << 48) >> 43);\n\tjumptable[curr_jmp_idx].instr[0] = mov;         // mov  x9, #0x0c14\n\tjumptable[curr_jmp_idx].instr[1] = movk1;       // movk x9, #0x5555, lsl #16\n\tjumptable[curr_jmp_idx].instr[2] = movk2;       // movk x9, #0x5555, lsl #32\n\tjumptable[curr_jmp_idx].instr[3] = 0xd61f0120;  // br   x9\n\n\tsymbol_address = (uint8_t *)(&jumptable[curr_jmp_idx].instr[0]);\n\tcurr_jmp_idx++;\n} else {\n\tsymbol_address = section_runtime_base(&sections[symbols[symbol_idx].st_shndx]) + symbols[symbol_idx].st_value;\n}\nuint32_t val;\nswitch (type)\n{\ncase R_AARCH64_CALL26:\n\t/* The mask separates opcode (6 bits) and the immediate value */\n\tuint32_t mask_bl = (0xffffffff << 26);\n\t/* S+A-P, divided by 4 */\n\tval = (symbol_address + relocations[i].r_addend - patch_offset) >> 2;\n\t/* Concatenate opcode and value to get final instruction */\n\t*((uint32_t *)patch_offset) &= mask_bl;\n\tval &= ~mask_bl;\n\t*((uint32_t *)patch_offset) |= val;\n\tbreak;\n...\n```\n\nIn the code we took the address of the first instruction `&jumptable[curr_jmp_idx].instr[0]` and wrote it in the `symbol_address`, further because the `type` is still `R_AARCH64_CALL26` it will be put into `bl` - jump to the relative address. Where our relative address is the first `mov` instruction. The whole `jumptable` code will be executed and finished with the `blr` instruction.\n\nThe final run:\n\n```\n$ gcc -o loader loader.c\n$ ./loader\nExecuting add5...\nadd5(42) = 47\nExecuting add10...\nadd10(42) = 52\nExecuting get_hello...\nget_hello() = Hello, world!\nExecuting get_var...\nget_var() = 5\nExecuting set_var(42)...\nExecuting get_var again...\nget_var() = 42\nExecuting say_hello...\nmy_puts executed\nHello, world!\n```\n\nThe final code is [here](https://github.com/cloudflare/cloudflare-blog/tree/master/2021-03-obj-file/4/3).\n\n## Summary\n\nThere are several things we covered in this blog post. We gave a brief introduction on how the binary got executed on Linux and how all components are linked together. We saw a big difference between x86 and aarch64 assembly. We learned how we can hook into the code and change its behavior. But just as it was said in the first blog post of this series, the most important thing is to remember to always think about security first. Processing external inputs should always be done with great care. Bounds and integrity checks have been omitted for the purposes of keeping the examples short, so readers should be aware that the code is not production ready and is designed for educational purposes only.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Linux](https://blog.cloudflare.com/tag/linux/) [Programming](https://blog.cloudflare.com/tag/programming/) [Deep Dive](https://blog.cloudflare.com/tag/deep-dive/)"
    },
    {
      "url": "https://blog.cloudflare.com/hostname-asn-lists/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/hostname-asn-lists/",
        "loadedTime": "2023-12-05T02:28:05.421Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/hostname-asn-lists/",
        "title": "Introducing hostname and ASN lists to simplify WAF rules creation",
        "description": "Today we are expanding Custom Lists by enabling you to create lists of hostnames and ASNs.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/15/2023\n5 min read\nIf you’re responsible for creating a Web Application Firewall (WAF) rule, you’ll almost certainly need to reference a large list of potential values that each field can have. And having to manually manage and enter all those fields, for numerous WAF rules, would be a guaranteed headache.\nThat’s why we introduced IP lists. Having a separate list of values that can be referenced, reused, and managed independently of the actual rule makes for a better WAF user experience. You can create a new list, such as $organization_ips, and then use it in a rule like “allow requests where source IP is in $organization_ips”. If you need to add or remove IPs, you do that in the list, without touching each of the rules that reference the list. You can even add a descriptive name to help track its content. It’s easy, clean, and organized.\nWhich led us, and our customers, to ask the next natural question: why stop at IPs?\nCloudflare’s WAF is highly configurable and allows you to write rules evaluating a set of hostnames, Autonomous System Numbers (ASNs), countries, header values, or values of JSON fields. But to do so, you’ve to input a list of items directly into the rule expression editor, with all the associated downsides: it’s slow (you need to modify each rule individually), prone to error, and sometimes impossible (given the 4 KB limit of a custom rule expression).\nWell, no longer! Today we are expanding Custom Lists by enabling you to create lists of hostnames and ASNs. The new list types are included in all Enterprise plans, so you’re free to start creating expansive lists the moment you read this.\nHostname \nYou can now create a list of hostnames by navigating to Configurations > Lists in your account. This is the same place where you can manage your IP lists and browse the available Managed IP Lists.\nOnce the list is created, you can use it in any WAF rule expression. Account WAF users will find this useful as they will be able to run a managed or custom ruleset only on traffic matching a set of hosts.\nUse of hostname lists in the filter of a custom ruleset (available to Account-level WAF users).\nSSL for SaaS users will find hostname lists useful as they can restrict specific rules or rulesets to run on a subset of hosts. The list can be updated programmatically (via the API) to add or remove hosts as they are onboarded to the account.\nA few things to know about lists: you can add domains and subdomains to a list, and a domain doesn’t automatically match subdomains. For example, if you add example.com to your list and use it in a custom rule to block traffic, requests for api.example.com won’t match. Hostname lists accept the * wildcard to include subdomains. For example, adding *.example.com to a list will match on api.example.com but it won’t match on ‘example.com’. Finally, example.com/path/subfolder wouldn’t be a valid entry — we are building string lists for this use case (more on that below).\nASN\nAn autonomous system (AS) is a large network or group of networks that has a unified routing policy. Every device that connects to the Internet is connected to an AS. Imagine an AS, as being like a town's post office, while an IP is the address of a single home. ASNs do not rotate like IPs do, making ASNs a better option when managing a larger portion of the IP space. Typically, each AS is operated by a single large organization, such as an Internet service provider, a large enterprise technology company, or a government agency.\nYou can use an ASN list to manage traffic from ISPs and cloud providers where bots generating automated traffic might be hosted. Using IPs would be less practical as the range of addresses would be too broad and change very frequently. Note that while ASNs are useful in this scenario, they should be used with caution, because blocking the wrong ASN could cause a large range of IPs to be affected.\nHow many lists do I get?\nEvery Enterprise account can create up to 10 Custom lists with a total of 10,000 items shared across all lists. An account is considered Enterprise when at least 1 Enterprise plan has been purchased. Quotas are shared across all data types (IP, ASN and Hostname) and they are defined at the account level, so you can use your lists across all your applications.\nFor example, an account with one (or more) Enterprise plan can have 8 IP lists with 1,000 items each, one Hostname list with 1,700 entries, and one list with 300 ASNs; no more lists or items can be added.\nEnterprise customers can increase their quotas by reaching out to their account team.\nAt the moment of writing Free, Professional and Business accounts have access only to IP Lists.\nWhere can I use them?\nThe following table summarizes what Custom List types will be available and on what fields they can be used. ASN and Hostname Lists are accessible from any WAF product built on the Ruleset Engine, including Custom, Rate Limiting and Managed rules.\nList type Rule fields where list can be used (dashboard) API fields where list can be used \nIP lists\tIP Source Address\tip.src\t\nHostname lists\tHostname\thttp.host\t\nASN lists\tASN\tip.src.asnum\t\nString: the future of lists (coming soon)\nThe next chapter for lists is a hyper-flexible string type. You’ll be able to use it against fields such as header, cookie, path, query, JSON body field, user agent, JA3, MIME type, and more. For each entry you will be able to specify different matching operators, such as ‘exact match’, ‘start with’, ’ends with’ or ‘contains’.\nCommon applications include listing all the user agents you want blocked, listing URLs you want to restrict access to, and more.\nMore complex use cases include creating a different rate limiting rule that applies to a group of users that can be identified using cookies, API keys, or session IDs. A string list will also allow you to collect JA3 fingerprints of known malicious bots.\nWe are still working on string lists, and it will be released in the coming months. In the meantime — if you’d like to start using our new and improved lists, you can jump right into your dash today.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nWAF Product News",
      "markdown": "11/15/2023\n\n*   [![Daniele Molteni](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2020/10/meProfessional-1-flipped.jpg)](https://blog.cloudflare.com/author/daniele/)\n\n5 min read\n\n![Introducing hostname and ASN lists to simplify WAF rules creation](https://blog.cloudflare.com/content/images/2023/08/image2-1.png)\n\nIf you’re responsible for creating a [Web Application Firewall (WAF)](https://www.cloudflare.com/learning/ddos/glossary/web-application-firewall-waf/) rule, you’ll almost certainly need to reference a large list of potential values that each field can have. And having to manually manage and enter all those fields, for numerous WAF rules, would be a guaranteed headache.\n\nThat’s why we introduced [IP lists](https://blog.cloudflare.com/introducing-ip-lists/). Having a separate list of values that can be referenced, reused, and managed independently of the actual rule makes for a better WAF user experience. You can create a new list, such as `$organization_ips`, and then use it in a rule like “allow requests where source IP is in `$organization_ips`”. If you need to add or remove IPs, you do that in the list, without touching each of the rules that reference the list. You can even add a descriptive name to help track its content. It’s easy, clean, and organized.\n\nWhich led us, and our customers, to ask the next natural question: why stop at IPs?\n\nCloudflare’s WAF is highly configurable and allows you to write rules evaluating a set of hostnames, Autonomous System Numbers (ASNs), countries, header values, or values of JSON fields. But to do so, you’ve to input a list of items directly into the rule expression editor, with all the associated downsides: it’s slow (you need to modify each rule individually), prone to error, and sometimes impossible (given the 4 KB limit of a custom rule expression).\n\nWell, no longer! Today we are expanding Custom Lists by enabling you to create lists of hostnames and ASNs. The new list types are included in all Enterprise plans, so you’re free to start creating expansive lists the moment you read this.\n\n### Hostname\n\nYou can now create a list of hostnames by navigating to **Configurations** > **Lists** in your account. This is the same place where you can manage your IP lists and browse the available Managed IP Lists.\n\nOnce the list is created, you can use it in any WAF rule expression. [Account WAF](https://blog.cloudflare.com/account-waf/) users will find this useful as they will be able to run a managed or custom ruleset only on traffic matching a set of hosts.\n\n![Use of hostname lists in the filter of a custom ruleset (available to Account-level WAF users).](https://blog.cloudflare.com/content/images/2023/08/image1-3.png)\n\nUse of hostname lists in the filter of a custom ruleset (available to Account-level WAF users).\n\nSSL for SaaS users will find hostname lists useful as they can restrict specific rules or rulesets to run on a subset of hosts. The list can be updated programmatically (via the API) to add or remove hosts as they are onboarded to the account.\n\nA few things to know about lists: you can add domains and subdomains to a list, and a domain doesn’t automatically match subdomains. For example, if you add example.com to your list and use it in a custom rule to block traffic, requests for api.example.com won’t match. Hostname lists accept the `*` wildcard to include subdomains. For example, adding `*.example.com` to a list will match on `api.example.com` but it won’t match on ‘example.com’. Finally, `example.com/path/subfolder` wouldn’t be a valid entry — we are building string lists for this use case (more on that below).\n\n### ASN\n\nAn [autonomous system](https://www.cloudflare.com/learning/network-layer/what-is-an-autonomous-system/) (AS) is a large network or group of networks that has a unified routing policy. Every device that connects to the Internet is connected to an AS. Imagine an AS, as being like a town's post office, while an IP is the address of a single home. ASNs do not rotate like IPs do, making ASNs a better option when managing a larger portion of the IP space. Typically, each AS is operated by a single large organization, such as an Internet service provider, a large enterprise technology company, or a government agency.\n\nYou can use an ASN list to manage traffic from ISPs and cloud providers where bots generating automated traffic might be hosted. Using IPs would be less practical as the range of addresses would be too broad and change very frequently. Note that while ASNs are useful in this scenario, they should be used with caution, because blocking the wrong ASN could cause a large range of IPs to be affected.\n\n### How many lists do I get?\n\nEvery Enterprise account can create up to 10 Custom lists with a total of 10,000 items shared across all lists. An account is considered Enterprise when at least 1 Enterprise plan has been purchased. Quotas are shared across all data types (IP, ASN and Hostname) and they are defined at the account level, so you can use your lists across all your applications.\n\nFor example, an account with one (or more) Enterprise plan can have 8 IP lists with 1,000 items each, one Hostname list with 1,700 entries, and one list with 300 ASNs; no more lists or items can be added.\n\nEnterprise customers can increase their quotas by reaching out to their account team.\n\nAt the moment of writing Free, Professional and Business accounts have access only to IP Lists.\n\n### Where can I use them?\n\nThe following table summarizes what Custom List types will be available and on what fields they can be used. ASN and Hostname Lists are accessible from any WAF product built on the [Ruleset](https://developers.cloudflare.com/ruleset-engine/) Engine, including Custom, Rate Limiting and Managed rules.\n\n| List type | Rule fields where list can be used (dashboard) | API fields where list can be used |\n| --- | --- | --- |\n| IP lists | IP Source Address | ip.src |\n| Hostname lists | Hostname | http.host |\n| ASN lists | ASN | ip.src.asnum |\n\n### String: the future of lists (coming soon)\n\nThe next chapter for lists is a hyper-flexible string type. You’ll be able to use it against fields such as header, cookie, path, query, JSON body field, user agent, JA3, MIME type, and more. For each entry you will be able to specify different matching operators, such as ‘exact match’, ‘start with’, ’ends with’ or ‘contains’.\n\nCommon applications include listing all the user agents you want blocked, listing URLs you want to restrict access to, and more.\n\nMore complex use cases include creating a different rate limiting rule that applies to a group of users that can be identified using cookies, API keys, or session IDs. A string list will also allow you to collect JA3 fingerprints of known malicious bots.\n\nWe are still working on string lists, and it will be released in the coming months. In the meantime — if you’d like to start using our new and improved lists, you can jump right into your dash today.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[WAF](https://blog.cloudflare.com/tag/waf/) [Product News](https://blog.cloudflare.com/tag/product-news/)"
    },
    {
      "url": "https://blog.cloudflare.com/workers-ai-streaming/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/workers-ai-streaming/",
        "loadedTime": "2023-12-05T02:28:11.255Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/workers-ai-streaming/",
        "title": "Streaming and longer context lengths for LLMs on Workers AI",
        "description": "Workers AI now supports streaming text responses for the LLM models in our catalog, including Llama-2, using server-sent events",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/14/2023\n5 min read\nThis post is also available in 简体中文, 繁體中文, 日本語, Deutsch, Français, Español, and 한국어.\nWorkers AI is our serverless GPU-powered inference platform running on top of Cloudflare’s global network. It provides a growing catalog of off-the-shelf models that run seamlessly with Workers and enable developers to build powerful and scalable AI applications in minutes. We’ve already seen developers doing amazing things with Workers AI, and we can’t wait to see what they do as we continue to expand the platform. To that end, today we’re excited to announce some of our most-requested new features: streaming responses for all Large Language Models (LLMs) on Workers AI, larger context and sequence windows, and a full-precision Llama-2 model variant.\nIf you’ve used ChatGPT before, then you’re familiar with the benefits of response streaming, where responses flow in token by token. LLMs work internally by generating responses sequentially using a process of repeated inference — the full output of a LLM model is essentially a sequence of hundreds or thousands of individual prediction tasks. For this reason, while it only takes a few milliseconds to generate a single token, generating the full response takes longer, on the order of seconds. The good news is we can start displaying the response as soon as the first tokens are generated, and append each additional token until the response is complete. This yields a much better experience for the end user — displaying text incrementally as it's generated not only provides instant responsiveness, but also gives the end-user time to read and interpret the text.\nAs of today, you can now use response streaming for any LLM model in our catalog, including the very popular Llama-2 model. Here’s how it works.\nServer-sent events: a little gem in the browser API\nServer-sent events are easy to use, simple to implement on the server side, standardized, and broadly available across many platforms natively or as a polyfill. Server-sent events fill a niche of handling a stream of updates from the server, removing the need for the boilerplate code that would otherwise be necessary to handle the event stream.\nEasy-to-use Streaming Bidirectional \nfetch\t✅\t\t\t\nServer-sent events\t✅\t✅\t\t\nWebsockets\t\t✅\t✅\t\nComparing fetch, server-sent events, and websockets\nTo get started using streaming on Workers AI’s text generation models with server-sent events, set the “stream” parameter to true in the input of request. This will change the response format and mime-type to text/event-stream.\nHere’s an example of using streaming with the REST API:\ncurl -X POST \\ \"https://api.cloudflare.com/client/v4/accounts/<account>/ai/run/@cf/meta/llama-2-7b-chat-int8\" \\ -H \"Authorization: Bearer <token>\" \\ -H \"Content-Type:application/json\" \\ -d '{ \"prompt\": \"where is new york?\", \"stream\": true }' data: {\"response\":\"New\"} data: {\"response\":\" York\"} data: {\"response\":\" is\"} data: {\"response\":\" located\"} data: {\"response\":\" in\"} data: {\"response\":\" the\"} ... data: [DONE]\nAnd here’s an example using a Worker script:\nimport { Ai } from \"@cloudflare/ai\"; export default { async fetch(request, env, ctx) { const ai = new Ai(env.AI, { sessionOptions: { ctx: ctx } }); const stream = await ai.run( \"@cf/meta/llama-2-7b-chat-int8\", { prompt: \"where is new york?\", stream: true } ); return new Response(stream, { headers: { \"content-type\": \"text/event-stream\" } } ); } }\nIf you want to consume the output event-stream from this Worker in a browser page, the client-side JavaScript is something like:\nconst source = new EventSource(\"/worker-endpoint\"); source.onmessage = (event) => { if(event.data==\"[DONE]\") { // SSE spec says the connection is restarted // if we don't explicitly close it source.close(); return; } const data = JSON.parse(event.data); el.innerHTML += data.response; }\nYou can use this simple code with any simple HTML page, complex SPAs using React or other Web frameworks.\nThis creates a much more interactive experience for the user, who now sees the page update as the response is incrementally created, instead of waiting with a spinner until the entire response sequence has been generated. Try it out streaming on ai.cloudflare.com.\nWorkers AI supports streaming text responses for the Llama-2 model and any future LLM models we are adding to our catalog.\nBut this is not all.\nHigher precision, longer context and sequence lengths\nAnother top request we heard from our community after the launch of Workers AI was for longer questions and answers in our Llama-2 model. In LLM terminology, this translates to higher context length (the number of tokens the model takes as input before making the prediction) and higher sequence length (the number of tokens the model generates in the response.)\nWe’re listening, and in conjunction with streaming, today we are adding a higher 16-bit full-precision Llama-2 variant to the catalog, and increasing the context and sequence lengths for the existing 8-bit version.\nModel Context length (in) Sequence length (out) \n@cf/meta/llama-2-7b-chat-int8\t2048 (768 before)\t1800 (256 before)\t\n@cf/meta/llama-2-7b-chat-fp16\t3072\t2500\t\nStreaming, higher precision, and longer context and sequence lengths provide a better user experience and enable new, richer applications using large language models in Workers AI.\nCheck the Workers AI developer documentation for more information and options. If you have any questions or feedback about Workers AI, please come see us in the Cloudflare Community and the Cloudflare Discord.\nIf you are interested in machine learning and serverless AI, the Cloudflare Workers AI team is building a global-scale platform and tools that enable our customers to run fast, low-latency inference tasks on top of our network. Check our jobs page for opportunities.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nWorkers AI Cloudflare Workers Developer Platform JavaScript Serverless",
      "markdown": "11/14/2023\n\n*   [![Jesse Kipp](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2020/12/jesse-headshot-square.jpg)](https://blog.cloudflare.com/author/jesse/)\n*   [![Celso Martinho](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/Celso-Martinho.png)](https://blog.cloudflare.com/author/celso/)\n\n5 min read\n\nThis post is also available in [简体中文](https://blog.cloudflare.com/zh-cn/workers-ai-streaming-zh-cn/), [繁體中文](https://blog.cloudflare.com/zh-tw/workers-ai-streaming-zh-tw/), [日本語](https://blog.cloudflare.com/ja-jp/workers-ai-streaming-ja-jp/), [Deutsch](https://blog.cloudflare.com/de-de/workers-ai-streaming-de-de/), [Français](https://blog.cloudflare.com/fr-fr/workers-ai-streaming-fr-fr/), [Español](https://blog.cloudflare.com/es-es/workers-ai-streaming-es-es/), and [한국어](https://blog.cloudflare.com/ko-kr/workers-ai-streaming-ko-kr/).\n\n![Streaming LLMs and longer context lengths available in Workers AI](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--3--2.png)\n\nWorkers AI is our serverless GPU-powered inference platform running on top of Cloudflare’s global network. It provides a growing catalog of off-the-shelf models that run seamlessly with Workers and enable developers to build powerful and scalable AI applications in minutes. We’ve already seen developers doing amazing things with Workers AI, and we can’t wait to see what they do as we continue to expand the platform. To that end, today we’re excited to announce some of our most-requested new features: streaming responses for all [Large Language Models](https://www.cloudflare.com/learning/ai/what-is-large-language-model/) (LLMs) on Workers AI, larger context and sequence windows, and a full-precision [Llama-2](https://developers.cloudflare.com/workers-ai/models/llm/) model variant.\n\nIf you’ve used ChatGPT before, then you’re familiar with the benefits of response streaming, where responses flow in token by token. LLMs work internally by generating responses sequentially using a process of repeated inference — the full output of a LLM model is essentially a sequence of hundreds or thousands of individual prediction tasks. For this reason, while it only takes a few milliseconds to generate a single token, generating the full response takes longer, on the order of seconds. The good news is we can start displaying the response as soon as the first tokens are generated, and append each additional token until the response is complete. This yields a much better experience for the end user —  displaying text incrementally as it's generated not only provides instant responsiveness, but also gives the end-user time to read and interpret the text.\n\nAs of today, you can now use response streaming for any LLM model in our catalog, including the very popular [Llama-2 model](https://developers.cloudflare.com/workers-ai/models/llm/). Here’s how it works.\n\n### Server-sent events: a little gem in the browser API\n\n[Server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events) are easy to use, simple to implement on the server side, standardized, and broadly available across many platforms natively or as a polyfill. Server-sent events fill a niche of handling a stream of updates from the server, removing the need for the boilerplate code that would otherwise be necessary to handle the event stream.\n\n|     | Easy-to-use | Streaming | Bidirectional |\n| --- | --- | --- | --- |\n| fetch | ✅   |     |     |\n| Server-sent events | ✅   | ✅   |     |\n| Websockets |     | ✅   | ✅   |\n\nComparing fetch, server-sent events, and websockets\n\nTo get started using streaming on Workers AI’s text generation models with server-sent events, set the “stream” parameter to true in the input of request. This will change the response format and `mime-type` to `text/event-stream`.\n\nHere’s an example of using streaming with the [REST API](https://developers.cloudflare.com/workers-ai/get-started/rest-api/):\n\n```\ncurl -X POST \\\n\"https://api.cloudflare.com/client/v4/accounts/<account>/ai/run/@cf/meta/llama-2-7b-chat-int8\" \\\n-H \"Authorization: Bearer <token>\" \\\n-H \"Content-Type:application/json\" \\\n-d '{ \"prompt\": \"where is new york?\", \"stream\": true }'\n\ndata: {\"response\":\"New\"}\n\ndata: {\"response\":\" York\"}\n\ndata: {\"response\":\" is\"}\n\ndata: {\"response\":\" located\"}\n\ndata: {\"response\":\" in\"}\n\ndata: {\"response\":\" the\"}\n\n...\n\ndata: [DONE]\n```\n\nAnd here’s an example using a Worker script:\n\n```\nimport { Ai } from \"@cloudflare/ai\";\nexport default {\n    async fetch(request, env, ctx) {\n        const ai = new Ai(env.AI, { sessionOptions: { ctx: ctx } });\n        const stream = await ai.run(\n            \"@cf/meta/llama-2-7b-chat-int8\",\n            { prompt: \"where is new york?\", stream: true  }\n        );\n        return new Response(stream,\n            { headers: { \"content-type\": \"text/event-stream\" } }\n        );\n    }\n}\n```\n\nIf you want to consume the output event-stream from this Worker in a browser page, the client-side JavaScript is something like:\n\n```\nconst source = new EventSource(\"/worker-endpoint\");\nsource.onmessage = (event) => {\n    if(event.data==\"[DONE]\") {\n        // SSE spec says the connection is restarted\n        // if we don't explicitly close it\n        source.close();\n        return;\n    }\n    const data = JSON.parse(event.data);\n    el.innerHTML += data.response;\n}\n```\n\nYou can use this simple code with any simple HTML page, complex SPAs using React or other Web frameworks.\n\nThis creates a much more interactive experience for the user, who now sees the page update as the response is incrementally created, instead of waiting with a spinner until the entire response sequence has been generated. Try it out streaming on [ai.cloudflare.com](https://ai.cloudflare.com/).\n\n![](https://blog.cloudflare.com/content/images/2023/10/llama-streaming.gif)\n\nWorkers AI supports streaming text responses for the [Llama-2](https://developers.cloudflare.com/workers-ai/models/llm/) model and any future LLM models we are adding to our catalog.\n\nBut this is not all.\n\n### Higher precision, longer context and sequence lengths\n\nAnother top request we heard from our community after the launch of Workers AI was for longer questions and answers in our Llama-2 model. In LLM terminology, this translates to higher context length (the number of tokens the model takes as input before making the prediction) and higher sequence length (the number of tokens the model generates in the response.)\n\nWe’re listening, and in conjunction with streaming, today we are adding a higher 16-bit full-precision Llama-2 variant to the catalog, and increasing the context and sequence lengths for the existing 8-bit version.\n\n| Model | Context length (in) | Sequence length (out) |\n| --- | --- | --- |\n| @cf/meta/llama-2-7b-chat-int8 | 2048 (768 before) | 1800 (256 before) |\n| @cf/meta/llama-2-7b-chat-fp16 | 3072 | 2500 |\n\nStreaming, higher precision, and longer context and sequence lengths provide a better user experience and enable new, richer applications using large language models in Workers AI.\n\nCheck the Workers AI [developer documentation](https://developers.cloudflare.com/workers-ai) for more information and options. If you have any questions or feedback about Workers AI, please come see us in the [Cloudflare Community](https://community.cloudflare.com/) and the [Cloudflare Discord](https://discord.gg/cloudflaredev).  \nIf you are interested in machine learning and serverless AI, the Cloudflare Workers AI team is building a global-scale platform and tools that enable our customers to run fast, low-latency inference tasks on top of our network. Check our [jobs page](https://www.cloudflare.com/careers/jobs/) for opportunities.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Workers AI](https://blog.cloudflare.com/tag/workers-ai/) [Cloudflare Workers](https://blog.cloudflare.com/tag/workers/) [Developer Platform](https://blog.cloudflare.com/tag/developer-platform/) [JavaScript](https://blog.cloudflare.com/tag/javascript/) [Serverless](https://blog.cloudflare.com/tag/serverless/)"
    },
    {
      "url": "https://blog.cloudflare.com/cloudflare-incident-on-october-30-2023/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/cloudflare-incident-on-october-30-2023/",
        "loadedTime": "2023-12-05T02:28:29.057Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/cloudflare-incident-on-october-30-2023/",
        "title": "Cloudflare incident on October 30, 2023",
        "description": "Multiple Cloudflare services were unavailable for 37 minutes on October 30, 2023, due to the misconfiguration of a deployment tool used by Workers KV. We’re sorry that this incident — and its resulting impact on our customers — occurred, and in the spirit of transparency and improvement, we are sharing the details here.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/01/2023\n8 min read\nThis post is also available in 简体中文, 繁體中文, 日本語 and 한국어, Deutsch, Français and Español.\nMultiple Cloudflare services were unavailable for 37 minutes on October 30, 2023. This was due to the misconfiguration of a deployment tool used by Workers KV. This was a frustrating incident, made more difficult by Cloudflare’s reliance on our own suite of products. We are deeply sorry for the impact it had on customers. What follows is a discussion of what went wrong, how the incident was resolved, and the work we are undertaking to ensure it does not happen again. \nWorkers KV is our globally distributed key-value store. It is used by both customers and Cloudflare teams alike to manage configuration data, routing lookups, static asset bundles, authentication tokens, and other data that needs low-latency access.\nDuring this incident, KV returned what it believed was a valid HTTP 401 (Unauthorized) status code instead of the requested key-value pair(s) due to a bug in a new deployment tool used by KV.\nThese errors manifested differently for each product depending on how KV is used by each service, with their impact detailed below.\nWhat was impacted\nA number of Cloudflare services depend on Workers KV for distributing configuration, routing information, static asset serving, and authentication state globally. These services instead received an HTTP 401 (Unauthorized) error when performing any get, put, delete, or list operation against a KV namespace.\nCustomers using the following Cloudflare products would have observed heightened error rates and/or would have been unable to access some or all features for the duration of the incident:\nProduct Impact \nWorkers KV\tCustomers with applications leveraging KV saw those applications fail during the duration of this incident, including both the KV API within Workers, and the REST API.\nWorkers applications not using KV were not impacted.\t\nPages\tApplications hosted on Pages were unreachable for the duration of the incident and returned HTTP 500 errors to users. New Pages deployments also returned HTTP 500 errors to users for the duration.\t\nAccess\tUsers who were unauthenticated could not log in; any origin attempting to validate the JWT using the /certs endpoint would fail; any application with a device posture policy failed for all users.\nExisting logged-in sessions that did not use the /certs endpoint or posture checks were unaffected. Overall, a large percentage of existing sessions were still affected.\t\nWARP / Zero Trust\tUsers were unable to register new devices or connect to resources subject to policies that enforce Device Posture checks or WARP Session timeouts.\nDevices already enrolled, resources not relying on device posture, or that had re-authorized outside of this window were unaffected.\t\nImages\tThe Images API returned errors during the incident. Existing image delivery was not impacted.\t\nCache Purge (single file)\tSingle file purge was partially unavailable for the duration of the incident as some data centers could not access configuration data in KV. Data centers that had existing configuration data locally cached were unaffected.\nOther cache purge mechanisms, including purge by tag, were unaffected.\t\nWorkers\tUploading or editing Workers through the dashboard, wrangler or API returned errors during the incident. Deployed Workers were not impacted, unless they used KV. \t\nAI Gateway\tAI Gateway was not able to proxy requests for the duration of the incident.\t\nWaiting Room\tWaiting Room configuration is stored at the edge in Workers KV. Waiting Room configurations, and configuration changes, were unavailable and the service failed open.\nWhen access to KV was restored, some Waiting Room users would have experienced queuing as the service came back up. \t\nTurnstile and Challenge Pages\tTurnstile's JavaScript assets are stored in KV, and the entry point for Turnstile (api.js) was not able to be served. Clients accessing pages using Turnstile could not initialize the Turnstile widget and would have failed closed during the incident window.\nChallenge Pages (which products like Custom, Managed and Rate Limiting rules use) also use Turnstile infrastructure for presenting challenge pages to users under specific conditions, and would have blocked users who were presented with a challenge during that period.\t\nCloudflare Dashboard\tParts of the Cloudflare dashboard that rely on Turnstile and/or our internal feature flag tooling (which uses KV for configuration) returned errors to users for the duration. \t\nTimeline\nAll timestamps referenced are in Coordinated Universal Time (UTC).\nTime Description \n2023-10-30 18:58 UTC\tThe Workers KV team began a progressive deployment of a new KV build to production.\t\n2023-10-30 19:29 UTC\tThe internal progressive deployment API returns staging build GUID to a call to list production builds. \t\n2023-10-30 19:40 UTC\tThe progressive deployment API was used to continue rolling out the release. This routed a percentage of traffic to the wrong destination, triggering alerting and leading to the decision to roll back.\t\n2023-10-30 19:54 UTC\tRollback via progressive deployment API attempted, traffic starts to fail at scale. — IMPACT START —\t\n2023-10-30 20:15 UTC\tCloudflare engineers manually edit (via break glass mechanisms) deployment routes to revert to last known good build for the majority of traffic.\t\n2023-10-30 20:29 UTC\tWorkers KV error rates return to normal pre-incident levels, and impacted services recover within the following minute.\t\n2023-10-30 20:31 UTC\tImpact resolved — IMPACT END — \t\nAs shown in the above timeline, there was a delay between the time we realized we were having an issue at 19:54 UTC and the time we were actually able to perform the rollback at 20:15 UTC.\nThis was caused by the fact that multiple tools within Cloudflare rely on Workers KV including Cloudflare Access. Access leverages Workers KV as part of its request verification process. Due to this, we were unable to leverage our internal tooling and had to use break-glass mechanisms to bypass the normal tooling. As described below, we had not spent sufficient time testing the rollback mechanisms. We plan to harden this moving forward.\nResolution\nCloudflare engineers manually switched (via break glass mechanism) the production route to the previous working version of Workers KV, which immediately eliminated the failing request path and subsequently resolved the issue with the Workers KV deployment.\nAnalysis\nWorkers KV is a low-latency key-value store that allows users to store persistent data on Cloudflare's network, as close to the users as possible. This distributed key-value store is used in many applications, some of which are first-party Cloudflare products like Pages, Access, and Zero Trust.\nThe Workers KV team was progressively deploying a new release using a specialized deployment tool. The deployment mechanism contains a staging and a production environment, and utilizes a process where the production environment is upgraded to the new version at progressive percentages until all production environments are upgraded to the most recent production build. The deployment tool had a latent bug with how it returns releases and their respective versions. Instead of returning releases from a single environment, the tool returned a broader list of releases than intended, resulting in production and staging releases being returned together.\nIn this incident, the service was deployed and tested in staging. But because of the deployment automation bug, when promoting to production, a script that had been deployed to the staging account was incorrectly referenced instead of the pre-production version on the production account. As a result, the deployment mechanism pointed the production environment to a version that was not running anywhere in the production environment, effectively black-holing traffic.\nWhen this happened, Workers KV became unreachable in production, as calls to the product were directed to a version that was not authorized for production access, returning a HTTP 401 error code. This caused dependent products which stored key-value pairs in KV to fail, regardless of whether the key-value pair was cached locally or not.\nAlthough automated alerting detected the issue immediately, there was a delay between the time we realized we were having an issue and the time we were actually able to perform the roll back. This was caused by the fact that multiple tools within Cloudflare rely on Workers KV including Cloudflare Access. Access uses Workers KV as part of the verification process for user JWTs (JSON Web Tokens).\nThese tools include the dashboard which was used to revert the change, and the authentication mechanism to access our continuous integration (CI) system. As Workers KV was down, so too were these services. Automatic rollbacks via our CI system had been successfully tested previously, but the authentication issues (Access relies on KV) due to the incident made accessing the necessary secrets to roll back the deploy impossible.\nThe fix ultimately was a manual change of the production build path to a previous and known good state. This path was known to have been deployed and was the previous production build before the attempted deployment.\nNext steps\nAs more teams at Cloudflare have built on Workers, we have \"organically\" ended up in a place where Workers KV now underpins a tremendous amount of our products and services. This incident has continued to reinforce the need for us to revisit how we can reduce the blast radius of critical dependencies, which includes improving the sophistication of our deployment tooling, its ease-of-use for our internal teams, and product-level controls for these dependencies. We’re prioritizing these efforts to ensure that there is not a repeat of this incident.\nThis also reinforces the need for Cloudflare to improve the tooling, and the safety of said tooling, around progressive deployments of Workers applications internally and for customers.\nThis includes (but is not limited) to the below list of key follow-up actions (in no specific order) this quarter:\nOnboard KV deployments to standardized Workers deployment models which use automated systems for impact detection and recovery.\nEnsure that the rollback process has access to a known good deployment identifier and that it works when Cloudflare Access is down.\nAdd pre-checks to deployments which will validate input parameters to ensure version mismatches don't propagate to production environments.\nHarden the progressive deployment tooling to operate in a way that is designed for multi-tenancy. The current design assumes a single-tenant model.\nAdd additional validation to progressive deployment scripts to verify that the deployment matches the app environment (production, staging, etc.).\nAgain, we’re extremely sorry this incident occurred, and take the impact of this incident on our customers extremely seriously.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nPost Mortem",
      "markdown": "11/01/2023\n\n*   [![Matt Silverlock](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/08/profile-1500px-square.jpeg)](https://blog.cloudflare.com/author/silverlock/)\n*   [![Kris Evans](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/08/1687236408752.jpeg)](https://blog.cloudflare.com/author/kris-evans/)\n\n8 min read\n\nThis post is also available in [简体中文](https://blog.cloudflare.com/zh-cn/cloudflare-incident-on-october-30-2023-zh-cn/), [繁體中文](https://blog.cloudflare.com/zh-tw/cloudflare-incident-on-october-30-2023-zh-tw/), [日本語](https://blog.cloudflare.com/ja-jp/cloudflare-incident-on-october-30-2023-ja-jp/) and [한국어](https://blog.cloudflare.com/ko-kr/cloudflare-incident-on-october-30-2023-ko-kr/), [Deutsch](https://blog.cloudflare.com/de-de/cloudflare-incident-on-october-30-2023-de-de/), [Français](https://blog.cloudflare.com/es-es/cloudflare-incident-on-october-30-2023-fr-fr/) and [Español](https://blog.cloudflare.com/es-es/cloudflare-incident-on-october-30-2023-es-es/).\n\n![](https://blog.cloudflare.com/content/images/2023/11/Vulnerabilitiy-1.png)\n\nMultiple Cloudflare services were unavailable for 37 minutes on October 30, 2023. This was due to the misconfiguration of a deployment tool used by Workers KV. This was a frustrating incident, made more difficult by Cloudflare’s reliance on our own suite of products. We are deeply sorry for the impact it had on customers. What follows is a discussion of what went wrong, how the incident was resolved, and the work we are undertaking to ensure it does not happen again.\n\nWorkers KV is our globally distributed key-value store. It is used by both customers and Cloudflare teams alike to manage configuration data, routing lookups, static asset bundles, authentication tokens, and other data that needs low-latency access.\n\nDuring this incident, KV returned what it believed was a valid HTTP 401 (Unauthorized) status code instead of the requested key-value pair(s) due to a bug in a new deployment tool used by KV.\n\nThese errors manifested differently for each product depending on how KV is used by each service, with their impact detailed below.\n\n### What was impacted\n\nA number of Cloudflare services depend on Workers KV for distributing configuration, routing information, static asset serving, and authentication state globally. These services instead received an HTTP 401 (Unauthorized) error when performing any get, put, delete, or list operation against a KV namespace.\n\nCustomers using the following Cloudflare products would have observed heightened error rates and/or would have been unable to access some or all features for the duration of the incident:\n\n| Product | Impact |\n| --- | --- |\n| Workers KV | Customers with applications leveraging KV saw those applications fail during the duration of this incident, including both the KV API within Workers, and the REST API.  <br>Workers applications not using KV were not impacted. |\n| Pages | Applications hosted on Pages were unreachable for the duration of the incident and returned HTTP 500 errors to users. New Pages deployments also returned HTTP 500 errors to users for the duration. |\n| Access | Users who were unauthenticated could not log in; any origin attempting to validate the JWT using the /certs endpoint would fail; any application with a device posture policy failed for all users.  <br>Existing logged-in sessions that did not use the /certs endpoint or posture checks were unaffected. Overall, a large percentage of existing sessions were still affected. |\n| WARP / Zero Trust | Users were unable to register new devices or connect to resources subject to policies that enforce Device Posture checks or WARP Session timeouts.  <br>Devices already enrolled, resources not relying on device posture, or that had re-authorized outside of this window were unaffected. |\n| Images | The Images API returned errors during the incident. Existing image delivery was not impacted. |\n| Cache Purge (single file) | Single file purge was partially unavailable for the duration of the incident as some data centers could not access configuration data in KV. Data centers that had existing configuration data locally cached were unaffected.  <br>Other cache purge mechanisms, including purge by tag, were unaffected. |\n| Workers | Uploading or editing Workers through the dashboard, wrangler or API returned errors during the incident. Deployed Workers were not impacted, unless they used KV. |\n| AI Gateway | AI Gateway was not able to proxy requests for the duration of the incident. |\n| Waiting Room | Waiting Room configuration is stored at the edge in Workers KV. Waiting Room configurations, and configuration changes, were unavailable and the service failed open.  <br>When access to KV was restored, some Waiting Room users would have experienced queuing as the service came back up. |\n| Turnstile and Challenge Pages | Turnstile's JavaScript assets are stored in KV, and the entry point for Turnstile (api.js) was not able to be served. Clients accessing pages using Turnstile could not initialize the Turnstile widget and would have failed closed during the incident window.  <br>Challenge Pages (which products like Custom, Managed and Rate Limiting rules use) also use Turnstile infrastructure for presenting challenge pages to users under specific conditions, and would have blocked users who were presented with a challenge during that period. |\n| Cloudflare Dashboard | Parts of the Cloudflare dashboard that rely on Turnstile and/or our internal feature flag tooling (which uses KV for configuration) returned errors to users for the duration. |\n\n### Timeline\n\n_All timestamps referenced are in Coordinated Universal Time (UTC)._\n\n| Time | Description |\n| --- | --- |\n| 2023-10-30 18:58 UTC | The Workers KV team began a progressive deployment of a new KV build to production. |\n| 2023-10-30 19:29 UTC | The internal progressive deployment API returns staging build GUID to a call to list production builds. |\n| 2023-10-30 19:40 UTC | The progressive deployment API was used to continue rolling out the release. This routed a percentage of traffic to the wrong destination, triggering alerting and leading to the decision to roll back. |\n| 2023-10-30 19:54 UTC | Rollback via progressive deployment API attempted, traffic starts to fail at scale. — IMPACT START — |\n| 2023-10-30 20:15 UTC | Cloudflare engineers manually edit (via break glass mechanisms) deployment routes to revert to last known good build for the majority of traffic. |\n| 2023-10-30 20:29 UTC | Workers KV error rates return to normal pre-incident levels, and impacted services recover within the following minute. |\n| 2023-10-30 20:31 UTC | Impact resolved — IMPACT END — |\n\nAs shown in the above timeline, there was a delay between the time we realized we were having an issue at 19:54 UTC and the time we were actually able to perform the rollback at 20:15 UTC.\n\nThis was caused by the fact that multiple tools within Cloudflare rely on Workers KV including Cloudflare Access. Access leverages Workers KV as part of its request verification process. Due to this, we were unable to leverage our internal tooling and had to use break-glass mechanisms to bypass the normal tooling. As described below, we had not spent sufficient time testing the rollback mechanisms. We plan to harden this moving forward.\n\n### Resolution\n\nCloudflare engineers manually switched (via break glass mechanism) the production route to the previous working version of Workers KV, which immediately eliminated the failing request path and subsequently resolved the issue with the Workers KV deployment.\n\n### Analysis\n\nWorkers KV is a low-latency key-value store that allows users to store persistent data on Cloudflare's network, as close to the users as possible. This distributed key-value store is used in many applications, some of which are first-party Cloudflare products like Pages, Access, and Zero Trust.\n\nThe Workers KV team was progressively deploying a new release using a specialized deployment tool. The deployment mechanism contains a staging and a production environment, and utilizes a process where the production environment is upgraded to the new version at progressive percentages until all production environments are upgraded to the most recent production build. The deployment tool had a latent bug with how it returns releases and their respective versions. Instead of returning releases from a single environment, the tool returned a broader list of releases than intended, resulting in production and staging releases being returned together.\n\nIn this incident, the service was deployed and tested in staging. But because of the deployment automation bug, when promoting to production, a script that had been deployed to the staging account was incorrectly referenced instead of the pre-production version on the production account. As a result, the deployment mechanism pointed the production environment to a version that was not running anywhere in the production environment, effectively black-holing traffic.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image1.png)\n\nWhen this happened, Workers KV became unreachable in production, as calls to the product were directed to a version that was not authorized for production access, returning a HTTP 401 error code. This caused dependent products which stored key-value pairs in KV to fail, regardless of whether the key-value pair was cached locally or not.\n\nAlthough automated alerting detected the issue immediately, there was a delay between the time we realized we were having an issue and the time we were actually able to perform the roll back. This was caused by the fact that multiple tools within Cloudflare rely on Workers KV including Cloudflare Access. Access uses Workers KV as part of the verification process for user JWTs (JSON Web Tokens).\n\nThese tools include the dashboard which was used to revert the change, and the authentication mechanism to access our continuous integration (CI) system. As Workers KV was down, so too were these services. Automatic rollbacks via our CI system had been successfully tested previously, but the authentication issues (Access relies on KV) due to the incident made accessing the necessary secrets to roll back the deploy impossible.\n\nThe fix ultimately was a manual change of the production build path to a previous and known good state. This path was known to have been deployed and was the previous production build before the attempted deployment.\n\n### Next steps\n\nAs more teams at Cloudflare have built on Workers, we have \"organically\" ended up in a place where Workers KV now underpins a tremendous amount of our products and services. This incident has continued to reinforce the need for us to revisit how we can reduce the blast radius of critical dependencies, which includes improving the sophistication of our deployment tooling, its ease-of-use for our internal teams, and product-level controls for these dependencies. We’re prioritizing these efforts to ensure that there is not a repeat of this incident.\n\nThis also reinforces the need for Cloudflare to improve the tooling, and the safety of said tooling, around progressive deployments of Workers applications internally and for customers.\n\nThis includes (but is not limited) to the below list of key follow-up actions (in no specific order) this quarter:\n\n1.  Onboard KV deployments to standardized Workers deployment models which use automated systems for impact detection and recovery.\n2.  Ensure that the rollback process has access to a known good deployment identifier and that it works when Cloudflare Access is down.\n3.  Add pre-checks to deployments which will validate input parameters to ensure version mismatches don't propagate to production environments.\n4.  Harden the progressive deployment tooling to operate in a way that is designed for multi-tenancy. The current design assumes a single-tenant model.\n5.  Add additional validation to progressive deployment scripts to verify that the deployment matches the app environment (production, staging, etc.).\n\nAgain, we’re extremely sorry this incident occurred, and take the impact of this incident on our customers extremely seriously.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Post Mortem](https://blog.cloudflare.com/tag/post-mortem/)"
    },
    {
      "url": "https://blog.cloudflare.com/post-mortem-on-cloudflare-control-plane-and-analytics-outage/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/post-mortem-on-cloudflare-control-plane-and-analytics-outage/",
        "loadedTime": "2023-12-05T02:28:27.462Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/post-mortem-on-cloudflare-control-plane-and-analytics-outage/",
        "title": "Post Mortem on Cloudflare Control Plane and Analytics Outage",
        "description": "Beginning on Thursday, November 2, 2023 at 11:43 UTC Cloudflare's control plane and analytics services experienced an outage. Here are the details",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/04/2023\n12 min read\nThis post is also available in 繁體中文, Français, Deutsch, Español, Português, 한국어, 简体中文 and 日本語.\nBeginning on Thursday, November 2, 2023, at 11:43 UTC Cloudflare's control plane and analytics services experienced an outage. The control plane of Cloudflare consists primarily of the customer-facing interface for all of our services including our website and APIs. Our analytics services include logging and analytics reporting.\nThe incident lasted from November 2 at 11:44 UTC until November 4 at 04:25 UTC. We were able to restore most of our control plane at our disaster recovery facility as of November 2 at 17:57 UTC. Many customers would not have experienced issues with most of our products after the disaster recovery facility came online. However, other services took longer to restore and customers that used them may have seen issues until we fully resolved the incident. Our raw log services were unavailable for most customers for the duration of the incident.\nServices have now been restored for all customers. Throughout the incident, Cloudflare's network and security services continued to work as expected. While there were periods where customers were unable to make changes to those services, traffic through our network was not impacted.\nThis post outlines the events that caused this incident, the architecture we had in place to prevent issues like this, what failed, what worked and why, and the changes we're making based on what we've learned over the last 36 hours. \nTo start, this never should have happened. We believed that we had high availability systems in place that should have stopped an outage like this, even when one of our core data center providers failed catastrophically. And, while many systems did remain online as designed, some critical systems had non-obvious dependencies that made them unavailable. I am sorry and embarrassed for this incident and the pain that it caused our customers and our team.\nIntended Design\nCloudflare's control plane and analytics systems run primarily on servers in three data centers around Hillsboro, Oregon. The three data centers are independent of one another, each have multiple utility power feeds, and each have multiple redundant and independent network connections.\nThe facilities were intentionally chosen to be at a distance apart that would minimize the chances that a natural disaster would cause all three to be impacted, while still close enough that they could all run active-active redundant data clusters. This means that they are continuously syncing data between the three facilities. By design, if any of the facilities goes offline then the remaining ones are able to continue to operate. \nThis is a system design that we began implementing four years ago. While most of our critical control plane systems had been migrated to the high availability cluster, some services, especially for some newer products, had not yet been added to the high availability cluster.\nIn addition, our logging systems were intentionally not part of the high availability cluster. The logic of that decision was that logging was already a distributed problem where logs were queued at the edge of our network and then sent back to the core in Oregon (or another regional facility for customers using regional services for logging). If our logging facility was offline then analytics logs would queue at the edge of our network until it came back online. We determined that analytics being delayed was acceptable.\nFlexential Data Center Power Failure\nThe largest of the three facilities in Oregon is run by Flexential. We refer to this facility as “PDX-DC04”. Cloudflare leases space in PDX-04 where we house our largest analytics cluster as well as more than a third of the machines for our high availability cluster. It is also the default location for services that have not yet been onboarded onto our high availability cluster. We are a relatively large customer of the facility, consuming approximately 10 percent of its total capacity.\nOn November 2 at 08:50 UTC Portland General Electric (PGE), the utility company that services PDX-04, had an unplanned maintenance event affecting one of their independent power feeds into the building. That event shut down one feed into PDX-04. The data center has multiple feeds with some level of independence that can power the facility. However, Flexential powered up their generators to effectively supplement the feed that was down.\nCounter to best practices, Flexential did not inform Cloudflare that they had failed over to generator power. None of our observability tools were able to detect that the source of power had changed. Had they informed us, we would have stood up a team to monitor the facility closely and move control plane services that were dependent on that facility out while it was degraded.\nIt is also unusual that Flexential ran both the one remaining utility feed and the generators at the same time. It is not unusual for utilities to ask data centers to drop off the grid when power demands are high and run exclusively on generators. Flexential operates 10 generators, inclusive of redundant units, capable of supporting the facility at full load. It would also have been possible for Flexential to run the facility only from the remaining utility feed. We haven't gotten a clear answer why they ran utility power and generator power.\nInformed Speculation On What Happened Next\nFrom this decision onward, we don't yet have clarity from Flexential on the root cause or some of the decisions they made or the events. We will update this post as we get more information from Flexential, as well as PGE, on what happened. Some of what follows is informed speculation based on the most likely series of events as well as what individual Flexential employees have shared with us unofficially.\nOne possible reason they may have left the utility line running is because Flexential was part of a program with PGE called DSG. DSG allows the local utility to run a data center's generators to help supply additional power to the grid. In exchange, the power company helps maintain the generators and supplies fuel. We have been unable to locate any record of Flexential informing us about the DSG program. We've asked if DSG was active at the time and have not received an answer. We do not know if it contributed to the decisions that Flexential made, but it could explain why the utility line continued to remain online after the generators were started.\nAt approximately 11:40 UTC, there was a ground fault on a PGE transformer at PDX-04. We believe, but have not been able to get confirmation from Flexential or PGE, that this was the transformer that stepped down power from the grid for the second feed that was still running as it entered the data center. It seems likely, though we have not been able to confirm with Flexential or PGE, that the ground fault was caused by the unplanned maintenance PGE was performing that impacted the first feed. Or it was a very unlucky coincidence.\nGround faults with high voltage (12,470 volt) power lines are very bad. Electrical systems are designed to quickly shut down to prevent damage when one occurs. Unfortunately, in this case, the protective measure also shut down all of PDX-04’s generators. This meant that the two sources of power generation for the facility — both the redundant utility lines as well as the 10 generators — were offline.\nFortunately, in addition to the generators, PDX-04 also contains a bank of UPS batteries. These batteries are supposedly sufficient to power the facility for approximately 10 minutes. That time is meant to be enough to bridge the gap between the power going out and the generators automatically starting up. If Flexential could get the generators or a utility feed restored within 10 minutes then there would be no interruption. In reality, the batteries started to fail after only 4 minutes based on what we observed from our own equipment failing. And it took Flexential far longer than 10 minutes to get the generators restored.\nAttempting to Restore Power\nWhile we haven't gotten official confirmation, we have been told by employees that three things hampered getting the generators back online. First, they needed to be physically accessed and manually restarted because of the way the ground fault had tripped circuits. Second, Flexential's access control system was not powered by the battery backups, so it was offline. And third, the overnight staffing at the site did not include an experienced operations or electrical expert — the overnight shift consisted of security and an unaccompanied technician who had only been on the job for a week.\nBetween 11:44 and 12:01 UTC, with the generators not fully restarted, the UPS batteries ran out of power and all customers of the data center lost power. Throughout this, Flexential never informed Cloudflare that there was any issue at the facility. We were first notified of issues in the data center when the two routers that connect the facility to the rest of the world went offline at 11:44 UTC. When we weren't able to reach the routers directly or through out-of-band management, we attempted to contact Flexential and dispatched our local team to physically travel to the facility. The first message to us from Flexential that they were experiencing an issue was at 12:28 UTC.\nWe are currently experiencing an issue with power at our [PDX-04] that began at approximately 0500AM PT [12:00 UTC]. Engineers are actively working to resolve the issue and restore service. We will communicate progress every 30 minutes or as more information becomes available as to the estimated time to restore. Thank you for your patience and understanding.\nDesigning for Data Center Level Failure\nWhile the PDX-04’s design was certified Tier III before construction and is expected to provide high availability SLAs, we planned for the possibility that it could go offline. Even well-run facilities can have bad days. And we planned for that. What we expected would happen in that case is that our analytics would be offline, logs would be queued at the edge and delayed, and certain lower priority services that were not integrated into our high availability cluster would go offline temporarily until they could be restored at another facility.\nThe other two data centers running in the area would take over responsibility for the high availability cluster and keep critical services online. Generally that worked as planned. Unfortunately, we discovered that a subset of services that were supposed to be on the high availability cluster had dependencies on services exclusively running in PDX-04.\nIn particular, two critical services that process logs and power our analytics — Kafka and ClickHouse — were only available in PDX-04 but had services that depended on them that were running in the high availability cluster. Those dependencies shouldn’t have been so tight, should have failed more gracefully, and we should have caught them.\nWe had performed testing of our high availability cluster by taking each (and both) of the other two data center facilities entirely offline. And we had also tested taking the high availability portion of PDX-04 offline. However, we had never tested fully taking the entire PDX-04 facility offline. As a result, we had missed the importance of some of these dependencies on our data plane.\nWe were also far too lax about requiring new products and their associated databases to integrate with the high availability cluster. Cloudflare allows multiple teams to innovate quickly. As such, products often take different paths toward their initial alpha. While, over time, our practice is to migrate the backend for these services to our best practices, we did not formally require that before products were declared generally available (GA). That was a mistake as it meant that the redundancy protections we had in place worked inconsistently depending on the product.\nMoreover, far too many of our services depend on the availability of our core facilities. While this is the way a lot of software services are created, it does not play to Cloudflare’s strength. We are good at distributed systems. Throughout this incident, our global network continued to perform as expected. While some of our products and features are configurable and serviceable through the edge of our network without needing the core, far too many today fail if the core is unavailable. We need to use the distributed systems products that we make available to all our customers for all our services, so they continue to function mostly as normal even if our core facilities are disrupted.\nDisaster Recovery\nAt 12:48 UTC, Flexential was able to get the generators restarted. Power returned to portions of the facility. In order to not overwhelm the system, when power is restored to a data center it is typically done gradually by powering back on one circuit at a time. Like the circuit breakers in a residential home, each customer is serviced by redundant breakers. When Flexential attempted to power back up Cloudflare's circuits, the circuit breakers were discovered to be faulty. We don't know if the breakers failed due to the ground fault or some other surge as a result of the incident, or if they'd been bad before, and it was only discovered after they had been powered off.\nFlexential began the process of replacing the failed breakers. That required them to source new breakers because more were bad than they had on hand in the facility. Because more services were offline than we expected, and because Flexential could not give us a time for restoration of our services, we made the call at 13:40 UTC to fail over to Cloudflare's disaster recovery sites located in Europe. Thankfully, we only needed to fail over a small percentage of Cloudflare’s overall control plane. Most of our services continued to run across our high availability systems across the two active core data centers.\nWe turned up the first services on the disaster recovery site at 13:43 UTC. Cloudflare's disaster recovery sites provide critical control plane services in the event of a disaster. While the disaster recovery site does not support some of our log processing services, it is designed to support the other portions of our control plane. \nWhen services were turned up there, we experienced a thundering herd problem where the API calls that had been failing overwhelmed our services. We implemented rate limits to get the request volume under control. During this period, customers of most products would have seen intermittent errors when making modifications through our dashboard or API. By 17:57 UTC, the services that had been successfully moved to the disaster recovery site were stable and most customers were no longer directly impacted. However, some systems still required manual configuration (e.g., Magic WAN) and some other services, largely related to log processing and some bespoke APIs, remained unavailable until we were able to restore PDX-04.\nSome Products and Features Delayed Restart\nA handful of products did not properly get stood up on our disaster recovery sites. These tended to be newer products where we had not fully implemented and tested a disaster recovery procedure. These included our Stream service for uploading new videos and some other services. Our team worked two simultaneous tracks to get these services restored: 1) reimplementing them on our disaster recovery sites; and 2) migrating them to our high-availability cluster.\nFlexential replaced our failed circuit breakers, restored both utility feeds, and confirmed clean power at 22:48 UTC. Our team was all-hands-on-deck and had worked all day on the emergency, so I made the call that most of us should get some rest and start the move back to PDX-04 in the morning. That decision delayed our full recovery, but I believe made it less likely that we’d compound this situation with additional mistakes.\nBeginning first thing on November 3, our team began restoring service in PDX-04. That began with physically booting our network gear then powering up thousands of servers and restoring their services. The state of our services in the data center was unknown as we believed multiple power cycles were likely to have occurred during the incident. Our only safe process to recover was to follow a complete bootstrap of the entire facility.\nThis involved a manual process of bringing our configuration management servers online to begin the restoration of the facility. Rebuilding these took 3 hours. From there, our team was able to bootstrap the rebuild of the rest of the servers that power our services. Each server took between 10 minutes and 2 hours to rebuild. While we were able to run this in parallel across multiple servers, there were inherent dependencies between services that required some to be brought back online in sequence.\nServices are now fully restored as of November 4, 2023, at 04:25 UTC. For most customers, because we also store analytics in our European core data centers, you should see no data loss in most analytics across our dashboard and APIs. However, some datasets which are not replicated in the EU will have persistent gaps. For customers that use our log push feature, your logs will not have been processed for the majority of the event, so anything you did not receive will not be recovered.\nLessons and Remediation\nWe have a number of questions that we need answered from Flexential. But we also must expect that entire data centers may fail. Google has a process where when there’s a significant event or crisis they can call a Code Yellow or Code Red. In these cases, most or all engineering resources are shifted to addressing the issue at hand.\nWe have not had such a process in the past, but it’s clear today we need to implement a version of it ourselves: Code Orange. We are shifting all non-critical engineering functions to focusing on ensuring high reliability of our control plane. As part of that, we expect the following changes:\nRemove dependencies on our core data centers for control plane configuration of all services and move them wherever possible to be powered first by our distributed network\nEnsure that the control plane running on the network continues to function even if all our core data centers are offline\nRequire that all products and features that are designated Generally Available must rely on the high availability cluster (if they rely on any of our core data centers), without having any software dependencies on specific facilities\nRequire all products and features that are designated Generally Available have a reliable disaster recovery plan that is tested\nTest the blast radius of system failures and minimize the number of services that are impacted by a failure\nImplement more rigorous chaos testing of all data center functions including the full removal of each of our core data center facilities\nThorough auditing of all core data centers and a plan to reaudit to ensure they comply with our standards\nLogging and analytics disaster recovery plan that ensures no logs are dropped even in the case of a failure of all our core facilities\nAs I said earlier, I am sorry and embarrassed for this incident and the pain that it caused our customers and our team. We have the right systems and procedures in place to be able to withstand even the cascading string of failures we saw at our data center provider, but we need to be more rigorous about enforcing that they are followed and tested for unknown dependencies. This will have my full attention and the attention of a large portion of our team through the balance of the year. And the pain from the last couple of days will make us better.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nOutage Post Mortem",
      "markdown": "11/04/2023\n\n*   [![Matthew Prince](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/06/Matthew-Prince-3.jpeg)](https://blog.cloudflare.com/author/matthew-prince/)\n\n12 min read\n\nThis post is also available in [繁體中文](https://blog.cloudflare.com/zh-tw/post-mortem-on-cloudflare-control-plane-and-analytics-outage-zh-tw/), [Français](https://blog.cloudflare.com/fr-fr/post-mortem-on-cloudflare-control-plane-and-analytics-outage-fr-fr/), [Deutsch](https://blog.cloudflare.com/de-de/post-mortem-on-cloudflare-control-plane-and-analytics-outage-de-de/), [Español](https://blog.cloudflare.com/es-es/post-mortem-on-cloudflare-control-plane-and-analytics-outage-es-es/), [Português](https://blog.cloudflare.com/pt-br/post-mortem-on-cloudflare-control-plane-and-analytics-outage-pt-br/), [한국어](https://blog.cloudflare.com/ko-kr/post-mortem-on-cloudflare-control-plane-and-analytics-outage-ko-kr/), [简体中文](https://blog.cloudflare.com/zh-cn/post-mortem-on-cloudflare-control-plane-and-analytics-outage-zh-cn/) and [日本語](https://blog.cloudflare.com/ja-jp/post-mortem-on-cloudflare-control-plane-and-analytics-outage-ja-jp/).\n\nBeginning on Thursday, November 2, 2023, at 11:43 UTC Cloudflare's control plane and analytics services experienced an outage. The control plane of Cloudflare consists primarily of the customer-facing interface for all of our services including our website and APIs. Our analytics services include logging and analytics reporting.\n\nThe incident lasted from November 2 at 11:44 UTC until November 4 at 04:25 UTC. We were able to restore most of our control plane at our disaster recovery facility as of November 2 at 17:57 UTC. Many customers would not have experienced issues with most of our products after the disaster recovery facility came online. However, other services took longer to restore and customers that used them may have seen issues until we fully resolved the incident. Our raw log services were unavailable for most customers for the duration of the incident.\n\nServices have now been restored for all customers. Throughout the incident, Cloudflare's network and security services continued to work as expected. While there were periods where customers were unable to make changes to those services, traffic through our network was not impacted.\n\nThis post outlines the events that caused this incident, the architecture we had in place to prevent issues like this, what failed, what worked and why, and the changes we're making based on what we've learned over the last 36 hours.\n\nTo start, this never should have happened. We believed that we had high availability systems in place that should have stopped an outage like this, even when one of our core data center providers failed catastrophically. And, while many systems did remain online as designed, some critical systems had non-obvious dependencies that made them unavailable. I am sorry and embarrassed for this incident and the pain that it caused our customers and our team.\n\n### Intended Design\n\nCloudflare's control plane and analytics systems run primarily on servers in three data centers around Hillsboro, Oregon. The three data centers are independent of one another, each have multiple utility power feeds, and each have multiple redundant and independent network connections.\n\nThe facilities were intentionally chosen to be at a distance apart that would minimize the chances that a natural disaster would cause all three to be impacted, while still close enough that they could all run active-active redundant data clusters. This means that they are continuously syncing data between the three facilities. By design, if any of the facilities goes offline then the remaining ones are able to continue to operate.\n\nThis is a system design that we began implementing four years ago. While most of our critical control plane systems had been migrated to the high availability cluster, some services, especially for some newer products, had not yet been added to the high availability cluster.\n\nIn addition, our logging systems were intentionally not part of the high availability cluster. The logic of that decision was that logging was already a distributed problem where logs were queued at the edge of our network and then sent back to the core in Oregon (or another regional facility for customers using regional services for logging). If our logging facility was offline then analytics logs would queue at the edge of our network until it came back online. We determined that analytics being delayed was acceptable.\n\n### Flexential Data Center Power Failure\n\nThe largest of the three facilities in Oregon is run by Flexential. We refer to this facility as “PDX-DC04”. Cloudflare leases space in PDX-04 where we house our largest analytics cluster as well as more than a third of the machines for our high availability cluster. It is also the default location for services that have not yet been onboarded onto our high availability cluster. We are a relatively large customer of the facility, consuming approximately 10 percent of its total capacity.\n\nOn November 2 at 08:50 UTC Portland General Electric (PGE), the utility company that services PDX-04, had an unplanned maintenance event affecting one of their independent power feeds into the building. That event shut down one feed into PDX-04. The data center has multiple feeds with some level of independence that can power the facility. However, Flexential powered up their generators to effectively supplement the feed that was down.\n\nCounter to best practices, Flexential did not inform Cloudflare that they had failed over to generator power. None of our observability tools were able to detect that the source of power had changed. Had they informed us, we would have stood up a team to monitor the facility closely and move control plane services that were dependent on that facility out while it was degraded.\n\nIt is also unusual that Flexential ran both the one remaining utility feed and the generators at the same time. It is not unusual for utilities to ask data centers to drop off the grid when power demands are high and run exclusively on generators. Flexential operates 10 generators, inclusive of redundant units, capable of supporting the facility at full load. It would also have been possible for Flexential to run the facility only from the remaining utility feed. We haven't gotten a clear answer why they ran utility power and generator power.\n\n### Informed Speculation On What Happened Next\n\nFrom this decision onward, we don't yet have clarity from Flexential on the root cause or some of the decisions they made or the events. We will update this post as we get more information from Flexential, as well as PGE, on what happened. Some of what follows is informed speculation based on the most likely series of events as well as what individual Flexential employees have shared with us unofficially.\n\nOne possible reason they may have left the utility line running is because Flexential was part of a program with PGE called DSG. DSG allows the local utility to run a data center's generators to help supply additional power to the grid. In exchange, the power company helps maintain the generators and supplies fuel. We have been unable to locate any record of Flexential informing us about the DSG program. We've asked if DSG was active at the time and have not received an answer. We do not know if it contributed to the decisions that Flexential made, but it could explain why the utility line continued to remain online after the generators were started.\n\nAt approximately 11:40 UTC, there was a ground fault on a PGE transformer at PDX-04. We believe, but have not been able to get confirmation from Flexential or PGE, that this was the transformer that stepped down power from the grid for the second feed that was still running as it entered the data center. It seems likely, though we have not been able to confirm with Flexential or PGE, that the ground fault was caused by the unplanned maintenance PGE was performing that impacted the first feed. Or it was a very unlucky coincidence.\n\nGround faults with high voltage (12,470 volt) power lines are very bad. Electrical systems are designed to quickly shut down to prevent damage when one occurs. Unfortunately, in this case, the protective measure also shut down all of PDX-04’s generators. This meant that the two sources of power generation for the facility — both the redundant utility lines as well as the 10 generators — were offline.\n\nFortunately, in addition to the generators, PDX-04 also contains a bank of UPS batteries. These batteries are supposedly sufficient to power the facility for approximately 10 minutes. That time is meant to be enough to bridge the gap between the power going out and the generators automatically starting up. If Flexential could get the generators or a utility feed restored within 10 minutes then there would be no interruption. In reality, the batteries started to fail after only 4 minutes based on what we observed from our own equipment failing. And it took Flexential far longer than 10 minutes to get the generators restored.\n\n### Attempting to Restore Power\n\nWhile we haven't gotten official confirmation, we have been told by employees that three things hampered getting the generators back online. First, they needed to be physically accessed and manually restarted because of the way the ground fault had tripped circuits. Second, Flexential's access control system was not powered by the battery backups, so it was offline. And third, the overnight staffing at the site did not include an experienced operations or electrical expert — the overnight shift consisted of security and an unaccompanied technician who had only been on the job for a week.\n\nBetween 11:44 and 12:01 UTC, with the generators not fully restarted, the UPS batteries ran out of power and all customers of the data center lost power. Throughout this, Flexential never informed Cloudflare that there was any issue at the facility. We were first notified of issues in the data center when the two routers that connect the facility to the rest of the world went offline at 11:44 UTC. When we weren't able to reach the routers directly or through out-of-band management, we attempted to contact Flexential and dispatched our local team to physically travel to the facility. The first message to us from Flexential that they were experiencing an issue was at 12:28 UTC.\n\n> _We are currently experiencing an issue with power at our \\[PDX-04\\] that began at approximately 0500AM PT \\[12:00 UTC\\]. Engineers are actively working to resolve the issue and restore service. We will communicate progress every 30 minutes or as more information becomes available as to the estimated time to restore. Thank you for your patience and understanding._\n\n### Designing for Data Center Level Failure\n\nWhile the PDX-04’s design was certified Tier III before construction and is expected to provide high availability SLAs, we planned for the possibility that it could go offline. Even well-run facilities can have bad days. And we planned for that. What we expected would happen in that case is that our analytics would be offline, logs would be queued at the edge and delayed, and certain lower priority services that were not integrated into our high availability cluster would go offline temporarily until they could be restored at another facility.\n\nThe other two data centers running in the area would take over responsibility for the high availability cluster and keep critical services online. Generally that worked as planned. Unfortunately, we discovered that a subset of services that were supposed to be on the high availability cluster had dependencies on services exclusively running in PDX-04.\n\nIn particular, two critical services that process logs and power our analytics — Kafka and ClickHouse — were only available in PDX-04 but had services that depended on them that were running in the high availability cluster. Those dependencies shouldn’t have been so tight, should have failed more gracefully, and we should have caught them.\n\nWe had performed testing of our high availability cluster by taking each (and both) of the other two data center facilities entirely offline. And we had also tested taking the high availability portion of PDX-04 offline. However, we had never tested fully taking the entire PDX-04 facility offline. As a result, we had missed the importance of some of these dependencies on our data plane.\n\nWe were also far too lax about requiring new products and their associated databases to integrate with the high availability cluster. Cloudflare allows multiple teams to innovate quickly. As such, products often take different paths toward their initial alpha. While, over time, our practice is to migrate the backend for these services to our best practices, we did not formally require that before products were declared generally available (GA). That was a mistake as it meant that the redundancy protections we had in place worked inconsistently depending on the product.\n\nMoreover, far too many of our services depend on the availability of our core facilities. While this is the way a lot of software services are created, it does not play to Cloudflare’s strength. We are good at distributed systems. Throughout this incident, our global network continued to perform as expected. While some of our products and features are configurable and serviceable through the edge of our network without needing the core, far too many today fail if the core is unavailable. We need to use the distributed systems products that we make available to all our customers for all our services, so they continue to function mostly as normal even if our core facilities are disrupted.\n\n### Disaster Recovery\n\nAt 12:48 UTC, Flexential was able to get the generators restarted. Power returned to portions of the facility. In order to not overwhelm the system, when power is restored to a data center it is typically done gradually by powering back on one circuit at a time. Like the circuit breakers in a residential home, each customer is serviced by redundant breakers. When Flexential attempted to power back up Cloudflare's circuits, the circuit breakers were discovered to be faulty. We don't know if the breakers failed due to the ground fault or some other surge as a result of the incident, or if they'd been bad before, and it was only discovered after they had been powered off.\n\nFlexential began the process of replacing the failed breakers. That required them to source new breakers because more were bad than they had on hand in the facility. Because more services were offline than we expected, and because Flexential could not give us a time for restoration of our services, we made the call at 13:40 UTC to fail over to Cloudflare's disaster recovery sites located in Europe. Thankfully, we only needed to fail over a small percentage of Cloudflare’s overall control plane. Most of our services continued to run across our high availability systems across the two active core data centers.\n\nWe turned up the first services on the disaster recovery site at 13:43 UTC. Cloudflare's disaster recovery sites provide critical control plane services in the event of a disaster. While the disaster recovery site does not support some of our log processing services, it is designed to support the other portions of our control plane.\n\nWhen services were turned up there, we experienced a thundering herd problem where the API calls that had been failing overwhelmed our services. We implemented rate limits to get the request volume under control. During this period, customers of most products would have seen intermittent errors when making modifications through our dashboard or API. By 17:57 UTC, the services that had been successfully moved to the disaster recovery site were stable and most customers were no longer directly impacted. However, some systems still required manual configuration (e.g., Magic WAN) and some other services, largely related to log processing and some bespoke APIs, remained unavailable until we were able to restore PDX-04.\n\n### Some Products and Features Delayed Restart\n\nA handful of products did not properly get stood up on our disaster recovery sites. These tended to be newer products where we had not fully implemented and tested a disaster recovery procedure. These included our Stream service for uploading new videos and some other services. Our team worked two simultaneous tracks to get these services restored: 1) reimplementing them on our disaster recovery sites; and 2) migrating them to our high-availability cluster.\n\nFlexential replaced our failed circuit breakers, restored both utility feeds, and confirmed clean power at 22:48 UTC. Our team was all-hands-on-deck and had worked all day on the emergency, so I made the call that most of us should get some rest and start the move back to PDX-04 in the morning. That decision delayed our full recovery, but I believe made it less likely that we’d compound this situation with additional mistakes.\n\nBeginning first thing on November 3, our team began restoring service in PDX-04. That began with physically booting our network gear then powering up thousands of servers and restoring their services. The state of our services in the data center was unknown as we believed multiple power cycles were likely to have occurred during the incident. Our only safe process to recover was to follow a complete bootstrap of the entire facility.\n\nThis involved a manual process of bringing our configuration management servers online to begin the restoration of the facility. Rebuilding these took 3 hours. From there, our team was able to bootstrap the rebuild of the rest of the servers that power our services. Each server took between 10 minutes and 2 hours to rebuild. While we were able to run this in parallel across multiple servers, there were inherent dependencies between services that required some to be brought back online in sequence.\n\nServices are now fully restored as of November 4, 2023, at 04:25 UTC. For most customers, because we also store analytics in our European core data centers, you should see no data loss in most analytics across our dashboard and APIs. However, some datasets which are not replicated in the EU will have persistent gaps. For customers that use our log push feature, your logs will not have been processed for the majority of the event, so anything you did not receive will not be recovered.\n\n### Lessons and Remediation\n\nWe have a number of questions that we need answered from Flexential. But we also must expect that entire data centers may fail. Google has a process where when there’s a significant event or crisis they can call a Code Yellow or Code Red. In these cases, most or all engineering resources are shifted to addressing the issue at hand.\n\nWe have not had such a process in the past, but it’s clear today we need to implement a version of it ourselves: Code Orange. We are shifting all non-critical engineering functions to focusing on ensuring high reliability of our control plane. As part of that, we expect the following changes:\n\n*   Remove dependencies on our core data centers for control plane configuration of all services and move them wherever possible to be powered first by our distributed network\n*   Ensure that the control plane running on the network continues to function even if all our core data centers are offline\n*   Require that all products and features that are designated Generally Available must rely on the high availability cluster (if they rely on any of our core data centers), without having any software dependencies on specific facilities\n*   Require all products and features that are designated Generally Available have a reliable disaster recovery plan that is tested\n*   Test the blast radius of system failures and minimize the number of services that are impacted by a failure\n*   Implement more rigorous chaos testing of all data center functions including the full removal of each of our core data center facilities\n*   Thorough auditing of all core data centers and a plan to reaudit to ensure they comply with our standards\n*   Logging and analytics disaster recovery plan that ensures no logs are dropped even in the case of a failure of all our core facilities\n\nAs I said earlier, I am sorry and embarrassed for this incident and the pain that it caused our customers and our team. We have the right systems and procedures in place to be able to withstand even the cascading string of failures we saw at our data center provider, but we need to be more rigorous about enforcing that they are followed and tested for unknown dependencies. This will have my full attention and the attention of a large portion of our team through the balance of the year. And the pain from the last couple of days will make us better.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Outage](https://blog.cloudflare.com/tag/outage/) [Post Mortem](https://blog.cloudflare.com/tag/post-mortem/)"
    },
    {
      "url": "https://blog.cloudflare.com/introducing-http-traffic-anomalies-notifications/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/introducing-http-traffic-anomalies-notifications/",
        "loadedTime": "2023-12-05T02:28:35.484Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/introducing-http-traffic-anomalies-notifications/",
        "title": "Introducing notifications for HTTP Traffic Anomalies",
        "description": "Today we're excited to announce Traffic Anomalies notifications, which proactively alert you when your Internet property is seeing an unexpected change in traffic patterns",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/31/2023\n5 min read\nThis post is also available in Deutsch, Français, Español, 简体中文, 繁體中文, 日本語 and 한국어.\nWhen it comes to managing Internet properties, the difference between a small technical hiccup and major incident is often a matter of speed. Proactive alerting plays a crucial role, which is why we were excited when we released HTTP Error Rate notifications — giving administrators visibility into when end users are experiencing errors. \nBut what if there are issues that don't show up as errors, like a sudden drop in traffic, or a spike? \nToday, we're excited to announce Traffic Anomalies notifications, available to enterprise customers. These notifications trigger when Cloudflare detects unexpected changes in traffic, giving another valuable perspective into the health of your systems.\nUnexpected changes in traffic could be indicative of many things. If you run an ecommerce site and see a spike in traffic that could be great news — maybe customers are flocking to your sale, or you just had an ad run on a popular TV show. However, it could also mean that something is going wrong: maybe someone accidentally turned off a firewall rule, and now you’re seeing more malicious traffic. Either way, you might want to know that something has changed. \nSimilarly, a sudden drop in traffic could mean many things. Perhaps it’s Friday afternoon and all of your employees are signing off and no longer accessing your company website. Or maybe a link to your site is broken, and now potential customers aren’t able to access it. You could be losing potential revenue every minute that traffic is low, so you’d want to know as soon as possible to investigate.\nHow can we tell when to alert? \nCalculating anomalies in time series datasets is difficult to do well. The simplest way to do it is to use basic thresholds. However, as we’ve previously blogged about, simple thresholds aren’t very accurate when trying to determine when things are actually going wrong. There are too many edge cases for them to work effectively. \nCalculating anomalies in HTTP errors is relatively easy. We know that in general there should be a very low number of errors, so any spike is bad and therefore alertable. That’s why we use Service Level Objectives (SLOs) to calculate anomalies for our HTTP Error Rate notifications. \nHowever, analyzing overall HTTP traffic behaves more similarly to Cloudflare Security Events: there’s some general baseline of events that is computed from historical trends. Any deviation from that baseline is alertable. Because of those similarities, we decided to use the same calculations for Traffic Anomalies notifications as we have previously used for Security Event notifications: z-scores. This involves comparing the current value to the average over a period of time. However, many standard deviations away from the average the current value is, is the z-score. \nPlot of HTTP traffic against z-scores. The blue is the HTTP traffic, purple is the positive z-score bound of the traffic, and green is the negative z-score bound of the traffic\nFor Traffic Anomalies notifications, we’re comparing the traffic over the past 5 minutes (the short window) to the average of the traffic over the past 4 hours (the long window). Positive z-scores indicate a spike, and negative z-scores indicate a drop. If the current value is more than 3.5 standard deviations away from the average, we alert. We measure every 5 minutes, so we can alert on any traffic spike or drop in a timely fashion.\nGreen bucket is the long window and the red bucket is the short window\nWhile our Security Event notifications only trigger when there is a spike in security events (a drop is almost always a good thing), in the case of Traffic Anomalies we send notifications for both spikes and drops. This is because a drop of HTTP traffic is likely indicative of a problem, and a surge could be good or bad. \nAs with Security Events, Traffic Anomalies notifications support minimum thresholds. This means that, even if we determine that an event is outside 3.5 standard deviations, if the number of events is insignificant, we don’t alert. A spike must be at least 200 requests, and a drop must fall by at least 200 requests. This makes the notifications less noisy, since we aren’t alerting on small spikes and drops. \nDigging a little deeper\nCloudflare stores sampled statistics on requests that go through its network in Clickhouse. Every minute, we take HTTP traffic from Clickhouse and store it in an instance of VictoriaMetrics, a time-series data storage solution. VictoriaMetrics gives us out-of-the-box algorithmic functions for free, and it has been a good fit for our use case. We chose VictoriaMetrics for a few reasons.\nFirstly, it's easy to configure and operate. As a team, we want to optimize for low operational burden and VictoriaMetrics has been great thus far. Secondly, VictoriaMetrics has the ability to scale horizontally, meaning we can run it in a highly available mode. For a system such as this where we want something reliable to compute time sensitive information for our customers, the high availability requirement is essential. Finally, in our tests, we found that VictoriaMetrics used around ⅓ of the memory that Prometheus, a similar alternative product, did for the same use case. \nOnce we have data in VictoriaMetrics, we can run queries against it and determine whether we need to alert our customers or not, based on notification configurations they have created ahead of time. To do this we leverage our existing Alert Notification System, which we blogged about initially in 2019. We know we can count on our current notification system for the last mile to deliver these critical notifications to our customers.\nData flow from HTTP request to notification\nSetting up the Notification \nTo configure this notification, navigate to the “Notifications” tab of the dashboard. Select Traffic Anomalies as your notification type. As with all Cloudflare notifications, you’re able to name and describe your notification, and choose how you want to be notified. \nTraffic Anomalies notification in the Dashboard\nYou can choose which domains you want monitored for Traffic Anomalies, whether you want to include traffic that’s already been mitigated by Cloudflare DoS or WAF products, and whether there are specific status codes you want included or excluded. You can also choose whether you want to be alerted on traffic spikes, drops, or both. \nWe’re excited to use this system to help serve our Enterprise customers with invaluable notifications regarding the overall health of their systems. Head over to the Notifications tab in the dash to check this new notification out now!\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nProduct News Notifications Network Services",
      "markdown": "10/31/2023\n\n*   [![Cathy Chi](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/1610954482110.jpg)](https://blog.cloudflare.com/author/cathy-chi/)\n*   [![Natasha Wissmann](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/05/IMG-1568.jpg)](https://blog.cloudflare.com/author/natasha/)\n\n5 min read\n\nThis post is also available in [Deutsch](https://blog.cloudflare.com/de-de/introducing-http-traffic-anomalies-notifications-de-de/), [Français](https://blog.cloudflare.com/fr-fr/introducing-http-traffic-anomalies-notifications-fr-fr/), [Español](https://blog.cloudflare.com/es-es/introducing-http-traffic-anomalies-notifications-es-es/), [简体中文](https://blog.cloudflare.com/zh-cn/introducing-http-traffic-anomalies-notifications-zh-cn/), [繁體中文](https://blog.cloudflare.com/zh-tw/introducing-http-traffic-anomalies-notifications-zh-tw/), [日本語](https://blog.cloudflare.com/ja-jp/introducing-http-traffic-anomalies-notifications-ja-jp/) and [한국어](https://blog.cloudflare.com/ko-kr/introducing-http-traffic-anomalies-notifications-ko-kr/).\n\n![Introducing notifications for HTTP Traffic Anomalies](https://blog.cloudflare.com/content/images/2023/10/Traffic-anomalies-1.png)\n\nWhen it comes to managing Internet properties, the difference between a small technical hiccup and major incident is often a matter of speed. Proactive alerting plays a crucial role, which is why we were excited when we released [HTTP Error Rate notifications](https://blog.cloudflare.com/smarter-origin-service-level-monitoring/) — giving administrators visibility into when end users are experiencing errors.\n\nBut what if there are issues that don't show up as errors, like a sudden drop in traffic, or a spike?\n\nToday, we're excited to announce Traffic Anomalies notifications, available to enterprise customers. These notifications trigger when Cloudflare detects unexpected changes in traffic, giving another valuable perspective into the health of your systems.\n\nUnexpected changes in traffic could be indicative of many things. If you run an ecommerce site and see a spike in traffic that could be great news — maybe customers are flocking to your sale, or you just had an ad run on a popular TV show. However, it could also mean that something is going wrong: maybe someone accidentally turned off a firewall rule, and now you’re seeing more malicious traffic. Either way, you might want to know that something has changed.\n\nSimilarly, a sudden drop in traffic could mean many things. Perhaps it’s Friday afternoon and all of your employees are signing off and no longer accessing your company website. Or maybe a link to your site is broken, and now potential customers aren’t able to access it. You could be losing potential revenue every minute that traffic is low, so you’d want to know as soon as possible to investigate.\n\n### How can we tell when to alert?\n\nCalculating anomalies in time series datasets is difficult to do well. The simplest way to do it is to use basic thresholds. However, as we’ve [previously blogged about](https://blog.cloudflare.com/smarter-origin-service-level-monitoring/), simple thresholds aren’t very accurate when trying to determine when things are actually going wrong. There are too many edge cases for them to work effectively.\n\nCalculating anomalies in HTTP errors is relatively easy. We know that in general there should be a very low number of errors, so any spike is bad and therefore alertable. That’s why we use [Service Level Objectives (SLOs)](https://sre.google/workbook/alerting-on-slos/) to calculate anomalies for our [HTTP Error Rate notifications](https://developers.cloudflare.com/notifications/notification-available/#traffic-monitoring).\n\nHowever, analyzing overall HTTP traffic behaves more similarly to [Cloudflare Security Events](https://blog.cloudflare.com/introducing-thresholds-in-security-event-alerting-a-z-score-love-story/): there’s some general baseline of events that is computed from historical trends. Any deviation from that baseline is alertable. Because of those similarities, we decided to use the same calculations for Traffic Anomalies notifications as we have previously used for Security Event notifications: [z-scores](https://blog.cloudflare.com/get-notified-when-your-site-is-under-attack/). This involves comparing the current value to the average over a period of time. However, many standard deviations away from the average the current value is, is the z-score.\n\n![](https://blog.cloudflare.com/content/images/2023/10/image4-6.png)\n\n_Plot of HTTP traffic against z-scores. The blue is the HTTP traffic, purple is the positive z-score bound of the traffic, and green is the negative z-score bound of the traffic_\n\nFor Traffic Anomalies notifications, we’re comparing the traffic over the past 5 minutes (the short window) to the average of the traffic over the past 4 hours (the long window). Positive z-scores indicate a spike, and negative z-scores indicate a drop. If the current value is more than 3.5 standard deviations away from the average, we alert. We measure every 5 minutes, so we can alert on any traffic spike or drop in a timely fashion.\n\n![](https://blog.cloudflare.com/content/images/2023/10/image2-9.png)\n\n_Green bucket is the long window and the red bucket is the short window_\n\nWhile our Security Event notifications only trigger when there is a spike in security events (a drop is almost always a good thing), in the case of Traffic Anomalies we send notifications for both spikes _and_ drops. This is because a drop of HTTP traffic is likely indicative of a problem, and a surge could be good or bad.\n\nAs with Security Events, Traffic Anomalies notifications support [minimum thresholds](https://blog.cloudflare.com/introducing-thresholds-in-security-event-alerting-a-z-score-love-story/). This means that, even if we determine that an event is outside 3.5 standard deviations, if the number of events is insignificant, we don’t alert. A spike must be at least 200 requests, and a drop must fall by at least 200 requests. This makes the notifications less noisy, since we aren’t alerting on small spikes and drops.\n\n### Digging a little deeper\n\nCloudflare stores sampled statistics on requests that go through its network [in Clickhouse](https://blog.cloudflare.com/http-analytics-for-6m-requests-per-second-using-clickhouse/). Every minute, we take HTTP traffic from Clickhouse and store it in an instance of VictoriaMetrics, a time-series data storage solution. VictoriaMetrics gives us out-of-the-box algorithmic functions for free, and it has been a good fit for our use case. We chose VictoriaMetrics for a few reasons.\n\nFirstly, it's easy to configure and operate. As a team, we want to optimize for low operational burden and VictoriaMetrics has been great thus far. Secondly, VictoriaMetrics has the ability to scale horizontally, meaning we can run it in a highly available mode. For a system such as this where we want something reliable to compute time sensitive information for our customers, the high availability requirement is essential. Finally, in our tests, we found that VictoriaMetrics used around ⅓ of the memory that Prometheus, a similar alternative product, did for the same use case.\n\nOnce we have data in VictoriaMetrics, we can run queries against it and determine whether we need to alert our customers or not, based on notification configurations they have created ahead of time. To do this we leverage our existing Alert Notification System, [which we blogged about initially in 2019](https://blog.cloudflare.com/new-tools-to-monitor-your-server-and-avoid-downtime/). We know we can count on our current notification system for the last mile to deliver these critical notifications to our customers.\n\n![](https://blog.cloudflare.com/content/images/2023/10/image1-9.png)\n\n_Data flow from HTTP request to notification_\n\n###### _Setting up the Notification_\n\nTo configure this notification, navigate to the “Notifications” tab of the dashboard. Select Traffic Anomalies as your notification type. As with all Cloudflare notifications, you’re able to name and describe your notification, and choose how you want to be notified.\n\n![](https://blog.cloudflare.com/content/images/2023/10/image5-3.png)\n\n_Traffic Anomalies notification in the Dashboard_\n\nYou can choose which domains you want monitored for Traffic Anomalies, whether you want to include traffic that’s already been mitigated by Cloudflare DoS or WAF products, and whether there are specific status codes you want included or excluded. You can also choose whether you want to be alerted on traffic spikes, drops, or both.\n\nWe’re excited to use this system to help serve our Enterprise customers with invaluable notifications regarding the overall health of their systems. Head over to the Notifications tab in the dash to check this new notification out now!\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Product News](https://blog.cloudflare.com/tag/product-news/) [Notifications](https://blog.cloudflare.com/tag/notifications/) [Network Services](https://blog.cloudflare.com/tag/network-services/)"
    },
    {
      "url": "https://blog.cloudflare.com/ddos-threat-report-2023-q3/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/ddos-threat-report-2023-q3/",
        "loadedTime": "2023-12-05T02:28:48.260Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/ddos-threat-report-2023-q3/",
        "title": "DDoS threat report for 2023 Q3",
        "description": "In the past quarter, DDoS attacks surged by 65%. Gaming and Gambling companies were the most attacked and Cloudflare mitigated thousands of hyper-volumetric DDoS attacks. The largest attacks we saw peaked at 201 million rps and 2.6 Tbps.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/26/2023\n15 min read\nThis blog post is also available in 简体中文, 繁體中文, 日本語, 한국어, Deutsch, Italiano, Français, Español, Português and Nederlands.\nWelcome to the third DDoS threat report of 2023. DDoS attacks, or distributed denial-of-service attacks, are a type of cyber attack that aims to disrupt websites (and other types of Internet properties) to make them unavailable for legitimate users by overwhelming them with more traffic than they can handle — similar to a driver stuck in a traffic jam on the way to the grocery store.\nWe see a lot of DDoS attacks of all types and sizes, and our network is one of the largest in the world spanning more than 300 cities in over 100 countries. Through this network we serve over 64 million HTTP requests per second at peak and about 2.3 billion DNS queries every day. On average, we mitigate 140 billion cyber threats each day. This colossal amount of data gives us a unique vantage point to understand the threat landscape and provide the community access to insightful and actionable DDoS trends.\nIn recent weeks, we've also observed a surge in DDoS attacks and other cyber attacks against Israeli newspaper and media websites, as well as financial institutions and government websites. Palestinian websites have also seen a significant increase in DDoS attacks. View the full coverage here.\nHTTP DDoS attacks against Israeli websites using Cloudflare\nThe global DDoS threat landscape\nIn the third quarter of 2023, Cloudflare faced one of the most sophisticated and persistent DDoS attack campaigns in recorded history.\nCloudflare mitigated thousands of hyper-volumetric HTTP DDoS attacks, 89 of which exceeded 100 million requests per second (rps) and with the largest peaking at 201 million rps — a figure three times higher than the previous largest attack on record (71M rps).\nThe campaign contributed to an overall increase of 65% in HTTP DDoS attack traffic in Q3 compared to the previous quarter. Similarly, L3/4 DDoS attacks also increased by 14% alongside numerous attacks in the terabit-per-second range — the largest attack targeted Cloudflare’s free DNS resolver 1.1.1.1 and peaked at 2.6 Tbps.\nGaming and Gambling companies were bombarded with the largest volume of HTTP DDoS attack traffic, overtaking the Cryptocurrency industry from last quarter.\nReminder: an interactive version of this report is also available as a Cloudflare Radar Report. On Radar, you can also dive deeper and explore traffic trends, attacks, outages and many more insights for your specific industry, network and country.\nHTTP DDoS attacks and hyper-volumetric attacks\nAn HTTP DDoS attack is a DDoS attack over the Hypertext Transfer Protocol (HTTP). It targets HTTP Internet properties such as mobile application servers, ecommerce websites, and API gateways.\nIllustration of an HTTP DDoS attack\nHTTP/2, which accounts for 62% of HTTP traffic, is a version of the protocol that’s meant to improve application performance. The downside is that HTTP/2 can also help improve a botnet’s performance.\nDistribution of HTTP versions by Radar\nCampaign of hyper-volumetric DDoS attacks exploiting HTTP/2 Rapid Resets\nStarting in late August 2023, Cloudflare and various other vendors were subject to a sophisticated and persistent DDoS attack campaign that exploited the HTTP/2 Rapid Reset vulnerability (CVE-2023-44487).\nIllustration of an HTTP/2 Rapid Reset DDoS attack\nThe DDoS campaign included thousands of hyper-volumetric DDoS attacks over HTTP/2 that peaked in the range of millions of requests per second. The average attack rate was 30M rps. Approximately 89 of the attacks peaked above 100M rps and the largest one we saw hit 201M rps.\nHTTP/2 Rapid Reset campaign of hyper-volumetric DDoS attacks\nCloudflare’s systems automatically detected and mitigated the vast majority of attacks. We deployed emergency countermeasures and improved our mitigation systems’ efficacy and efficiency to ensure the availability of our network and of our customers’.\nCheck out our engineering blog that dives deep into the land of HTTP/2, what we learned and what actions we took to make the Internet safer.\nHyper-volumetric DDoS attacks enabled by VM-based botnets\nAs we’ve seen in this campaign and previous ones, botnets that leverage cloud computing platforms and exploit HTTP/2 are able to generate up to x5,000 more force per botnet node. This allowed them to launch hyper-volumetric DDoS attacks with a small botnet ranging 5-20 thousand nodes alone. To put that into perspective, in the past, IoT based botnets consisted of fleets of millions of nodes and barely managed to reach a few million requests per second.\nComparison of an Internet of Things (IoT) based botnet and a Virtual Machine (VM) based botnet\nWhen analyzing the two-month-long DDoS campaign, we can see that Cloudflare infrastructure was the main target of the attacks. More specifically, 19% of all attacks targeted Cloudflare websites and infrastructure. Another 18% targeted Gaming companies, and 10% targeted well known VoIP providers.\nTop industries targeted by the HTTP/2 Rapid Reset DDoS attacks\nHTTP DDoS attack traffic increased by 65%\nThe attack campaign contributed to an overall increase in the amount of attack traffic. Last quarter, the volume of HTTP DDoS attacks increased by 15% QoQ. This quarter, it grew even more. Attacks volume increased by 65% QoQ to a total staggering figure of 8.9 trillion HTTP DDoS requests that Cloudflare systems automatically detected and mitigated.\nAggregated volume of HTTP DDoS attack requests by quarter\nAlongside the 65% increase in HTTP DDoS attacks, we also saw a minor increase of 14% in L3/4 DDoS attacks — similar to the figures we saw in the first quarter of this year.\nL3/4 DDoS attack by quarter\nA rise in large volumetric DDoS attacks contributing to this increase. In Q3, our DDoS defenses automatically detected and mitigated numerous DDoS attacks in the terabit-per-second range. The largest attacks we saw peaked at 2.6 Tbps. This attack was part of a broader campaign that targeted Cloudflare’s free DNS resolver 1.1.1.1. It was a UDP flood that was launched by a Mirai-variant botnet.\nTop sources of HTTP DDoS attacks\nWhen comparing the global and country-specific HTTP DDoS attack request volume, we see that the US remains the largest source of HTTP DDoS attacks. One out of every 25 HTTP DDoS requests originated from the US. China remains in second place. Brazil replaced Germany as the third-largest source of HTTP DDoS attacks, as Germany fell to fourth place.\nHTTP DDoS attacks: Top sources compared to all attack traffic\nSome countries naturally receive more traffic due to various factors such as the population and Internet usage, and therefore also receive/generate more attacks. So while it’s interesting to understand the total amount of attack traffic originating from or targeting a given country, it is also helpful to remove that bias by normalizing the attack traffic by all traffic to a given country.\nWhen doing so, we see a different pattern. The US doesn’t even make it into the top ten. Instead, Mozambique is in first place (again). One out of every five HTTP requests that originated from Mozambique was part of an HTTP DDoS attack traffic.\nEgypt remains in second place — approximately 13% of requests originating from Egypt were part of an HTTP DDoS attack. Libya and China follow as the third and fourth-largest source of HTTP DDoS attacks.\nHTTP DDoS attacks: Top sources compared to their own traffic\nTop sources of L3/4 DDoS attacks\nWhen we look at the origins of L3/4 DDoS attacks, we ignore the source IP address because it can be spoofed. Instead, we rely on the location of Cloudflare’s data center where the traffic was ingested. Thanks to our large network and global coverage, we’re able to achieve geographical accuracy to understand where attacks come from.\nIn Q3, approximately 36% of all L3/4 DDoS attack traffic that we saw originated from the US. Far behind, Germany came in second place with 8% and the UK followed in third place with almost 5%.\nL3/4 DDoS attacks: Top sources compared to all attack traffic\nWhen normalizing the data, we see that Vietnam dropped to the second-largest source of L3/4 DDoS attacks after being first for two consecutive quarters. New Caledonia, a French territory comprising dozens of islands in the South Pacific, grabbed the first place. Two out of every four bytes ingested in Cloudflare’s data centers in New Caledonia were attacks.\nL3/4 DDoS attacks: Top sources compared to their own traffic\nTop attacked industries by HTTP DDoS attacks\nIn terms of absolute volume of HTTP DDoS attack traffic, the Gaming and Gambling industry jumps to first place overtaking the Cryptocurrency industry. Over 5% of all HTTP DDoS attack traffic that Cloudflare saw targeted the Gaming and Gambling industry.\nHTTP DDoS attacks: Top attacked industries compared to all attack traffic\nThe Gaming and Gambling industry has long been one of the most attacked industries compared to others. But when we look at the HTTP DDoS attack traffic relative to each specific industry, we see a different picture. The Gaming and Gambling industry has so much user traffic that, despite being the most attacked industry by volume, it doesn’t even make it into the top ten when we put it into the per-industry context.\nInstead, what we see is that the Mining and Metals industry was targeted by the most attacks compared to its total traffic — 17.46% of all traffic to Mining and Metals companies were DDoS attack traffic.\nFollowing closely in second place, 17.41% of all traffic to Non-profits were HTTP DDoS attacks. Many of these attacks are directed at more than 2,400 Non-profit and independent media organizations in 111 countries that Cloudflare protects for free as part of Project Galileo, which celebrated its ninth anniversary this year. Over the past quarter alone, Cloudflare mitigated an average of 180.5 million cyber threats against Galileo-protected websites every day.\nHTTP DDoS attacks: Top attacked industries compared to their own traffic\nPharmaceuticals, Biotechnology and Health companies came in third, and US Federal Government websites in fourth place. Almost one out of every 10 HTTP requests to US Federal Government Internet properties were part of an attack. In fifth place, Cryptocurrency and then Farming and Fishery not far behind.\nTop attacked industries by region\nNow let’s dive deeper to understand which industries were targeted the most in each region.\nHTTP DDoS attacks: Top industries targeted by HTTP DDoS attacks by region\nRegional deepdives\nAfrica\nAfter two consecutive quarters as the most attacked industry, the Telecommunications industry dropped from first place to fourth. Media Production companies were the most attacked industry in Africa. The Banking, Financial Services and Insurance (BFSI) industry follows as the second most attacked. Gaming and Gambling companies in third.\nAsia\nThe Cryptocurrency industry remains the most attacked in APAC for the second consecutive quarter. Gaming and Gambling came in second place. Information Technology and Services companies in third.\nEurope\nFor the fourth consecutive quarter, the Gaming and Gambling industry remains the most attacked industry in Europe. Retail companies came in second, and Computer Software companies in third.\nLatin America\nFarming was the most targeted industry in Latin America in Q3. It accounted for a whopping 53% of all attacks towards Latin America. Far behind, Gaming and Gambling companies were the second most targeted. Civic and Social Organizations were in third.\nMiddle East\nRetail companies were the most targeted in the Middle East in Q3. Computer Software companies came in second and the Gaming and Gambling industry in third.\nNorth America\nAfter two consecutive quarters, the Marketing and Advertising industry dropped from the first place to the second. Computer Software took the lead. In third place, Telecommunications companies.\nOceania\nThe Telecommunications industry was, by far, the most targeted in Oceania in Q3 — over 45% of all attacks to Oceania. Cryptocurrency and Computer Software companies came in second and third places respectively.\nTop attacked industries by L3/4 DDoS attacks\nWhen descending the layers of the OSI model, the Internet networks and services that were most targeted belonged to the Information Technology and Services industry. Almost 35% of all L3/4 DDoS attack traffic (in bytes) targeted the Information Technology and Internet industry.\nFar behind, Telecommunication companies came in second with a mere share of 3%. Gaming and Gambling came in third, Banking, Financial Services and Insurance companies (BFSI) in fourth.\nL3/4 DDoS attacks: Top attacked industries compared to all attack traffic\nWhen comparing the attacks on industries to all traffic for that specific industry, we see that the Music industry jumps to the first place, followed by Computer and Network Security companies, Information Technology and Internet companies and Aviation and Aerospace.\nL3/4 DDoS attacks: Top attacked industries compared to their own traffic\nTop attacked countries by HTTP DDoS attacks\nWhen examining the total volume of attack traffic, the US remains the main target of HTTP DDoS attacks. Almost 5% of all HTTP DDoS attack traffic targeted the US. Singapore came in second and China in third.\nHTTP DDoS attacks: Top attacked countries compared to all traffic\nIf we normalize the data per country and region and divide the attack traffic by the total traffic, we get a different picture. The top three most attacked countries are Island nations.\nAnguilla, a small set of islands east of Puerto Rico, jumps to the first place as the most attacked country. Over 75% of all traffic to Anguilla websites were HTTP DDoS attacks. In second place, American Samoa, a group of islands east of Fiji. In third, the British Virgin Islands.\nIn fourth place, Algeria, and then Kenya, Russia, Vietnam, Singapore, Belize, and Japan.\nHTTP DDoS attacks: Top attacked countries compared to their own traffic\nTop attacked countries by L3/4 DDoS attacks\nFor the second consecutive quarter, Chinese Internet networks and services remain the most targeted by L3/4 DDoS attacks. These China-bound attacks account for 29% of all attacks we saw in Q3.\nFar, far behind, the US came in second place (3.5%) and Taiwan in third place (3%).\nL3/4 DDoS attacks: Top attacked countries compared to all traffic\nWhen normalizing the amount of attack traffic compared to all traffic to a country, China remains in first place and the US disappears from the top ten. Cloudflare saw that 73% of traffic to China Internet networks were attacks. However, the normalized ranking changes from second place on, with the Netherlands receiving the second-highest proportion of attack traffic (representing 35% of the country’s overall traffic), closely followed by Thailand, Taiwan and Brazil.\nL3/4 DDoS attacks: Top attacked countries compared to their own traffic\nTop attack vectors\nThe Domain Name System, or DNS, serves as the phone book of the Internet. DNS helps translate the human-friendly website address (e.g., www.cloudflare.com) to a machine-friendly IP address (e.g., 104.16.124.96). By disrupting DNS servers, attackers impact the machines’ ability to connect to a website, and by doing so making websites unavailable to users.\nFor the second consecutive quarter, DNS-based DDoS attacks were the most common. Almost 47% of all attacks were DNS-based. This represents a 44% increase compared to the previous quarter. SYN floods remain in second place, followed by RST floods, UDP floods, and Mirai attacks.\nTop attack vectors\nEmerging threats - reduced, reused and recycled\nAside from the most common attack vectors, we also saw significant increases in lesser known attack vectors. These tend to be very volatile as threat actors try to “reduce, reuse and recycle” older attack vectors. These tend to be UDP-based protocols that can be exploited to launch amplification and reflection DDoS attacks.\nOne well-known tactic that we continue to see is the use of amplification/reflection attacks. In this attack method, the attacker bounces traffic off of servers, and aims the responses towards their victim. Attackers are able to aim the bounced traffic to their victim by various methods such as IP spoofing.\nAnother form of reflection can be achieved differently in an attack named ‘DNS Laundering attack’. In a DNS Laundering attack, the attacker will query subdomains of a domain that is managed by the victim’s DNS server. The prefix that defines the subdomain is randomized and is never used more than once or twice in such an attack. Due to the randomization element, recursive DNS servers will never have a cached response and will need to forward the query to the victim’s authoritative DNS server. The authoritative DNS server is then bombarded by so many queries until it cannot serve legitimate queries or even crashes all together.\nIllustration of a reflection and amplification attack\nOverall in Q3, Multicast DNS (mDNS) based DDoS attacks was the attack method that increased the most. In second place were attacks that exploit the Constrained Application Protocol (CoAP), and in third, the Encapsulating Security Payload (ESP). Let’s get to know those attack vectors a little better.\nMain emerging threats\nmDNS DDoS attacks increased by 456%\nMulticast DNS (mDNS) is a UDP-based protocol that is used in local networks for service/device discovery. Vulnerable mDNS servers respond to unicast queries originating outside the local network, which are ‘spoofed’ (altered) with the victim's source address. This results in amplification attacks. In Q3, we noticed a large increase of mDNS attacks; a 456% increase compared to the previous quarter.\nCoAP DDoS attacks increased by 387%\nThe Constrained Application Protocol (CoAP) is designed for use in simple electronics and enables communication between devices in a low-power and lightweight manner. However, it can be abused for DDoS attacks via IP spoofing or amplification, as malicious actors exploit its multicast support or leverage poorly configured CoAP devices to generate large amounts of unwanted network traffic. This can lead to service disruption or overloading of the targeted systems, making them unavailable to legitimate users.\nESP DDoS attacks increased by 303%\nThe Encapsulating Security Payload (ESP) protocol is part of IPsec and provides confidentiality, authentication, and integrity to network communications. However, it could potentially be abused in DDoS attacks if malicious actors exploit misconfigured or vulnerable systems to reflect or amplify traffic towards a target, leading to service disruption. Like with other protocols, securing and properly configuring the systems using ESP is crucial to mitigate the risks of DDoS attacks.\nRansom DDoS attacks\nOccasionally, DDoS attacks are carried out to extort ransom payments. We’ve been surveying Cloudflare customers over three years now, and have been tracking the occurrence of Ransom DDoS attack events.\nComparison of Ransomware and Ransom DDoS attacks\nUnlike Ransomware attacks, where victims typically fall prey to downloading a malicious file or clicking on a compromised email link which locks, deletes, or leaks their files until a ransom is paid, Ransom DDoS attacks can be much simpler for threat actors to execute. Ransom DDoS attacks bypass the need for deceptive tactics such as luring victims into opening dubious emails or clicking on fraudulent links, and they don't necessitate a breach into the network or access to corporate resources.\nOver the past quarter, reports of Ransom DDoS attacks continue to decrease. Approximately 8% of respondents reported being threatened or subject to Random DDoS attacks, which continues a decline we've been tracking throughout the year. Hopefully it is because threat actors have realized that organizations will not pay them (which is our recommendation).\nRansom DDoS attacks by quarter\nHowever, keep in mind that this is also very seasonal, and we can expect an increase in ransom DDoS attacks during the months of November and December. If we look at Q4 numbers from the past three years, we can see that Ransom DDoS attacks have been significantly increasing YoY in November. In previous Q4s, it reached a point where one out of every four respondents reported being subject to Ransom DDoS attacks.\nImproving your defenses in the era of hyper-volumetric DDoS attacks\nIn the past quarter, we saw an unprecedented surge in DDoS attack traffic. This surge was largely driven by the hyper-volumetric HTTP/2 DDoS attack campaign.\nCloudflare customers using our HTTP reverse proxy, i.e. our CDN/WAF services, are already protected from these and other HTTP DDoS attacks. Cloudflare customers that are using non-HTTP services and organizations that are not using Cloudflare at all are strongly encouraged to use an automated, always-on HTTP DDoS Protection service for their HTTP applications.\nIt’s important to remember that security is a process, not a single product or flip of a switch. Atop of our automated DDoS protection systems, we offer comprehensive bundled features such as firewall, bot detection, API protection, and caching to bolster your defenses. Our multi-layered approach optimizes your security posture and minimizes potential impact. We’ve also put together a list of recommendations to help you optimize your defenses against DDoS attacks, and you can follow our step-by-step wizards to secure your applications and prevent DDoS attacks.\n...\nReport methodologies\nLearn more about our methodologies and how we generate these insights: https://developers.cloudflare.com/radar/reference/quarterly-ddos-reports\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nDDoS Attacks Cloudflare Radar DDoS Reports Insights",
      "markdown": "10/26/2023\n\n*   [![Omer Yoachimik](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/06/Screenshot-2023-06-02-at-9.38.13-AM.png)](https://blog.cloudflare.com/author/omer/)\n*   [![Jorge Pacheco](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/04/CV_Profile.jpeg)](https://blog.cloudflare.com/author/jorge/)\n\n15 min read\n\nThis blog post is also available in [简体中文](https://blog.cloudflare.com/zh-cn/ddos-threat-report-2023-q3-zh-cn/), [繁體中文](https://blog.cloudflare.com/zh-tw/ddos-threat-report-2023-q3-zh-tw/), [日本語](https://blog.cloudflare.com/ja-jp/ddos-threat-report-2023-q3-ja-jp/), [한국어](https://blog.cloudflare.com/ko-kr/ddos-threat-report-2023-q3-ko-kr/), [Deutsch](https://blog.cloudflare.com/de-de/ddos-threat-report-2023-q3-de-de/), [Italiano](https://blog.cloudflare.com/it-it/ddos-threat-report-2023-q3-it-it/), [Français](https://blog.cloudflare.com/fr-fr/ddos-threat-report-2023-q3-fr-fr/), [Español](https://blog.cloudflare.com/es-es/ddos-threat-report-2023-q3-es-es/), [Português](https://blog.cloudflare.com/pt-br/ddos-threat-report-2023-q3-pt-br/) and [Nederlands](https://blog.cloudflare.com/nl-nl/ddos-threat-report-2023-q3-nl-nl/).\n\n![DDoS threat report for 2023 Q3](https://blog.cloudflare.com/content/images/2023/10/image19.png)\n\nWelcome to the third DDoS threat report of 2023. DDoS attacks, or [distributed denial-of-service attacks](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/), are a type of cyber attack that aims to disrupt websites (and other types of Internet properties) to make them unavailable for legitimate users by overwhelming them with more traffic than they can handle — similar to a driver stuck in a traffic jam on the way to the grocery store.\n\nWe see a lot of DDoS attacks of all types and sizes, and our [network](https://www.cloudflare.com/network/) is one of the largest in the world spanning more than 300 cities in over 100 countries. Through this network we serve over 64 million HTTP requests per second at peak and about 2.3 billion DNS queries every day. On average, we mitigate 140 billion cyber threats each day. This colossal amount of data gives us a unique vantage point to understand the threat landscape and provide the community access to insightful and actionable DDoS trends.\n\nIn recent weeks, we've also observed a surge in DDoS attacks and other cyber attacks against Israeli newspaper and media websites, as well as financial institutions and government websites. Palestinian websites have also seen a significant increase in DDoS attacks. View the full coverage [here](https://blog.cloudflare.com/cyber-attacks-in-the-israel-hamas-war/).\n\n![HTTP DDoS attacks against Israeli websites using Cloudflare](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--7--1.png)\n\nHTTP DDoS attacks against Israeli websites using Cloudflare\n\n## The global DDoS threat landscape\n\nIn the third quarter of 2023, Cloudflare faced one of the most sophisticated and persistent DDoS attack campaigns in recorded history.\n\n1.  Cloudflare mitigated thousands of hyper-volumetric HTTP DDoS attacks, 89 of which exceeded 100 million requests per second (rps) and with the largest peaking at 201 million rps — a figure three times higher than the previous [largest attack on record](https://blog.cloudflare.com/cloudflare-mitigates-record-breaking-71-million-request-per-second-ddos-attack/?) (71M rps).\n2.  The campaign contributed to an overall increase of 65% in HTTP DDoS attack traffic in Q3 compared to the previous quarter. Similarly, L3/4 DDoS attacks also increased by 14% alongside numerous attacks in the terabit-per-second range — the largest attack targeted Cloudflare’s free DNS resolver [1.1.1.1](https://www.cloudflare.com/learning/dns/what-is-1.1.1.1/) and peaked at 2.6 Tbps.\n3.  Gaming and Gambling companies were bombarded with the largest volume of HTTP DDoS attack traffic, overtaking the Cryptocurrency industry from last quarter.\n\n_Reminder: an interactive version of this report is also available as a_ [_Cloudflare Radar Report_](https://radar.cloudflare.com/reports/ddos-2023-q3)_. On_ [_Radar_](https://radar.cloudflare.com/)_, you can also dive deeper and explore traffic trends, attacks, outages and many more insights for your specific industry, network and country._\n\n### HTTP DDoS attacks and hyper-volumetric attacks\n\nAn [HTTP DDoS attack](https://www.cloudflare.com/learning/ddos/http-flood-ddos-attack/) is a DDoS attack over the [Hypertext Transfer Protocol (HTTP)](https://www.cloudflare.com/learning/ddos/glossary/hypertext-transfer-protocol-http/). It targets HTTP Internet properties such as mobile application servers, ecommerce websites, and API gateways.\n\n![Illustration of an HTTP DDoS attack](https://blog.cloudflare.com/content/images/2023/10/Untitled.png)\n\nIllustration of an HTTP DDoS attack\n\n[HTTP/2](https://developers.cloudflare.com/support/network/understanding-cloudflare-http2-and-http3-support/#http2), which accounts for 62% of HTTP traffic, is a version of the protocol that’s meant to improve application performance. The downside is that HTTP/2 can also help _improve_ a botnet’s performance.\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0-1.png)\n\nDistribution of HTTP versions by Radar\n\n### Campaign of hyper-volumetric DDoS attacks exploiting HTTP/2 Rapid Resets\n\nStarting in late August 2023, Cloudflare and various other vendors were subject to a sophisticated and persistent DDoS attack campaign that exploited the [HTTP/2 Rapid Reset](https://blog.cloudflare.com/zero-day-rapid-reset-http2-record-breaking-ddos-attack/) vulnerability ([CVE-2023-44487](https://www.cve.org/CVERecord?id=CVE-2023-44487)).\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--1--1.png)\n\nIllustration of an HTTP/2 Rapid Reset DDoS attack\n\nThe DDoS campaign included thousands of hyper-volumetric DDoS attacks over HTTP/2 that peaked in the range of millions of requests per second. The average attack rate was 30M rps. Approximately 89 of the attacks peaked above 100M rps and the largest one we saw hit 201M rps.\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--2--1.png)\n\nHTTP/2 Rapid Reset campaign of hyper-volumetric DDoS attacks\n\nCloudflare’s systems automatically detected and mitigated the vast majority of attacks. We deployed emergency countermeasures and improved our mitigation systems’ efficacy and efficiency to ensure the availability of our network and of our customers’.\n\nCheck out our engineering [blog](https://blog.cloudflare.com/technical-breakdown-http2-rapid-reset-ddos-attack/) that dives deep into the land of HTTP/2, what we learned and what actions we took to make the Internet safer.\n\n### Hyper-volumetric DDoS attacks enabled by VM-based botnets\n\nAs we’ve seen in this campaign and previous [ones](https://blog.cloudflare.com/ddos-threat-report-2023-q1/), botnets that leverage cloud computing platforms and exploit HTTP/2 are able to generate up to **x5,000** more force per botnet node. This allowed them to launch hyper-volumetric DDoS attacks with a small botnet ranging 5-20 thousand nodes alone. To put that into perspective, in the past, IoT based botnets consisted of fleets of millions of nodes and barely managed to reach a few million requests per second.\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--3--1.png)\n\nComparison of an Internet of Things (IoT) based botnet and a Virtual Machine (VM) based botnet\n\nWhen analyzing the two-month-long DDoS campaign, we can see that Cloudflare infrastructure was the main target of the attacks. More specifically, 19% of all attacks targeted Cloudflare websites and infrastructure. Another 18% targeted Gaming companies, and 10% targeted well known VoIP providers.\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--4--1.png)\n\nTop industries targeted by the HTTP/2 Rapid Reset DDoS attacks\n\n### HTTP DDoS attack traffic increased by 65%\n\nThe attack campaign contributed to an overall increase in the amount of attack traffic. Last quarter, the volume of HTTP DDoS attacks increased by 15% QoQ. This quarter, it grew even more. Attacks volume increased by 65% QoQ to a total staggering figure of 8.9 trillion HTTP DDoS requests that Cloudflare systems automatically detected and mitigated.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Untitled9.png)\n\nAggregated volume of HTTP DDoS attack requests by quarter\n\nAlongside the 65% increase in HTTP DDoS attacks, we also saw a minor increase of 14% in L3/4 DDoS attacks — similar to the figures we saw in the first quarter of this year.\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--5--1.png)\n\nL3/4 DDoS attack by quarter\n\nA rise in large volumetric DDoS attacks contributing to this increase. In Q3, our DDoS defenses automatically detected and mitigated numerous DDoS attacks in the terabit-per-second range. The largest attacks we saw peaked at 2.6 Tbps. This attack was part of a broader campaign that targeted Cloudflare’s free DNS resolver [1.1.1.1](https://www.cloudflare.com/learning/dns/what-is-1.1.1.1/). It was a [UDP flood](https://www.cloudflare.com/learning/ddos/udp-flood-ddos-attack/) that was launched by a [Mirai-variant botnet](https://www.cloudflare.com/learning/ddos/glossary/mirai-botnet/).\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-007.png)\n\n## Top sources of HTTP DDoS attacks\n\nWhen comparing the global and country-specific HTTP DDoS attack request volume, we see that the US remains the largest source of HTTP DDoS attacks. One out of every 25 HTTP DDoS requests originated from the US. China remains in second place. Brazil replaced Germany as the third-largest source of HTTP DDoS attacks, as Germany fell to fourth place.\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--6-.png)\n\nHTTP DDoS attacks: Top sources compared to all attack traffic\n\nSome countries naturally receive more traffic due to various factors such as the population and Internet usage, and therefore also receive/generate more attacks. So while it’s interesting to understand the total amount of attack traffic originating from or targeting a given country, it is also helpful to remove that bias by normalizing the attack traffic by all traffic to a given country.\n\nWhen doing so, we see a different pattern. The US doesn’t even make it into the top ten. Instead, Mozambique is in first place (again). One out of every five HTTP requests that originated from Mozambique was part of an HTTP DDoS attack traffic.\n\nEgypt remains in second place — approximately 13% of requests originating from Egypt were part of an HTTP DDoS attack. Libya and China follow as the third and fourth-largest source of HTTP DDoS attacks.\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--8-.png)\n\nHTTP DDoS attacks: Top sources compared to their own traffic\n\n## Top sources of L3/4 DDoS attacks\n\nWhen we look at the origins of L3/4 DDoS attacks, we ignore the source IP address because it can be [spoofed](https://www.cloudflare.com/learning/ddos/glossary/ip-spoofing/). Instead, we rely on the location of Cloudflare’s data center where the traffic was ingested. Thanks to our large network and global coverage, we’re able to achieve geographical accuracy to understand where attacks come from.\n\nIn Q3, approximately 36% of all [L3/4 DDoS attack](https://www.cloudflare.com/learning/ddos/layer-3-ddos-attacks/) traffic that we saw originated from the US. Far behind, Germany came in second place with 8% and the UK followed in third place with almost 5%.\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--9-.png)\n\nL3/4 DDoS attacks: Top sources compared to all attack traffic\n\nWhen normalizing the data, we see that Vietnam dropped to the second-largest source of L3/4 DDoS attacks after being first for two consecutive quarters. New Caledonia, a French territory comprising dozens of islands in the South Pacific, grabbed the first place. Two out of every four bytes ingested in Cloudflare’s data centers in New Caledonia were attacks.\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--10-.png)\n\nL3/4 DDoS attacks: Top sources compared to their own traffic\n\n## Top attacked industries by HTTP DDoS attacks\n\nIn terms of absolute volume of HTTP DDoS attack traffic, the Gaming and Gambling industry jumps to first place overtaking the Cryptocurrency industry. Over 5% of all HTTP DDoS attack traffic that Cloudflare saw targeted the Gaming and Gambling industry.\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--11--2.png)\n\nHTTP DDoS attacks: Top attacked industries compared to all attack traffic\n\nThe Gaming and Gambling industry has long been one of the most attacked industries compared to others. But when we look at the HTTP DDoS attack traffic relative to each specific industry, we see a different picture. The Gaming and Gambling industry has so much user traffic that, despite being the most attacked industry _by volume_, it doesn’t even make it into the top ten when we put it into the per-industry context.\n\nInstead, what we see is that the Mining and Metals industry was targeted by the most attacks compared to its total traffic — 17.46% of all traffic to Mining and Metals companies were DDoS attack traffic.\n\nFollowing closely in second place, 17.41% of all traffic to Non-profits were HTTP DDoS attacks. Many of these attacks are directed at more than 2,400 Non-profit and independent media organizations in 111 countries that Cloudflare protects for free as part of Project Galileo, which celebrated its [ninth anniversary](https://blog.cloudflare.com/nine-years-of-project-galileo-and-how-the-last-year-has-changed-it/) this year. Over the past quarter alone, Cloudflare mitigated an average of 180.5 million cyber threats against Galileo-protected websites every day.\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--12--2.png)\n\nHTTP DDoS attacks: Top attacked industries compared to their own traffic\n\nPharmaceuticals, Biotechnology and Health companies came in third, and US Federal Government websites in fourth place. Almost one out of every 10 HTTP requests to US Federal Government Internet properties were part of an attack. In fifth place, Cryptocurrency and then Farming and Fishery not far behind.\n\n### Top attacked industries by region\n\nNow let’s dive deeper to understand which industries were targeted the most in each region.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Top-Attacked-Industry-by-Region-Q3-2023.png)\n\nHTTP DDoS attacks: Top industries targeted by HTTP DDoS attacks by region\n\n## Regional deepdives\n\n### Africa\n\nAfter two consecutive quarters as the most attacked industry, the Telecommunications industry dropped from first place to fourth. Media Production companies were the most attacked industry in Africa. The Banking, Financial Services and Insurance (BFSI) industry follows as the second most attacked. Gaming and Gambling companies in third.\n\n### Asia\n\nThe Cryptocurrency industry remains the most attacked in APAC for the second consecutive quarter. Gaming and Gambling came in second place. Information Technology and Services companies in third.\n\n### Europe\n\nFor the fourth consecutive quarter, the Gaming and Gambling industry remains the most attacked industry in Europe. Retail companies came in second, and Computer Software companies in third.\n\n### Latin America\n\nFarming was the most targeted industry in Latin America in Q3. It accounted for a whopping 53% of all attacks towards Latin America. Far behind, Gaming and Gambling companies were the second most targeted. Civic and Social Organizations were in third.\n\n### Middle East\n\nRetail companies were the most targeted in the Middle East in Q3. Computer Software companies came in second and the Gaming and Gambling industry in third.\n\n### North America\n\nAfter two consecutive quarters, the Marketing and Advertising industry dropped from the first place to the second. Computer Software took the lead. In third place, Telecommunications companies.\n\n### Oceania\n\nThe Telecommunications industry was, by far, the most targeted in Oceania in Q3 — over 45% of all attacks to Oceania. Cryptocurrency and Computer Software companies came in second and third places respectively.\n\n## Top attacked industries by L3/4 DDoS attacks\n\nWhen descending the layers of the [OSI model](https://www.cloudflare.com/learning/ddos/glossary/open-systems-interconnection-model-osi/), the Internet networks and services that were most targeted belonged to the Information Technology and Services industry. Almost 35% of all L3/4 DDoS attack traffic (in bytes) targeted the Information Technology and Internet industry.\n\nFar behind, Telecommunication companies came in second with a mere share of 3%. Gaming and Gambling came in third, Banking, Financial Services and Insurance companies (BFSI) in fourth.\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--13--1.png)\n\nL3/4 DDoS attacks: Top attacked industries compared to all attack traffic\n\nWhen comparing the attacks on industries to all traffic for that specific industry, we see that the Music industry jumps to the first place, followed by Computer and Network Security companies, Information Technology and Internet companies and Aviation and Aerospace.\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--14--1.png)\n\nL3/4 DDoS attacks: Top attacked industries compared to their own traffic\n\n## Top attacked countries by HTTP DDoS attacks\n\nWhen examining the total volume of attack traffic, the US remains the main target of HTTP DDoS attacks. Almost 5% of all HTTP DDoS attack traffic targeted the US. Singapore came in second and China in third.\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--15--2.png)\n\nHTTP DDoS attacks: Top attacked countries compared to all traffic\n\nIf we normalize the data per country and region and divide the attack traffic by the total traffic, we get a different picture. The top three most attacked countries are Island nations.\n\nAnguilla, a small set of islands east of Puerto Rico, jumps to the first place as the most attacked country. Over 75% of all traffic to Anguilla websites were HTTP DDoS attacks. In second place, American Samoa, a group of islands east of Fiji. In third, the British Virgin Islands.\n\nIn fourth place, Algeria, and then Kenya, Russia, Vietnam, Singapore, Belize, and Japan.\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--16-.png)\n\nHTTP DDoS attacks: Top attacked countries compared to their own traffic\n\n### Top attacked countries by L3/4 DDoS attacks\n\nFor the second consecutive quarter, Chinese Internet networks and services remain the most targeted by L3/4 DDoS attacks. These China-bound attacks account for 29% of all attacks we saw in Q3.\n\nFar, far behind, the US came in second place (3.5%) and Taiwan in third place (3%).\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--17-.png)\n\nL3/4 DDoS attacks: Top attacked countries compared to all traffic\n\nWhen normalizing the amount of attack traffic compared to all traffic to a country, China remains in first place and the US disappears from the top ten. Cloudflare saw that 73% of traffic to China Internet networks were attacks. However, the normalized ranking changes from second place on, with the Netherlands receiving the second-highest proportion of attack traffic (representing 35% of the country’s overall traffic), closely followed by Thailand, Taiwan and Brazil.\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--18-.png)\n\nL3/4 DDoS attacks: Top attacked countries compared to their own traffic\n\n## Top attack vectors\n\nThe Domain Name System, or [DNS](https://www.cloudflare.com/learning/dns/what-is-dns/), serves as the phone book of the Internet. DNS helps translate the human-friendly website address (e.g., [www.cloudflare.com](https://www.cloudflare.com/)) to a machine-friendly IP address (e.g., 104.16.124.96). By disrupting DNS servers, attackers impact the machines’ ability to connect to a website, and by doing so making websites unavailable to users.\n\nFor the second consecutive quarter, [DNS-based DDoS attacks](https://www.cloudflare.com/learning/ddos/dns-flood-ddos-attack/) were the most common. Almost 47% of all attacks were DNS-based. This represents a 44% increase compared to the previous quarter. [SYN floods](https://www.cloudflare.com/learning/ddos/syn-flood-ddos-attack/) remain in second place, followed by RST floods, [UDP floods](https://www.cloudflare.com/learning/ddos/udp-flood-ddos-attack/), and [Mirai attacks](https://www.cloudflare.com/learning/ddos/glossary/mirai-botnet/).\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--19-.png)\n\nTop attack vectors\n\n### Emerging threats - _reduced, reused and recycled_\n\nAside from the most common attack vectors, we also saw significant increases in lesser known attack vectors. These tend to be very volatile as threat actors try to _“reduce, reuse and recycle”_ older attack vectors. These tend to be UDP-based protocols that can be exploited to launch amplification and reflection DDoS attacks.\n\nOne well-known tactic that we continue to see is the use of amplification/reflection attacks. In this attack method, the attacker bounces traffic off of servers, and aims the responses towards their victim. Attackers are able to aim the bounced traffic to their victim by various methods such as [IP spoofing](https://www.cloudflare.com/learning/ddos/glossary/ip-spoofing/).\n\nAnother form of reflection can be achieved differently in an attack named ‘DNS Laundering attack’. In a DNS Laundering attack, the attacker will query subdomains of a domain that is managed by the victim’s DNS server. The prefix that defines the subdomain is randomized and is never used more than once or twice in such an attack. Due to the randomization element, recursive DNS servers will never have a cached response and will need to forward the query to the victim’s authoritative DNS server. The authoritative DNS server is then bombarded by so many queries until it cannot serve legitimate queries or even crashes all together.\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--20-.png)\n\nIllustration of a reflection and amplification attack\n\nOverall in Q3, Multicast DNS (mDNS) based DDoS attacks was the attack method that increased the most. In second place were attacks that exploit the Constrained Application Protocol (CoAP), and in third, the Encapsulating Security Payload (ESP). Let’s get to know those attack vectors a little better.\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--21-.png)\n\nMain emerging threats\n\n### mDNS DDoS attacks increased by 456%\n\nMulticast DNS (mDNS) is a UDP-based protocol that is used in local networks for service/device discovery. Vulnerable mDNS servers respond to unicast queries originating outside the local network, which are ‘spoofed’ (altered) with the victim's source address. This results in amplification attacks. In Q3, we noticed a large increase of mDNS attacks; a 456% increase compared to the previous quarter.\n\n### CoAP DDoS attacks increased by 387%\n\nThe Constrained Application Protocol (CoAP) is designed for use in simple electronics and enables communication between devices in a low-power and lightweight manner. However, it can be abused for DDoS attacks via [IP spoofing](https://www.cloudflare.com/learning/ddos/glossary/ip-spoofing/) or amplification, as malicious actors exploit its multicast support or leverage poorly configured CoAP devices to generate large amounts of unwanted network traffic. This can lead to service disruption or overloading of the targeted systems, making them unavailable to legitimate users.\n\n### ESP DDoS attacks increased by 303%\n\nThe Encapsulating Security Payload ([ESP](https://www.cloudflare.com/learning/network-layer/what-is-ipsec/#:~:text=Encapsulating%20Security%20Protocol%20(ESP))) protocol is part of [IPsec](https://www.cloudflare.com/learning/network-layer/what-is-ipsec/) and provides confidentiality, authentication, and integrity to network communications. However, it could potentially be abused in DDoS attacks if malicious actors exploit misconfigured or vulnerable systems to reflect or amplify traffic towards a target, leading to service disruption. Like with other protocols, securing and properly configuring the systems using ESP is crucial to mitigate the risks of DDoS attacks.\n\n## Ransom DDoS attacks\n\nOccasionally, DDoS attacks are carried out to extort ransom payments. We’ve been surveying Cloudflare customers over three years now, and have been tracking the occurrence of [Ransom DDoS attack](https://www.cloudflare.com/learning/ddos/ransom-ddos-attack/) events.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Untitled--1--1.png)\n\nComparison of Ransomware and Ransom DDoS attacks\n\nUnlike [Ransomware](https://www.cloudflare.com/learning/security/ransomware/what-is-ransomware/) attacks, where victims typically fall prey to downloading a malicious file or clicking on a compromised email link which locks, deletes, or leaks their files until a ransom is paid, [Ransom DDoS attacks](https://www.cloudflare.com/learning/ddos/ransom-ddos-attack/) can be much simpler for threat actors to execute. Ransom DDoS attacks bypass the need for deceptive tactics such as luring victims into opening dubious emails or clicking on fraudulent links, and they don't necessitate a breach into the network or access to corporate resources.\n\nOver the past quarter, reports of Ransom DDoS attacks continue to decrease. Approximately 8% of respondents reported being threatened or subject to Random DDoS attacks, which continues a decline we've been tracking throughout the year. Hopefully it is because threat actors have realized that organizations will not pay them (which is our [recommendation](https://www.cloudflare.com/ransom-ddos/)).\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--22-.png)\n\nRansom DDoS attacks by quarter\n\nHowever, keep in mind that this is also very seasonal, and we can expect an increase in ransom DDoS attacks during the months of November and December. If we look at Q4 numbers from the past three years, we can see that Ransom DDoS attacks have been significantly increasing YoY in November. In previous Q4s, it reached a point where one out of every four respondents reported being subject to Ransom DDoS attacks.\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--23-.png)\n\n## Improving your defenses in the era of hyper-volumetric DDoS attacks\n\nIn the past quarter, we saw an unprecedented surge in DDoS attack traffic. This surge was largely driven by the hyper-volumetric HTTP/2 DDoS attack campaign.\n\nCloudflare customers using our HTTP reverse proxy, i.e. our CDN/WAF services, are [already protected](https://www.cloudflare.com/h2/) from these and other HTTP DDoS attacks. Cloudflare customers that are using non-HTTP services and organizations that are not using Cloudflare at all are strongly encouraged to use an automated, always-on HTTP DDoS Protection service for their HTTP applications.\n\nIt’s important to remember that security is a process, not a single product or flip of a switch. Atop of our automated DDoS protection systems, we offer comprehensive bundled features such as [firewall](https://developers.cloudflare.com/firewall/cf-firewall-rules/), [bot detection](https://developers.cloudflare.com/bots/), [API protection](https://developers.cloudflare.com/api-shield/), and [caching](https://developers.cloudflare.com/cache/) to bolster your defenses. Our multi-layered approach optimizes your security posture and minimizes potential impact. We’ve also put together a [list of recommendations](https://developers.cloudflare.com/ddos-protection/best-practices/respond-to-ddos-attacks/) to help you optimize your defenses against DDoS attacks, and you can follow our step-by-step wizards to [secure your applications](https://developers.cloudflare.com/learning-paths/application-security/) and [prevent DDoS attacks](https://developers.cloudflare.com/learning-paths/prevent-ddos-attacks/).\n\n...  \n**Report methodologies**  \nLearn more about our methodologies and how we generate these insights: [https://developers.cloudflare.com/radar/reference/quarterly-ddos-reports](https://developers.cloudflare.com/radar/reference/quarterly-ddos-reports)\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[DDoS](https://blog.cloudflare.com/tag/ddos/) [Attacks](https://blog.cloudflare.com/tag/attacks/) [Cloudflare Radar](https://blog.cloudflare.com/tag/cloudflare-radar/) [DDoS Reports](https://blog.cloudflare.com/tag/ddos-reports/) [Insights](https://blog.cloudflare.com/tag/insights/)"
    },
    {
      "url": "https://blog.cloudflare.com/email-routing-subdomains/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/email-routing-subdomains/",
        "loadedTime": "2023-12-05T02:28:48.782Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/email-routing-subdomains/",
        "title": "Email Routing subdomain support, new APIs and security protocols",
        "description": "It's been two years since we announced Email Routing, our solution to create custom email addresses for your domains and route incoming emails to your preferred mailbox. Since then, the team has worked hard to evolve the product and add more powerful features to meet our users' expectations.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/26/2023\n11 min read\nIt's been two years since we announced Email Routing, our solution to create custom email addresses for your domains and route incoming emails to your preferred mailbox. Since then, the team has worked hard to evolve the product and add more powerful features to meet our users' expectations. Examples include Route to Workers, which allows you to process your Emails programmatically using Workers scripts, Public APIs, Audit Logs, or DMARC Management. \nWe also made significant progress in supporting more email security extensions and protocols, protecting our customers from unwanted traffic, and keeping our IP space reputation for email egress impeccable to maximize our deliverability rates to whatever inbox upstream provider you chose.\nSince leaving beta, Email Routing has grown into one of our most popular products; it’s used by more than one million different customer zones globally, and we forward around 20 million messages daily to every major email platform out there. Our product is mature, robust enough for general usage, and suitable for any production environment. And it keeps evolving: today, we announce three new features that will help make Email Routing more secure, flexible, and powerful than ever.\nNew security protocols\nThe SMTP email protocol has been around since the early 80s. Naturally, it wasn't designed with the best security practices and requirements in mind, at least not the ones that the Internet expects today. For that reason, several protocol revisions and extensions have been standardized and adopted by the community over the years. Cloudflare is known for being an early adopter of promising emerging technologies; Email Routing already supports things like SPF, DKIM signatures, DMARC policy enforcement, TLS transport, STARTTLS, and IPv6 egress, to name a few. Today, we are introducing support for two new standards to help increase email security and improve deliverability to third-party upstream email providers.\nARC\nAuthenticated Received Chain (ARC) is an email authentication system designed to allow an intermediate email server (such as Email Routing) to preserve email authentication results. In other words, with ARC, we can securely preserve the results of validating sender authentication mechanisms like SPF and DKIM, which we support when the email is received, and transport that information to the upstream provider when we forward the message. ARC establishes a chain of trust with all the hops the message has passed through. So, if it was tampered with or changed in one of the hops, it is possible to see where by following that chain.\nWe began rolling out ARC support to Email Routing a few weeks ago. Here’s how it works:\nAs you can see, joe@example.com sends an Email to henry@domain.example, an Email Routing address, which in turn is forwarded to the final address, example@gmail.com.\nEmail Routing will use @example.com’s DMARC policy to check the SPF and DKIM alignments (SPF, DKIM, and DMARC help authenticate email senders by verifying that the emails came from the domain that they claim to be from.) It then stores this authentication result by adding a Arc-Authentication-Results header in the message:\nARC-Authentication-Results: i=1; mx.cloudflare.net; dkim=pass header.d=cloudflare.com header.s=example09082023 header.b=IRdayjbb; dmarc=pass header.from=example.com policy.dmarc=reject; spf=none (mx.cloudflare.net: no SPF records found for postmaster@example.com) smtp.helo=smtp.example.com; spf=pass (mx.cloudflare.net: domain of joe@example.com designates 2a00:1440:4824:20::32e as permitted sender) smtp.mailfrom=joe@example.com; arc=none smtp.remote-ip=2a00:1440:4824:20::32e \nThen we take a snapshot of all the headers and the body of the original message, and we generate an Arc-Message-Signature header with a DKIM-like cryptographic signature (in fact ARC uses the same DKIM keys):\nARC-Message-Signature: i=1; a=rsa-sha256; s=2022; d=email.cloudflare.net; c=relaxed/relaxed; h=To:Date:Subject:From:reply-to:cc:resent-date:resent-from:resent-to :resent-cc:in-reply-to:references:list-id:list-help:list-unsubscribe :list-subscribe:list-post:list-owner:list-archive; t=1697709687; bh=sN/+...aNbf==; \nFinally, before forwarding the message to example@gmail.com, Email Routing generates the Arc-Seal header, another DKIM-like signature, composed out of the Arc-Authentication-Results and Arc-Message-Signature, and cryptographically “seals” the message:\nARC-Seal: i=1; a=rsa-sha256; s=2022; d=email.cloudflare.net; cv=none; b=Lx35lY6..t4g==; \nWhen Gmail receives the message from Email Routing, it not only normally authenticates the last hop domain.example domain (Email Routing uses SRS), but it also checks the ARC seal header, which provides the authentication results of the original sender.\nARC increases the traceability of the message path through email intermediaries, allowing for more informed delivery decisions by those who receive emails as well as higher deliverability rates for those who transport them, like Email Routing. It has been adopted by all the major email providers like Gmail and Microsoft. You can read more about the ARC protocol in the RFC8617.\nMTA-STS\nAs we said earlier, SMTP is an old protocol. Initially Email communications were done in the clear, in plain-text and unencrypted. At some point in time in the late 90s, the email providers community standardized STARTTLS, also known as Opportunistic TLS. The STARTTLS extension allowed a client in a SMTP session to upgrade to TLS encrypted communications.\nWhile at the time this seemed like a step forward in the right direction, we later found out that because STARTTLS can start with an unencrypted plain-text connection, and that can be hijacked, the protocol is susceptible to man-in-the-middle attacks.\nA few years ago MTA Strict Transport Security (MTA-STS) was introduced by email service providers including Microsoft, Google and Yahoo as a solution to protect against downgrade and man-in-the-middle attacks in SMTP sessions, as well as solving the lack of security-first communication standards in email. \nSuppose that example.com uses Email Routing. Here’s how you can enable MTA-STS for it.\nFirst, log in to the Cloudflare dashboard and select your account and zone. Then go to DNS > Records and create a new CNAME record with the name “_mta-sts” that points to Cloudflare’s record “_mta-sts.mx.cloudflare.net”. Make sure to disable the proxy mode.\nConfirm that the record was created:\n$ dig txt _mta-sts.example.com _mta-sts.example.com. 300 IN CNAME _mta-sts.mx.cloudflare.net. _mta-sts.mx.cloudflare.net. 300 IN TXT \"v=STSv1; id=20230615T153000;\" \nThis tells the other end client that is trying to connect to us that we support MTA-STS.\nNext you need an HTTPS endpoint at mta-sts.example.com to serve your policy file. This file defines the mail servers in the domain that use MTA-STS. The reason why HTTPS is used here instead of DNS is because not everyone uses DNSSEC yet, so we want to avoid another MITM attack vector.\nTo do this you need to deploy a very simple Worker that allows Email clients to pull Cloudflare’s Email Routing policy file using the “well-known” URI convention. Go to your Account > Workers & Pages and press Create Application. Pick the “MTA-STS” template from the list.\nThis Worker simply proxies https://mta-sts.mx.cloudflare.net/.well-known/mta-sts.txt to your own domain. After deploying it, go to the Worker configuration, then Triggers > Custom Domains and Add Custom Domain.\nYou can then confirm that your policy file is working:\n$ curl https://mta-sts.example.com/.well-known/mta-sts.txt version: STSv1 mode: enforce mx: *.mx.cloudflare.net max_age: 86400 \nThis says that we enforce MTA-STS. Capable email clients will only deliver email to this domain over a secure connection to the specified MX servers. If no secure connection can be established the email will not be delivered. \nEmail Routing also supports MTA-STS upstream, which greatly improves security when forwarding your Emails to service providers like Gmail or Microsoft, and others.\nWhile enabling MTA-STS involves a few steps today, we plan to simplify things for you and automatically configure MTA-STS for your domains from the Email Routing dashboard as a future improvement.\nSending emails and replies from Workers\nLast year we announced Email Workers, allowing anyone using Email Routing to associate a Worker script to an Email address rule, and programmatically process their incoming emails in any way they want. Workers is our serverless compute platform, it provides hundreds of features and APIs, like databases and storage. Email Workers opened doors to a flood of use-cases and applications that weren’t possible before like implementing allow/block lists, advanced rules, notifications to messaging applications, honeypot aggregators and more.\nStill, you could only act on the incoming email event. You could read and process the email message, you could even manipulate and create some headers, but you couldn’t rewrite the body of the message or create new emails from scratch.\nToday we’re announcing two new powerful Email Workers APIs that will further enhance what you can do with Email Routing and Workers.\nSend emails from Workers\nNow you can send an email from any Worker, from scratch, whenever you want, not just when you receive incoming messages, to any email address verified on Email Routing under your account. Here are a few practical examples where sending email from Workers to your verified addresses can be helpful:\nDaily digests with the news from your favorite publications.\nAlert messages whenever the weather conditions are adverse.\nAutomatic notifications when systems complete tasks.\nReceive a message composed of the inputs of a form online on a contact page.\nLet's see a simple example of a Worker sending an email. First you need to create “send_email” bindings in your wrangler.toml configuration:\nsend_email = [ {type = \"send_email\", name = \"EMAIL_OUT\"} ] \nAnd then creating a new message and sending it in a Workers is as simple as:\nimport { EmailMessage } from \"cloudflare:email\"; import { createMimeMessage } from \"mimetext\"; export default { async fetch(request, env) { const msg = createMimeMessage(); msg.setSender({ name: \"Workers AI story\", addr: \"joe@example.com\" }); msg.setRecipient(\"mary@domain.example\"); msg.setSubject(\"An email generated in a worker\"); msg.addMessage({ contentType: 'text/plain', data: `Congratulations, you just sent an email from a worker.` }); var message = new EmailMessage( \"joe@example.com\", \"mary@domain.example\", msg.asRaw() ); try { await env.EMAIL_OUT.send(message); } catch (e) { return new Response(e.message); } return new Response(\"email sent!\"); }, }; \nThis example makes use of mimetext, an open-source raw email message generator.\nAgain, for security reasons, you can only send emails to the addresses for which you confirmed ownership in Email Routing under your Cloudflare account. If you’re looking for sending email campaigns or newsletters to destination addresses that you do not control or larger subscription groups, you should consider other options like our MailChannels integration.\nSince sending Emails from Workers is not tied to the EmailEvent, you can send them from any type of Worker, including Cron Triggers and Durable Objects, whenever you want, you control all the logic.\nReply to emails\nOne of our most-requested features has been to provide a way to programmatically respond to incoming emails. It has been possible to do this with Email Workers in a very limited capacity by returning a permanent SMTP error message — but this may or may not be visible to the end user depending on the client implementation. \nexport default { async email(message, env, ctx) { message.setReject(\"Address not allowed\"); } } \nAs of today, you can now truly reply to incoming emails with another new message and implement smart auto-responders programmatically, adding any content and context in the main body of the message. Think of a customer support email automatically generating a ticket and returning the link to the sender, an out-of-office reply with instructions when you're on vacation, or a detailed explanation of why you rejected an email. Here’s a code example:\nimport { EmailMessage } from \"cloudflare:email\"; import { createMimeMessage } from \"mimetext\"; export default { async email(message, env, ctx) { const ticket = createTicket(message); const msg = createMimeMessage(); msg.setHeader(\"In-Reply-To\", message.headers.get(\"Message-ID\")); msg.setSender({ name: \"Thank you for you contact\", addr: \"joe@example.com\" }); msg.setRecipient(\"mary@domain.example\"); msg.setSubject(\"An email generated in a worker\"); msg.addMessage({ contentType: 'text/plain', data: `We got your message, your ticket number is ${ ticket }` }); const replyMessage = new EmailMessage( \"joe@example.com\", message.from, msg.asRaw() ); await message.reply(replyMessage); } }\nTo mitigate security risks and abuse, replying to incoming emails has a few requirements:\nThe incoming email has to have valid DMARC.\nThe email can only be replied to once.\nThe In-Reply-To header of the reply message must match the Message-ID of the incoming message.\nThe recipient of the reply must match the incoming sender.\nThe outgoing sender domain must match the same domain that received the email.\nIf these and other internal conditions are not met, then reply() will fail with an exception, otherwise you can freely compose your reply message and send it back to the original sender.\nFor more information the documentation to these APIs is available in our Developer Docs.\nSubdomains support\nThis is a big one.\nEmail Routing is a zone-level feature. A zone has a top-level domain (the same as the zone name) and it can have subdomains (managed under the DNS feature.) As an example, I can have the example.com zone, and then the mail.example.com and corp.example.com subdomains under it. However, we can only use Email Routing with the top-level domain of the zone, example.com in this example. While this is fine for the vast majority of use cases, some customers — particularly bigger organizations with complex email requirements — have asked for more flexibility. \nThis changes today. Now you can use Email Routing with any subdomain of any zone in your account. To make this possible we redesigned the dashboard UI experience to make it easier to get you started and manage all your Email Routing domains and subdomains, rules and destination addresses in one single place. Let’s see how it works.\nTo add Email Routing features to a new subdomain, log in to the Cloudflare dashboard and select your account and zone. Then go to Email > Email Routing > Settings and click “Add subdomain”.\nOnce the subdomain is added and the DNS records are configured, you can see it in the Settings list under the Subdomains section:\nNow you can go to Email > Email Routing > Routing rules and create new custom addresses that will show you the option of using either the top domain of the zone or any other configured subdomain.\nAfter the new custom address for the subdomain is created you can see it in the list with all the other addresses, and manage it from there.\nIt’s this easy.\nFinal words\nWe hope you enjoy the new features that we are announcing today. Still, we want to be clear: there are no changes in pricing, and Email Routing is still free for Cloudflare customers.\nEver since Email Routing was launched, we’ve been listening to customers’ feedback and trying to adjust our roadmap to both our requirements and their own ideas and requests. Email shouldn't be difficult; our goal is to listen, learn and keep improving the service with better, more powerful features.\nYou can find detailed information about the new features and more in our Email Routing Developer Docs.\nIf you have any questions or feedback about Email Routing, please come see us in the Cloudflare Community and the Cloudflare Discord.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nEmail Routing Subdomains Email Workers Cloudflare Workers Developer Platform",
      "markdown": "10/26/2023\n\n*   [![Celso Martinho](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/Celso-Martinho.png)](https://blog.cloudflare.com/author/celso/)\n*   [![André Cruz](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/02/andre2.jpg)](https://blog.cloudflare.com/author/andre-cruz/)\n*   [![Nelson Duarte](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/10/me_1x1.jpg)](https://blog.cloudflare.com/author/nelson-duarte/)\n\n11 min read\n\n![Email Routing subdomain support, new APIs and security protocols](https://blog.cloudflare.com/content/images/2023/10/Email-Routing.png)\n\nIt's been two years since we announced Email Routing, our solution to create custom email addresses for your domains and route incoming emails to your preferred mailbox. Since then, the team has worked hard to evolve the product and add more powerful features to meet our users' expectations. Examples include [Route to Workers](https://blog.cloudflare.com/announcing-route-to-workers/), which allows you to [process your Emails programmatically](https://developers.cloudflare.com/email-routing/email-workers/) using Workers scripts, [Public APIs](https://blog.cloudflare.com/email-routing-leaves-beta/), Audit Logs, or [DMARC Management](https://blog.cloudflare.com/dmarc-management/).\n\nWe also made significant progress in supporting more email security extensions and protocols, protecting our customers from unwanted traffic, and keeping our IP space reputation for email egress impeccable to maximize our deliverability rates to whatever inbox upstream provider you chose.\n\nSince [leaving beta](https://blog.cloudflare.com/email-routing-leaves-beta/), Email Routing has grown into one of our most popular products; it’s used by more than one million different customer zones globally, and we forward around 20 million messages daily to every major email platform out there. Our product is mature, robust enough for general usage, and suitable for any production environment. And it keeps evolving: today, we announce three new features that will help make Email Routing more secure, flexible, and powerful than ever.\n\n## New security protocols\n\nThe SMTP email protocol has been around since the early 80s. Naturally, it wasn't designed with the best security practices and requirements in mind, at least not the ones that the Internet expects today. For that reason, several protocol revisions and extensions have been standardized and adopted by the community over the years. Cloudflare is known for being an early adopter of promising emerging technologies; Email Routing already [supports](https://developers.cloudflare.com/email-routing/postmaster/) things like SPF, DKIM signatures, DMARC policy enforcement, TLS transport, STARTTLS, and IPv6 egress, to name a few. Today, we are introducing support for two new standards to help [increase email security](https://www.cloudflare.com/zero-trust/products/email-security/) and improve deliverability to third-party upstream email providers.\n\n### ARC\n\n[Authenticated Received Chain](https://arc-spec.org/) (ARC) is an email authentication system designed to allow an intermediate email server (such as Email Routing) to preserve email authentication results. In other words, with ARC, we can securely preserve the results of validating sender authentication mechanisms like SPF and DKIM, which we support when the email is received, and transport that information to the upstream provider when we forward the message. ARC establishes a chain of trust with all the hops the message has passed through. So, if it was tampered with or changed in one of the hops, it is possible to see where by following that chain.\n\nWe began rolling out ARC support to Email Routing a few weeks ago. Here’s how it works:\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--1--2.png)\n\nAs you can see, `joe@example.com` sends an Email to `henry@domain.example`, an Email Routing address, which in turn is forwarded to the final address, `example@gmail.com`.\n\nEmail Routing will use `@example.com`’s DMARC policy to check the SPF and DKIM alignments (SPF, DKIM, and DMARC [help authenticate](https://www.cloudflare.com/learning/email-security/dmarc-dkim-spf/) email senders by verifying that the emails came from the domain that they claim to be from.) It then stores this authentication result by adding a `Arc-Authentication-Results` header in the message:\n\n```\nARC-Authentication-Results: i=1; mx.cloudflare.net; dkim=pass header.d=cloudflare.com header.s=example09082023 header.b=IRdayjbb; dmarc=pass header.from=example.com policy.dmarc=reject; spf=none (mx.cloudflare.net: no SPF records found for postmaster@example.com) smtp.helo=smtp.example.com; spf=pass (mx.cloudflare.net: domain of joe@example.com designates 2a00:1440:4824:20::32e as permitted sender) smtp.mailfrom=joe@example.com; arc=none smtp.remote-ip=2a00:1440:4824:20::32e\n```\n\nThen we take a snapshot of all the headers and the body of the original message, and we generate an `Arc-Message-Signature` header with a DKIM-like cryptographic signature (in fact ARC uses the same DKIM keys):\n\n```\nARC-Message-Signature: i=1; a=rsa-sha256; s=2022; d=email.cloudflare.net; c=relaxed/relaxed; h=To:Date:Subject:From:reply-to:cc:resent-date:resent-from:resent-to :resent-cc:in-reply-to:references:list-id:list-help:list-unsubscribe :list-subscribe:list-post:list-owner:list-archive; t=1697709687; bh=sN/+...aNbf==;\n```\n\nFinally, before forwarding the message to `example@gmail.com`, Email Routing generates the `Arc-Seal` header, another DKIM-like signature, composed out of the `Arc-Authentication-Results` and `Arc-Message-Signature`, and cryptographically “seals” the message:\n\n```\nARC-Seal: i=1; a=rsa-sha256; s=2022; d=email.cloudflare.net; cv=none; b=Lx35lY6..t4g==;\n```\n\nWhen Gmail receives the message from Email Routing, it not only normally authenticates the last hop domain.example domain (Email Routing uses [SRS](https://developers.cloudflare.com/email-routing/postmaster/#sender-rewriting)), but it also checks the ARC seal header, which provides the authentication results of the original sender.\n\nARC increases the traceability of the message path through email intermediaries, allowing for more informed delivery decisions by those who receive emails as well as higher deliverability rates for those who transport them, like Email Routing. It has been adopted by all the major email providers like [Gmail](https://support.google.com/a/answer/175365?hl=en) and Microsoft. You can read more about the ARC protocol in the [RFC8617](https://datatracker.ietf.org/doc/html/rfc8617).\n\n### MTA-STS\n\nAs we said earlier, SMTP is an old protocol. Initially Email communications were done in the clear, in plain-text and unencrypted. At some point in time in the late 90s, the email providers community standardized STARTTLS, also known as Opportunistic TLS. The [STARTTLS extension](https://datatracker.ietf.org/doc/html/rfc3207) allowed a client in a SMTP session to upgrade to TLS encrypted communications.\n\nWhile at the time this seemed like a step forward in the right direction, we later found out that because STARTTLS can start with an unencrypted plain-text connection, and that can be hijacked, the protocol is [susceptible to man-in-the-middle attacks](https://lwn.net/Articles/866481/).\n\nA few years ago MTA Strict Transport Security ([MTA-STS](https://datatracker.ietf.org/doc/html/rfc8461)) was introduced by email service providers including Microsoft, Google and Yahoo as a solution to protect against downgrade and man-in-the-middle attacks in SMTP sessions, as well as solving the lack of security-first communication standards in email.\n\nSuppose that `example.com` uses Email Routing. Here’s how you can enable MTA-STS for it.\n\nFirst, log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and zone. Then go to **DNS** > **Records** and create a new CNAME record with the name “`_mta-sts`” that points to Cloudflare’s record “`_mta-sts.mx.cloudflare.net`”. Make sure to disable the proxy mode.\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0-2.png)\n\nConfirm that the record was created:\n\n```\n$ dig txt _mta-sts.example.com\n_mta-sts.example.com.\t300\tIN\tCNAME\t_mta-sts.mx.cloudflare.net.\n_mta-sts.mx.cloudflare.net. 300\tIN\tTXT\t\"v=STSv1; id=20230615T153000;\"\n```\n\nThis tells the other end client that is trying to connect to us that we support MTA-STS.\n\nNext you need an HTTPS endpoint at `mta-sts.example.com` to serve your policy file. This file defines the mail servers in the domain that use MTA-STS. The reason why HTTPS is used here instead of DNS is because not everyone uses DNSSEC yet, so we want to avoid another MITM attack vector.\n\nTo do this you need to deploy a very simple Worker that allows Email clients to pull Cloudflare’s Email Routing [policy](https://mta-sts.mx.cloudflare.net/.well-known/mta-sts.txt) file using the [“well-known” URI](https://en.wikipedia.org/wiki/Well-known_URI) convention. Go to your **Account** > **Workers & Pages** and press **Create Application**. Pick the “MTA-STS” template from the list.\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--2--2.png)\n\nThis Worker simply proxies `https://mta-sts.mx.cloudflare.net/.well-known/mta-sts.txt` to your own domain. After deploying it, go to the Worker configuration, then **Triggers** > **Custom Domains** and **Add Custom Domain**.\n\n![](https://blog.cloudflare.com/content/images/2023/10/customdomains.png)\n\nYou can then confirm that your policy file is working:\n\n```\n$ curl https://mta-sts.example.com/.well-known/mta-sts.txt\nversion: STSv1\nmode: enforce\nmx: *.mx.cloudflare.net\nmax_age: 86400\n```\n\nThis says that we enforce MTA-STS. Capable email clients will only deliver email to this domain over a secure connection to the specified MX servers. If no secure connection can be established the email will not be delivered.\n\nEmail Routing also supports MTA-STS upstream, which greatly improves security when forwarding your Emails to service providers like [Gmail](https://support.google.com/a/answer/9261504?hl=en) or [Microsoft](https://learn.microsoft.com/en-us/purview/enhancing-mail-flow-with-mta-sts), and others.\n\nWhile enabling MTA-STS involves a few steps today, we plan to simplify things for you and automatically configure MTA-STS for your domains from the Email Routing dashboard as a future improvement.\n\n## Sending emails and replies from Workers\n\nLast year we announced [Email Workers](https://developers.cloudflare.com/email-routing/email-workers/), allowing anyone using Email Routing to associate a Worker script to an Email address rule, and programmatically process their incoming emails in any way they want. [Workers](https://developers.cloudflare.com/workers/) is our serverless compute platform, it provides hundreds of features and APIs, like [databases](https://developers.cloudflare.com/workers/databases/) and [storage](https://developers.cloudflare.com/r2/api/workers/workers-api-reference/). Email Workers opened doors to a flood of use-cases and applications that weren’t possible before like implementing allow/block lists, advanced rules, notifications to messaging applications, honeypot aggregators and more.\n\nStill, you could only act on the incoming email event. You could read and process the email message, you could even manipulate and create some headers, but you couldn’t rewrite the body of the message or create new emails from scratch.\n\nToday we’re announcing two new powerful Email Workers APIs that will further enhance what you can do with Email Routing and Workers.\n\n### Send emails from Workers\n\nNow you can send an email from any Worker, from scratch, whenever you want, not just when you receive incoming messages, to any email address verified on Email Routing under your account. Here are a few practical examples where sending email from Workers to your verified addresses can be helpful:\n\n*   Daily digests with the news from your favorite publications.\n*   Alert messages whenever the weather conditions are adverse.\n*   Automatic notifications when systems complete tasks.\n*   Receive a message composed of the inputs of a form online on a contact page.\n\nLet's see a simple example of a Worker sending an email. First you need to create “`send_email`” bindings in your wrangler.toml configuration:\n\n```\nsend_email = [\n    {type = \"send_email\", name = \"EMAIL_OUT\"}\n ]\n```\n\nAnd then creating a new message and sending it in a Workers is as simple as:\n\n```\nimport { EmailMessage } from \"cloudflare:email\";\nimport { createMimeMessage } from \"mimetext\";\n\nexport default {\n async fetch(request, env) {\n   const msg = createMimeMessage();\n   msg.setSender({ name: \"Workers AI story\", addr: \"joe@example.com\" });\n   msg.setRecipient(\"mary@domain.example\");\n   msg.setSubject(\"An email generated in a worker\");\n   msg.addMessage({\n       contentType: 'text/plain',\n       data: `Congratulations, you just sent an email from a worker.`\n   });\n\n   var message = new EmailMessage(\n     \"joe@example.com\",\n     \"mary@domain.example\",\n     msg.asRaw()\n   );\n   try {\n     await env.EMAIL_OUT.send(message);\n   } catch (e) {\n     return new Response(e.message);\n   }\n\n   return new Response(\"email sent!\");\n },\n};\n```\n\nThis example makes use of [mimetext](https://muratgozel.github.io/MIMEText/), an open-source raw email message generator.\n\nAgain, for security reasons, you can only send emails to the addresses for which you confirmed ownership in Email Routing under your Cloudflare account. If you’re looking for sending email campaigns or newsletters to destination addresses that you do not control or larger subscription groups, you should consider other options like our [MailChannels integration](https://blog.cloudflare.com/sending-email-from-workers-with-mailchannels/).\n\nSince sending Emails from Workers is not tied to the EmailEvent, you can send them from any type of Worker, including [Cron Triggers](https://developers.cloudflare.com/workers/configuration/cron-triggers/) and [Durable Objects](https://developers.cloudflare.com/durable-objects/), whenever you want, you control all the logic.\n\n### Reply to emails\n\nOne of our most-requested features has been to provide a way to programmatically respond to incoming emails. It has been possible to do this with Email Workers in a very limited capacity by returning a permanent SMTP error message — but this may or may not be visible to the end user depending on the client implementation.\n\n```\nexport default {\n  async email(message, env, ctx) {\n      message.setReject(\"Address not allowed\");\n  }\n}\n\n```\n\nAs of today, you can now truly reply to incoming emails with another new message and implement smart auto-responders programmatically, adding any content and context in the main body of the message. Think of a customer support email automatically generating a ticket and returning the link to the sender, an out-of-office reply with instructions when you're on vacation, or a detailed explanation of why you rejected an email. Here’s a code example:\n\n```\nimport { EmailMessage } from \"cloudflare:email\";\nimport { createMimeMessage } from \"mimetext\";\n\nexport default {\n  async email(message, env, ctx) {\n\n    const ticket = createTicket(message);\n\n    const msg = createMimeMessage();\n    msg.setHeader(\"In-Reply-To\", message.headers.get(\"Message-ID\"));\n    msg.setSender({ name: \"Thank you for you contact\", addr: \"joe@example.com\" });\n    msg.setRecipient(\"mary@domain.example\");\n    msg.setSubject(\"An email generated in a worker\");\n    msg.addMessage({\n      contentType: 'text/plain',\n      data: `We got your message, your ticket number is ${ ticket }`\n    });\n\n    const replyMessage = new EmailMessage(\n      \"joe@example.com\",\n      message.from,\n      msg.asRaw()\n    );\n\n    await message.reply(replyMessage);\n  }\n}\n```\n\nTo mitigate security risks and abuse, replying to incoming emails has a few requirements:\n\n*   The incoming email has to have valid DMARC.\n*   The email can only be replied to once.\n*   The `In-Reply-To` header of the reply message must match the `Message-ID` of the incoming message.\n*   The recipient of the reply must match the incoming sender.\n*   The outgoing sender domain must match the same domain that received the email.\n\nIf these and other internal conditions are not met, then `reply()` will fail with an exception, otherwise you can freely compose your reply message and send it back to the original sender.\n\nFor more information the documentation to these APIs is available in our [Developer Docs](https://developers.cloudflare.com/email-routing/email-workers/runtime-api/).\n\n## Subdomains support\n\nThis is a big one.\n\nEmail Routing is a [zone-level](https://developers.cloudflare.com/fundamentals/concepts/accounts-and-zones/#zones) feature. A zone has a top-level domain (the same as the zone name) and it can have subdomains (managed under the DNS feature.) As an example, I can have the `example.com`  zone, and then the `mail.example.com` and `corp.example.com` subdomains under it. However, we can only use Email Routing with the top-level domain of the zone, `example.com` in this example. While this is fine for the vast majority of use cases, some customers — particularly bigger organizations with complex email requirements — have asked for more flexibility.\n\nThis changes today. Now you can use Email Routing with any subdomain of any zone in your account. To make this possible we redesigned the dashboard UI experience to make it easier to get you started and manage all your Email Routing domains and subdomains, rules and destination addresses in one single place. Let’s see how it works.\n\nTo add Email Routing features to a new subdomain, log in to the [Cloudflare dashboard](https://dash.cloudflare.com/) and select your account and zone. Then go to **Email** > **Email Routing** > **Settings** and click “Add subdomain”.\n\n![](https://blog.cloudflare.com/content/images/2023/10/prev-req-rec.png)\n\nOnce the subdomain is added and the DNS records are configured, you can see it in the **Settings** list under the **Subdomains** section:\n\n![](https://blog.cloudflare.com/content/images/2023/10/Domain.png)\n\nNow you can go to **Email** > **Email Routing** > **Routing rules** and create new custom addresses that will show you the option of using either the top domain of the zone or any other configured subdomain.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Screenshot-2023-10-25-at-11.55.31-AM.png)\n\nAfter the new custom address for the subdomain is created you can see it in the list with all the other addresses, and manage it from there.\n\n![](https://blog.cloudflare.com/content/images/2023/10/custom-addresses.png)\n\nIt’s this easy.\n\n## Final words\n\nWe hope you enjoy the new features that we are announcing today. Still, we want to be clear: there are no changes in pricing, and Email Routing is still free for Cloudflare customers.\n\nEver since Email Routing was launched, we’ve been listening to customers’ feedback and trying to adjust our roadmap to both our requirements and their own ideas and requests. Email shouldn't be difficult; our goal is to listen, learn and keep improving the service with better, more powerful features.\n\nYou can find detailed information about the new features and more in our Email Routing [Developer Docs](https://developers.cloudflare.com/email-routing).\n\nIf you have any questions or feedback about Email Routing, please come see us in the [Cloudflare Community](https://community.cloudflare.com/new-topic?category=Feedback/Previews%20%26%20Betas&tags=email) and the [Cloudflare Discord](https://discord.gg/cloudflaredev).\n\n![](https://blog.cloudflare.com/content/images/2023/10/Email-Routing-spot.png)\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Email Routing](https://blog.cloudflare.com/tag/email-routing/) [Subdomains](https://blog.cloudflare.com/tag/subdomains/) [Email Workers](https://blog.cloudflare.com/tag/email-workers/) [Cloudflare Workers](https://blog.cloudflare.com/tag/workers/) [Developer Platform](https://blog.cloudflare.com/tag/developer-platform/)"
    },
    {
      "url": "https://blog.cloudflare.com/introducing-har-sanitizer-secure-har-sharing/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/introducing-har-sanitizer-secure-har-sharing/",
        "loadedTime": "2023-12-05T02:28:48.254Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/introducing-har-sanitizer-secure-har-sharing/",
        "title": "Introducing HAR Sanitizer: secure HAR sharing",
        "description": "As a follow-up to the most recent Okta breach, we are making a HAR file sanitizer available to everyone, not just Cloudflare customers, at no cost.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/26/2023\n5 min read\nOn Wednesday, October 18th, 2023, Cloudflare’s Security Incident Response Team (SIRT) discovered an attack on our systems that originated from an authentication token stolen from one of Okta’s support systems. No Cloudflare customer information or systems were impacted by the incident, thanks to the real-time detection and rapid action of our Security Incident Response Team (SIRT) in tandem with our Zero Trust security posture and use of hardware keys. With that said, we’d rather not repeat the experience — and so we have built a new security tool that can help organizations render this type of attack obsolete for good.\nThe bad actor in the Okta breach compromised user sessions by capturing session tokens from administrators at Cloudflare and other impacted organizations. They did this by infiltrating Okta’s customer support system and stealing one of the most common mechanisms for troubleshooting — an HTTP Response Archive (HAR) file.\nHAR files contain a record of a user’s browser session, a kind of step-by-step audit, that a user can share with someone like a help desk agent to diagnose an issue. However, the file can also contain sensitive information that can be used to launch an attack.\nAs a follow-up to the Okta breach, we are making a HAR file sanitizer available to everyone, not just Cloudflare customers, at no cost. We are publishing this tool under an open source license and are making it available to any support, engineering or security team. At Cloudflare, we are committed to making the Internet a better place and using HAR files without the threat of stolen sessions should be part of the future of the Internet.\nHAR Files - a look back in time\nImagine being able to rewind time and revisit every single step a user took during a web session, scrutinizing each request and the responses the browser received.\nHAR (HTTP Archive) files are a JSON formatted archive file of a web browser’s interaction with a web application. HAR files provide a detailed snapshot of every request, including headers, cookies, and other types of data sent to a web server by the browser. This makes them an invaluable resource to troubleshoot web application issues especially for complex, layered web applications.\nThe snapshot that a HAR file captures can contain the following information:\nComplete Request and Response Headers: Every piece of data sent and received, including method types (GET, POST, etc.), status codes, URLs, cookies, and more.\nPayload Content: Details of what was actually exchanged between the client and server, which can be essential for diagnosing issues related to data submission or retrieval.\nTiming Information: Precise timing breakdowns of each phase – from DNS lookup, connection time, SSL handshake, to content download – giving insight into performance bottlenecks.\nThis information can be difficult to gather from an application’s logs due to the diverse nature of devices, browsers and networks used to access an application. A user would need to take dozens of manual steps. A HAR file gives them a one-click option to share diagnostic information with another party. The file is also standard, providing the developers, support teams, and administrators on the other side of the exchange with a consistent input to their own tooling. This minimizes the frustrating back-and-forth where teams try to recreate a user-reported problem, ensuring that everyone is, quite literally, on the same page.\nHAR files as an attack vector\nHAR files, while powerful, come with a cautionary note. Within the set of information they contain, session cookies make them a target for malicious actors.\nThe Role of Session Cookies\nBefore diving into the risks, it's crucial to understand the role of session cookies. A session cookie is sent from a server and stored on a user's browser to maintain stateful information across web sessions for that user. In simpler terms, it’s how the browser keeps you logged into an application for a period of time even if you close the page. Generally, these cookies live in local memory on a user’s browser and are not often shared. However, a HAR file is one of the most common ways that a session cookie could be inadvertently shared.\nDangers of a stolen session cookie\nIf a HAR file with a valid session cookie is shared, then there are a number of potential security threats that user, and company, may be exposed to:\nUnauthorized Access: The biggest risk is unauthorized access. If a HAR file with a session cookie lands in the wrong hands, it grants entry to the user’s account for that application. For platforms that store personal data or financial details, the consequences of such a breach can be catastrophic. Especially if the session cookie of a user with administrative or elevated permissions is stolen.\nSession Hijacking: Armed with a session cookie, attackers can impersonate legitimate users, a tactic known as session hijacking. This can lead to a range of malicious activities, from spreading misinformation to siphoning off funds.\nPersistent Exposure: Unlike other forms of data, a session cookie's exposure risk doesn't necessarily end when a user session does. Depending on the cookie's lifespan, malicious actors could gain prolonged access, repeatedly compromising a user's digital interactions.\nGateway to Further Attacks: With access to a user's session, especially an administrator’s, attackers can probe for other vulnerabilities, exploit platform weaknesses, or jump to other applications.\nMitigating the impact of a stolen HAR file\nThankfully, there are ways to render a HAR file inert even if stolen by an attacker. One of the most effective methods is to “sanitize” a HAR file of any session related information before sharing it for debugging purposes.\nThe HAR sanitizer we are introducing today allows a user to upload any HAR file, and the tool will strip out any session related cookies or JSON Web Tokens (JWT). The tool is built entirely on Cloudflare Workers, and all sanitization is done client-side which means Cloudflare never sees the full contents of the session token.\nJust enough sanitization\nBy default, the sanitizer will remove all session-related cookies and tokens — but there are some cases where these are essential for troubleshooting. For these scenarios, we are implementing a way to conditionally strip “just enough” data from the HAR file to render them safe, while still giving support teams the information they need.\nThe first product we’ve optimized the HAR sanitizer for is Cloudflare Access. Access relies on a user’s JWT — a compact token often used for secure authentication — to verify that a user should have access to the requested resource. This means a JWT plays a crucial role in troubleshooting issues with Cloudflare Access. We have tuned the HAR sanitizer to strip the cryptographic signature out of the Access JWT, rendering it inert, while still providing useful information for internal admins and Cloudflare support to debug issues.\nBecause HAR files can include a diverse array of data types, selectively sanitizing them is not a case of ‘one size fits all’. We will continue to expand support for other popular authentication tools to ensure we strip out “just enough” information.\nWhat’s next\nOver the coming months, we will launch additional security controls in Cloudflare Zero Trust to further mitigate attacks stemming from session tokens stolen from HAR files. This will include:\nEnhanced Data Loss Prevention (DLP) file type scanning to include HAR file and session token detections, to ensure users in your organization can not share unsanitized files.\nExpanded API CASB scanning to detect HAR files with session tokens in collaboration tools like Zendesk, Jira, Drive and O365.\nAutomated HAR sanitization of data in popular collaboration tools.\nAs always, we continue to expand our Cloudflare One Zero Trust suite to protect organizations of all sizes against an ever-evolving array of threats. Ready to get started? Sign up here to begin using Cloudflare One at no cost for teams of up to 50 users.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nTools Open Source",
      "markdown": "10/26/2023\n\n*   [![Kenny Johnson](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2020/12/0DA903DE-E315-43D5-B0DE-39870448303D.jpeg)](https://blog.cloudflare.com/author/kenny/)\n\n5 min read\n\n![Introducing HAR Sanitizer: secure HAR sharing](https://blog.cloudflare.com/content/images/2023/10/image2-8.png)\n\nOn Wednesday, October 18th, 2023, Cloudflare’s Security Incident Response Team (SIRT) discovered an attack on our systems that originated from an [authentication token stolen from one of Okta’s support systems](https://blog.cloudflare.com/how-cloudflare-mitigated-yet-another-okta-compromise/). No Cloudflare customer information or systems were impacted by the incident, thanks to the real-time detection and rapid action of our Security Incident Response Team (SIRT) in tandem with our [Zero Trust security posture](https://www.cloudflare.com/learning/security/glossary/what-is-zero-trust/) and use of hardware keys. With that said, we’d rather not repeat the experience — and so we have built a new security tool that can help organizations render this type of attack obsolete for good.\n\nThe bad actor in the Okta breach compromised user sessions by capturing session tokens from administrators at Cloudflare and other impacted organizations. They did this by infiltrating Okta’s customer support system and stealing one of the most common mechanisms for troubleshooting — an HTTP Response Archive (HAR) file.\n\nHAR files contain a record of a user’s browser session, a kind of step-by-step audit, that a user can share with someone like a help desk agent to diagnose an issue. However, the file can also contain sensitive information that can be used to launch an attack.\n\nAs a follow-up to the Okta breach, we are making a [HAR file sanitizer](https://har-sanitizer.pages.dev/) available to everyone, not just Cloudflare customers, at no cost. We are publishing this tool under an [open source license](https://github.com/cloudflare/har-sanitizer) and are making it available to any support, engineering or security team. At Cloudflare, we are committed to making the Internet a better place and using HAR files without the threat of stolen sessions should be part of the future of the Internet.\n\n## HAR Files - a look back in time\n\nImagine being able to rewind time and revisit every single step a user took during a web session, scrutinizing each request and the responses the browser received.\n\n[HAR (HTTP Archive)](https://en.wikipedia.org/wiki/HAR_%28file_format%29) files are a JSON formatted archive file of a web browser’s interaction with a web application. HAR files provide a detailed snapshot of every request, including headers, cookies, and other types of data sent to a web server by the browser. This makes them an invaluable resource to troubleshoot web application issues especially for complex, layered web applications.\n\nThe snapshot that a HAR file captures can contain the following information:\n\n**Complete Request and Response Headers:** Every piece of data sent and received, including method types (GET, POST, etc.), status codes, URLs, cookies, and more.\n\n**Payload Content:** Details of what was actually exchanged between the client and server, which can be essential for diagnosing issues related to data submission or retrieval.\n\n**Timing Information:** Precise timing breakdowns of each phase – from DNS lookup, connection time, SSL handshake, to content download – giving insight into performance bottlenecks.\n\nThis information can be difficult to gather from an application’s logs due to the diverse nature of devices, browsers and networks used to access an application. A user would need to take dozens of manual steps. A HAR file gives them a one-click option to share diagnostic information with another party. The file is also standard, providing the developers, support teams, and administrators on the other side of the exchange with a consistent input to their own tooling. This minimizes the frustrating back-and-forth where teams try to recreate a user-reported problem, ensuring that everyone is, quite literally, on the same page.\n\n## HAR files as an attack vector\n\nHAR files, while powerful, come with a cautionary note. Within the set of information they contain, session cookies make them a target for malicious actors.\n\n### The Role of Session Cookies\n\nBefore diving into the risks, it's crucial to understand the role of session cookies. A session cookie is sent from a server and stored on a user's browser to maintain stateful information across web sessions for that user. In simpler terms, it’s how the browser keeps you logged into an application for a period of time even if you close the page. Generally, these cookies live in local memory on a user’s browser and are not often shared. However, a HAR file is one of the most common ways that a session cookie could be inadvertently shared.\n\n### Dangers of a stolen session cookie\n\nIf a HAR file with a valid session cookie is shared, then there are a number of potential security threats that user, and company, may be exposed to:\n\n**Unauthorized Access:** The biggest risk is unauthorized access. If a HAR file with a session cookie lands in the wrong hands, it grants entry to the user’s account for that application. For platforms that store personal data or financial details, the consequences of such a breach can be catastrophic. Especially if the session cookie of a user with administrative or elevated permissions is stolen.\n\n**Session Hijacking:** Armed with a session cookie, attackers can impersonate legitimate users, a tactic known as session hijacking. This can lead to a range of malicious activities, from spreading misinformation to siphoning off funds.\n\n**Persistent Exposure:** Unlike other forms of data, a session cookie's exposure risk doesn't necessarily end when a user session does. Depending on the cookie's lifespan, malicious actors could gain prolonged access, repeatedly compromising a user's digital interactions.\n\n**Gateway to Further Attacks:** With access to a user's session, especially an administrator’s, attackers can probe for other vulnerabilities, exploit platform weaknesses, or jump to other applications.\n\n## Mitigating the impact of a stolen HAR file\n\nThankfully, there are ways to render a HAR file inert even if stolen by an attacker. One of the most effective methods is to “sanitize” a HAR file of any session related information before sharing it for debugging purposes.\n\nThe [HAR sanitizer](https://har-sanitizer.pages.dev/) we are introducing today allows a user to upload any HAR file, and the tool will strip out any session related cookies or JSON Web Tokens (JWT). The tool is built entirely on Cloudflare Workers, and all sanitization is done client-side which means Cloudflare never sees the full contents of the session token.\n\n![](https://blog.cloudflare.com/content/images/2023/10/image1-8.png)\n\n### Just enough sanitization\n\nBy default, the sanitizer will remove all session-related cookies and tokens — but there are some cases where these are essential for troubleshooting. For these scenarios, we are implementing a way to conditionally strip “just enough” data from the HAR file to render them safe, while still giving support teams the information they need.\n\nThe first product we’ve optimized the HAR sanitizer for is [Cloudflare Access](https://developers.cloudflare.com/cloudflare-one/policies/access/). Access relies on a user’s [JWT](https://developers.cloudflare.com/cloudflare-one/identity/authorization-cookie/application-token/) — a compact token often used for secure authentication — to verify that a user should have access to the requested resource. This means a JWT plays a crucial role in troubleshooting issues with Cloudflare Access. We have tuned the HAR sanitizer to strip the cryptographic signature out of the Access JWT, rendering it inert, while still providing useful information for internal admins and Cloudflare support to debug issues.\n\nBecause HAR files can include a diverse array of data types, selectively sanitizing them is not a case of ‘one size fits all’. We will continue to expand support for other popular authentication tools to ensure we strip out “just enough” information.\n\n## What’s next\n\nOver the coming months, we will launch additional security controls in Cloudflare Zero Trust to further mitigate attacks stemming from session tokens stolen from HAR files. This will include:\n\n*   Enhanced Data Loss Prevention (DLP) file type scanning to include HAR file and session token detections, to ensure users in your organization can not share unsanitized files.\n*   Expanded API CASB scanning to detect HAR files with session tokens in collaboration tools like Zendesk, Jira, Drive and O365.\n*   Automated HAR sanitization of data in popular collaboration tools.\n\nAs always, we continue to expand our Cloudflare One Zero Trust suite to protect organizations of all sizes against an ever-evolving array of threats. Ready to get started? [Sign up here](https://www.cloudflare.com/products/zero-trust/) to begin using Cloudflare One at no cost for teams of up to 50 users.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Tools](https://blog.cloudflare.com/tag/tools/) [Open Source](https://blog.cloudflare.com/tag/open-source/)"
    },
    {
      "url": "https://blog.cloudflare.com/cache-reserve-goes-ga/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/cache-reserve-goes-ga/",
        "loadedTime": "2023-12-05T02:29:03.349Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/cache-reserve-goes-ga/",
        "title": "Cache Reserve goes GA: enhanced control to minimize egress costs",
        "description": "We're thrilled to announce the graduation of Cache Reserve from beta to GA, accompanied by the introduction of several exciting new features. These new features include adding Cache Reserve into the analytics shown on the Cache overview section of the Cloudflare dashboard",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/25/2023\n7 min read\nEveryone is chasing the highest cache ratio possible. Serving more content from Cloudflare’s cache means it loads faster for visitors, saves website operators money on egress fees from origins, and provides multiple layers of resiliency and protection to make sure that content is available to be served and websites scale effortlessly. A year ago we introduced Cache Reserve to help customer’s serve as much content as possible from Cloudflare’s cache.\nToday, we are thrilled to announce the graduation of Cache Reserve from beta to General Availability (GA), accompanied by the introduction of several exciting new features. These new features include adding Cache Reserve into the analytics shown on the Cache overview section of the Cloudflare dashboard, giving customers the ability to see how they are using Cache Reserve over time. We have also added the ability for customers to delete all data in Cache Reserve without losing content in the edge cache. This is useful for customers who are no longer using Cache Reserve storage.\nWe’re also introducing new tools that give organizations more granular control over which files are saved to Cache Reserve, based on valuable feedback we received during the beta. The default configuration of Cache Reserve is to cache all available cacheable files, but some beta customers reported that they didn’t want certain rapidly-changing files cached. Based on their feedback, we’ve added the ability to define Cache Reserve eligibility within Cache Rules. This new rule lets users be very specific about which traffic is admitted to Cache Reserve.\nTo experience Cache Reserve firsthand visit the Cache Reserve section on the Cloudflare dashboard, press a single button to enable Cache Reserve, and experience cost-efficient, high-performance content delivery.\nCaching background\nContent delivery begins when a client or browser makes a request, be it for a webpage, video, application, or even a cat picture. This request travels to an origin server, aka the host of the requested content. The origin assembles the necessary data, packages it, and dispatches it back to the client. It's at this moment that website operators often incur a fee for transferring the content from their host to the requesting visitor. This per-GB of data “transferred” is a frequent line item on monthly hosting bills for website operators; we refer to them as egress fees or an “egress tax,” and have blogged previously on why we think it is bad practice.\nDuring its return voyage to the client, Cloudflare has the ability to cache the origin’s response. Caching enables subsequent visitors, who are requesting the same content, to receive it from one of our cache servers rather than the origin server. Since the file is now served from Cloudflare's servers it saves the website operator from egress fees. It also means better performance, due to Cloudflare’s cache servers typically being physically situated much closer to end users than the customer’s own origin servers.\nServing files from cache is a fundamental, and often essential strategy for delivering content over the Internet efficiently. We can evaluate the efficacy of a cache by looking at its “hit/miss” ratio: when website content is served from a cache server it’s known as a cache hit. But when content is not in cache, and we need to go back to the origin server to get a fresh copy of the content, we call it a cache miss.\nWhy cache misses happen\nSometimes eligible content may not be served from cache for a variety of reasons. One scenario occurs when Cloudflare must revalidate with the origin to see if a fresh copy is available. This situation arises when a customer has configured a resource’s time-to-live (TTL) to specify how long cached content should be served to visitors, and when to consider it outdated (stale). How long a user specifies something is safe to be served from cache is only a part of the story, though. Content delivery networks (CDNs) also need to consider how to best utilize storage for all of their customers and perform network optimizations to ensure the right assets are cached in the right locations.\nCDNs must decide whether to evict content before their specified TTL to optimize storage for other assets when cache space nears full capacity. At Cloudflare, our eviction strategy prioritizes content based on its popularity, employing an algorithm known as \"least recently used\" or LRU. This means that even if the content’s TTL specifies that content should be cached for a long time, we may still need to evict it earlier if it's less frequently requested than other resources, to make room for more frequently accessed content.\nThis approach can sometimes perplex users who wonder why a cache miss occurs unexpectedly. Without eviction, we'd be forced to store content in data centers farther from the requesting visitors, hindering asset performance and introducing inefficiencies into Cloudflare's network operations.\nSome customers, however, possess large content libraries that may not all be requested very frequently but which they’d still like to shield from being served by their origin. In a traditional caching setup, these assets might be evicted as they become less popular and, when requested again, fetched from the origin, resulting in egress fees. Cache Reserve is the solution for scenarios like this one, allowing customers to deliver assets from Cloudflare’s network, rather than their origin server — avoiding any associated egress tax, and providing better performance.\nCache Reserve basics\nCache Reserve combines several Cloudflare technologies, including tiered cache and R2 storage, to seamlessly provide organizations with a way to ensure their assets are never evicted from Cloudflare’s network, even if they are infrequently accessed by users. Once admitted to Cache Reserve, content can be stored for a much longer period of time — 30 days by default — without being subjected to LRU eviction. If another request for the content arrives during that period, it can be extended for another 30-day period (and so on) or until the TTL signifies that we should no longer serve that content from cache. Cache Reserve serves as a safety net to backstop all cacheable content, so customers can sleep well at night without having to worry about unwanted cache eviction and origin egress fees.\nConfiguration of Cache Reserve is simple and efficient, on average taking seconds to configure and start seeing hit ratios increase dramatically. By simply pressing a single button in the Cache Reserve section of Cloudflare’s dashboard, all eligible content will be written to Cache Reserve on a miss and retrieved before Cloudflare would otherwise ask the origin for the resource. For more information about what’s required to use Cache Reserve, please review the documentation.\nCustomers are also seeing significant savings when using Cache Reserve, often seeing it cost only a fraction of what they would otherwise pay for the egress from their hosting provider. As Docker put it,\n“The 2% cache hit ratio improvement enabled by Cache Reserve has eliminated roughly two-thirds of our S3 egress. The reduction in egress charges is almost an order of magnitude larger than the price we paid for Cache Reserve.”\nBrett Inman, Docker | Senior Manager of Engineering\nWhat’s new with Cache Reserve?\nSince we’ve last blogged about Cache Reserve we have made three important updates to the product that improve the quality of life for users. \nNew analytics\nPreviously, Cache Reserve analytics provided views of how much storage had been used by a particular website and estimates of the number of operations used in a particular time period. We’ve improved analytics to be more similar to traditional cache analytics, allowing customers to view storage and operations in a customized time series from the cache analytics dashboard. \nAdditionally, the updated Cache Reserve analytics will provide you an estimate of how much egress you’re saving by using the product.\nIn the coming months we will also provide greater visibility into the largest and most requested items being served from Cache Reserve.\nCache Reserve delete storage\nCache Reserve users who want to change, remove or stop using their Reserve altogether have asked for a simple way to wipe their storage without impacting their use of Cloudflare’s traditional edge cache. Previously clearing Cache Reserve would be achieved by purging content. This could be problematic because purging also wipes content cached in the traditional edge cache which could lead to additional origin fetches and egress fees.\nWe’ve built in a new way for customers to completely remove their Cache Reserve storage with the push of a button, which can be found in the Cache Reserve dashboard. When performing this action you will need to wait until Cache Reserve is cleared before re-enabling. This period can differ depending on how much is stored in your Cache Reserve, but in general can take around 24 hours. \nThe Cache Reserve delete button differs from purging. Purge will still allow for you to invalidate resources across all of Cloudflare’s Caches — including both Cache Reserve and the edge cache with a single request. The Cache Reserve delete button will actively remove the entire storage in the Reserve only. Currently, this action can be performed for the entire Cache Reserve storage associated with a zone. \nIntegration into Cache Rules\nOne of the most requested Cache Reserve features we heard from early adopters is the ability to specify what parts of their website should be eligible for storage in Cache Reserve. Previously, when a user enabled Cache Reserve, all of a website’s assets that were eligible for Cache Reserve could be stored in the Reserve. For egress sensitive customers, this is the path we still recommend. However, for customers that really want to customize what is eligible for Cache Reserve, you can now use Cache Rules to specify assets that should be stored in Cache Reserve based on the usual Cache Rules fields (hostnames, paths, URLs, etc.) and also by using specific new rules configurations like the minimum size of a resource. For example, you can specify that all assets that should be written to Cache Reserve have a minimum size of 100kb. By using the new rules functionality, Cache Reserve customers can customize how their Reserve is built while still maintaining utilization of the edge cache, and saving even more money. \nTry out Cache Reserve today!\nYou can easily sign up for Cache Reserve in the Cloudflare Dashboard by navigating to the Cache section, clicking on Cache Reserve, and pushing enable storage sync. Try it out and let us know what you think!\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nCache Reserve General Availability Application Services Performance",
      "markdown": "10/25/2023\n\n*   [![Alex Krivit](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2018/08/RE0BQ6EF_400x400.jpg)](https://blog.cloudflare.com/author/alex/)\n\n7 min read\n\n![Cache Reserve goes GA: enhanced control to minimize egress costs](https://blog.cloudflare.com/content/images/2023/10/Cache-Reserve.png)\n\nEveryone is chasing the highest cache ratio possible. Serving more content from Cloudflare’s cache means it loads faster for visitors, saves website operators money on [egress fees](https://www.cloudflare.com/learning/cloud/what-are-data-egress-fees/) from origins, and provides multiple layers of resiliency and protection to make sure that content is available to be served and websites scale effortlessly. A year ago we introduced [Cache Reserve](https://blog.cloudflare.com/introducing-cache-reserve/) to help customer’s serve as much content as possible from Cloudflare’s cache.\n\nToday, we are thrilled to announce the **graduation of Cache Reserve from beta to General Availability (GA)**, accompanied by the introduction of several exciting new features. These new features include adding Cache Reserve into the analytics shown on the [_Cache overview_](https://dash.cloudflare.com/caching) section of the Cloudflare dashboard, giving customers the ability to see how they are using Cache Reserve over time. We have also added the ability for customers to delete all data in Cache Reserve without losing content in the edge cache. This is useful for customers who are no longer using Cache Reserve storage.\n\nWe’re also introducing new tools that give organizations more granular control over which files are saved to Cache Reserve, based on valuable feedback we received during the beta. The default configuration of Cache Reserve is to cache all available cacheable files, but some beta customers reported that they didn’t want certain rapidly-changing files cached. Based on their feedback, we’ve added the ability to define Cache Reserve eligibility within [Cache Rules](https://blog.cloudflare.com/cache-rules-go-ga/). This new rule lets users be very specific about which traffic is admitted to Cache Reserve.\n\nTo experience Cache Reserve firsthand visit the [Cache Reserve](https://dash.cloudflare.com/caching/cache-reserve) section on the Cloudflare dashboard, press a single button to enable Cache Reserve, and experience cost-efficient, high-performance content delivery.\n\n### Caching background\n\nContent delivery begins when a client or browser makes a request, be it for a webpage, video, application, or even a cat picture. This request travels to an origin server, aka the host of the requested content. The origin assembles the necessary data, packages it, and dispatches it back to the client. It's at this moment that website operators often incur a fee for transferring the content from their host to the requesting visitor. This per-GB of data “transferred” is a frequent line item on monthly hosting bills for website operators; we refer to them as [egress fees](https://www.cloudflare.com/learning/cloud/what-are-data-egress-fees/) or an “egress tax,” and have blogged previously on why we think it is [bad practice](https://blog.cloudflare.com/aws-egregious-egress/).\n\nDuring its return voyage to the client, Cloudflare has the ability to cache the origin’s response. Caching enables subsequent visitors, who are requesting the same content, to receive it from one of our cache servers rather than the origin server. Since the file is now served from Cloudflare's servers it saves the website operator from egress fees. It also means better performance, due to Cloudflare’s cache servers typically being physically situated much closer to end users than the customer’s own origin servers.\n\nServing files from cache is a fundamental, and often essential strategy for delivering content over the Internet efficiently. We can evaluate the efficacy of a cache by looking at its “hit/miss” ratio: when website content is served from a cache server it’s known as a cache **hit**. But when content is not in cache, and we need to go back to the origin server to get a fresh copy of the content, we call it a cache **miss**.\n\n### Why cache misses happen\n\nSometimes eligible content may not be served from cache for a variety of reasons. One scenario occurs when Cloudflare must [revalidate](https://blog.cloudflare.com/introducing-smart-edge-revalidation/#:~:text=So%20What%20Is%20Revalidation%3F) with the origin to see if a fresh copy is available. This situation arises when a customer has configured a resource’s [time-to-live (TTL)](https://www.cloudflare.com/learning/cdn/glossary/time-to-live-ttl/) to specify how long cached content should be served to visitors, and when to consider it outdated (stale). How long a _user_ specifies something is safe to be served from cache is only a part of the story, though. [Content delivery networks (CDNs)](https://www.cloudflare.com/learning/cdn/what-is-a-cdn/) also need to consider how to best utilize storage for all of their customers and perform network optimizations to ensure the right assets are cached in the right locations.\n\nCDNs must decide whether to evict content before their specified TTL to optimize storage for other assets when cache space nears full capacity. At Cloudflare, our eviction strategy prioritizes content based on its popularity, employing an algorithm known as \"least recently used\" or LRU. This means that even if the content’s TTL specifies that content should be cached for a long time, we may still need to evict it earlier if it's less frequently requested than other resources, to make room for more frequently accessed content.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Cache-Reserve-Response-Flow.png)\n\nThis approach can sometimes perplex users who wonder why a cache miss occurs unexpectedly. Without eviction, we'd be forced to store content in data centers farther from the requesting visitors, hindering asset performance and introducing inefficiencies into Cloudflare's network operations.\n\nSome customers, however, possess large content libraries that may not all be requested very frequently but which they’d still like to shield from being served by their origin. In a traditional caching setup, these assets might be evicted as they become less popular and, when requested again, fetched from the origin, resulting in egress fees. Cache Reserve is the solution for scenarios like this one, allowing customers to deliver assets from Cloudflare’s network, rather than their origin server — avoiding any associated egress tax, and providing better performance.\n\n### Cache Reserve basics\n\nCache Reserve combines several Cloudflare technologies, including [tiered cache](https://developers.cloudflare.com/cache/how-to/tiered-cache/) and [R2](https://developers.cloudflare.com/r2/) storage, to seamlessly provide organizations with a way to ensure their assets are never evicted from Cloudflare’s network, even if they are infrequently accessed by users. Once [admitted](https://developers.cloudflare.com/cache/about/cache-reserve/#cache-reserve-asset-eligibility) to Cache Reserve, content can be stored for a much longer period of time — 30 days by [default](https://developers.cloudflare.com/cache/about/cache-reserve/) — without being subjected to LRU eviction. If another request for the content arrives during that period, it can be extended for another 30-day period (and so on) or until the TTL signifies that we should no longer serve that content from cache. Cache Reserve serves as a safety net to backstop all cacheable content, so customers can sleep well at night without having to worry about unwanted cache eviction and origin egress fees.\n\nConfiguration of Cache Reserve is simple and efficient, on average taking seconds to configure and start seeing hit ratios increase dramatically. By simply pressing a [single button](https://dash.cloudflare.com/caching/cache-reserve) in the Cache Reserve section of Cloudflare’s dashboard, all [eligible content](https://developers.cloudflare.com/cache/advanced-configuration/cache-reserve/#cache-reserve-asset-eligibility) will be written to Cache Reserve on a miss and retrieved before Cloudflare would otherwise ask the origin for the resource. For more information about what’s required to use Cache Reserve, please review the [documentation](https://developers.cloudflare.com/cache/advanced-configuration/cache-reserve/).\n\nCustomers are also seeing significant savings when using Cache Reserve, often seeing it cost only a fraction of what they would otherwise pay for the egress from their hosting provider. As [Docker](https://www.cloudflare.com/case-studies/docker/) put it,\n\n> “The 2% cache hit ratio improvement enabled by Cache Reserve has eliminated roughly two-thirds of our S3 egress. The reduction in egress charges is almost an order of magnitude larger than the price we paid for Cache Reserve.”  \n> **Brett Inman**, Docker | Senior Manager of Engineering\n\n### What’s new with Cache Reserve?\n\nSince we’ve last [blogged](https://blog.cloudflare.com/cache-reserve-open-beta/) about Cache Reserve we have made three important updates to the product that improve the quality of life for users.\n\n#### New analytics\n\nPreviously, Cache Reserve analytics provided views of how much storage had been used by a particular website and estimates of the number of operations used in a particular time period. We’ve improved analytics to be more similar to traditional cache analytics, allowing customers to view storage and operations in a customized time series from the cache analytics dashboard.\n\nAdditionally, the updated Cache Reserve analytics will provide you an estimate of how much egress you’re saving by using the product.\n\nIn the coming months we will also provide greater visibility into the largest and most requested items being served from Cache Reserve.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Screenshot-2023-10-08-at-7.45.35-PM.png)\n\n#### Cache Reserve delete storage\n\nCache Reserve users who want to change, remove or stop using their Reserve altogether have asked for a simple way to wipe their storage without impacting their use of Cloudflare’s traditional edge cache. Previously clearing Cache Reserve would be achieved by purging content. This could be problematic because purging also wipes content cached in the traditional edge cache which could lead to additional origin fetches and egress fees.\n\nWe’ve built in a new way for customers to completely remove their Cache Reserve storage with the push of a button, which can be found in the Cache Reserve [dashboard](https://dash.cloudflare.com/caching/cache-reserve). When performing this action you will need to wait until Cache Reserve is cleared before re-enabling. This period can differ depending on how much is stored in your Cache Reserve, but in general can take around 24 hours.  \n\nThe Cache Reserve delete button differs from purging. **Purge** will still allow for you to invalidate resources across _all_ of Cloudflare’s Caches — including both Cache Reserve and the edge cache with a single request. The Cache Reserve delete button will actively remove the entire storage in the Reserve only. Currently, this action can be performed for the entire Cache Reserve storage associated with a zone.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Screenshot-2023-10-08-at-7.46.10-PM.png)\n\n#### Integration into Cache Rules\n\nOne of the most requested Cache Reserve features we heard from early adopters is the ability to specify what parts of their website should be eligible for storage in Cache Reserve. Previously, when a user enabled Cache Reserve, all of a website’s assets that were [eligible](https://developers.cloudflare.com/cache/advanced-configuration/cache-reserve/#cache-reserve-asset-eligibility) for Cache Reserve could be stored in the Reserve. For egress sensitive customers, this is the path we still recommend. However, for customers that really want to customize what is eligible for Cache Reserve, you can now use [Cache Rules](https://dash.cloudflare.com/caching/cache-rules) to specify assets that should be stored in Cache Reserve based on the usual Cache Rules fields (hostnames, paths, URLs, etc.) and also by using specific new rules configurations like the minimum size of a resource. For example, you can specify that all assets that should be written to Cache Reserve have a minimum size of 100kb. By using the new rules functionality, Cache Reserve customers can customize how their Reserve is built while still maintaining utilization of the edge cache, and saving even more money.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Screenshot-2023-10-08-at-4.14.48-PM--1-.png)\n\n### Try out Cache Reserve today!\n\nYou can easily sign up for Cache Reserve in the Cloudflare Dashboard by navigating to the Cache section, clicking on [Cache Reserve](https://dash.cloudflare.com/caching/cache-reserve), and pushing enable storage sync. Try it out and let us know what you think!\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Cache Reserve](https://blog.cloudflare.com/tag/cache-reserve/) [General Availability](https://blog.cloudflare.com/tag/general-availability/) [Application Services](https://blog.cloudflare.com/tag/application-services/) [Performance](https://blog.cloudflare.com/tag/performance/)"
    },
    {
      "url": "https://blog.cloudflare.com/q3-2023-internet-disruption-summary/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/q3-2023-internet-disruption-summary/",
        "loadedTime": "2023-12-05T02:29:18.393Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/q3-2023-internet-disruption-summary/",
        "title": "Q3 2023 Internet disruption summary",
        "description": "In this post, we review selected Internet disruptions observed by Cloudflare during the third quarter of 2023, supported by traffic graphs from Cloudflare Radar and other internal Cloudflare tools, and grouped by associated cause or common geography.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/25/2023\n15 min read\nThis post is also available in 简体中文, 繁體中文, 日本語, 한국어, Deutsch, Français and Español.\nCloudflare operates in more than 300 cities in over 100 countries, where we interconnect with over 12,500 network providers in order to provide a broad range of services to millions of customers. The breadth of both our network and our customer base provides us with a unique perspective on Internet resilience, enabling us to observe the impact of Internet disruptions.\nWe have been publishing these summaries since the first quarter of 2022, and over that time, the charts on Cloudflare Radar have evolved. Many of the traffic graphs in early editions of this summary were screenshots from the relevant traffic pages on Radar. Late last year, we launched the ability to download graphs, and earlier this year, to embed dynamic graphs, and these summaries have taken advantage of those capabilities where possible. Sharp-eyed readers may notice an additional evolution in some of the graphs below: yellow highlighting indicating an observed “traffic anomaly”. Identification of such anomalies, along with the ability to be notified about them, as well as a timeline enhancement (embedded below) to the Cloudflare Radar Outage Center, were launched as part of Birthday Week at the end of September. More information on these new features can be found in our announcement blog post.\nAs we have seen in previous quarters, Iraq pursued an aggressive plan of government-directed Internet shutdowns intended to prevent cheating on exams, and several other African countries implemented politically motivated shutdowns. Damage to several submarine cables, as well as planned maintenance to others, caused Internet disruptions across a number of countries during the third quarter. Natural disasters, including wildfires and an earthquake, caused issues with connectivity, as did power outages in multiple countries. An acknowledged cyberattack resulted in a major US university intentionally disconnecting from the Internet, while a number of other major Internet providers acknowledged problems on their networks without ever disclosing the root cause of those problems.\n(Note that the Internet disruptions related to the Israel/Palestine conflict are not covered in this post, as they began on October 7 in Q4 of 2023. Disruptions related to this conflict are being tracked, with additional insights found on the Cloudflare blog and @CloudflareRadar on X/Twitter.)\nGovernment directed\nBecause the Internet has become a critical communications tool, Internet shutdowns are often used by governments as a means of controlling communication both within a country and with the outside world. These government-directed shutdowns are imposed for a variety of reasons, including during periods of civil unrest and protests around elections, and as a deterrent against cheating during exams.\nIraq\nAs we have discussed in past summaries, Internet shutdowns are used by some governments in an attempt to prevent cheating on national high school or baccalaureate exams. These shutdowns have a nationwide impact, and it isn’t clear whether they are ultimately successful at mitigating cheating. As we have also discussed in the past, such shutdowns frequently occur in Iraq, and that was certainly the case during the third quarter, with rounds of shutdowns occurring during all three months.\nThe first round of exam-related Internet shutdowns during the quarter in Iraq was a continuation of a set that started in June, and continued on into July, targeting cheating on 9th and 12th grade exams. On ten days between July 4 and July 17, Internet connectivity was shut down on AS203214 (HulumTele), AS59588 (ZAINAS-IQ), AS199739 (Earthlink), AS203735 (Capacities-LTD), AS51684 (ASIACELL), and AS58322 (Halasat) in Iraq (except for the Kurdistan Region) between 04:00 - 08:00 local time (01:00 - 05:00 UTC).\nDuring the second week of August, several networks in the Kurdistan region of Iraq again implemented daily exam-related Internet shutdowns due to a second round of exams for 12th grade students. These shutdowns took place between 06:00 - 08:00 local time (03:00 - 05:00 UTC), and impacted AS21277 (Newroz Telecom), AS48492 (IQ-Online), and AS59625 (KorekTel) from August 6-13. These two hour shutdowns were similar to those seen in the region in June.\nA second round of 9th grade exams in August drove a week of Internet shutdowns across Iraq (except the Kurdistan region) between August 21 and August 29. Connectivity was shut down between 04:00 - 08:00 local time (01:00 - 05:00 UTC) across the same networks impacted by the shutdowns implemented in July.\nFollowing the second round of 9th grade exams in August, the second round of 12th grade exams in Iraq (except the Kurdistan region) occurred in September, and with these exams, came yet another round of Internet shutdowns. Impacting the same set of network providers as the previous two months, these shutdowns occurred between September 17-30. However, while they started at the same time (04:00 local time, 01:00 UTC), they were shorter than previous rounds, ending an hour earlier (07:00 local time, 04:00 UTC).\nSenegal\nOn July 31, following the arrest of the Senegalese opposition leader, the Senegalese Ministry of Communication, Telecommunications and the Digital Economy once again ordered the disconnection of mobile Internet connectivity in Senegal as shown in the communiqué below. These disruptions to mobile Internet access were visible on two of the four network providers within the country: AS37196 (Sudatel Senegal) and AS37649 (Tigo/Free).\nAs shown in the graphs below, the shutdowns began mid-morning local time, generally between 08:00 and 10:00, from July 31 through August 5, and ended early the next morning, generally between midnight and 02:00. The final shutdown on August 5 was an exception, ending at 22:00 local time on both networks. (Senegal is UTC+0, so the local times are the same as UTC.)\nEthiopia\nFollowing days of clashes between the federal military and local militia, mobile Internet connectivity was shut down in Amhara, Ethiopia. Cloudflare saw traffic to the region drop around 21:00 local time (18:00 UTC) on August 2. This is the second time that authorities have shut down mobile Internet connectivity in Amhara in 2023 — the first time was on April 6 after protests broke out following the federal government’s move to disband regional security forces. (Note that the country is no stranger to Internet shutdowns, as they have taken such action multiple times over the last several years.) Despite calls to restore connectivity, mobile Internet remained unavailable through the end of the third quarter, as seen in the figure below.\nGabon\nOn August 26, following contentious presidential elections in Gabon, Internet connectivity was shut down in order to \"prevent the spread of calls for violence\". As shown in the figure below, traffic began to fall just before 17:00 local time (16:00 UTC), and remained at zero through approximately 07:30 local time (06:30 UTC) on August 30. Connectivity was restored hours after military officers seized power in the country, placing President Ali Bongo under house arrest and naming a new leader after the country’s election body announced Bongo had won a third term.\nCable cuts\nCameroon\nOn July 7, an X/Twitter post from Cameroon Telecommunications alerted subscribers to disruptions to voice and data services, with a subsequent post nearly six hours later noting that services had been re-established. Although these posts did not provide details on the cause of the disruption, a Facebook post from the operator included an attached communiqué explaining that “The optical fibre has been severed by road maintenance operations, causing major disruptions in the delivery of our services.” The figure below shows the impact of this fiber damage, with traffic from AS15964 (CAMNET-AS) dropping sharply around 11:30 local time (10:30 UTC), and returning to expected levels by 18:00 local time (17:00 UTC).\nLiberia\nDamage to the Africa Coast to Europe (ACE) submarine cable disrupted Internet connectivity in Liberia on July 28. A Facebook post from the Liberia Telecommunications Authority (LTA) noted “The Liberia Telecommunications Authority(LTA) announces the temporary interruption of all nationwide Internet services due to the breakdown of the Africa Coast to Europe Cable in Ivory Coast.” and also highlighted that the ACE cable serves as the “sole source of internet connectivity between Europe and Liberia”. The figure below shows a near complete loss of traffic starting at 13:00 local time (13:00 UTC) and gradually recovering over the next several hours, returning to expected levels by 17:00 local time (17:00 UTC).\nTogo, Benin, Namibia, and the Republic of Congo (Brazzaville)\nOn August 6, the West African Cable System (WACS) and the South Atlantic 3 (SAT–3) undersea cables were damaged by an undersea landslide in the Congo Canyon, located at the mouth of the Congo River. The damage to the cables impacted Internet connectivity in Togo, Benin, Namibia, and the Republic of Congo (Brazzaville). Social media posts from Telecom Namibia and Canalbox Congo alerted subscribers that connectivity would be impacted as a result of the damage to the cables.\nCable repair ship CS Leon Thevenin was called upon to perform repairs, but it took several weeks for it to arrive at the site of the damage, and then additional time to perform the repairs, which were reportedly completed on September 6. Network operators in impacted countries were able to shift some traffic to alternate cables, such as Google’s Equiano cable, which went live in February 2023.\nAs such, the graphs below illustrate that there was not a complete loss of traffic for impacted countries. To that end, traffic in Togo appeared to recover several weeks before the cable repairs were completed. The full impact is harder to see in the graphs for Benin, Namibia, and the Republic of Congo (Brazzaville) because the selected timeframe is long enough to force data aggregation at a daily level, but it is clearly visible in graphs covering shorter periods of time (with data aggregation at an hourly level) during the weeks after the cable cut occurred.\nSouth Sudan\nHighlighting the interconnected nature of the Internet, fiber cuts in Uganda caused a brief Internet disruption for customers on MTN South Sudan (AS37594) on August 14, occurring between 13:00 - 15:00 local time (11:00 - 13:00 UTC), and impacting an estimated 438,000 users. An X/Twitter post from the provider that afternoon told subscribers “We sincerely apologize for the network issues experienced over the last couple of hours. It was due to multiple fiber cuts in Uganda.”\nCyberattack\nUniversity of Michigan\nOn August 27, a “significant security concern” led the University of Michigan to shut down the Internet on the Ann Arbor, Flint and Dearborn campuses. Although the shutdown occurred at the start of the new school year, classes continued as scheduled, but an announcement posted by the University detailed the impact of disconnecting from the Internet, including potential delays in financial aid refunds and the unavailability of certain campus systems. The impact of the disconnection can be seen in the figure below, appearing as a significant drop in traffic starting just before 14:00 local time (18:00 UTC) on August 27, and lasting until just after 08:00 local time (12:00 UTC) on August 30 on AS36375 (UMICH-AS-5), the primary autonomous system used by the University of Michigan.\nFire\nLahaina, Hawaii\nIn early August, a series of wildfires broke out in the state of Hawaii, predominantly on the island of Maui. The town of Lahaina was one of the hardest hit, with the fires killing nearly 100 people, as well as destroying homes, businesses, and infrastructure, causing power outages and disrupting Internet connectivity. The graph below shows traffic to Cloudflare from Lahaina dropping to near zero around 21:00 local time on August 7 (07:00 UTC on August 8), and remaining at minimal levels through August 30. Some recovery of Internet traffic can be seen through the end of September as cleanup and repairs progressed, and as wireless operators deployed temporary network assets in an effort to restore some service capacity.\nEarthquake\nMorocco\nAt 23:11 local time on September 8 (22:11 UTC), a magnitude 6.8 earthquake occurred in Morocco, centered 79 kilometers (49 miles) southwest of Marrakesh. Nearly 3,000 deaths were reported as a result of the quake, and significant damage was reported, including the collapse of schools, houses, and historic buildings. Power outages and infrastructure damage also impacted Internet connectivity in the region, leading to largely localized disruptions.\nThe country-level graph below shows a nominal loss of traffic in Morocco after the earthquake, remaining slightly lower than expected for approximately four days. However, the impacts are more evident at a regional level, with the earthquake causing an immediate 64% drop in traffic in Marrkesh-Safi, a 64% loss in Souss-Massa, and a 49% decline in Casablanca-Settat. Peak traffic levels in these regions remained slightly lower than those seen in previous weeks for several days after the earthquake occurred.\nPower outages\nCuraçao\nOn July 27, a malfunction at a major Aqualectra Utility power distribution center resulted in 70% of neighborhoods in Curaçao losing power. The power outage resulted in an island-wide Internet disruption. As seen in the graph below, Internet traffic fell sharply at around 12:30 local time (16:30 UTC), remaining largely flat for approximately five hours before starting to recover around 17:30 local time (21:30 UTC). The start of the recovery aligns with the timing of a Facebook post made at 18:00 local time by Aqualectra Utility noting that “55% of Curaçao’s power supply has been restored.” The ongoing traffic increase is in line with additional neighborhoods having power restored, with traffic returning to expected levels by around 22:00 local time (2:00 UTC on July 28).\nBrazil\nA widespread power outage in Brazil starting at 08:30 local time (11:30 UTC) on August 15 resulted in a nominal disruption to Internet traffic within the country. Although the power outage represented a loss of approximately 27% of the total electric load at the time it occurred, the impact to the country’s Internet traffic was much lower, as seen in the graph below. Traffic returned to expected levels by around 11:30 local time (14:30 UTC).\nKenya\nA “system disturbance” at 21:45 local time (18:45 UTC) on August 25 led to “loss of bulk power supply to various parts of the country” in Kenya, according to an X/Twitter post from Kenya Power. The impact of the power outage is visible in the graph below, with traffic dropping as power is lost. Subsequent updates from Kenya Power on August 26 (1, 2, 3) highlighted the progress made in restoring electricity across the country. Internet traffic from the country returned to expected levels by 03:00 on August 27 (00:00 UTC).\nFrench Guiana\nAn 11-hour Internet disruption in French Guiana on August 27 was the result of a power outage caused by “a problem that occurred at the energy evacuation station which connects Petit-Saut to the Kourou-Saint-Laurent line”. The power outage caused a nationwide drop in Internet traffic between 11:00 local time (14:00 UTC) and 22:00 local time (01:00 UTC on August 28), visible in the graph below.\nTunisia\nA fire at the Tunisian Company of Electricity and Gas power station in Rades, Ben Arous Governorate caused a widespread power outage in Tunisia, resulting in an Internet disruption starting at 01:00 local time (00:00 UTC) on September 20. Traffic remained lower than expected for approximately five hours, as shown in the graph below, in line with a published report that noted “The unexpected outage lasted for over four hours in some areas of the country.”\nBarbados\nA September 21 Facebook post from The Barbados Light & Power Company Limited noted that the company was aware of an outage affecting customers, and that they were “working to promptly and safely restore power in the shortest time possible.” This outage resulted in a significant drop in Internet traffic from the country starting at 11:30 local time (15:30 UTC). A subsequent Facebook post from the utility company at 20:00 local time (00:00 UTC on September 22) noted that power had been restored to all customers. Ahead of full power restoration, Internet traffic had returned to expected levels around 17:00 local time (21:00 UTC).\nMaintenance\nGuinea\nLa Guinéenne de la Large Bande, also known as GUILAB, is the company responsible for managing the capacity allocated to the country of Guinea on the Africa Coast to Europe (ACE) submarine cable. According to a (translation of the) communiqué posted by the company on Facebook, planned maintenance on the cable would be taking place between 22:00 on July 14 and 06:18 \"sharp\" on July 15 (22:00 on July 14 and 06:18 on July 15 UTC). This maintenance resulted in a complete Internet outage in Guinea, as seen in the graph below. It appears that the ACE submarine cable is Guinea’s sole international Internet connection, with no other backup submarine or terrestrial connectivity.\nPalau\nJust a few days later, planned maintenance to another submarine cable took Palau, an island country in the western Pacific, completely offline for several days. According to a press release from the Palau National Communications Corporation (PNCC) posted to their Facebook page, “BSCC (Belau Submarine Cable Corporation) has been notified that an emergency repair will be undertaken on the SEA – US cable network in Guam from Tuesday, July 18th 7:00 a.m. Palau time, and expected to be completed 5:00 p.m. Saturday, July 22nd. … For safety reasons, repairs can only be undertaken when the cable is not powered. Since BSCC’s Palau Cable Network No 1 connects to SEA – US for onward transport to Guam, BSCC will be unable to provide service for the duration of the repair. BSCC will be unable to provide any international connectivity for Palau. The only available international connection will be via PNCC satellite connection, which will provide limited capacity compared to normal cable service.”\nThe graph below shows that Cloudflare did not see any appreciable traffic from Palau’s backup satellite connection during the duration of the repairs, as traffic dropped to zero at 07:00 local time on July 18 (22:00 UTC on July 17), and remained there until around 18:00 local time on July 21 (09:00 UTC), as the repairs were completed earlier than expected. A PNCC press release confirmed this early completion, noting “PNCC is pleased to inform the public that Internet and Mobile Data services for our customers have been restored, due to the early completion today of the emergency repairs on the SEA-US Submarine Cable System, our main off-island internet connection.”\nUnspecified issues\nSpectrum (Charter Communications)\nAt 14:03 Eastern Time (18:03 UTC) on August 17, the X/Twitter support account for Spectrum, a brand of US-based Internet service provider Charter Communications, posted a statement that noted “We are aware of an outage affecting customers in Alabama, Georgia and Tennessee. We apologize for the inconvenience and are working to resolve as quickly as possible. Thank you.” The graphs below show the varied impacts to traffic seen from Spectrum (AS20115) across the listed states, as well as Texas, which wasn’t initially cited by Spectrum as having an issue, though customers quickly called it out.\nA near complete outage was observed in Tennessee between 12:30 - 14:00 local time (17:30 - 19:00 UTC), while a brief drop in traffic at 12:00 local time (17:00 UTC) and quick recovery ahead of another drop at 13:30 local time (18:30 UTC) was seen in Alabama. Georgia also saw an initial drop in traffic at 13:00 local time (17:00 UTC) ahead of a larger fall at 14:30 local time (18:30 UTC), while traffic from Texas only experienced a decline at 13:30 local time (18:30 UTC). Traffic volumes from all four impacted states recovered within several hours — approximately three hours after the initial post, Spectrum’s support account stated “We have received confirmation repairs have been completed and services have been restored to affected customers in the Alabama, Georgia and Tennessee area.”\nStarlink\nOn September 12, satellite Internet service provider SpaceX Starlink experienced a brief but complete outage. The graph below shows traffic from AS14593 (SPACEX-STARLINK) dropping at 23:15 UTC, but quickly recovering, returning to normal within 90 minutes. At 00:33 UTC on September 13, Starlink shared an X/Twitter post stating “Starlink is currently in a network outage, and we are actively implementing a solution. We appreciate your patience, we'll share an update once this issue is resolved” and just over an hour later, posted “The network issue has been fully resolved”.\nSky UK\nDuring the evening (UTC) of September 19, numerous complaints could be found on social media about a nationwide outage across the United Kingdom on Sky Broadband (AS5607). A sharp drop in traffic from Sky Broadband can be seen in the graph below starting at 21:00 UTC, but a full outage did not appear to have taken place. Traffic volumes below expected levels lasted until approximately 01:00 UTC on September 20. While the issue was acknowledged by Sky’s support account on X/Twitter, no root cause for the disruption was ever provided.\nConclusion\nAs we’ve noted in past quarterly summaries, this report is intended as a summary overview of observed disruptions, and not an exhaustive or complete list of issues that have occurred during the quarter. Some disruptions not covered here were visible in our data, but never acknowledged by the impacted provider, while others were reported by industry colleagues based on their measurement methodologies, but not clearly obvious in our traffic graphs.\nAs we indicated above, the Cloudflare Radar Outage Center now includes information on observed traffic anomalies as well as verified outages. Interested users can subscribe to notifications for both anomalies and outages — our blog post includes more information on how to do so.\nVisit Cloudflare Radar for additional insights around Internet disruptions. Follow us on social media at @CloudflareRadar (Twitter), cloudflare.social/@radar (Mastodon), and radar.cloudflare.com (Bluesky), or contact us via email.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nCloudflare Radar Internet Traffic Outage Internet Shutdown Internet Quality",
      "markdown": "10/25/2023\n\n*   [![David Belson](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/11/David-Belson.jpeg)](https://blog.cloudflare.com/author/david-belson/)\n\n15 min read\n\nThis post is also available in [简体中文](https://blog.cloudflare.com/zh-cn/q3-2023-internet-disruption-summary-zh-cn/), [繁體中文](https://blog.cloudflare.com/zh-tw/q3-2023-internet-disruption-summary-zh-tw/), [日本語](https://blog.cloudflare.com/ja-jp/q3-2023-internet-disruption-summary-ja-jp/), [한국어](https://blog.cloudflare.com/ko-kr/q3-2023-internet-disruption-summary-ko-kr/), [Deutsch](https://blog.cloudflare.com/de-de/q3-2023-internet-disruption-summary-de-de/), [Français](https://blog.cloudflare.com/fr-fr/q3-2023-internet-disruption-summary-fr-fr/) and [Español](https://blog.cloudflare.com/es-es/q3-2023-internet-disruption-summary-es-es/).\n\n![Q3 2023 Internet disruption summary](https://blog.cloudflare.com/content/images/2023/10/image12-1.png)\n\nCloudflare operates in more than 300 cities in over 100 countries, where we interconnect with over 12,500 network providers in order to provide a broad range of services to millions of customers. The breadth of both our network and our customer base provides us with a unique perspective on Internet resilience, enabling us to observe the impact of Internet disruptions.\n\nWe have been publishing these summaries since the first quarter of 2022, and over that time, the charts on Cloudflare Radar have evolved. Many of the traffic graphs in early editions of this summary were screenshots from the relevant traffic pages on Radar. Late last year, we launched the ability to download graphs, and earlier this year, to embed dynamic graphs, and these summaries have taken advantage of those capabilities where possible. Sharp-eyed readers may notice an additional evolution in some of the graphs below: yellow highlighting indicating an observed “traffic anomaly”. Identification of such anomalies, along with the ability to be notified about them, as well as a timeline enhancement (embedded below) to the [Cloudflare Radar Outage Center](https://radar.cloudflare.com/outage-center/), were launched as part of Birthday Week at the end of September. More information on these new features can be found in our [announcement blog post](https://blog.cloudflare.com/traffic-anomalies-notifications-radar/).\n\nAs we have seen in previous quarters, Iraq pursued an aggressive plan of [government-directed](#governmentdirected) Internet shutdowns intended to prevent cheating on exams, and several other African countries implemented politically motivated shutdowns. [Damage](#cablecuts) to several submarine cables, as well as [planned maintenance](#maintenance) to others, caused Internet disruptions across a number of countries during the third quarter. Natural disasters, including [wildfires](#fire) and an [earthquake](#earthquake), caused issues with connectivity, as did [power outages](#poweroutages) in multiple countries. An acknowledged [cyberattack](#cyberattack) resulted in a major US university intentionally disconnecting from the Internet, while a number of other major Internet providers [acknowledged problems](#unspecifiedissues) on their networks without ever disclosing the root cause of those problems.\n\n_(Note that the Internet disruptions related to the Israel/Palestine conflict are not covered in this post, as they began on October 7 in Q4 of 2023. Disruptions related to this conflict are being tracked, with additional insights found on the [Cloudflare blog](https://blog.cloudflare.com/internet-traffic-patterns-in-israel-and-palestine-following-the-october-2023-attacks/) and [@CloudflareRadar](https://twitter.com/CloudflareRadar) on X/Twitter.)_\n\n## Government directed\n\nBecause the Internet has become a critical communications tool, Internet shutdowns are often used by governments as a means of controlling communication both within a country and with the outside world. These government-directed shutdowns are imposed for a variety of reasons, including during periods of civil unrest and protests around elections, and as a deterrent against cheating during exams.\n\n### Iraq\n\nAs we have discussed in past summaries, Internet shutdowns are used by some governments in an attempt to prevent cheating on national high school or baccalaureate exams. These shutdowns have a nationwide impact, and it isn’t clear whether they are ultimately successful at mitigating cheating. As we have also discussed in the past, such shutdowns frequently occur in [Iraq](https://radar.cloudflare.com/iq), and that was certainly the case during the third quarter, with rounds of shutdowns occurring during all three months.\n\nThe first round of exam-related Internet shutdowns during the quarter in Iraq was a continuation of a set that started in June, and continued on into July, targeting cheating on 9th and 12th grade exams. On ten days between July 4 and July 17, Internet connectivity was shut down on [AS203214 (HulumTele)](https://radar.cloudflare.com/as203214), [AS59588 (ZAINAS-IQ)](https://radar.cloudflare.com/as59588), [AS199739 (Earthlink)](https://radar.cloudflare.com/as199739), [AS203735 (Capacities-LTD)](https://radar.cloudflare.com/as203735), [AS51684 (ASIACELL)](https://radar.cloudflare.com/as51684), and [AS58322 (Halasat)](https://radar.cloudflare.com/as58322) in Iraq (except for the Kurdistan Region) between 04:00 - 08:00 local time (01:00 - 05:00 UTC).\n\nDuring the second week of August, several networks in the Kurdistan region of Iraq again implemented daily exam-related Internet shutdowns due to a [second round of exams for 12th grade students](https://twitter.com/964English/status/1688157983170322432). These shutdowns took place between 06:00 - 08:00 local time (03:00 - 05:00 UTC), and impacted [AS21277 (Newroz Telecom)](https://radar.cloudflare.com/as21277), [AS48492 (IQ-Online)](https://radar.cloudflare.com/as48492), and [AS59625 (KorekTel)](https://radar.cloudflare.com/as59625) from August 6-13. These two hour shutdowns were [similar to those seen in the region](https://blog.cloudflare.com/q2-2023-internet-disruption-summary/) in June.\n\nA second round of 9th grade exams in August drove a week of Internet shutdowns across Iraq (except the Kurdistan region) between August 21 and August 29. Connectivity was shut down between 04:00 - 08:00 local time (01:00 - 05:00 UTC) across the same networks impacted by the shutdowns implemented in July.\n\nFollowing the second round of 9th grade exams in August, the second round of 12th grade exams in Iraq (except the Kurdistan region) occurred in September, and with these exams, came yet another round of Internet shutdowns. Impacting the same set of network providers as the previous two months, these shutdowns occurred between September 17-30. However, while they started at the same time (04:00 local time, 01:00 UTC), they were shorter than previous rounds, ending an hour earlier (07:00 local time, 04:00 UTC).\n\n### Senegal\n\nOn July 31, following the arrest of the Senegalese opposition leader, the Senegalese Ministry of Communication, Telecommunications and the Digital Economy [once again](https://blog.cloudflare.com/q2-2023-internet-disruption-summary/) [ordered the disconnection of mobile Internet connectivity](https://pulse.internetsociety.org/shutdowns/senegal-government-imposes-mobile-internet-shutdown) in [Senegal](https://radar.cloudflare.com/sn) as shown in the communiqué below. These disruptions to mobile Internet access were visible on two of the four network providers within the country: [AS37196 (Sudatel Senegal)](https://radar.cloudflare.com/as37196) and [AS37649 (Tigo/Free)](https://radar.cloudflare.com/as37649).\n\n![](https://blog.cloudflare.com/content/images/2023/10/Screenshot-2023-10-23-at-17.14.06.png)\n\nAs shown in the graphs below, the shutdowns began mid-morning local time, generally between 08:00 and 10:00, from July 31 through August 5, and ended early the next morning, generally between midnight and 02:00. The final shutdown on August 5 was an exception, ending at 22:00 local time on both networks. (Senegal is UTC+0, so the local times are the same as UTC.)\n\n### Ethiopia\n\nFollowing days of clashes between the federal military and local militia, [mobile Internet connectivity was shut down](https://www.nasdaq.com/articles/mobile-internet-outages-in-ethiopias-amhara-amid-fighting-residents-say) in Amhara, [Ethiopia](https://radar.cloudflare.com/et). Cloudflare saw traffic to the region drop around 21:00 local time (18:00 UTC) on August 2. This is the second time that authorities have shut down mobile Internet connectivity in Amhara in 2023 — the first time was on April 6 after protests broke out following the federal government’s move to disband regional security forces. (Note that the country is no stranger to Internet shutdowns, as they have taken such action [multiple times over the last several years](https://docs.google.com/spreadsheets/d/1DvPAuHNLp5BXGb0nnZDGNoiIwEeu2ogdXEIDvT4Hyfk/edit?usp=sharing).) Despite [calls to restore connectivity](https://www.accessnow.org/press-release/amhara-internet-shutdown/), mobile Internet remained unavailable through the end of the third quarter, as seen in the figure below.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Sep-30---Ethiopia---Amhara.png)\n\n### Gabon\n\nOn August 26, following contentious presidential elections in [Gabon](https://radar.cloudflare.com/ga), Internet connectivity was shut down in order to [\"prevent the spread of calls for violence\"](https://www.france24.com/fr/afrique/20230826-pr%C3%A9sidentielle-au-gabon-couvre-feu-instaur%C3%A9-et-internet-coup%C3%A9). As shown in the figure below, traffic began to fall just before 17:00 local time (16:00 UTC), and remained at zero through approximately 07:30 local time (06:30 UTC) on August 30. [Connectivity was restored](https://www.reuters.com/world/africa/gabonese-military-officers-announce-they-have-seized-power-2023-08-30/) hours after military officers seized power in the country, placing President Ali Bongo under house arrest and naming a new leader after the country’s election body announced Bongo had won a third term.\n\n## Cable cuts\n\n### Cameroon\n\nOn July 7, an X/Twitter [post](https://twitter.com/Camtelonline/status/1677298034852483075) from [Cameroon Telecommunications](https://camtel.cm/) alerted subscribers to disruptions to voice and data services, with a subsequent [post](https://twitter.com/Camtelonline/status/1677384867238252545) nearly six hours later noting that services had been re-established. Although these posts did not provide details on the cause of the disruption, a [Facebook post](https://www.facebook.com/camtelonline/posts/pfbid02VFYsyC5tKEPr2TLcEXRd9ShE8gq8pWHYA253reirbGHjb9tb3UCptjc3CogYLJ1il) from the operator included an attached communiqué explaining that “_The optical fibre has been severed by road maintenance operations, causing major disruptions in the delivery of our services._” The figure below shows the impact of this fiber damage, with traffic from [AS15964 (CAMNET-AS)](https://radar.cloudflare.com/as15964) dropping sharply around 11:30 local time (10:30 UTC), and returning to expected levels by 18:00 local time (17:00 UTC).\n\n### Liberia\n\nDamage to the [Africa Coast to Europe (ACE)](https://www.submarinecablemap.com/submarine-cable/africa-coast-to-europe-ace) submarine cable disrupted Internet connectivity in [Liberia](https://radar.cloudflare.com/lr) on July 28. A [Facebook post](https://www.facebook.com/TelecommunicationsAuthorityLIBERA/posts/pfbid0ZHbnyKHtkUPycRke7dspbQQ1GdKpfTdc2fQm1HmSLaip1ds28kwm7gRYLoBeoVUnl) from the [Liberia Telecommunications Authority (LTA)](http://lta.gov.lr/) noted “The Liberia Telecommunications Authority(LTA) announces the temporary interruption of all nationwide Internet services due to the breakdown of the Africa Coast to Europe Cable in Ivory Coast.” and also highlighted that the ACE cable serves as the “sole source of internet connectivity between Europe and Liberia”. The figure below shows a near complete loss of traffic starting at 13:00 local time (13:00 UTC) and gradually recovering over the next several hours, returning to expected levels by 17:00 local time (17:00 UTC).\n\n### Togo, Benin, Namibia, and the Republic of Congo (Brazzaville)\n\nOn August 6, the [West African Cable System (WACS)](https://www.submarinecablemap.com/submarine-cable/west-africa-cable-system-wacs) and the [South Atlantic 3 (SAT–3)](https://www.submarinecablemap.com/submarine-cable/sat-3wasc) undersea cables were [damaged by an undersea landslide](https://www.kentik.com/blog/dual-subsea-cable-cuts-disrupt-african-internet/) in the Congo Canyon, located at the mouth of the Congo River. The damage to the cables impacted Internet connectivity in [Togo](https://radar.cloudflare.com/tg), [Benin](https://radar.cloudflare.com/bj), [Namibia](https://radar.cloudflare.com/na), and the [Republic of Congo (Brazzaville)](https://radar.cloudflare.com/cg). Social media posts from [Telecom Namibia](https://twitter.com/TelecomNamibia/status/1688867775601844224) and [Canalbox Congo](https://twitter.com/canalboxcongo/status/1688856878988623872) alerted subscribers that connectivity would be impacted as a result of the damage to the cables.\n\nCable repair ship [CS Leon Thevenin](https://www.marinetraffic.com/en/ais/home/shipid:761048/zoom:10) was called upon to perform repairs, but it took [several weeks for it to arrive](https://twitter.com/philBE2/status/1696246908954542552) at the site of the damage, and then [additional time to perform the repairs](https://twitter.com/philBE2/status/1699071287807955020), which were [reportedly completed on September 6](https://www.news24.com/news24/tech-and-trends/news/internet-speed-relief-a-snapped-undersea-cable-is-now-fixed-20230907). Network operators in impacted countries were able to [shift some traffic to alternate cables](https://www.kentik.com/blog/dual-subsea-cable-cuts-disrupt-african-internet/), such as Google’s [Equiano](https://www.submarinecablemap.com/submarine-cable/equiano) cable, which went live in February 2023.\n\nAs such, the graphs below illustrate that there was not a complete loss of traffic for impacted countries. To that end, traffic in Togo appeared to recover several weeks before the cable repairs were completed. The full impact is harder to see in the graphs for Benin, Namibia, and the Republic of Congo (Brazzaville) because the selected timeframe is long enough to force data aggregation at a daily level, but it is clearly visible in graphs covering shorter periods of time (with data aggregation at an hourly level) during the weeks after the cable cut occurred.\n\n### South Sudan\n\nHighlighting the interconnected nature of the Internet, fiber cuts in [Uganda](https://radar.cloudflare.com/ug) caused a brief Internet disruption for customers on [MTN South Sudan (AS37594)](https://radar.cloudflare.com/as37594) on August 14, occurring between 13:00 - 15:00 local time (11:00 - 13:00 UTC), and [impacting an estimated 438,000 users](https://stats.labs.apnic.net/cgi-bin/aspop?c=ss). An X/Twitter [post](https://twitter.com/MTNSSD/status/1691086728457531392) from the provider that afternoon told subscribers “_We sincerely apologize for the network issues experienced over the last couple of hours. It was due to multiple fiber cuts in Uganda._”\n\n## Cyberattack\n\n### University of Michigan\n\nOn August 27, a “significant security concern” led the [University of Michigan](https://umich.edu/) to [shut down the Internet](https://www.mlive.com/news/ann-arbor/2023/08/significant-security-concern-prompted-internet-outage-at-university-of-michigan.html) on the Ann Arbor, Flint and Dearborn campuses. Although the shutdown occurred at the start of the new school year, classes continued as scheduled, but an [announcement](https://web.archive.org/web/20230829122845/https://www.umich.edu/announcements/) posted by the University detailed the impact of disconnecting from the Internet, including potential delays in financial aid refunds and the unavailability of certain campus systems. The impact of the disconnection can be seen in the figure below, appearing as a significant drop in traffic starting just before 14:00 local time (18:00 UTC) on August 27, and lasting until just after 08:00 local time (12:00 UTC) on August 30 on [AS36375 (UMICH-AS-5)](https://radar.cloudflare.com/as36375), the primary autonomous system used by the University of Michigan.\n\n## Fire\n\n### Lahaina, Hawaii\n\nIn early August, a [series of wildfires](https://en.wikipedia.org/wiki/2023_Hawaii_wildfires) broke out in the state of Hawaii, predominantly on the island of Maui. The town of [Lahaina](https://en.wikipedia.org/wiki/Lahaina,_Hawaii) was one of the hardest hit, with the fires [killing nearly 100 people](https://mauinow.com/2023/10/05/one-more-lahaina-fire-victim-identified-by-police-death-toll-is-98-unaccounted-for-is-12/), as well as destroying homes, businesses, and infrastructure, causing power outages and disrupting Internet connectivity. The graph below shows traffic to Cloudflare from Lahaina dropping to near zero around 21:00 local time on August 7 (07:00 UTC on August 8), and remaining at minimal levels through August 30. Some recovery of Internet traffic can be seen through the end of September as cleanup and repairs progressed, and as wireless operators [deployed temporary network assets](https://www.fiercewireless.com/wireless/wireless-operators-scramble-restore-service-after-maui-wildfires) in an effort to restore some service capacity.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Sep-30---US---Hawaii---Lahaina.png)\n\n## Earthquake\n\n### Morocco\n\nAt 23:11 local time on September 8 (22:11 UTC), a [magnitude 6.8 earthquake](https://twitter.com/LastQuake/status/1700271514925297843?s=20) occurred in [Morocco](https://radar.cloudflare.com/ma), centered 79 kilometers (49 miles) southwest of Marrakesh. Nearly 3,000 deaths were reported as a result of the quake, and [significant damage was reported](https://en.wikipedia.org/wiki/2023_Marrakesh%E2%80%93Safi_earthquake#Impact), including the collapse of schools, houses, and historic buildings. Power outages and infrastructure damage also impacted Internet connectivity in the region, leading to largely localized disruptions.\n\nThe country-level graph below shows a nominal loss of traffic in Morocco after the earthquake, remaining slightly lower than expected for approximately four days. However, the impacts are more evident at a regional level, with the earthquake causing an immediate 64% drop in traffic in Marrkesh-Safi, a 64% loss in Souss-Massa, and a 49% decline in Casablanca-Settat. Peak traffic levels in these regions remained slightly lower than those seen in previous weeks for several days after the earthquake occurred.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Sep-8-13-Morocco---regional.jpeg)\n\n## Power outages\n\n### Curaçao\n\nOn July 27, a [malfunction at a major Aqualectra Utility power distribution center](https://twitter.com/CloudflareRadar/status/1684688380200718336) resulted in 70% of neighborhoods in [Curaçao](https://radar.cloudflare.com/cw) losing power. The power outage resulted in an island-wide Internet disruption. As seen in the graph below, Internet traffic fell sharply at around 12:30 local time (16:30 UTC), remaining largely flat for approximately five hours before starting to recover around 17:30 local time (21:30 UTC). The start of the recovery aligns with the timing of a [Facebook post made at 18:00 local time](https://www.facebook.com/AqualectraUtilityCuracao/posts/pfbid02pAwrM5qat5vsRpxpGeDz9AfJyqzeyXYT4AVecxjCJWN93QyKsGRCktzWnCLWsHX5l) by Aqualectra Utility noting that “_55% of Curaçao’s power supply has been restored._” The ongoing traffic increase is in line with additional neighborhoods having power restored, with traffic returning to expected levels by around 22:00 local time (2:00 UTC on July 28).\n\n### Brazil\n\nA [widespread power outage in Brazil](https://abcnews.go.com/Business/wireStory/widespread-blackout-leaves-parts-brazil-electricity-102282449) starting at 08:30 local time (11:30 UTC) on August 15 resulted in a nominal disruption to Internet traffic within the country. Although the power outage represented a loss of approximately [27% of the total electric load](https://www.linkedin.com/pulse/bazil-blackout-15th-august-2023-member-ieee-cigre-eei) at the time it occurred, the impact to the country’s Internet traffic was much lower, as seen in the graph below. Traffic returned to expected levels by around 11:30 local time (14:30 UTC).\n\n### Kenya\n\nA “system disturbance” at 21:45 local time (18:45 UTC) on August 25 led to “loss of bulk power supply to various parts of the country” in [Kenya](https://radar.cloudflare.com/ke), according to an [X/Twitter post](https://twitter.com/CloudflareRadar/status/1695235744258838585) from [Kenya Power](https://www.kplc.co.ke/). The impact of the power outage is visible in the graph below, with traffic dropping as power is lost. Subsequent updates from Kenya Power on August 26 ([1](https://twitter.com/KenyaPower_Care/status/1695195264670183754), [2](https://twitter.com/KenyaPower_Care/status/1695229820823556483), [3](https://twitter.com/KenyaPower/status/1695353129368277431)) highlighted the progress made in restoring electricity across the country. Internet traffic from the country returned to expected levels by 03:00 on August 27 (00:00 UTC).\n\n### French Guiana\n\nAn 11-hour Internet disruption in [French Guiana](https://radar.cloudflare.com/gf) on August 27 was the result of a [power outage](https://www-franceguyane-fr.translate.goog/actualite/faitsdivers/coupure-delectricite-saint-laurent-du-maroni-et-ses-environs-retrouvent-le-courant-950591.php?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en&_x_tr_pto=wapp) caused by “_a problem that occurred at the energy evacuation station which connects Petit-Saut to the Kourou-Saint-Laurent line_”. The power outage caused a nationwide drop in Internet traffic between 11:00 local time (14:00 UTC) and 22:00 local time (01:00 UTC on August 28), visible in the graph below.\n\n### Tunisia\n\nA fire at the [Tunisian Company of Electricity and Gas](https://www.steg.com.tn/fr/index.html) power station in Rades, Ben Arous Governorate caused a [widespread power outage](https://crisis24.garda.com/alerts/2023/09/tunisia-power-outages-ongoing-nationwide-early-sept-20) in [Tunisia](https://radar.cloudflare.com/tn), resulting in an Internet disruption starting at 01:00 local time (00:00 UTC) on September 20. Traffic remained lower than expected for approximately five hours, as shown in the graph below, in line with a [published report](https://africannugget.com/new-tunisia-suffers-widespread-power-outage/) that noted “_The unexpected outage lasted for over four hours in some areas of the country._”\n\n### Barbados\n\nA September 21 [Facebook post](https://www.facebook.com/blpconline/posts/pfbid024uwgnaqzD57ca5sJ21mZcnffdHeMkPzthekwaUNWaio7PuyWJNvEVBpPquY2KAqWl) from [The Barbados Light & Power Company Limited](https://www.blpc.com.bb/) noted that the company was aware of an outage affecting customers, and that they were “_working to promptly and safely restore power in the shortest time possible._” This outage resulted in a significant drop in Internet traffic from the country starting at 11:30 local time (15:30 UTC). A subsequent [Facebook post](https://www.facebook.com/blpconline/posts/pfbid02dG4fzDSJfjS7Q2NnacvLjU6rSp52UyjPMerX9xjnJQNcEwr2KVq27twKRLaufwLEl) from the utility company at 20:00 local time (00:00 UTC on September 22) noted that power had been restored to all customers. Ahead of full power restoration, Internet traffic had returned to expected levels around 17:00 local time (21:00 UTC).\n\n## Maintenance\n\n### Guinea\n\n[La Guinéenne de la Large Bande](https://en.wikipedia.org/wiki/Guin%C3%A9enne_de_Large_Bande), also known as GUILAB, is the company responsible for managing the capacity allocated to the country of [Guinea](https://radar.cloudflare.com/gn) on the [Africa Coast to Europe (ACE)](https://www.submarinecablemap.com/submarine-cable/africa-coast-to-europe-ace) submarine cable. According to a (translation of the) [communiqué posted by the company](https://www.facebook.com/photo?fbid=1096640891463744&set=a.787672269027276) on Facebook, planned maintenance on the cable would be taking place between 22:00 on July 14 and 06:18 \"sharp\" on July 15 (22:00 on July 14 and 06:18 on July 15 UTC). This maintenance resulted in a complete Internet outage in Guinea, as seen in the graph below. It appears that the ACE submarine cable is Guinea’s sole international Internet connection, with no other backup submarine or terrestrial connectivity.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Screenshot-2023-10-23-at-18.02.31.png)\n\n### Palau\n\nJust a few days later, planned maintenance to another submarine cable took [Palau](https://radar.cloudflare.com/pw), an island country in the western Pacific, completely offline for several days. According to a [press release from the Palau National Communications Corporation (PNCC)](https://www.facebook.com/connectingpalau/posts/pfbid0213hUTxWFruhe3K211F6VRNL9iLZzV5Y3vMvJoscDgQyQeVdZ1a5ip81dkFnp5egsl) posted to their Facebook page, “_[BSCC (Belau Submarine Cable Corporation)](https://belaucable.com/) has been notified that an emergency repair will be undertaken on the [SEA – US cable network](https://www.submarinecablemap.com/submarine-cable/sea-us) in Guam from Tuesday, July 18th 7:00 a.m. Palau time, and expected to be completed 5:00 p.m. Saturday, July 22nd. … For safety reasons, repairs can only be undertaken when the cable is not powered. Since BSCC’s Palau Cable Network No 1 connects to SEA – US for onward transport to Guam, BSCC will be unable to provide service for the duration of the repair. BSCC will be unable to provide any international connectivity for Palau. The only available international connection will be via PNCC satellite connection, which will provide limited capacity compared to normal cable service._”\n\nThe graph below shows that Cloudflare did not see any appreciable traffic from Palau’s backup satellite connection during the duration of the repairs, as traffic dropped to zero at 07:00 local time on July 18 (22:00 UTC on July 17), and remained there until around 18:00 local time on July 21 (09:00 UTC), as the repairs were completed earlier than expected. A [PNCC press release](https://www.pnccpalau.com/img/pages/news/2023/2023072118/palaus-internet-service-restored-effective-july-21-2023.jpg) confirmed this early completion, noting “_PNCC is pleased to inform the public that Internet and Mobile Data services for our customers have been restored, due to the early completion today of the emergency repairs on the SEA-US Submarine Cable System, our main off-island internet connection._”\n\n## Unspecified issues\n\n### Spectrum (Charter Communications)\n\nAt 14:03 Eastern Time (18:03 UTC) on August 17, the X/Twitter support account for [Spectrum](https://www.spectrum.com/), a brand of US-based Internet service provider [Charter Communications](https://corporate.charter.com/about-charter), posted a statement that noted “_We are aware of an outage affecting customers in Alabama, Georgia and Tennessee. We apologize for the inconvenience and are working to resolve as quickly as possible. Thank you._” The graphs below show the varied impacts to traffic seen from [Spectrum (AS20115)](https://radar.cloudflare.com/as20115) across the listed states, as well as Texas, which wasn’t initially cited by Spectrum as having an issue, though customers quickly called it out.\n\nA near complete outage was observed in Tennessee between 12:30 - 14:00 local time (17:30 - 19:00 UTC), while a brief drop in traffic at 12:00 local time (17:00 UTC) and quick recovery ahead of another drop at 13:30 local time (18:30 UTC) was seen in Alabama. Georgia also saw an initial drop in traffic at 13:00 local time (17:00 UTC) ahead of a larger fall at 14:30 local time (18:30 UTC), while traffic from Texas only experienced a decline at 13:30 local time (18:30 UTC). Traffic volumes from all four impacted states recovered within several hours — approximately three hours after the initial post, Spectrum’s support account [stated](https://twitter.com/Ask_Spectrum/status/1692283651088892075) “_We have received confirmation repairs have been completed and services have been restored to affected customers in the Alabama, Georgia and Tennessee area._”\n\n![](https://blog.cloudflare.com/content/images/2023/10/Aug-17---US---Tennessee.png)\n\n![](https://blog.cloudflare.com/content/images/2023/10/Aug-17---US---Alabama.png)\n\n![](https://blog.cloudflare.com/content/images/2023/10/Aug-17---US---Georgia.png)\n\n![](https://blog.cloudflare.com/content/images/2023/10/Aug-17---US---Texas.png)\n\n### Starlink\n\nOn September 12, satellite Internet service provider [SpaceX Starlink](https://www.starlink.com/) experienced a brief but complete outage. The graph below shows traffic from [AS14593 (SPACEX-STARLINK)](https://radar.cloudflare.com/AS14593) dropping at 23:15 UTC, but quickly recovering, returning to normal within 90 minutes. At 00:33 UTC on September 13, [Starlink shared an X/Twitter post](https://twitter.com/Starlink/status/1701756057171951888) stating “_Starlink is currently in a network outage, and we are actively implementing a solution. We appreciate your patience, we'll share an update once this issue is resolved_” and just over an hour later, posted “_The network issue has been fully resolved_”.\n\n### Sky UK\n\nDuring the evening (UTC) of September 19, [numerous complaints could be found on social media](https://twitter.com/search?f=live&q=outage%20(%40skyuk)%20until%3A2023-09-20%20since%3A2023-09-19&src=typed_query) about a nationwide outage across the [United Kingdom](https://radar.cloudflare.com/gb) on [Sky Broadband (AS5607)](https://radar.cloudflare.com/as5607). A sharp drop in traffic from Sky Broadband can be seen in the graph below starting at 21:00 UTC, but a full outage did not appear to have taken place. Traffic volumes below expected levels lasted until approximately 01:00 UTC on September 20. While the issue was [acknowledged by Sky’s support account](https://twitter.com/SkyHelpTeam/status/1704416323717980310) on X/Twitter, no root cause for the disruption was ever provided.\n\n## Conclusion\n\nAs we’ve noted in past quarterly summaries, this report is intended as a summary overview of observed disruptions, and not an exhaustive or complete list of issues that have occurred during the quarter. Some disruptions not covered here were visible in our data, but never acknowledged by the impacted provider, while others were reported by industry colleagues based on their measurement methodologies, but not clearly obvious in our traffic graphs.\n\nAs we indicated above, the Cloudflare Radar Outage Center now includes information on observed traffic anomalies as well as verified outages. Interested users can subscribe to notifications for both anomalies and outages — our [blog post](https://blog.cloudflare.com/traffic-anomalies-notifications-radar/) includes more information on how to do so.\n\nVisit [Cloudflare Radar](https://radar.cloudflare.com/) for additional insights around Internet disruptions. Follow us on social media at [@CloudflareRadar](https://twitter.com/CloudflareRadar) (Twitter), [cloudflare.social/@radar](https://cloudflare.social/@radar) (Mastodon), and [radar.cloudflare.com](https://bsky.app/profile/radar.cloudflare.com) (Bluesky), or contact us via email.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Cloudflare Radar](https://blog.cloudflare.com/tag/cloudflare-radar/) [Internet Traffic](https://blog.cloudflare.com/tag/internet-traffic/) [Outage](https://blog.cloudflare.com/tag/outage/) [Internet Shutdown](https://blog.cloudflare.com/tag/internet-shutdown/) [Internet Quality](https://blog.cloudflare.com/tag/internet-quality/)"
    },
    {
      "url": "https://blog.cloudflare.com/page/142/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/page/142/",
        "loadedTime": "2023-12-05T02:29:33.547Z",
        "referrerUrl": "https://blog.cloudflare.com/",
        "depth": 1,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/page/142/",
        "title": "The Cloudflare Blog (Page 142)",
        "description": null,
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "September 21, 2010 12:25AM \nWelcome to the Team!\nLife @ Cloudflare Cloudflare History Community \nWe have a new face around Cloudflare. Damon Billian came on-board in early September and is here to focus on the Cloudflare community. We believe in building a strong community since one of the powerful things about Cloudflare is that with every new site on the system, the system gets smarter....\nSeptember 17, 2010 10:58PM \nInaugural Cloudflare T-shirts\nCloudflare History SWAG \nThere is excitement around the office today, our first Cloudflare t-shirts arrived. We can't wait to start giving them out to our beta users!...\nSeptember 02, 2010 8:38PM \nIntroducing Igor, Alex, Phil, Isla, and Vera: Five of Our Ninja Nameservers\nCloudflare History \nIf you're a big enterprise and you want performance and security you buy an expensive piece of hardware and contract with a content delivery network or highly connected data center. For the vast majority of websites online, this simply isn't an option....\nSeptember 02, 2010 12:05AM \nAnd Then There Were Three: Cloudflare's New Data Center\nData Center USA California Cloudflare Network North America \nCloudflare has expanded its data centers to three adding a new location in San Jose this week. By adding data centers to our platform, Cloudflare can deliver even faster site performance....\nJune 14, 2010 4:44PM \n24 Hours in Las Vegas!\nCloudflare History Events \nTo celebrate the milestone of getting to 100 websites in the Cloudflare Private Beta, the team took time away from the office to enjoy some fun. We headed to Las Vegas for 24 hours where we did it up right. The adventure started by enjoying Ka, a Cirque du Soleil show at the MGM....\nMarch 01, 2010 10:04PM \nWhat's in a name?\nCloudflare History Brand \nWhen Michelle, Lee and I started working on the business plan that turned into Cloudflare one of the first questions was what we should call it? We needed a placeholder so we initially chose \"Project WebWall.\"...\nJanuary 05, 2010 1:54AM \nCloudflare's New Home\nCloudflare History \nCloudflare opened its first office today in downtown Palo Alto, CA. Our team has grown from 3 people to five and we are excited to have Matthieu and Ian onboard. Matthieu and Ian are engineers that will help make Cloudflare awesome for its users....\nNovember 12, 2009 12:52AM \nTaking Cloudflare to the Next Level\nCloudflare History \nAs many of you know, we have been working on Cloudflare for several months. We are excited to announce that today we closed our Series A financing from Venrock and UV Partners (now called Pelion Ventures)....\nApril 29, 2009 12:23AM \nCloudflare: Winner of the 2009 Harvard Business School Business Plan Competition\nCloudflare History \nWe're excited to announce that we just got word that Cloudflare won the 2009 Harvard Business Plan competition. We're extremely excited and flattered by the recognition. The competition was a terrific experience for Michelle and me....",
      "markdown": "September 21, 2010 12:25AM\n\n[\n\n## Welcome to the Team!\n\n](https://blog.cloudflare.com/welcome-to-the-team/)[Life @ Cloudflare](https://blog.cloudflare.com/tag/life-at-cloudflare/) [Cloudflare History](https://blog.cloudflare.com/tag/cloudflare-history/) [Community](https://blog.cloudflare.com/tag/community/)\n\nWe have a new face around Cloudflare. Damon Billian came on-board in early September and is here to focus on the Cloudflare community. We believe in building a strong community since one of the powerful things about Cloudflare is that with every new site on the system, the system gets smarter....\n\n*   [![Michelle Zatlyn](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/Michelle-Zatlyn-2.png)](https://blog.cloudflare.com/author/michelle-zatlyn/)\n\nSeptember 17, 2010 10:58PM\n\n[\n\n## Inaugural Cloudflare T-shirts\n\n](https://blog.cloudflare.com/inaugural-cloudflare-t-shirts/)[Cloudflare History](https://blog.cloudflare.com/tag/cloudflare-history/) [SWAG](https://blog.cloudflare.com/tag/swag/)\n\nThere is excitement around the office today, our first Cloudflare t-shirts arrived. We can't wait to start giving them out to our beta users!...\n\n*   [![Michelle Zatlyn](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/Michelle-Zatlyn-2.png)](https://blog.cloudflare.com/author/michelle-zatlyn/)\n\nSeptember 02, 2010 8:38PM\n\n[\n\n## Introducing Igor, Alex, Phil, Isla, and Vera: Five of Our Ninja Nameservers\n\n](https://blog.cloudflare.com/introducing-igor-alex-phil-isla-and-vera-five/)[Cloudflare History](https://blog.cloudflare.com/tag/cloudflare-history/)\n\nIf you're a big enterprise and you want performance and security you buy an expensive piece of hardware and contract with a content delivery network or highly connected data center. For the vast majority of websites online, this simply isn't an option....\n\n*   [![Matthew Prince](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/06/Matthew-Prince-3.jpeg)](https://blog.cloudflare.com/author/matthew-prince/)\n\nSeptember 02, 2010 12:05AM\n\n[\n\n## And Then There Were Three: Cloudflare's New Data Center\n\n](https://blog.cloudflare.com/and-then-there-were-threecloudflares-new-data/)[Data Center](https://blog.cloudflare.com/tag/data-center/) [USA](https://blog.cloudflare.com/tag/usa/) [California](https://blog.cloudflare.com/tag/california/) [Cloudflare Network](https://blog.cloudflare.com/tag/cloudflare-network/) [North America](https://blog.cloudflare.com/tag/north-america/)\n\nCloudflare has expanded its data centers to three adding a new location in San Jose this week. By adding data centers to our platform, Cloudflare can deliver even faster site performance....\n\n*   [![Michelle Zatlyn](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/Michelle-Zatlyn-2.png)](https://blog.cloudflare.com/author/michelle-zatlyn/)\n\nJune 14, 2010 4:44PM\n\n[\n\n## 24 Hours in Las Vegas!\n\n](https://blog.cloudflare.com/24-hours-in-las-vegas/)[Cloudflare History](https://blog.cloudflare.com/tag/cloudflare-history/) [Events](https://blog.cloudflare.com/tag/events/)\n\nTo celebrate the milestone of getting to 100 websites in the Cloudflare Private Beta, the team took time away from the office to enjoy some fun. We headed to Las Vegas for 24 hours where we did it up right. The adventure started by enjoying Ka, a Cirque du Soleil show at the MGM....\n\n*   [![Michelle Zatlyn](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/Michelle-Zatlyn-2.png)](https://blog.cloudflare.com/author/michelle-zatlyn/)\n\nMarch 01, 2010 10:04PM\n\n[\n\n## What's in a name?\n\n](https://blog.cloudflare.com/whats-in-a-name/)[Cloudflare History](https://blog.cloudflare.com/tag/cloudflare-history/) [Brand](https://blog.cloudflare.com/tag/brand/)\n\nWhen Michelle, Lee and I started working on the business plan that turned into Cloudflare one of the first questions was what we should call it? We needed a placeholder so we initially chose \"Project WebWall.\"...\n\n*   [![Matthew Prince](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/06/Matthew-Prince-3.jpeg)](https://blog.cloudflare.com/author/matthew-prince/)\n\nJanuary 05, 2010 1:54AM\n\n[\n\n## Cloudflare's New Home\n\n](https://blog.cloudflare.com/cloudflares-new-home/)[Cloudflare History](https://blog.cloudflare.com/tag/cloudflare-history/)\n\nCloudflare opened its first office today in downtown Palo Alto, CA. Our team has grown from 3 people to five and we are excited to have Matthieu and Ian onboard. Matthieu and Ian are engineers that will help make Cloudflare awesome for its users....\n\n*   [![Michelle Zatlyn](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/Michelle-Zatlyn-2.png)](https://blog.cloudflare.com/author/michelle-zatlyn/)\n\nNovember 12, 2009 12:52AM\n\n[\n\n## Taking Cloudflare to the Next Level\n\n](https://blog.cloudflare.com/taking-cloudflare-to-the-next-level/)[Cloudflare History](https://blog.cloudflare.com/tag/cloudflare-history/)\n\nAs many of you know, we have been working on Cloudflare for several months. We are excited to announce that today we closed our Series A financing from Venrock and UV Partners (now called Pelion Ventures)....\n\n*   [![Michelle Zatlyn](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/Michelle-Zatlyn-2.png)](https://blog.cloudflare.com/author/michelle-zatlyn/)\n\nApril 29, 2009 12:23AM\n\n[\n\n## Cloudflare: Winner of the 2009 Harvard Business School Business Plan Competition\n\n](https://blog.cloudflare.com/cloudflare-winner-of-the-2009-harvard-busines/)[Cloudflare History](https://blog.cloudflare.com/tag/cloudflare-history/)\n\nWe're excited to announce that we just got word that Cloudflare won the 2009 Harvard Business Plan competition. We're extremely excited and flattered by the recognition. The competition was a terrific experience for Michelle and me....\n\n*   [![Matthew Prince](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/06/Matthew-Prince-3.jpeg)](https://blog.cloudflare.com/author/matthew-prince/)"
    },
    {
      "url": "https://blog.cloudflare.com/the-epyc-journey-continues-to-milan-in-cloudflares-11th-generation-edge-server/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/the-epyc-journey-continues-to-milan-in-cloudflares-11th-generation-edge-server/",
        "loadedTime": "2023-12-05T02:29:48.177Z",
        "referrerUrl": "https://blog.cloudflare.com/cloudflare-gen-12-server-bigger-better-cooler-in-a-2u1n-form-factor/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/the-epyc-journey-continues-to-milan-in-cloudflares-11th-generation-edge-server/",
        "title": "The EPYC journey continues to Milan in Cloudflare’s 11th generation Edge Server",
        "description": "At Cloudflare we aim to introduce a new server platform to our edge network every 12 to 18 months or so, to ensure that we keep up with the latest industry technologies and developments.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "The EPYC journey continues to Milan in Cloudflare’s 11th generation Edge Server\n08/31/2021\n10 min read\nWhen I was interviewing to join Cloudflare in 2014 as a member of the SRE team, we had just introduced our generation 4 server, and I was excited about the prospects. Since then, Cloudflare, the industry and I have all changed dramatically. The best thing about working for a rapidly growing company like Cloudflare is that as the company grows, new roles open up to enable career development. And so, having left the SRE team last year, I joined the recently formed hardware engineering team, a team that simply didn’t exist in 2014.\nWe aim to introduce a new server platform to our edge network every 12 to 18 months or so, to ensure that we keep up with the latest industry technologies and developments. We announced the generation 9 server in October 2018 and we announced the generation 10 server in February 2020. We consider this length of cycle optimal: short enough to stay nimble and take advantage of the latest technologies, but long enough to offset the time taken by our hardware engineers to test and validate the entire platform. When we are shipping servers to over 200 cities around the world with a variety of regulatory standards, it’s essential to get things right the first time.\nWe continually work with our silicon vendors to receive product roadmaps and stay on top of the latest technologies. Since mid-2020, the hardware engineering team at Cloudflare has been working on our generation 11 server.\nRequests per Watt is one of our defining characteristics when testing new hardware and we use it to identify how much more efficient a new hardware generation is than the previous generation. We continually strive to reduce our operational costs and power consumption reduction is one of the most important parts of this. It’s good for the planet and we can fit more servers into a rack, reducing our physical footprint.\nThe design of these Generation 11 x86 servers has been in parallel with our efforts to design next-generation edge servers using the Ampere Altra Arm architecture. You can read more about our tests in a blog post by my colleague Sung and we will document our work on Arm at the edge in a subsequent blog post.\nWe evaluated Intel’s latest generation of “Ice Lake” Xeon processors. Although Intel’s chips were able to compete with AMD in terms of raw performance, the power consumption was several hundred watts higher per server - that’s enormous. This meant that Intel’s Performance per Watt was unattractive.\nWe previously described how we had deployed AMD EPYC 7642’s processors in our generation 10 server. This has 48 cores and is based on AMD’s 2nd generation EPYC architecture, code named Rome. For our generation 11 server, we evaluated 48, 56 and 64 core samples based on AMD’s 3rd generation EPYC architecture, code named Milan. We were interested to find that comparing the two 48 core processors directly, we saw a performance boost of several percent in the 3rd generation EPYC architecture. We therefore had high hopes for the 56 core and 64 core chips.\nSo, based on the samples we received from our vendors and our subsequent testing, hardware from AMD and Ampere made the shortlist for our generation 11 server. On this occasion, we decided that Intel did not meet our requirements. However, it’s healthy that Intel and AMD compete and innovate in the x86 space and we look forward to seeing how Intel’s next generation shapes up.\nTesting and validation process\nBefore we go on to talk about the hardware, I’d like to say a few words about the testing process we went through to test out generation 11 servers.\nAs we elected to proceed with AMD chips, we were able to use our generation 10 servers as our Engineering Validation Test platform, with the only changes being the new silicon and updated firmware. We were able to perform these upgrades ourselves in our hardware validation lab.\nCloudflare’s network is built with commodity hardware and we source the hardware from multiple vendors, known as ODMs (Original Design Manufacturer) who build the servers to our specifications.\nWhen you are working with bleeding edge silicon and experimental firmware, not everything is plain sailing. We worked with one of our ODMs to eliminate an issue which was causing the Linux kernel to panic on boot. Once resolved, we used a variety of synthetic benchmarking tools to verify the performance including cf_benchmark, as well as an internal tool which applies a synthetic load to our entire software stack.\nOnce we were satisfied, we ordered Design Validation Test samples, which were manufactured by our ODMs with the new silicon. We continued to test these and iron out the inevitable issues that arise when you are developing custom hardware. To ensure that performance matched our expectations, we used synthetic benchmarking to test the new silicon. We also began testing it in our production environment by gradually introducing customer traffic to them as confidence grew.\nOnce the issues were resolved, we ordered the Product Validation Test samples, which were again manufactured by our ODMs, taking into account the feedback obtained in the DVT phase. As these are intended to be production grade, we work with the broader Cloudflare teams to deploy these units like a mass production order.\nCPU\nPreviously: AMD EPYC 7642 48-Core Processor\nNow: AMD EPYC 7713 64-Core Processor\nAMD EPYC 7642 AMD EPYC 7643 AMD EPYC 7663 AMD EPYC 7713 \nStatus\tIncumbent\tCandidate\tCandidate\tCandidate\t\nCore Count\t48\t48\t56\t64\t\nThread Count\t96\t96\t112\t128\t\nBase Clock\t2.3GHz\t2.3GHz\t2.0GHz\t2.0GHz\t\nMax Boost Clock\t3.3GHz\t3.6GHz\t3.5GHz\t3.675GHz\t\nTotal L3 Cache\t256MB\t256MB\t256MB\t256MB\t\nDefault TDP\t225W\t225W\t240W\t225W\t\nConfigurable TDP\t240W\t240W\t240W\t240W\t\nIn the above chart, TDP refers to Thermal Design Power, a measure of the heat dissipated. All of the above processors have a configurable TDP - assuming the cooling solution is capable - giving more performance at the expense of increased power consumption. We tested all processors configured at their highest supported TDP.\nThe 64 core processors have 33% more cores than the 48 core processors so you might hypothesize that we would see a corresponding 33% increase in performance, although our benchmarks saw slightly more modest gains. This can be explained because the 64 core processors have lower base clock frequencies to fit within the same 225W power envelope.\nIn production testing, we found that the 64 core EPYC 7713 gave us around a 29% performance boost over the incumbent, whilst having similar power consumption and thermal properties. \nMemory\nPreviously: 256GB DDR4-2933\nNow: 384GB DDR4-3200\nHaving made a decision about the processor, the next step was to determine the optimal amount of memory for our workload. We ran a series of experiments with our chosen EPYC 7713 processor and 256GB, 384GB and 512GB memory configurations. We started off by running synthetic benchmarks with tools such as STREAM to ensure that none of the configurations performed unexpectedly poorly and to generate a baseline understanding of the performance.\nAfter the synthetic benchmarks, we proceeded to test the various configurations with production workloads to empirically determine the optimal quantity. We use Prometheus and Grafana to gather and display a rich set of metrics from all of our servers so that we can monitor and spot trends, and we re-used the same infrastructure for our performance analysis.\nAs well as measuring available memory, previous experience has shown us that one of the best ways to ensure that we have enough memory is to observe request latency and disk IO performance. If there is insufficient memory, we expect to see request latency and disk IO volume and latency to increase. The reason for this is that our core HTTP server uses memory to cache web assets and if there is insufficient memory the assets will be ejected from memory prematurely and more assets will be fetched from disk instead of memory, degrading performance.\nLike most things in life, it’s a balancing act. We want enough memory to take advantage of the fact that serving web assets directly from memory is much faster than even the best NVMe disks. We also want to future proof our platform to enable the new features such as the ones that we recently announced in security week and developer week. However, we don’t want to spend unnecessarily on excess memory that will never be used. We found that the 512GB configuration did not provide a performance boost to justify the extra cost and settled on the 384GB configuration.\nWe also tested the performance impact of switching from DDR4-2933 to DDR4-3200 memory. We found that it provided a performance boost of several percent and the pricing has improved to the point where it is cost beneficial to make the change.\nDisk\nPreviously: 3x Samsung PM983 x 960GB\nNow: 2x Samsung PM9A3 x 1.92TB\nWe validated samples by studying the manufacturer’s data sheets and testing using fio to ensure that the results being obtained in our test environment were in line with the published specifications. We also developed an automation framework to help compare different drive models using fio. The framework helps us to restore the drives close to factory settings, precondition the drives, perform the sequential and random tests in our environment, and analyze the data results to evaluate the bandwidth and latency results. Since our SSD samples were arriving in our test center at different months, having an automated framework helped in dealing with speedy evaluations by reducing our time spent testing and doing analysis.\nFor Gen 11 we decided to move to a 2x 2TB configuration from the original 3x 1TB configuration giving us an extra 1 TB of storage. This also meant we could use the higher performance of a 2TB drive and save around 6W of power since there is one less SSD. \nAfter analyzing the performances of various 2TB drives, their latencies and endurances, we chose Samsung’s PM9A3 SSDs as our Gen11 drives. The results we obtained below were consistent with the manufacturer's claims.\nSequential performance:\nRandom Performance:\nCompared to our previous generation drives, we could see a 1.5x - 2x improvement in read and write bandwidths. The higher values for the PM9A3 can be attributed to the fact that these are PCIe 4.0 drives, have more intelligent SSD controllers and an upgraded NAND architecture.\nNetwork\nPreviously: Mellanox ConnectX-4 dual-port 25G\nNow: Mellanox ConnectX-4 dual-port 25G\nThere is no change on the network front; the Mellanox ConnectX-4 is a solid performer which continues to meet our needs. We investigated higher speed Ethernet, but we do not currently see this as beneficial. Cloudflare’s network is built on cheap commodity hardware and the highly distributed nature of Cloudflare’s network means we don’t have discrete DDoS scrubbing centres. All points of presence operate as scrubbing centres. This means that we distribute the load across our entire network and do not need to employ higher speed and more expensive Ethernet devices.\nOpen source firmware\nTransparency, security and integrity is absolutely critical to us at Cloudflare. Last year, we described how we had deployed Platform Secure Boot to create trust that we were running the software that we thought we were.\nNow, we are pleased to announce that we are deploying open source firmware to our servers using OpenBMC. With access to the source code, we have been able to configure BMC features such as the fan PID controller, having BIOS POST codes recorded and accessible, and managing networking ports and devices. Prior to OpenBMC, requesting these features from our vendors led to varying results and misunderstandings of the scope and capabilities of the BMC. After working with the BMC source code much more directly, we have the flexibility to work on features ourselves to our liking, or understand why the BMC is incapable of running our desired software.\nWhilst our current BMC is an industry standard, we feel that OpenBMC better suits our needs and gives us advantages such as allowing us to deal with upstream security issues without a dependency on our vendors. Some opportunities with security include integration of desired authentication modules, usage of specific software packages, staying up to date with the latest Linux kernel, and controlling a variety of attack vectors. Because we have a kernel lockdown implemented, flashing tooling is difficult to use in our environment. With access to source code of the flashing tools, we have an understanding of what the tools need access to, and assess whether or not this meets our standard of security.\nSummary\nThe jump between our generation 9 and generation 10 servers was enormous. To summarise, we changed from a dual-socket Intel platform to a single socket AMD platform. We upgraded the SATA SSDs to NVMe storage devices, and physically the multi-node chassis changed to a 1U form factor.\nAt the start of the generation 11 project we weren’t sure if we would be making such radical changes again. However, after a thorough testing of the latest chips and a review of how well the generation 10 server has performed in production for over a year, our generation 11 server built upon the solid foundations of generation 10 and ended up as a refinement rather than total revamp. Despite this, and bearing in mind that performance varies by time of day and geography, we are pleased that generation 11 is capable of serving approximately 29% more requests than generation 10 without an increase in power consumption.\nThanks to Denny Mathew and Ryan Chow’s work on benchmarking and OpenBMC, respectively.\nIf you are interested in working with bleeding edge hardware, open source server firmware, solving interesting problems, helping to improve our performance, and are interested in helping us work on our generation 12 server platform (amongst many other things!), we’re hiring.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nCloudflare Network EPYC AMD Hardware",
      "markdown": "## The EPYC journey continues to Milan in Cloudflare’s 11th generation Edge Server\n\n08/31/2021\n\n*   [![Chris Howells](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/08/photo.jpg)](https://blog.cloudflare.com/author/chris-howells/)\n\n10 min read\n\n![](https://blog.cloudflare.com/content/images/2021/08/Gen-11.png)\n\nWhen I was interviewing to join Cloudflare in 2014 as a member of the SRE team, we had just introduced our [generation 4 server](https://blog.cloudflare.com/a-tour-inside-cloudflares-latest-generation-servers/), and I was excited about the prospects. Since then, Cloudflare, the industry and I have all changed dramatically. The best thing about working for a rapidly growing company like Cloudflare is that as the company grows, new roles open up to enable career development. And so, having left the SRE team last year, I joined the recently formed hardware engineering team, a team that simply didn’t exist in 2014.\n\nWe aim to introduce a new server platform to our edge network every 12 to 18 months or so, to ensure that we keep up with the latest industry technologies and developments. We announced the [generation 9 server](https://blog.cloudflare.com/a-tour-inside-cloudflares-g9-servers/) in October 2018 and we announced the [generation 10 server](https://blog.cloudflare.com/a-tour-inside-cloudflares-g9-servers/) in February 2020. We consider this length of cycle optimal: short enough to stay nimble and take advantage of the latest technologies, but long enough to offset the time taken by our hardware engineers to test and validate the entire platform. When we are [shipping servers to over 200 cities](https://www.cloudflare.com/en-gb/network/) around the world with a variety of regulatory standards, it’s essential to get things right the first time.\n\nWe continually work with our silicon vendors to receive product roadmaps and stay on top of the latest technologies. Since mid-2020, the hardware engineering team at Cloudflare has been working on our generation 11 server.\n\nRequests per Watt is one of our defining characteristics when testing new hardware and we use it to identify how much more efficient a new hardware generation is than the previous generation. We continually strive to reduce our operational costs and [power consumption reduction](https://blog.cloudflare.com/the-climate-and-cloudflare/) is one of the most important parts of this. It’s good for the planet and we can fit more servers into a rack, reducing our physical footprint.\n\nThe design of these Generation 11 x86 servers has been in parallel with our efforts to design next-generation edge servers using the [Ampere Altra](https://blog.cloudflare.com/arms-race-ampere-altra-takes-on-aws-graviton2/) Arm architecture. You can read more about our tests in a blog post by my colleague Sung and we will document our work on Arm at the edge in a subsequent blog post.\n\nWe evaluated Intel’s latest generation of “Ice Lake” Xeon processors. Although Intel’s chips were able to compete with AMD in terms of raw performance, the power consumption was several hundred watts higher per server - that’s enormous. This meant that Intel’s Performance per Watt was unattractive.\n\nWe previously described how we had deployed AMD EPYC 7642’s processors in our generation 10 server. This has 48 cores and is based on AMD’s 2nd generation EPYC architecture, code named Rome. For our generation 11 server, we evaluated 48, 56 and 64 core samples based on AMD’s 3rd generation EPYC architecture, code named Milan. We were interested to find that comparing the two 48 core processors directly, we saw a performance boost of several percent in the [3rd generation EPYC](https://www.amd.com/en/events/epyc) architecture. We therefore had high hopes for the 56 core and 64 core chips.\n\nSo, based on the samples we received from our vendors and our subsequent testing, hardware from AMD and Ampere made the shortlist for our generation 11 server. On this occasion, we decided that Intel did not meet our requirements. However, it’s healthy that Intel and AMD compete and innovate in the x86 space and we look forward to seeing how Intel’s next generation shapes up.\n\n![](https://blog.cloudflare.com/content/images/2021/08/IMG_4118.jpeg)\n\n### Testing and validation process\n\nBefore we go on to talk about the hardware, I’d like to say a few words about the testing process we went through to test out generation 11 servers.\n\nAs we elected to proceed with AMD chips, we were able to use our generation 10 servers as our Engineering Validation Test platform, with the only changes being the new silicon and updated firmware. We were able to perform these upgrades ourselves in our hardware validation lab.\n\nCloudflare’s network is built with commodity hardware and we source the hardware from multiple vendors, known as ODMs (Original Design Manufacturer) who build the servers to our specifications.\n\nWhen you are working with bleeding edge silicon and experimental firmware, not everything is plain sailing. We worked with one of our ODMs to eliminate an issue which was causing the Linux kernel to panic on boot. Once resolved, we used a variety of synthetic benchmarking tools to verify the performance including [cf\\_benchmark](https://github.com/cloudflare/cf_benchmark), as well as an internal tool which applies a synthetic load to our entire software stack.\n\nOnce we were satisfied, we ordered Design Validation Test samples, which were manufactured by our ODMs with the new silicon. We continued to test these and iron out the inevitable issues that arise when you are developing custom hardware. To ensure that performance matched our expectations, we used synthetic benchmarking to test the new silicon. We also began testing it in our production environment by gradually introducing customer traffic to them as confidence grew.\n\nOnce the issues were resolved, we ordered the Product Validation Test samples, which were again manufactured by our ODMs, taking into account the feedback obtained in the DVT phase. As these are intended to be production grade, we work with the broader Cloudflare teams to deploy these units like a mass production order.\n\n![](https://blog.cloudflare.com/content/images/2021/08/20210722_113627.jpeg)\n\n### CPU\n\nPreviously: AMD EPYC 7642 48-Core Processor  \nNow: AMD EPYC 7713 64-Core Processor\n\n![](https://blog.cloudflare.com/content/images/2021/08/IMG_4203.jpeg)\n\n|     | [AMD EPYC 7642](https://www.amd.com/en/products/cpu/amd-epyc-7642) | [AMD EPYC 7643](https://www.amd.com/en/products/cpu/amd-epyc-7643) | [AMD EPYC 7663](https://www.amd.com/en/products/cpu/amd-epyc-7663) | [AMD EPYC 7713](https://www.amd.com/en/products/cpu/amd-epyc-7713) |\n| --- | --- | --- | --- | --- |\n| Status | Incumbent | Candidate | Candidate | Candidate |\n| Core Count | 48  | 48  | 56  | 64  |\n| Thread Count | 96  | 96  | 112 | 128 |\n| Base Clock | 2.3GHz | 2.3GHz | 2.0GHz | 2.0GHz |\n| Max Boost Clock | 3.3GHz | 3.6GHz | 3.5GHz | 3.675GHz |\n| Total L3 Cache | 256MB | 256MB | 256MB | 256MB |\n| Default TDP | 225W | 225W | 240W | 225W |\n| Configurable TDP | 240W | 240W | 240W | 240W |\n\nIn the above chart, TDP refers to Thermal Design Power, a measure of the heat dissipated. All of the above processors have a configurable TDP - assuming the cooling solution is capable - giving more performance at the expense of increased power consumption. We tested all processors configured at their highest supported TDP.\n\nThe 64 core processors have 33% more cores than the 48 core processors so you might hypothesize that we would see a corresponding 33% increase in performance, although our benchmarks saw slightly more modest gains. This can be explained because the 64 core processors have lower base clock frequencies to fit within the same 225W power envelope.\n\nIn production testing, we found that the 64 core EPYC 7713 gave us around a 29% performance boost over the incumbent, whilst having similar power consumption and thermal properties.\n\n![](https://blog.cloudflare.com/content/images/2021/08/IMG_4196.jpeg)\n\n## Memory\n\n![](https://blog.cloudflare.com/content/images/2021/08/_MG_4100.jpeg)\n\nPreviously: 256GB DDR4-2933  \nNow: 384GB DDR4-3200\n\nHaving made a decision about the processor, the next step was to determine the optimal amount of memory for our workload. We ran a series of experiments with our chosen EPYC 7713 processor and 256GB, 384GB and 512GB memory configurations. We started off by running synthetic benchmarks with tools such as [STREAM](https://www.cs.virginia.edu/stream/) to ensure that none of the configurations performed unexpectedly poorly and to generate a baseline understanding of the performance.\n\nAfter the synthetic benchmarks, we proceeded to test the various configurations with production workloads to empirically determine the optimal quantity. We use Prometheus and Grafana to gather and display a rich set of metrics from all of our servers so that we can monitor and spot trends, and we re-used the same infrastructure for our performance analysis.\n\nAs well as measuring available memory, previous experience has shown us that one of the best ways to ensure that we have enough memory is to observe request latency and disk IO performance. If there is insufficient memory, we expect to see request latency and disk IO volume and latency to increase. The reason for this is that our core HTTP server uses memory to cache web assets and if there is insufficient memory the assets will be ejected from memory prematurely and more assets will be fetched from disk instead of memory, degrading performance.\n\nLike most things in life, it’s a balancing act. We want enough memory to take advantage of the fact that serving web assets directly from memory is much faster than even the best NVMe disks. We also want to future proof our platform to enable the new features such as the ones that we recently announced in [security week](https://blog.cloudflare.com/tag/security-week/) and [developer week](https://blog.cloudflare.com/tag/developer-week/). However, we don’t want to spend unnecessarily on excess memory that will never be used. We found that the 512GB configuration did not provide a performance boost to justify the extra cost and settled on the 384GB configuration.\n\nWe also tested the performance impact of switching from DDR4-2933 to DDR4-3200 memory. We found that it provided a performance boost of several percent and the pricing has improved to the point where it is cost beneficial to make the change.\n\n## Disk\n\nPreviously: 3x Samsung PM983 x 960GB  \nNow: 2x Samsung PM9A3 x 1.92TB\n\n![](https://blog.cloudflare.com/content/images/2021/08/20210722_113829.jpeg)\n\nWe validated samples by studying the manufacturer’s data sheets and [testing using fio](https://fio.readthedocs.io/en/latest/fio_doc.html) to ensure that the results being obtained in our test environment were in line with the published specifications. We also developed an automation framework to help compare different drive models using fio. The framework helps us to restore the drives close to factory settings, precondition the drives, perform the sequential and random tests in our environment, and analyze the data results to evaluate the bandwidth and latency results. Since our SSD samples were arriving in our test center at different months, having an automated framework helped in dealing with speedy evaluations by reducing our time spent testing and doing analysis.\n\nFor Gen 11 we decided to move to a 2x 2TB configuration from the original 3x 1TB configuration giving us an extra 1 TB of storage. This also meant we could use the higher performance of a 2TB drive and save around 6W of power since there is one less SSD.\n\nAfter analyzing the performances of various 2TB drives, their latencies and endurances, we chose Samsung’s PM9A3 SSDs as our Gen11 drives. The results we obtained below were consistent with the manufacturer's claims.\n\nSequential performance:\n\n![](https://blog.cloudflare.com/content/images/2021/08/pasted-image-0-5.png)\n\n![](https://blog.cloudflare.com/content/images/2021/08/pasted-image-0--1--2.png)\n\nRandom Performance:\n\n![](https://blog.cloudflare.com/content/images/2021/08/pasted-image-0--2-.png)\n\n![](https://blog.cloudflare.com/content/images/2021/08/pasted-image-0--3-.png)\n\nCompared to our previous generation drives, we could see a 1.5x - 2x improvement in read and write bandwidths. The higher values for the PM9A3 can be attributed to the fact that these are PCIe 4.0 drives, have more intelligent SSD controllers and an upgraded NAND architecture.\n\n### Network\n\nPreviously: Mellanox ConnectX-4 dual-port 25G  \nNow: Mellanox ConnectX-4 dual-port 25G\n\nThere is no change on the network front; the Mellanox ConnectX-4 is a solid performer which continues to meet our needs. We investigated higher speed Ethernet, but we do not currently see this as beneficial. Cloudflare’s network is built on cheap commodity hardware and the highly distributed nature of Cloudflare’s network means we don’t have discrete DDoS scrubbing centres. All points of presence operate as scrubbing centres. This means that we distribute the load across our entire network and do not need to employ higher speed and more expensive Ethernet devices.\n\n### Open source firmware\n\nTransparency, security and integrity is absolutely critical to us at Cloudflare. Last year, we described how we had [deployed Platform Secure Boot](https://blog.cloudflare.com/anchoring-trust-a-hardware-secure-boot-story/) to create trust that we were running the software that we thought we were.\n\nNow, we are pleased to announce that we are deploying open source firmware to our servers using OpenBMC. With access to the source code, we have been able to configure BMC features such as the fan PID controller, having BIOS POST codes recorded and accessible, and managing networking ports and devices. Prior to OpenBMC, requesting these features from our vendors led to varying results and misunderstandings of the scope and capabilities of the BMC. After working with the BMC source code much more directly, we have the flexibility to work on features ourselves to our liking, or understand why the BMC is incapable of running our desired software.\n\nWhilst our current BMC is an industry standard, we feel that OpenBMC better suits our needs and gives us advantages such as allowing us to deal with upstream security issues without a dependency on our vendors. Some opportunities with security include integration of desired authentication modules, usage of specific software packages, staying up to date with the latest Linux kernel, and controlling a variety of attack vectors. Because we have a kernel lockdown implemented, flashing tooling is difficult to use in our environment. With access to source code of the flashing tools, we have an understanding of what the tools need access to, and assess whether or not this meets our standard of security.\n\n### Summary  \n\n![](https://blog.cloudflare.com/content/images/2021/08/IMG_4195.jpeg)\n\nThe jump between our generation 9 and generation 10 servers was enormous. To summarise, we changed from a dual-socket Intel platform to a single socket AMD platform. We upgraded the SATA SSDs to NVMe storage devices, and physically the multi-node chassis changed to a 1U form factor.\n\nAt the start of the generation 11 project we weren’t sure if we would be making such radical changes again. However, after a thorough testing of the latest chips and a review of how well the generation 10 server has performed in production for over a year, our generation 11 server built upon the solid foundations of generation 10 and ended up as a refinement rather than total revamp. Despite this, and bearing in mind that performance varies by time of day and geography, we are pleased that generation 11 is capable of serving approximately 29% more requests than generation 10 without an increase in power consumption.\n\n![](https://blog.cloudflare.com/content/images/2021/08/Screenshot-2021-06-22-at-15.27.27.png)\n\nThanks to Denny Mathew and Ryan Chow’s work on benchmarking and OpenBMC, respectively.\n\nIf you are interested in working with bleeding edge hardware, open source server firmware, solving interesting problems, helping to improve our performance, and are interested in helping us work on our generation 12 server platform (amongst many other things!), [we’re hiring](https://www.cloudflare.com/en-gb/careers/jobs/?department=Infrastructure&location=default).\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Cloudflare Network](https://blog.cloudflare.com/tag/cloudflare-network/) [EPYC](https://blog.cloudflare.com/tag/epyc/) [AMD](https://blog.cloudflare.com/tag/amd/) [Hardware](https://blog.cloudflare.com/tag/hardware/)"
    },
    {
      "url": "https://blog.cloudflare.com/a-tour-inside-cloudflares-g9-servers/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/a-tour-inside-cloudflares-g9-servers/",
        "loadedTime": "2023-12-05T02:29:57.455Z",
        "referrerUrl": "https://blog.cloudflare.com/cloudflare-gen-12-server-bigger-better-cooler-in-a-2u1n-form-factor/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/a-tour-inside-cloudflares-g9-servers/",
        "title": "A Tour Inside Cloudflare's G9 Servers",
        "description": "From a G4 server comprising of 12 Intel Sandybridge CPU cores, our G9 server has 192 Intel Skylake CPU cores ready to handle today’s Cloudflare’s network.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/10/2018\n9 min read\nCloudflare operates at a significant scale, handling nearly 10% of the Internet HTTP requests that is at peak more than 25 trillion requests through our network every month. To ensure this is as efficient as possible, we own and operate all the equipment in our 154 locations around the world in order to process the volume of traffic that flows through our network. We spend a significant amount of time specing and designing servers that makes up our network to meet our ever changing and growing demands. On regular intervals, we will take everything we've learned about our last generation of hardware and refresh each component with the next generation…\nIf the above paragraph sounds familiar, it’s a reflecting glance to where we were 5 years ago using today’s numbers. We’ve done so much progress engineering and developing our tools with the latest tech through the years by pushing ourselves at getting smarter in what we do.\nHere though we’re going to blog about muscle.\nSince the last time we blogged about our G4 servers, we’ve iterated one generation each of the past 5 years. Our latest generation is now the G9 server. From a G4 server comprising 12 Intel Sandybridge CPU cores, our G9 server has 192 Intel Skylake CPU cores ready to handle today’s load across Cloudflare’s network. This server is QCT’s T42S-2U multi-node server where we have 4 nodes per chassis, therefore each node has 48 cores. Maximizing compute density is the primary goal since rental colocation space and power are costly. This 2U4N chassis form factor has served us well for the past 3 generations, we’re revisiting this option once more.\nExploded picture of the G9 server’s main components. 4 sleds represent 4 nodes, each with 2 24-core Intel CPUs\nEach high-level hardware component has gone through their own upgrade as well for a balanced scale up keeping our stack CPU bound, making this generation the most radical revision since we moved on from using HP 5 years ago. Let’s glance through each of those components.\nHardware Changes\nCPU\nPreviously: 2x 12 core Intel Xeon Silver 4116 2.1Ghz 85W\nNow: 2x 24 core Intel custom off-roadmap 1.9Ghz 150W\nThe performance of our infrastructure is heavily directed by how much compute we can squeeze in a given physical space and power. In essence, requests per second (RPS) per Watt is a critical metric that Qualcomm’s ARM64 46 core Falkor chip had a big advantage over Intel’s Skylake 4116.\nIntel proposed to co-innovate with us an off-roadmap 24-core Xeon Gold CPU specifically made for our workload offering considerable value in Performance per Watt. For this generation, we continue using Intel as system solutions are widely available while we’re working on realizing ARM64’s benefits to production. We expect this CPU to perform with better RPS per Watt right off the bat; increasing the RPS by 200% from doubling the amount of cores, and increasing the power consumption by 174% from increasing the CPUs TDP from 85W to 150W each.\nDisk\nPreviously: 6x Micron 1100 512G SATA SSD\nNow: 6x Intel S4500 480G SATA SSD\nWith all the requests we foresee for G9 to process, we need to tame down the outlying and long-tail latencies we have seen in our previous SSDs. Lowering p99 and p999 latency has been a serious endeavor. To help save milliseconds in disk response time for 0.01% or even 0.001% of all the traffic we see isn’t a joke!\nDatacenter grade SSDs in Intel S4500 will proliferate our fleet. These disks come with better endurance to last over the expected service life of our servers and better performance consistency with lower p95+ latency.\nNetwork\nPreviously: dual-port 10G Solarflare Flareon Ultra SFN8522\nNow: dual-port 25G Mellanox ConnectX-4 Lx OCP\nOur DDoS mitigation program is all done in userspace, so network adapter model can be anything on market as long as it supports XDP. We went with Mellanox for their solid reliability and their readily available 2x25G CX4 model. Upgrading to 25G intra-rack ethernet network is easy future-proofing since the 10G SFP+ ethernet port shares the same physical form factor as the 25G’s SFP28. Switch and NIC vendors offer models that can be configured as either 10G or 25G.\nAnother change is the adapter’s form factor itself being an OCP mezzanine instead of the more conventional Low Profile sized card. QCT is a server system integrator participating in the Open Compute Project, a non-profit organization establishing an open source hardware ecosystem founded by Facebook, Intel, and Rackspace. Their T42S-2U motherboards each include 2 PCIe x16 Gen3 expansion slots: 1 fit for a regular I/O card and 1 for an OCP mezzanine. The form factor change allows us to occupy the OCP slot leaving the regular slot free to integrate something else that may not be offered with an OCP form factor like a high capacity NVMe SSD or a GPU. We like that our server has the room for upgrades if needed.\nBoth Low Profile and OCP adapters offer the same throughput and features\nRear side of G9 chassis showing all 4 sled nodes, each leaving room to add on a PCI card\nMemory\nPreviously: 192G (12x16G) DDR4 2400Mhz RDIMM\nNow: 256G (8x32G) DDR4 2666Mhz RDIMM\nGoing from 192G (12x16G) to 256G (8x32G) made practical sense. The motherboard has 12 DIMM channels, which were all populated in the G8. We want to have the ability to upgrade just in case, as well as keeping memory configuration balanced and at optimal bandwidth capacity. 8x32G works well leaving 4 channels open for future upgrades.\nPhysical stress test\nOur software stack scales nicely enough that we can confidently assume we’ll double the amount of requests having twice the amount of CPU cores compared to G8. What we need to ensure before we ship any G9 servers out to our current 154 and future PoPs is that there won’t be any design issues pertaining to thermal nor power failures. At the extreme case that all of our cores run up 100% load, would that cause our server to run above operating temperature? How much power would a whole server with 192 cores totaling 1200W TDP consume? We set out to record both by applying a stress test to the whole system.\nTemperature readings were recorded off of ipmitool sdr list, then graphed showing socket and motherboard temperature. For 2U4N being such a compact form factor, it’s worth monitoring that a server running hot isn’t literally running hot. The red lines represent the 4 nodes that compose the whole G9 server under test; blue lines represent G8 nodes (we didn’t stress the G8’s so their temperature readings are constant).\nBoth graphs are looking fine and not out of control mostly thanks to the T42S-2U’s 4 80mm x 80mm fans capable of blowing over 90CFM; which we managed to reach their max spec RPM.\nRecording the new system’s max power consumption is critical information we need to properly design our rack stack choosing the right Power Distribution Unit and ensuring we’re below the budgeted power while keeping adequate phase balancing. For example, a typical 3-phase US-rated 24-Amp PDU gives you a max power rating of 8.6 kilowatts. We wouldn’t be able to fit 9 servers powered by that same PDU if each were running at 1kW without any way to cap them. \nAbove right graph shows our max power to be 1.9kW as the red line, or crudely 475W per node which is excellent in a modern server. Notice the blue and yellow lines representing the G9’s 2 power supplies summing the total power. The yellow line PSU appearing off is intentional as part of our testing procedure to show the PSU’s resilience in abrupt power changes.\nStressing out all available CPU, I/O, and memory along with maxing out fan RPMs combined is a good indicator for the highest possible heat and power draw this server can do. Hopefully we won’t ever see such an extreme case like this in live production environments, and we expect much milder actual results (read: we don’t think catastrophic failures to be possible). \nFirst Impression in live production\nWe increased capacity to one of our most loaded PoPs by adding G9 servers. The following time graphs represent a 24 hour range with how G9 performance compares with G8 in a live PoP.\nGreat! They're doing over 2x the requests compared to G8 with about 10% less CPU usage. Note that all results here are based from non-optimized systems, so we could add more load on the G9 and have their CPU usage comparable to the G8. Additionally, they're doing that amount with better CPU processing time shown as nginx execution time. You can see the latency gap between generations widening as we go towards the 99.9th percentile:\nLong-tail latencies for NGINX CPU processing time (lower is better)\nTalking about latency, let’s check how our new SSDs are doing on that front:\nCache disk IOPS and latency (lower is better)\nThe trend still holds that G9 is doing better. It’s a good thing that the G9’s SSDs aren’t seeing as many IOPS since it means we’re not hitting cache disks as often and are able to store and process more on CPU and memory. We’ve cut the read cache hits and latency by half. Less writes results in better performance consistency and longevity.\nAnother metric that G9 does more is power consumption, doing about 55% more than the G8. While it’s not a piece of information to brag about, it is expected when older CPUs were once rated at 85W TDP to now using ones with 150W TDP; and when considering how much work the G9 servers do:\nG9 is actually 1.5x more power efficient than G8. Temperature readings were checked as well. Inlet and outlet chassis temps, as well as CPU temps, are well within operating temperatures.\nNow that’s muscle! In other words for every 3 G8 servers, just 2 of those G9's would take on the same workload. If one of our racks normally would have 9 G8 servers, we can switch those out with only 6 G9's. Inversely planning to turn up a cage of 10 G9 racks would be the same as if we did 15 G8 racks! \nWe have big plans to cover our entire network with G9 servers, with most of them planned for the existing cities your site most likely uses. By 2019, you’ll benefit with increased bandwidth and lower wait times. And we’ll benefit in expanding and turning up datacenters quicker and more efficiently.\nWhat's next?\nGen X? Right now is exciting times at Cloudflare. Many teams and engineers are testing, porting, and implementing new stuff that can help us lower operating costs, explore new products and possibilities, and improve Quality of Service. We’re tackling problems and taking on projects that are unique in the industry.\nServerless computing like Cloudflare Workers and beyond will ask for new challenges to our infrastructure as all of our customers can program their features on Cloudflare’s edge network. \nThe network architecture that was conventionally made up of routers, switches, and servers has been merged into 3-in-1 box solutions allowing Cloudflare services to be set up into locations that weren’t possible before.\nThe advent of NVMe and persistent memory, as well as the possibility of turning SSDs into DRAM, is redefining how we design cache servers and handle tiered caching. SSDs and memory aren’t treated as separate entities like they used to.\nHardware brings the company together like a rug in a living room. See how many links I mentioned above to show you how we’re one team dedicated to build a better Internet. Everything that we do here roots down to how we manage the tons of aluminum and silicon we’ve invested. There's a lot here to develop our hardware to help Cloudflare grow to where we envision ourselves to be. If you’d like to contribute, we’d love to hear from you. \nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nSpeed & Reliability Network Hardware",
      "markdown": "10/10/2018\n\n*   [![Rob Dinh](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2020/12/Screen-Shot-2020-12-08-at-07.04.53.png)](https://blog.cloudflare.com/author/robdinh/)\n\n9 min read\n\nCloudflare operates at a significant scale, handling nearly 10% of the Internet HTTP requests that is at peak more than 25 trillion requests through our network every month. To ensure this is as efficient as possible, we own and operate all the equipment in our [154 locations around the world](https://www.cloudflare.com/network-map) in order to process the volume of traffic that flows through our network. We spend a significant amount of time specing and designing servers that makes up our network to meet our ever changing and growing demands. On regular intervals, we will take everything we've learned about our last generation of hardware and refresh each component with the next generation…\n\nIf the above paragraph sounds familiar, it’s a reflecting glance to where we were [5 years ago](https://blog.cloudflare.com/a-tour-inside-cloudflares-latest-generation-servers/) using today’s numbers. We’ve done so much progress engineering and developing our tools with the latest tech through the years by pushing ourselves at getting smarter in what we do.\n\nHere though we’re going to blog about muscle.\n\nSince the last time we blogged about our G4 servers, we’ve iterated one generation each of the past 5 years. Our latest generation is now the G9 server. From a G4 server comprising 12 Intel Sandybridge CPU cores, our G9 server has 192 Intel Skylake CPU cores ready to handle today’s load across Cloudflare’s network. This server is [QCT’s T42S-2U multi-node server](https://www.qct.io/product/index/Server/rackmount-server/Multi-node-Server/QuantaPlex-T42S-2U-4Node) where we have 4 nodes per chassis, therefore each node has 48 cores. Maximizing compute density is the primary goal since rental colocation space and power are costly. This 2U4N chassis form factor has served us well for the past 3 generations, we’re revisiting this option once more.\n\n![](https://lh3.googleusercontent.com/MSUVbbiNbqCozo5hb0cnaWlQeUUxq3JdbJoZvUrGnY0ly9unm8JfFMKeTcKHp0P49HmMHdJ8f1Fvw7lHu8R1FmEQKKpbUjtP4qdkWMviOf0-NKmksk_Uy_7rFEhGxk2muRYO68Me)\n\n_Exploded picture of the G9 server’s main components. 4 sleds represent 4 nodes, each with 2 24-core Intel CPUs_\n\nEach high-level hardware component has gone through their own upgrade as well for a balanced scale up keeping our stack CPU bound, making this generation the most radical revision since we moved on from using HP 5 years ago. Let’s glance through each of those components.\n\n### Hardware Changes\n\nCPU\n\n*   Previously: 2x 12 core Intel Xeon Silver 4116 2.1Ghz 85W\n*   Now: 2x 24 core Intel custom off-roadmap 1.9Ghz 150W\n\nThe performance of our infrastructure is heavily directed by how much compute we can squeeze in a given physical space and power. In essence, requests per second (RPS) per Watt is a critical metric that [Qualcomm’s ARM64 46 core Falkor chip](https://blog.cloudflare.com/arm-takes-wing/) had a [big advantage](https://www.datacenterknowledge.com/design/cloudflare-bets-arm-servers-it-expands-its-data-center-network) over Intel’s Skylake 4116.\n\nIntel proposed to co-innovate with us an off-roadmap 24-core Xeon Gold CPU specifically made for our workload offering considerable value in Performance per Watt. For this generation, we continue using Intel as system solutions are widely available while we’re working on [realizing ARM64’s benefits to production](https://blog.cloudflare.com/porting-our-software-to-arm64/). We expect this CPU to perform with better RPS per Watt right off the bat; increasing the RPS by 200% from doubling the amount of cores, and increasing the power consumption by 174% from increasing the CPUs TDP from 85W to 150W each.\n\n### Disk\n\n*   Previously: 6x Micron 1100 512G SATA SSD\n*   Now: 6x Intel S4500 480G SATA SSD\n\nWith all the requests we foresee for G9 to process, we need to tame down the outlying and long-tail latencies we have seen in our previous SSDs. Lowering p99 and p999 latency has been a serious endeavor. To help save milliseconds in disk response time for 0.01% or even 0.001% of all the traffic we see [isn’t a joke](https://blog.cloudflare.com/how-we-scaled-nginx-and-saved-the-world-54-years-every-day/)!\n\nDatacenter grade SSDs in [Intel S4500](http://static6.arrow.com/aropdfconversion/8cb17527534c0532edc2710e8132aeea4d8e6f24/pgurl_intel-ssd-dc-s4500-series-240gb-2_5in-sata-6gbs-3d1-tlcordering.p.pdf) will proliferate our fleet. These disks come with better endurance to last over the expected service life of our servers and better performance consistency with lower p95+ latency.\n\n### Network\n\n*   Previously: dual-port 10G Solarflare Flareon Ultra SFN8522\n*   Now: dual-port 25G Mellanox ConnectX-4 Lx OCP\n\nOur [DDoS mitigation program](https://blog.cloudflare.com/meet-gatebot-a-bot-that-allows-us-to-sleep/) is all done in userspace, so network adapter model can be anything on market as long as it [supports XDP](https://netdevconf.org/2.1/papers/Gilberto_Bertin_XDP_in_practice.pdf). We went with Mellanox for their solid reliability and their readily available 2x25G CX4 model. Upgrading to 25G intra-rack ethernet network is easy future-proofing since the 10G SFP+ ethernet port shares the same physical form factor as the 25G’s SFP28. Switch and NIC vendors offer models that can be configured as either 10G or 25G.\n\nAnother change is the adapter’s form factor itself being an OCP mezzanine instead of the more conventional Low Profile sized card. QCT is a server system integrator participating in the [Open Compute Project](https://www.opencompute.org/), a non-profit organization establishing an open source hardware ecosystem founded by Facebook, Intel, and Rackspace. Their T42S-2U motherboards each include 2 PCIe x16 Gen3 expansion slots: 1 fit for a regular I/O card and 1 for an OCP mezzanine. The form factor change allows us to occupy the OCP slot leaving the regular slot free to integrate something else that may not be offered with an OCP form factor like a high capacity NVMe SSD or a GPU. We like that our server has the room for upgrades if needed.\n\n![](https://blog.cloudflare.com/content/images/2018/10/image3.png)\n\n_Both Low Profile and OCP adapters offer the same throughput and features_\n\n![](https://lh4.googleusercontent.com/qncCFb7XX0ZvJyvXxWcJdv6rJ_6Yqs_BYRj5xZokMll0d3sFgJX5eBqi1N8KUIuqyBV_niKttp0PsYgkDsnpQsQBOmXGEYPCkgANeTldyf-75KjYkR3DpueCUK2hCoOy4NjmQqsI)\n\n_Rear side of G9 chassis showing all 4 sled nodes, each leaving room to add on a PCI card_\n\n#### Memory\n\n*   Previously: 192G (12x16G) DDR4 2400Mhz RDIMM\n*   Now: 256G (8x32G) DDR4 2666Mhz RDIMM\n\nGoing from 192G (12x16G) to 256G (8x32G) made practical sense. The motherboard has 12 DIMM channels, which were all populated in the G8. We want to have the ability to upgrade just in case, as well as keeping memory configuration balanced and at optimal bandwidth capacity. 8x32G works well leaving 4 channels open for future upgrades.\n\n### Physical stress test\n\nOur software stack scales nicely enough that we can confidently assume we’ll double the amount of requests having twice the amount of CPU cores compared to G8. What we need to ensure before we ship any G9 servers out to our current 154 and future PoPs is that there won’t be any design issues pertaining to thermal nor power failures. At the extreme case that all of our cores run up 100% load, would that cause our server to run above operating temperature? How much power would a whole server with 192 cores totaling 1200W TDP consume? We set out to record both by applying a stress test to the whole system.\n\nTemperature readings were recorded off of _ipmitool sdr list_, then graphed showing socket and motherboard temperature. For 2U4N being such a compact form factor, it’s worth monitoring that a server running hot isn’t _literally_ running hot. The red lines represent the 4 nodes that compose the whole G9 server under test; blue lines represent G8 nodes (we didn’t stress the G8’s so their temperature readings are constant).\n\n![](https://lh4.googleusercontent.com/ddFg5dcptq_R6eQp6I6X2RqrzzPCDL1OWBN1qb0KBGSixf-oJaDxnnC1gjtownqAaru6WyexTLpubnapjdKhE8CB3BXrVIZa6tnOy86CX6JrtPOXUAtvFGBtT4dvYx9QApHjaHTy)\n\n![](https://lh4.googleusercontent.com/Xchiev2ccIEYdrFIiB2dpF1SI1rbOIz24S0o_mTJklyy--Pjcz8LWPrxhEIXX3zg-0b9VEEHA2V5v-SzQPV4_F4i6NOBofxl8j7SS0qa_6FVqdmzezaYGXXA39OzJBv_6YIyG0Pl)\n\nBoth graphs are looking fine and not out of control mostly thanks to the T42S-2U’s 4 80mm x 80mm fans capable of blowing over 90CFM; which we managed to reach their max spec RPM.\n\nRecording the new system’s max power consumption is critical information we need to properly design our rack stack choosing the right Power Distribution Unit and ensuring we’re below the budgeted power while keeping adequate phase balancing. For example, a typical 3-phase US-rated 24-Amp PDU gives you a max power rating of 8.6 kilowatts. We wouldn’t be able to fit 9 servers powered by that same PDU if each were running at 1kW without any way to cap them.\n\n![](https://lh4.googleusercontent.com/pirTvrYl4aQuSd575px16hVHA55qFKhqTDzan9mn6HCz9G1Z83CjXO_y9z50wjgveFP07rQFfAlMK3h7ts8e85zgMYXcy1OiVh0bLdbQx8pORSNkZr7MOCtBDqLb_Akoy37cbCVx)\n\n![](https://lh4.googleusercontent.com/GujZRUCJu-WHNb4H8K-D16X7kyCBd_-Wqg18shh4_4nFAES6uhYu4CpR1fESeL4wUT_rhy1-WOsLVzP5lW-3gekMqJzrCjKSteBm_99OLWu_edtJplUQgNAo6cke9KidPtWf9cwy)\n\nAbove right graph shows our max power to be 1.9kW as the red line, or crudely 475W per node which is excellent in a modern server. Notice the blue and yellow lines representing the G9’s 2 power supplies summing the total power. The yellow line PSU appearing off is intentional as part of our testing procedure to show the PSU’s resilience in abrupt power changes.\n\nStressing out all available CPU, I/O, and memory along with maxing out fan RPMs combined is a good indicator for the highest possible heat and power draw this server can do. Hopefully we won’t ever see such an extreme case like this in live production environments, and we expect much milder actual results (read: we don’t think catastrophic failures to be possible).\n\n### First Impression in live production\n\nWe increased capacity to one of our most loaded PoPs by adding G9 servers. The following time graphs represent a 24 hour range with how G9 performance compares with G8 in a live PoP.\n\n![](https://blog.cloudflare.com/content/images/2018/10/Screen-Shot-2018-10-03-at-6.44.41-PM-1.png)\n\nGreat! They're doing over 2x the requests compared to G8 with about 10% less CPU usage. Note that all results here are based from non-optimized systems, so we could add more load on the G9 and have their CPU usage comparable to the G8. Additionally, they're doing that amount with better CPU processing time shown as nginx execution time. You can see the latency gap between generations widening as we go towards the 99.9th percentile:\n\n![](https://blog.cloudflare.com/content/images/2018/10/Screen-Shot-2018-10-03-at-6.53.13-PM-1.png)\n\n_Long-tail latencies for NGINX CPU processing time (lower is better)_\n\nTalking about latency, let’s check how our new SSDs are doing on that front:\n\n![](https://blog.cloudflare.com/content/images/2018/10/Screen-Shot-2018-10-04-at-4.59.41-PM.png)\n\n_Cache disk IOPS and latency (lower is better)_\n\nThe trend still holds that G9 is doing better. It’s a good thing that the G9’s SSDs aren’t seeing as many IOPS since it means we’re not hitting cache disks as often and are able to store and process more on CPU and memory. We’ve cut the read cache hits and latency by half. Less writes results in better performance consistency and longevity.\n\nAnother metric that G9 does more is power consumption, doing about 55% more than the G8. While it’s not a piece of information to brag about, it is expected when older CPUs were once rated at 85W TDP to now using ones with 150W TDP; and when considering how much work the G9 servers do:\n\n![](https://blog.cloudflare.com/content/images/2018/10/Screen-Shot-2018-10-03-at-7.12.17-PM-1.png)\n\nG9 is actually 1.5x more power efficient than G8. Temperature readings were checked as well. Inlet and outlet chassis temps, as well as CPU temps, are well within operating temperatures.\n\nNow that’s muscle! In other words for every 3 G8 servers, just 2 of those G9's would take on the same workload. If one of our racks normally would have 9 G8 servers, we can switch those out with only 6 G9's. Inversely planning to turn up a cage of 10 G9 racks would be the same as if we did 15 G8 racks!\n\nWe have [big plans](https://www.bizjournals.com/sanfrancisco/news/2018/06/14/cloudflare-to-expand-to-200-cities-by-2019.html) to cover our entire network with G9 servers, with most of them planned for the existing cities your site most likely uses. By 2019, you’ll benefit with increased bandwidth and lower wait times. And we’ll benefit in expanding and turning up datacenters quicker and more efficiently.\n\n### What's next?\n\nGen X? Right now is exciting times at Cloudflare. Many teams and engineers are testing, porting, and implementing new stuff that can help us lower operating costs, explore new products and possibilities, and improve Quality of Service. We’re tackling problems and taking on projects that are unique in the industry.\n\nServerless computing like [Cloudflare Workers](https://blog.cloudflare.com/introducing-cloudflare-workers/) and beyond will ask for new challenges to our infrastructure as all of our customers can program their features on Cloudflare’s edge network.\n\nThe network architecture that was conventionally made up of routers, switches, and servers has been merged into 3-in-1 box solutions allowing Cloudflare services to be set up into locations that weren’t possible before.\n\nThe advent of NVMe and persistent memory, as well as the possibility of turning SSDs into DRAM, is redefining how we design cache servers and handle [tiered caching](https://www.cloudflare.com/products/argo-smart-routing/). SSDs and memory aren’t treated as separate entities like they used to.\n\nHardware brings the company together like a rug in a living room. See how many links I mentioned above to show you how we’re one team dedicated to build a better Internet. Everything that we do here roots down to how we manage the tons of aluminum and silicon we’ve invested. There's a lot here to develop our hardware to help Cloudflare grow to where we envision ourselves to be. If you’d like to contribute, we’d love to hear from you.  \n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Speed & Reliability](https://blog.cloudflare.com/tag/speed-and-reliability/) [Network](https://blog.cloudflare.com/tag/network/) [Hardware](https://blog.cloudflare.com/tag/hardware/)"
    },
    {
      "url": "https://blog.cloudflare.com/workers-ai/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/workers-ai/",
        "loadedTime": "2023-12-05T02:29:55.759Z",
        "referrerUrl": "https://blog.cloudflare.com/cloudflare-gen-12-server-bigger-better-cooler-in-a-2u1n-form-factor/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/workers-ai/",
        "title": "Workers AI: serverless GPU-powered inference on Cloudflare’s global network",
        "description": "We are excited to launch Workers AI - an AI inference as a service platform, empowering developers to run AI models with just a few lines of code, all powered by our global network of GPUs",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "09/27/2023\n11 min read\nThis post is also available in 简体中文, 日本語, 한국어, Français, Deutsch and Español.\nIf you're anywhere near the developer community, it's almost impossible to avoid the impact that AI’s recent advancements have had on the ecosystem. Whether you're using AI in your workflow to improve productivity, or you’re shipping AI based features to your users, it’s everywhere. The focus on AI improvements are extraordinary, and we’re super excited about the opportunities that lay ahead, but it's not enough.\nNot too long ago, if you wanted to leverage the power of AI, you needed to know the ins and outs of machine learning, and be able to manage the infrastructure to power it.\nAs a developer platform with over one million active developers, we believe there is so much potential yet to be unlocked, so we’re changing the way AI is delivered to developers. Many of the current solutions, while powerful, are based on closed, proprietary models and don't address privacy needs that developers and users demand. Alternatively, the open source scene is exploding with powerful models, but they’re simply not accessible enough to every developer. Imagine being able to run a model, from your code, wherever it’s hosted, and never needing to find GPUs or deal with setting up the infrastructure to support it.\nThat's why we are excited to launch Workers AI - an AI inference as a service platform, empowering developers to run AI models with just a few lines of code, all powered by our global network of GPUs. It's open and accessible, serverless, privacy-focused, runs near your users, pay-as-you-go, and it's built from the ground up for a best in class developer experience.\nWorkers AI - making inference just work\nWe’re launching Workers AI to put AI inference in the hands of every developer, and to actually deliver on that goal, it should just work out of the box. How do we achieve that?\nAt the core of everything, it runs on the right infrastructure - our world-class network of GPUs\nWe provide off-the-shelf models that run seamlessly on our infrastructure\nFinally, deliver it to the end developer, in a way that’s delightful. A developer should be able to build their first Workers AI app in minutes, and say “Wow, that’s kinda magical!”.\nSo what exactly is Workers AI? It’s another building block that we’re adding to our developer platform - one that helps developers run well-known AI models on serverless GPUs, all on Cloudflare’s trusted global network. As one of the latest additions to our developer platform, it works seamlessly with Workers + Pages, but to make it truly accessible, we’ve made it platform-agnostic, so it also works everywhere else, made available via a REST API.\nModels you know and love\nWe’re launching with a curated set of popular, open source models, that cover a wide range of inference tasks:\nText generation (large language model): meta/llama-2-7b-chat-int8\nAutomatic speech recognition (ASR): openai/whisper\nTranslation: meta/m2m100-1.2\nText classification: huggingface/distilbert-sst-2-int8\nImage classification: microsoft/resnet-50\nEmbeddings: baai/bge-base-en-v1.5\nYou can browse all available models in your Cloudflare dashboard, and soon you’ll be able to dive into logs and analytics on a per model basis!\nThis is just the start, and we’ve got big plans. After launch, we’ll continue to expand based on community feedback. Even more exciting - in an effort to take our catalog from zero to sixty, we’re announcing a partnership with Hugging Face, a leading AI community + hub. The partnership is multifaceted, and you can read more about it here, but soon you’ll be able to browse and run a subset of the Hugging Face catalog directly in Workers AI.\nAccessible to everyone\nPart of the mission of our developer platform is to provide all the building blocks that developers need to build the applications of their dreams. Having access to the right blocks is just one part of it — as a developer your job is to put them together into an application. Our goal is to make that as easy as possible.\nTo make sure you could use Workers AI easily regardless of entry point, we wanted to provide access via: Workers or Pages to make it easy to use within the Cloudflare ecosystem, and via REST API if you want to use Workers AI with your current stack.\nHere’s a quick CURL example that translates some text from English to French:\ncurl https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai/run/@cf/meta/m2m100-1.2b \\ -H \"Authorization: Bearer {API_TOKEN}\" \\ -d '{ \"text\": \"I'll have an order of the moule frites\", \"target_lang\": \"french\" }' \nAnd here are what the response looks like:\n{ \"result\": { \"answer\": \"Je vais commander des moules frites\" }, \"success\": true, \"errors\":[], \"messages\":[] } \nUse it with any stack, anywhere - your favorite Jamstack framework, Python + Django/Flask, Node.js, Ruby on Rails, the possibilities are endless. And deploy.\nDesigned for developers\nDeveloper experience is really important to us. In fact, most of this post has been about just that. Making sure it works out of the box. Providing popular models that just work. Being accessible to all developers whether you build and deploy with Cloudflare or elsewhere. But it’s more than that - the experience should be frictionless, zero to production should be fast, and it should feel good along the way.\nLet’s walk through another example to show just how easy it is to use! We’ll run Llama 2, a popular large language model open sourced by Meta, in a worker.\nWe’ll assume you have some of the basics already complete (Cloudflare account, Node, NPM, etc.), but if you don’t this guide will get you properly set up!\n1. Create a Workers project\nCreate a new project named workers-ai by running:\n$ npm create cloudflare@latest \nWhen setting up your workers-ai worker, answer the setup questions as follows:\nEnter workers-ai for the app name\nChoose Hello World script for the type of application\nSelect yes to using TypeScript\nSelect yes to using Git\nSelect no to deploying\nLastly navigate to your new app directory:\ncd workers-ai \n2. Connect Workers AI to your worker\nCreate a Workers AI binding, which allows your worker to access the Workers AI service without having to manage an API key yourself.\nTo bind Workers AI to your worker, add the following to the end of your wrangler.toml file:\n[ai] binding = \"AI\" #available in your worker via env.AI \nYou can also bind Workers AI to a Pages Function. For more information, refer to Functions Bindings.\n3. Install the Workers AI client library\nnpm install @cloudflare/ai \n4. Run an inference task in your worker\nUpdate the source/index.ts with the following code:\nimport { Ai } from '@cloudflare/ai' export default { async fetch(request, env) { const ai = new Ai(env.AI); const input = { prompt: \"What's the origin of the phrase 'Hello, World'\" }; const output = await ai.run('@cf/meta/llama-2-7b-chat-int8', input ); return new Response(JSON.stringify(output)); }, }; \n5. Develop locally with Wrangler\nWhile in your project directory, test Workers AI locally by running:\n$ npx wrangler dev --remote \nNote - These models currently only run on Cloudflare’s network of GPUs (and not locally), so setting --remote above is a must, and you’ll be prompted to log in at this point.\nWrangler will give you a URL (most likely localhost:8787). Visit that URL, and you’ll see a response like this\n{ \"response\": \"Hello, World is a common phrase used to test the output of a computer program, particularly in the early stages of programming. The phrase \"Hello, World!\" is often the first program that a beginner learns to write, and it is included in many programming language tutorials and textbooks as a way to introduce basic programming concepts. The origin of the phrase \"Hello, World!\" as a programming test is unclear, but it is believed to have originated in the 1970s. One of the earliest known references to the phrase is in a 1976 book called \"The C Programming Language\" by Brian Kernighan and Dennis Ritchie, which is considered one of the most influential books on the development of the C programming language. } \n6. Deploy your worker\nFinally, deploy your worker to make your project accessible on the Internet:\n$ npx wrangler deploy # Outputs: https://workers-ai.<YOUR_SUBDOMAIN>.workers.dev \nAnd that’s it. You can literally go from zero to deployed AI in minutes. This is obviously a simple example, but shows how easy it is to run Workers AI from any project.\nPrivacy by default\nWhen Cloudflare was founded, our value proposition had three pillars: more secure, more reliable, and more performant. Over time, we’ve realized that a better Internet is also a more private Internet, and we want to play a role in building it.\nThat’s why Workers AI is private by default - we don’t train our models, LLM or otherwise, on your data or conversations, and our models don’t learn from your usage. You can feel confident using Workers AI in both personal and business settings, without having to worry about leaking your data. Other providers only offer this fundamental feature with their enterprise version. With us, it’s built in for everyone.\nWe’re also excited to support data localization in the future. To make this happen, we have an ambitious GPU rollout plan - we’re launching with seven sites today, roughly 100 by the end of 2023, and nearly everywhere by the end of 2024. Ultimately, this will empower developers to keep delivering killer AI features to their users, while staying compliant with their end users’ data localization requirements.\nThe power of the platform\nVector database - Vectorize\nWorkers AI is all about running Inference, and making it really easy to do so, but sometimes inference is only part of the equation. Large language models are trained on a fixed set of data, based on a snapshot at a specific point in the past, and have no context on your business or use case. When you submit a prompt, information specific to you can increase the quality of results, making it more useful and relevant. That’s why we’re also launching Vectorize, our vector database that’s designed to work seamlessly with Workers AI. Here’s a quick overview of how you might use Workers AI + Vectorize together.\nExample: Use your data (knowledge base) to provide additional context to an LLM when a user is chatting with it.\nGenerate initial embeddings: run your data through Workers AI using an embedding model. The output will be embeddings, which are numerical representations of those words.\nInsert those embeddings into Vectorize: this essentially seeds the vector database with your data, so we can later use it to retrieve embeddings that are similar to your users’ query\nGenerate embedding from user question: when a user submits a question to your AI app, first, take that question, and run it through Workers AI using an embedding model.\nGet context from Vectorize: use that embedding to query Vectorize. This should output embeddings that are similar to your user’s question.\nCreate context aware prompt: Now take the original text associated with those embeddings, and create a new prompt combining the text from the vector search, along with the original question\nRun prompt: run this prompt through Workers AI using an LLM model to get your final result\nAI Gateway\nThat covers a more advanced use case. On the flip side, if you are running models elsewhere, but want to get more out of the experience, you can run those APIs through our AI gateway to get features like caching, rate-limiting, analytics and logging. These features can be used to protect your end point, monitor and optimize costs, and also help with data loss prevention. Learn more about AI gateway here.\nStart building today\nTry it out for yourself, and let us know what you think. Today we’re launching Workers AI as an open Beta for all Workers plans - free or paid. That said, it’s super early, so…\nWarning - It’s an early beta\nUsage is not currently recommended for production apps, and limits + access are subject to change.\nLimits\nWe’re initially launching with limits on a per-model basis\n@cf/meta/llama-2-7b-chat-int8: 50 reqs/min globally\nCheckout our docs for a full overview of our limits.\nPricing\nWhat we released today is just a small preview to give you a taste of what’s coming (we simply couldn’t hold back), but we’re looking forward to putting the full-throttle version of Workers AI in your hands.\nWe realize that as you approach building something, you want to understand: how much is this going to cost me? Especially with AI costs being so easy to get out of hand. So we wanted to share the upcoming pricing of Workers AI with you.\nWhile we won’t be billing on day one, we are announcing what we expect our pricing will look like.\nUsers will be able to choose from two ways to run Workers AI:\nRegular Twitch Neurons (RTN) - running wherever there's capacity at $0.01 / 1k neurons\nFast Twitch Neurons (FTN) - running at nearest user location at $0.125 / 1k neurons\nYou may be wondering — what’s a neuron?\nNeurons are a way to measure AI output that always scales down to zero (if you get no usage, you will be charged for 0 neurons). To give you a sense of what you can accomplish with a thousand neurons, you can: generate 130 LLM responses, 830 image classifications, or 1,250 embeddings.\nOur goal is to help our customers pay only for what they use, and choose the pricing that best matches their use case, whether it’s price or latency that is top of mind.\nWhat’s on the roadmap?\nWorkers AI is just getting started, and we want your feedback to help us make it great. That said, there are some exciting things on the roadmap.\nMore models, please\nWe're launching with a solid set of models that just work, but will continue to roll out new models based on your feedback. If there’s a particular model you'd love to see on Workers AI, pop into our Discord and let us know!\nIn addition to that, we're also announcing a partnership with Hugging Face, and soon you'll be able to access and run a subset of the Hugging Face catalog directly from Workers AI.\nAnalytics + observability\nUp to this point, we’ve been hyper focussed on one thing - making it really easy for any developer to run powerful AI models in just a few lines of code. But that’s only one part of the story. Up next, we’ll be working on some analytics and observability capabilities to give you insights into your usage + performance + spend on a per-model basis, plus the ability to fig into your logs if you want to do some exploring.\nA road to global GPU coverage\nOur goal is to be the best place to run inference on Region: Earth, so we're adding GPUs to our data centers as fast as we can.\nWe plan to be in 100 data centers by the end of this year\nAnd nearly everywhere by the end of 2024\n\nWe’re really excited to see you build - head over to our docs to get started.\nIf you need inspiration, want to share something you’re building, or have a question - pop into our Developer Discord.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nBirthday Week Cloudflare Workers AI Developer Platform Database",
      "markdown": "09/27/2023\n\n*   [![Phil Wittig](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/09/phil.jpeg)](https://blog.cloudflare.com/author/phil/)\n*   [![Rita Kozlov](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/05/Rita-Kozlov.jpeg)](https://blog.cloudflare.com/author/rita/)\n*   [![Rebecca Weekly](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/04/RWeekly---Retouched-16.jpg)](https://blog.cloudflare.com/author/rebecca-weekly/)\n*   [![Celso Martinho](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/Celso-Martinho.png)](https://blog.cloudflare.com/author/celso/)\n\n11 min read\n\nThis post is also available in [简体中文](https://blog.cloudflare.com/zh-cn/workers-ai-zh-cn/), [日本語](https://blog.cloudflare.com/ja-jp/workers-ai-ja-jp/), [한국어](https://blog.cloudflare.com/ko-kr/workers-ai-ko-kr/), [Français](https://blog.cloudflare.com/fr-fr/workers-ai-fr-fr/), [Deutsch](https://blog.cloudflare.com/de-de/workers-ai-de-de/) and [Español](https://blog.cloudflare.com/es-es/workers-ai-es-es/).\n\n![](https://blog.cloudflare.com/content/images/2023/09/image1-29.png)\n\nIf you're anywhere near the developer community, it's almost impossible to avoid the impact that AI’s recent advancements have had on the ecosystem. Whether you're using [AI](https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/) in your workflow to improve productivity, or you’re shipping AI based features to your users, it’s everywhere. The focus on AI improvements are extraordinary, and we’re super excited about the opportunities that lay ahead, but it's not enough.\n\nNot too long ago, if you wanted to leverage the power of AI, you needed to know the ins and outs of machine learning, and be able to manage the infrastructure to power it.\n\nAs a developer platform with over one million active developers, we believe there is so much potential yet to be unlocked, so we’re changing the way AI is delivered to developers. Many of the current solutions, while powerful, are based on closed, proprietary models and don't address privacy needs that developers and users demand. Alternatively, the open source scene is exploding with powerful models, but they’re simply not accessible enough to every developer. Imagine being able to run a model, from your code, wherever it’s hosted, and never needing to find GPUs or deal with setting up the infrastructure to support it.\n\nThat's why we are excited to launch Workers AI - an AI inference as a service platform, empowering developers to run AI models with just a few lines of code, all powered by our global network of GPUs. It's open and accessible, serverless, privacy-focused, runs near your users, pay-as-you-go, and it's built from the ground up for a best in class developer experience.\n\n## Workers AI - making inference **just work**\n\nWe’re launching Workers AI to put AI inference in the hands of every developer, and to actually deliver on that goal, it should **just work** out of the box. How do we achieve that?\n\n*   At the core of everything, it runs on the right infrastructure - our world-class network of GPUs\n*   We provide off-the-shelf models that run seamlessly on our infrastructure\n*   Finally, deliver it to the end developer, in a way that’s delightful. A developer should be able to build their first Workers AI app in minutes, and say “Wow, that’s kinda magical!”.\n\nSo what exactly is Workers AI? It’s another building block that we’re adding to our developer platform - one that helps developers run well-known AI models on serverless GPUs, all on Cloudflare’s trusted global network. As one of the latest additions to our developer platform, it works seamlessly with Workers + Pages, but to make it truly accessible, we’ve made it platform-agnostic, so it also works everywhere else, made available via a REST API.\n\n## Models you know and love\n\nWe’re launching with a curated set of popular, open source models, that cover a wide range of inference tasks:\n\n*   **Text generation (large language model):** meta/llama-2-7b-chat-int8\n*   **Automatic speech recognition (ASR):** openai/whisper\n*   **Translation:** meta/m2m100-1.2\n*   **Text classification:** huggingface/distilbert-sst-2-int8\n*   **Image classification:** microsoft/resnet-50\n*   **Embeddings:** baai/bge-base-en-v1.5\n\nYou can browse all available models in your Cloudflare dashboard, and soon you’ll be able to dive into logs and analytics on a per model basis!\n\n![](https://blog.cloudflare.com/content/images/2023/09/image4-14.png)\n\nThis is just the start, and we’ve got big plans. After launch, we’ll continue to expand based on community feedback. Even more exciting - in an effort to take our catalog from zero to sixty, we’re announcing a partnership with Hugging Face, a leading AI community + hub. The partnership is multifaceted, and you can read more about it [here](https://blog.cloudflare.com/best-place-region-earth-inference), but soon you’ll be able to browse and run a subset of the Hugging Face catalog directly in Workers AI.\n\n## Accessible to everyone\n\nPart of the mission of our developer platform is to provide **all** the building blocks that developers need to build the applications of their dreams. Having access to the right blocks is just one part of it — as a developer your job is to put them together into an application. Our goal is to make that as easy as possible.\n\nTo make sure you could use Workers AI easily regardless of entry point, we wanted to provide access via: Workers or Pages to make it easy to use within the Cloudflare ecosystem, and via REST API if you want to use Workers AI with your current stack.\n\nHere’s a quick CURL example that translates some text from English to French:\n\n```\ncurl https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai/run/@cf/meta/m2m100-1.2b \\\n-H \"Authorization: Bearer {API_TOKEN}\" \\\n\t-d '{ \"text\": \"I'll have an order of the moule frites\", \"target_lang\": \"french\" }'\n```\n\nAnd here are what the response looks like:\n\n```\n{\n  \"result\": {\n    \"answer\": \"Je vais commander des moules frites\"\n  },\n  \"success\": true,\n  \"errors\":[],\n  \"messages\":[]\n}\n```\n\nUse it with any stack, anywhere - your favorite Jamstack framework, Python + Django/Flask, Node.js, Ruby on Rails, the possibilities are endless. And deploy.\n\n## Designed for developers\n\nDeveloper experience is really important to us. In fact, most of this post has been about just that. Making sure it works out of the box. Providing popular models that just work. Being accessible to all developers whether you build and deploy with Cloudflare or elsewhere. But it’s more than that - the experience should be frictionless, zero to production should be fast, and it should feel good along the way.\n\nLet’s walk through another example to show just how easy it is to use! We’ll run Llama 2, a popular [large language model](https://www.cloudflare.com/learning/ai/what-is-large-language-model/) open sourced by Meta, in a worker.\n\nWe’ll assume you have some of the basics already complete (Cloudflare account, Node, NPM, etc.), but if you don’t [this guide](https://developers.cloudflare.com/workers-ai/get-started/local-dev-setup/) will get you properly set up!\n\n### 1\\. Create a Workers project\n\nCreate a new project named workers-ai by running:\n\n```\n$ npm create cloudflare@latest\n```\n\nWhen setting up your workers-ai worker, answer the setup questions as follows:\n\n*   Enter **workers-ai** for the app name\n*   Choose **Hello World** script for the type of application\n*   Select **yes** to using TypeScript\n*   Select **yes** to using Git\n*   Select **no** to deploying\n\nLastly navigate to your new app directory:\n\n```\ncd workers-ai\n```\n\n### 2\\. Connect Workers AI to your worker\n\nCreate a Workers AI binding, which allows your worker to access the Workers AI service without having to manage an API key yourself.\n\nTo bind Workers AI to your worker, add the following to the end of your **wrangler.toml** file:\n\n```\n[ai]\nbinding = \"AI\" #available in your worker via env.AI\n```\n\nYou can also bind Workers AI to a Pages Function. For more information, refer to [Functions Bindings](https://developers.cloudflare.com/pages/platform/functions/bindings/#ai).\n\n### 3\\. Install the Workers AI client library\n\n```\nnpm install @cloudflare/ai\n```\n\n### 4\\. Run an inference task in your worker\n\nUpdate the **source/index.ts** with the following code:\n\n```\nimport { Ai } from '@cloudflare/ai'\nexport default {\n  async fetch(request, env) {\n    const ai = new Ai(env.AI);\n    const input = { prompt: \"What's the origin of the phrase 'Hello, World'\" };\n    const output = await ai.run('@cf/meta/llama-2-7b-chat-int8', input );\n    return new Response(JSON.stringify(output));\n  },\n};\n```\n\n### 5\\. Develop locally with Wrangler\n\nWhile in your project directory, test Workers AI locally by running:\n\n```\n$ npx wrangler dev --remote\n```\n\n**Note -** These models currently only run on Cloudflare’s network of GPUs (and not locally), so setting `--remote` above is a must, and you’ll be prompted to log in at this point.\n\nWrangler will give you a URL (most likely localhost:8787). Visit that URL, and you’ll see a response like this\n\n```\n{\n  \"response\": \"Hello, World is a common phrase used to test the output of a computer program, particularly in the early stages of programming. The phrase \"Hello, World!\" is often the first program that a beginner learns to write, and it is included in many programming language tutorials and textbooks as a way to introduce basic programming concepts. The origin of the phrase \"Hello, World!\" as a programming test is unclear, but it is believed to have originated in the 1970s. One of the earliest known references to the phrase is in a 1976 book called \"The C Programming Language\" by Brian Kernighan and Dennis Ritchie, which is considered one of the most influential books on the development of the C programming language.\n}\n```\n\n### 6\\. Deploy your worker\n\nFinally, deploy your worker to make your project accessible on the Internet:\n\n```\n$ npx wrangler deploy\n# Outputs: https://workers-ai.<YOUR_SUBDOMAIN>.workers.dev\n```\n\nAnd that’s it. You can literally go from zero to deployed AI in minutes. This is obviously a simple example, but shows how easy it is to run Workers AI from any project.\n\n## Privacy by default\n\nWhen Cloudflare was founded, our value proposition had three pillars: more secure, more reliable, and more performant. Over time, we’ve realized that a better Internet is also a more private Internet, and we want to play a role in building it.\n\nThat’s why Workers AI is private by default - we don’t train our models, LLM or otherwise, on your data or conversations, and our models don’t learn from your usage. You can feel confident using Workers AI in both personal and business settings, without having to worry about leaking your data. Other providers only offer this fundamental feature with their enterprise version. With us, it’s built in for everyone.\n\nWe’re also excited to support data localization in the future. To make this happen, we have an ambitious GPU rollout plan - we’re launching with seven sites today, roughly 100 by the end of 2023, and nearly everywhere by the end of 2024. Ultimately, this will empower developers to keep delivering killer AI features to their users, while staying compliant with their end users’ data localization requirements.\n\n## The power of the platform\n\n#### Vector database - Vectorize\n\nWorkers AI is all about running Inference, and making it really easy to do so, but sometimes inference is only part of the equation. Large language models are trained on a fixed set of data, based on a snapshot at a specific point in the past, and have no context on your business or use case. When you submit a prompt, information specific to you can increase the quality of results, making it more useful and relevant. That’s why we’re also launching Vectorize, our [vector database](https://www.cloudflare.com/learning/ai/what-is-vector-database/) that’s designed to work seamlessly with Workers AI. Here’s a quick overview of how you might use Workers AI + Vectorize together.\n\nExample: Use your data (knowledge base) to provide additional context to an LLM when a user is chatting with it.\n\n1.  **Generate initial embeddings:** run your data through Workers AI using an [embedding model](https://www.cloudflare.com/learning/ai/what-are-embeddings/). The output will be embeddings, which are numerical representations of those words.\n2.  ******Insert those embeddings into Vectorize:****** this essentially seeds the vector database with your data, so we can later use it to retrieve embeddings that are similar to your users’ query\n3.  ******Generate embedding from user question:****** when a user submits a question to your AI app, first, take that question, and run it through Workers AI using an embedding model.\n4.  ******Get context from Vectorize:****** use that embedding to query Vectorize. This should output embeddings that are similar to your user’s question.\n5.  ******Create context aware prompt:****** Now take the original text associated with those embeddings, and create a new prompt combining the text from the vector search, along with the original question\n6.  **Run prompt:** run this prompt through Workers AI using an LLM model to get your final result\n\n#### AI Gateway\n\nThat covers a more advanced use case. On the flip side, if you are running models elsewhere, but want to get more out of the experience, you can run those APIs through our AI gateway to get features like caching, rate-limiting, analytics and logging. These features can be used to protect your end point, monitor and optimize costs, and also help with data loss prevention. Learn more about AI gateway [here](https://blog.cloudflare.com/announcing-ai-gateway).\n\n## Start building today\n\nTry it out for yourself, and let us know what you think. Today we’re launching Workers AI as an open Beta for all Workers plans - free or paid. That said, it’s super early, so…\n\n#### Warning - It’s an early beta\n\nUsage is **not currently recommended for production apps**, and limits + access are subject to change.\n\n#### Limits\n\nWe’re initially launching with limits on a per-model basis\n\n*   @cf/meta/llama-2-7b-chat-int8: 50 reqs/min globally\n\nCheckout our [docs](https://developers.cloudflare.com/workers-ai/platform/limits/) for a full overview of our limits.\n\n#### Pricing\n\nWhat we released today is just a small preview to give you a taste of what’s coming (we simply couldn’t hold back), but we’re looking forward to putting the full-throttle version of Workers AI in your hands.\n\nWe realize that as you approach building something, you want to understand: how much is this going to cost me? Especially with AI costs being so easy to get out of hand. So we wanted to share the upcoming pricing of Workers AI with you.\n\nWhile we won’t be billing on day one, we are announcing what we expect our pricing will look like.\n\nUsers will be able to choose from two ways to run Workers AI:\n\n*   **Regular Twitch Neurons (RTN)** \\- running wherever there's capacity at $0.01 / 1k neurons\n*   ****Fast Twitch Neurons (FTN)**** \\- running at nearest user location at $0.125 / 1k neurons\n\nYou may be wondering — what’s a neuron?\n\nNeurons are a way to measure AI output that always scales down to zero (if you get no usage, you will be charged for 0 neurons). To give you a sense of what you can accomplish with a thousand neurons, you can: generate 130 LLM responses, 830 image classifications, or 1,250 embeddings.\n\nOur goal is to help our customers pay only for what they use, and choose the pricing that best matches their use case, whether it’s price or latency that is top of mind.\n\n### What’s on the roadmap?\n\nWorkers AI is just getting started, and we want your feedback to help us make it great. That said, there are some exciting things on the roadmap.\n\n#### More models, please\n\nWe're launching with a solid set of models that just work, but will continue to roll out new models based on your feedback. If there’s a particular model you'd love to see on Workers AI, pop into our [Discord](https://discord.cloudflare.com/) and let us know!\n\nIn addition to that, we're also announcing a [partnership with Hugging Face](https://blog.cloudflare.com/best-place-region-earth-inference), and soon you'll be able to access and run a subset of the Hugging Face catalog directly from Workers AI.\n\n#### Analytics + observability\n\nUp to this point, we’ve been hyper focussed on one thing - making it really easy for any developer to run powerful AI models in just a few lines of code. But that’s only one part of the story. Up next, we’ll be working on some analytics and observability capabilities to give you insights into your usage + performance + spend on a per-model basis, plus the ability to fig into your logs if you want to do some exploring.\n\n#### A road to global GPU coverage\n\nOur goal is to be the best place to run inference on Region: Earth, so we're adding GPUs to our data centers as fast as we can.\n\n**We plan to be in 100 data centers by the end of this year**\n\n![](https://blog.cloudflare.com/content/images/2023/09/image3-28.png)\n\n**And nearly everywhere by the end of 2024**\n\n![](https://blog.cloudflare.com/content/images/2023/09/unnamed-3.png)\n\n  \n**We’re really excited to see you build** - head over to [our docs](https://developers.cloudflare.com/workers-ai/) to get started.\n\nIf you need inspiration, want to share something you’re building, or have a question - pop into our [Developer Discord](https://discord.com/invite/cloudflaredev).\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [Cloudflare Workers](https://blog.cloudflare.com/tag/workers/) [AI](https://blog.cloudflare.com/tag/ai/) [Developer Platform](https://blog.cloudflare.com/tag/developer-platform/) [Database](https://blog.cloudflare.com/tag/database/)"
    },
    {
      "url": "https://blog.cloudflare.com/cloudflares-gen-x-servers-for-an-accelerated-future/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/cloudflares-gen-x-servers-for-an-accelerated-future/",
        "loadedTime": "2023-12-05T02:30:09.352Z",
        "referrerUrl": "https://blog.cloudflare.com/cloudflare-gen-12-server-bigger-better-cooler-in-a-2u1n-form-factor/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/cloudflares-gen-x-servers-for-an-accelerated-future/",
        "title": "Cloudflare’s Gen X: \nServers for an Accelerated Future",
        "description": "We designed and built Cloudflare’s network to be able to grow capacity quickly and inexpensively; to allow every server, in every city, to run every service; and to allow us to shift customers and traffic across our network efficiently.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "02/24/2020\n6 min read\n“Every server can run every service.”\nWe designed and built Cloudflare’s network to be able to grow capacity quickly and inexpensively; to allow every server, in every city, to run every service; and to allow us to shift customers and traffic across our network efficiently. We deploy standard, commodity hardware, and our product developers and customers do not need to worry about the underlying servers. Our software automatically manages the deployment and execution of our developers’ code and our customers’ code across our network. Since we manage the execution and prioritization of code running across our network, we are both able to optimize the performance of our highest tier customers and effectively leverage idle capacity across our network.\nAn alternative approach might have been to run several fragmented networks with specialized servers designed to run specific features, such as the Firewall, DDoS protection or Workers. However, we believe that approach would have resulted in wasted idle resources and given us less flexibility to build new software or adopt the newest available hardware. And a single optimization target means we can provide security and performance at the same time.\nWe use Anycast to route a web request to the nearest Cloudflare data center (from among 200 cities), improving performance and maximizing the surface area to fight attacks. \nOnce a datacenter is selected, we use Unimog, Cloudflare’s custom load balancing system, to dynamically balance requests across diverse generations of servers. We load balance at different layers: between cities, between physical deployments located across a city, between external Internet ports, between internal cables, between servers, and even between logical CPU threads within a server.\nAs demand grows, we can scale out by simply adding new servers, points of presence (PoPs), or cities to the global pool of available resources. If any server component has a hardware failure, it is gracefully de-prioritized or removed from the pool, to be batch repaired by our operations team. This architecture has enabled us to have no dedicated Cloudflare staff at any of the 200 cities, instead relying on help for infrequent physical tasks from the ISPs (or data centers) hosting our equipment.\nGen X: Intel Not Inside\nWe recently turned up our tenth generation of servers, “Gen X”, already deployed across major US cities, and in the process of being shipped worldwide. Compared with our prior server (Gen 9), it processes as much as 36% more requests while costing substantially less. Additionally, it enables a ~50% decrease in L3 cache miss rate and up to 50% decrease in NGINX p99 latency, powered by a CPU rated at 25% lower TDP (thermal design power) per core.\nNotably, for the first time, Intel is not inside. We are not using their hardware for any major server components such as the CPU, board, memory, storage, network interface card (or any type of accelerator). Given how critical Intel is to our industry, this would until recently have been unimaginable, and is in contrast with prior generations which made extensive use of their hardware.\nIntel-based Gen 9 server\nThis time, AMD is inside.\nWe were particularly impressed by the 2nd Gen AMD EPYC processors because they proved to be far more efficient for our customers’ workloads. Since the pendulum of technology leadership swings back and forth between providers, we wouldn’t be surprised if that changes over time. However, we were happy to adapt quickly to the components that made the most sense for us.\nCompute\nCPU efficiency is very important to our server design. Since we have a compute-heavy workload, our servers are typically limited by the CPU before other components. Cloudflare’s software stack scales quite well with additional cores. So, we care more about core-count and power-efficiency than dimensions such as clock speed.\nWe selected the AMD EPYC 7642 processor in a single-socket configuration for Gen X. This CPU has 48-cores (96 threads), a base clock speed of 2.4 GHz, and an L3 cache of 256 MB. While the rated power (225W) may seem high, it is lower than the combined TDP in our Gen 9 servers and we preferred the performance of this CPU over lower power variants. Despite AMD offering a higher core count option with 64-cores, the performance gains for our software stack and usage weren’t compelling enough.\nWe have deployed the AMD EPYC 7642 in half a dozen Cloudflare data centers; it is considerably more powerful than a dual-socket pair of high-core count Intel processors (Skylake as well as Cascade Lake) we used in the last generation.\nReaders of our blog might remember our excitement around ARM processors. We even ported the entirety of our software stack to run on ARM, just as it does with x86, and have been maintaining that ever since even though it calls for slightly more work for our software engineering teams. We did this leading up to the launch of Qualcomm’s Centriq server CPU, which eventually got shuttered. While none of the off-the-shelf ARM CPUs available this moment are interesting to us, we remain optimistic about high core count offerings launching in 2020 and beyond, and look forward to a day when our servers are a mix of x86 (Intel and AMD) and ARM.\nWe aim to replace servers when the efficiency gains enabled by new equipment outweigh their cost.\nThe performance we’ve seen from the AMD EPYC 7642 processor has encouraged us to accelerate replacement of multiple generations of Intel-based servers.\nCompute is our largest investment in a server. Our heaviest workloads, from the Firewall to Workers (our serverless offering), often require more compute than other server resources. Also, the average size in kilobytes of a web request across our network tends to be small, influenced in part by the relative popularity of APIs and mobile applications. Our approach to server design is very different than traditional content delivery networks engineered to deliver large object video libraries, for whom servers focused on storage might make more sense, and re-architecting to offer serverless is prohibitively capital intensive.\nOur Gen X server is intentionally designed with an “empty” PCIe slot for a potential add on card, if it can perform some functions more efficiently than the primary CPU. Would that be a GPU, FPGA, SmartNIC, custom ASIC, TPU or something else? We’re intrigued to explore the possibilities.\nIn accompanying blog posts over the next few days, our hardware engineers will describe how AMD 7642 performed against the benchmarks we care about. We are thankful for their hard work.\nMemory, Storage & Network\nSince we are typically limited by CPU, Gen X represented an opportunity to grow components such as RAM and SSD more slowly than compute. \nFor memory, we continued to use 256GB of RAM, as in our prior generation, but rated higher at 2933MHz. For storage, we continue to have ~3TB, but moved to 3x1TB form factor using NVME flash (instead of SATA) with increased available IOPS and higher endurance, which enables full disk encryption using LUKS without penalty. For the network card, we continue to use Mellanox 2x25G NIC.\nWe moved from our multi-node chassis back to a simple 1U form factor, designed to be lighter and less error prone during operational work at the data center. We also added multiple new ODM partners to diversify how we manufacture our equipment and to take advantage of additional global warehousing. \nNetwork Expansion\nOur newest generation of servers give us the flexibility to continue to build out our network even closer to every user on Earth. We’re proud of the hard work from across engineering teams on Gen X, and are grateful for the support of our partners. Be on the lookout for more blogs about these servers in the coming days.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nHardware Cloudflare Network Data Center Partners Speed & Reliability",
      "markdown": "02/24/2020\n\n*   [![Nitin Rao](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2019/08/WtxhLSEUSgqYRMdQUwWp_1204a20b3d1d96bba523a6a2a5fa3cd73bd4fe59240a27ad6eb8c064c6792446.jpg)](https://blog.cloudflare.com/author/nitin-rao/)\n*   [![John Graham-Cumming](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2017/03/url-2.jpg)](https://blog.cloudflare.com/author/john-graham-cumming/)\n\n6 min read\n\n![](https://blog.cloudflare.com/content/images/2020/02/gen-x-color-Friday--twitter@2x.png)\n\n> _“Every server can run every service.”_\n\nWe designed and built Cloudflare’s network to be able to grow capacity quickly and inexpensively; to allow every server, in every city, to run every service; and to allow us to shift customers and traffic across our network efficiently. We deploy standard, commodity hardware, and our product developers and customers do not need to worry about the underlying servers. Our software automatically manages the deployment and execution of our developers’ code and our customers’ code across our network. Since we manage the execution and prioritization of code running across our network, we are both able to optimize the performance of our highest tier customers and effectively leverage idle capacity across our network.\n\nAn alternative approach might have been to run several fragmented networks with specialized servers designed to run specific features, such as the [Firewall](https://www.cloudflare.com/waf/), [DDoS protection](https://www.cloudflare.com/ddos/) or [Workers](https://workers.cloudflare.com/). However, we believe that approach would have resulted in wasted idle resources and given us less flexibility to build new software or adopt the newest available hardware. And a single optimization target means we can provide security and performance at the same time.\n\nWe use Anycast to route a web request to the nearest Cloudflare data center (from among [200 cities](https://www.cloudflare.com/network/)), improving performance and maximizing the surface area to fight attacks.\n\nOnce a datacenter is selected, we use Unimog, Cloudflare’s custom load balancing system, to dynamically balance requests across diverse generations of servers. We load balance at different layers: between cities, between physical deployments located across a city, between external Internet ports, between internal cables, between servers, and even between logical CPU threads within a server.\n\nAs demand grows, we can scale out by simply adding new servers, points of presence (PoPs), or cities to the global pool of available resources. If any server component has a hardware failure, it is gracefully de-prioritized or removed from the pool, to be batch repaired by our operations team. This architecture has enabled us to have no dedicated Cloudflare staff at any of the 200 cities, instead relying on help for infrequent physical tasks from the ISPs (or data centers) hosting our equipment.\n\n### Gen X: Intel Not Inside\n\n![](https://blog.cloudflare.com/content/images/2020/02/AMD_EPYC-39.jpg)\n\nWe recently turned up our tenth generation of servers, “Gen X”, already deployed across major US cities, and in the process of being shipped worldwide. Compared with our prior server ([Gen 9](https://blog.cloudflare.com/a-tour-inside-cloudflares-g9-servers/)), it processes as much as 36% more requests while costing substantially less. Additionally, it enables a ~50% decrease in L3 cache miss rate and up to 50% decrease in NGINX p99 latency, powered by a CPU rated at 25% lower TDP (thermal design power) per core.\n\nNotably, for the first time, Intel is not inside. We are not using their hardware for any major server components such as the CPU, board, memory, storage, network interface card (or any type of accelerator). Given how critical Intel is to our industry, this would until recently have been unimaginable, and is in contrast with [prior](https://blog.cloudflare.com/a-tour-inside-cloudflares-g9-servers/) [generations](https://blog.cloudflare.com/a-tour-inside-cloudflares-latest-generation-servers/) which made extensive use of their hardware.\n\n![](https://blog.cloudflare.com/content/images/2020/02/AMD2.png)\n\n_Intel-based Gen 9 server_\n\nThis time, AMD is inside.\n\nWe were particularly impressed by the 2nd Gen AMD EPYC processors because they proved to be far more efficient for our customers’ workloads. Since the pendulum of technology leadership swings back and forth between providers, we wouldn’t be surprised if that changes over time. However, we were happy to adapt quickly to the components that made the most sense for us.\n\n### Compute\n\n![](https://blog.cloudflare.com/content/images/2020/02/pasted-image-0.png)\n\nCPU efficiency is very important to our server design. Since we have a compute-heavy workload, our servers are typically limited by the CPU before other components. Cloudflare’s software stack scales quite well with additional cores. So, we care more about core-count and power-efficiency than dimensions such as clock speed.\n\nWe selected the AMD EPYC 7642 processor in a single-socket configuration for Gen X. This CPU has 48-cores (96 threads), a base clock speed of 2.4 GHz, and an L3 cache of 256 MB. While the rated power (225W) may seem high, it is lower than the combined TDP in our Gen 9 servers and we preferred the performance of this CPU over lower power variants. Despite AMD offering a higher core count option with 64-cores, the performance gains for our software stack and usage weren’t compelling enough.\n\nWe have deployed the AMD EPYC 7642 in half a dozen Cloudflare data centers; it is considerably more powerful than a dual-socket pair of [high-core count Intel processors](https://blog.cloudflare.com/a-tour-inside-cloudflares-g9-servers/) (Skylake as well as Cascade Lake) we used in the last generation.\n\nReaders of our blog might remember our excitement around ARM processors. We even ported the entirety of our software stack to run on ARM, just as it does with x86, and have been maintaining that ever since even though it calls for slightly more work for our software engineering teams. We did this leading up to the launch of [Qualcomm’s Centriq server CPU](https://blog.cloudflare.com/arm-takes-wing/), which eventually got shuttered. While none of the off-the-shelf ARM CPUs available this moment are interesting to us, we remain optimistic about high core count offerings launching in 2020 and beyond, and look forward to a day when our servers are a mix of x86 (Intel and AMD) and ARM.\n\nWe aim to replace servers when the efficiency gains enabled by new equipment outweigh their cost.\n\nThe performance we’ve seen from the AMD EPYC 7642 processor has encouraged us to accelerate replacement of multiple generations of Intel-based servers.\n\nCompute is our largest investment in a server. Our heaviest workloads, from the Firewall to [Workers](https://blog.cloudflare.com/cloud-computing-without-containers/) (our serverless offering), often require more compute than other server resources. Also, the average size in kilobytes of a web request across our network tends to be small, influenced in part by the relative popularity of APIs and mobile applications. Our approach to server design is very different than traditional [content delivery networks](https://www.cloudflare.com/learning/cdn/what-is-a-cdn/) engineered to deliver large object video libraries, for whom servers focused on storage might make more sense, and re-architecting to offer serverless is prohibitively capital intensive.\n\nOur Gen X server is intentionally designed with an “empty” PCIe slot for a potential add on card, if it can perform some functions more efficiently than the primary CPU. Would that be a GPU, FPGA, SmartNIC, custom ASIC, TPU or something else? We’re intrigued to explore the possibilities.\n\nIn accompanying blog posts over the next few days, our hardware engineers will describe how AMD 7642 performed against the benchmarks we care about. We are thankful for their hard work.\n\n## Memory, Storage & Network\n\nSince we are typically limited by CPU, Gen X represented an opportunity to grow components such as RAM and SSD more slowly than compute.\n\n![](https://blog.cloudflare.com/content/images/2020/02/AMD_EPYC-13.jpg)\n\nFor memory, we continued to use 256GB of RAM, as in our prior generation, but rated higher at 2933MHz. For storage, we continue to have ~3TB, but moved to 3x1TB form factor using NVME flash (instead of SATA) with increased available IOPS and higher endurance, which enables [full disk encryption using LUKS without penalty](https://www.usenix.org/conference/vault20/presentation/korchagin). For the network card, we continue to use Mellanox 2x25G NIC.\n\n![](https://blog.cloudflare.com/content/images/2020/02/AMD_EPYC-5.jpg)\n\nWe moved from our multi-node chassis back to a simple 1U form factor, designed to be lighter and less error prone during operational work at the data center. We also added multiple new ODM partners to diversify how we manufacture our equipment and to take advantage of additional global warehousing.\n\n![](https://blog.cloudflare.com/content/images/2020/02/AMD_EPYC-7.jpg)\n\n### Network Expansion\n\n![](https://blog.cloudflare.com/content/images/2020/02/AMD_EPYC-35.jpg)\n\nOur newest generation of servers give us the flexibility to continue to build out our network even closer to every user on Earth. We’re proud of the hard work from across engineering teams on Gen X, and are grateful for the support of our partners. Be on the lookout for more blogs about these servers in the coming days.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Hardware](https://blog.cloudflare.com/tag/hardware/) [Cloudflare Network](https://blog.cloudflare.com/tag/cloudflare-network/) [Data Center](https://blog.cloudflare.com/tag/data-center/) [Partners](https://blog.cloudflare.com/tag/partners/) [Speed & Reliability](https://blog.cloudflare.com/tag/speed-and-reliability/)"
    },
    {
      "url": "https://blog.cloudflare.com/page/3/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/page/3/",
        "loadedTime": "2023-12-05T02:30:21.042Z",
        "referrerUrl": "https://blog.cloudflare.com/page/2/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/page/3/",
        "title": "The Cloudflare Blog (Page 3)",
        "description": null,
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "September 29, 2023 2:05PM \nPost-quantum cryptography goes GA\nBirthday Week General Availability Product News Research \nCloudflare announces Post-Quantum Cryptography as a Generally Available system...\nSeptember 29, 2023 2:00PM \nEncrypted Client Hello - the last puzzle piece to privacy\nBirthday Week Product News Encrypted SNI Research \nWe're excited to announce a contribution to improving privacy for everyone on the Internet. Encrypted Client Hello, a new standard that prevents networks from snooping on which websites a user is visiting, is now available on all Cloudflare plans....\nSeptember 29, 2023 2:00PM \nCloudflare now uses post-quantum cryptography to talk to your origin server\nBirthday Week Post-Quantum Research Cryptography Security \nStarting today, you can secure the connection between Cloudflare and your origin server with post-quantum cryptography...\nSeptember 29, 2023 2:00PM \nPrivacy-preserving measurement and machine learning\nBirthday Week Privacy Machine Learning Research \nCloudflare is implementing DAP (Distributed Aggregation Protocol) – a way of aggregating data without exposing individual measurements that uses multi-party computation...\nSeptember 29, 2023 2:00PM \nNetwork performance update: Birthday Week 2023\nBirthday Week Cloudflare Radar Network Performance Update \nIn this post we are going to share the most recent updates since our last post in June, and tell you about our tools and processes that we use to monitor and improve our network performance...\nSeptember 29, 2023 2:00PM \nSee what threats are lurking in your Office 365 with Cloudflare Email Retro Scan\nBirthday Week \nWe are now announcing the ability for Cloudflare customers to scan old messages within their Office 365 Inboxes for threats. This Retro Scan will let you look back seven days and see what threats your current email security tool has missed...\nSeptember 29, 2023 2:00PM \nDetecting zero-days before zero-day\nBirthday Week WAF Security AI \nIn this blog post we talk about our approach and ongoing research into detecting novel web attack vectors in our WAF before they are seen by a security researcher....\nSeptember 29, 2023 2:00PM \nCloudflare is free of CAPTCHAs; Turnstile is free for everyone\nBirthday Week Product News Turnstile CAPTCHA Speed & Reliability Security Bots \nNow that we’ve eliminated CAPTCHAs at Cloudflare, we want to hasten the demise of CAPTCHAs across the internet. We’re thrilled to announce that Turnstile is generally available, and Turnstile’s ‘Managed’ mode is now completely free to everyone for unlimited use....\nSeptember 29, 2023 2:00PM \nEasily manage AI crawlers with our new bot categories\nBirthday Week \nManage AI crawlers, out of the box with Cloudflare...\nSeptember 28, 2023 2:02PM \nHyperdrive: making databases feel like they’re global\nBirthday Week Product News Database Developer Platform \nHyperdrive makes accessing your existing databases from Cloudflare Workers, wherever they are running, hyper fast...\nSeptember 28, 2023 2:00PM \nRace ahead with Cloudflare Pages build caching\nBirthday Week Cloudflare Pages Beta Speed \nUnleash the fast & furious in your builds with Cloudflare Pages' build caching. Reduce build times by caching previously computed project components. Now in Beta for select frameworks and package managers....\nSeptember 28, 2023 2:00PM \nRe-introducing the Cloudflare Workers Playground\nBirthday Week Developers Cloudflare Workers Dashboard \nToday, we’re excited to announce an updated Cloudflare Workers playground, where users can develop and test Workers before sharing or deploying them...\nSeptember 28, 2023 2:00PM \nRunning Serverless Puppeteer with Workers and Durable Objects\nBirthday Week Product News Durable Objects JavaScript Developers \nWe’ve heard from developers that configuring and maintaining their own serverless browser automation systems can be quite painful. The Workers Browser Rendering API solves this...\nSeptember 28, 2023 2:00PM \nA Socket API that works across JavaScript runtimes — announcing a WinterCG spec and Node.js implementation of connect()\nBirthday Week Product News Cloudflare Workers Developers TCP Node.js \nEngineers from Cloudflare and Vercel have published a specification of the connect() sockets API for review by the community, along with a Node.js compatible implementation of connect() that developers can start using today...\nSeptember 28, 2023 2:00PM \nCloudflare Integrations Marketplace introduces three new partners: Sentry, Momento and Turso\nBirthday Week Product News Developers Cloudflare Workers Observability Edge Database Cache Partners \nEarlier this year, we introduced integrations with Supabase, PlanetScale, Neon and Upstash. Today, we are thrilled to introduce our newest additions to Cloudflare’s Integrations Marketplace – Sentry, Turso and Momento...\nSeptember 28, 2023 2:00PM \nCloudflare is now powering Microsoft Edge Secure Network\nBirthday Week Privacy \nThis integration will allow Microsoft Edge users who opt in to browse the Internet more privately, without being tracked across websites...\nSeptember 28, 2023 2:00PM \nD1: open beta is here\nBirthday Week Developer Platform Database D1 \nD1 is now in open beta, and the theme is “scale”: with higher per-database storage limits and the ability to create more databases, we’re unlocking the ability for developers to build production-scale applications on D1...\nSeptember 28, 2023 2:00PM \nNew Workers pricing — never pay to wait on I/O again\nBirthday Week Developers Cloudflare Workers Product News \nAnnouncing new pricing for Cloudflare Workers, where you are billed based on CPU time, and never for the idle time that your Worker spends waiting on network requests and other I/O....\nSeptember 27, 2023 2:02PM \nThe best place on Region: Earth for inference\nBirthday Week \nToday, we’re excited to make a series of announcements that we believe will make a similar impact as Workers did in the future of computing...\nSeptember 27, 2023 2:00PM \nYou can now use WebGPU in Cloudflare Workers\nBirthday Week Cloudflare Workers Standards \nToday, we are introducing WebGPU support to Cloudflare Workers. This blog will explain why it's important, why we did it, how you can use it, and what comes next...",
      "markdown": "September 29, 2023 2:05PM\n\n[\n\n## Post-quantum cryptography goes GA\n\n](https://blog.cloudflare.com/post-quantum-cryptography-ga/)[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [General Availability](https://blog.cloudflare.com/tag/general-availability/) [Product News](https://blog.cloudflare.com/tag/product-news/) [Research](https://blog.cloudflare.com/tag/research/)\n\nCloudflare announces Post-Quantum Cryptography as a Generally Available system...\n\n*   [![Wesley Evans](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/08/Headshot.jpeg)](https://blog.cloudflare.com/author/wesley/)\n*   [![Bas Westerbaan](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/11/me.png)](https://blog.cloudflare.com/author/bas/)\n*   [![Christopher Patton](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2018/09/0-8.jpg)](https://blog.cloudflare.com/author/christopher-patton/)\n*   [![Peter Wu](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/05/_tmp_mini_magick20221201-42-hdr80s.jpg)](https://blog.cloudflare.com/author/peter-wu/)\n*   [![Vânia Gonçalves](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/08/cover_photo.jpg)](https://blog.cloudflare.com/author/vania/)\n\nSeptember 29, 2023 2:00PM\n\n[\n\n## Encrypted Client Hello - the last puzzle piece to privacy\n\n](https://blog.cloudflare.com/announcing-encrypted-client-hello/)[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [Product News](https://blog.cloudflare.com/tag/product-news/) [Encrypted SNI](https://blog.cloudflare.com/tag/encrypted-sni/) [Research](https://blog.cloudflare.com/tag/research/)\n\nWe're excited to announce a contribution to improving privacy for everyone on the Internet. Encrypted Client Hello, a new standard that prevents networks from snooping on which websites a user is visiting, is now available on all Cloudflare plans....\n\n*   [![Achiel van der Mandele](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2019/04/avatar.png)](https://blog.cloudflare.com/author/achiel/)\n*   [![Alessandro Ghedini](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2020/09/Ii4TgsoTda2layE131Dk_1204a20b3d1d96bba523a6a2a5fa3cd73bd4fe59240a27ad6eb8c064c6792446.jpg)](https://blog.cloudflare.com/author/alessandro-ghedini/)\n*   [![Christopher Wood](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/05/Chris_Wood.png)](https://blog.cloudflare.com/author/christopher/)\n*   [![Rushil Mehra](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/02/signal-2023-02-27-082155_002-2.jpeg)](https://blog.cloudflare.com/author/rushil-mehra/)\n\nSeptember 29, 2023 2:00PM\n\n[\n\n## Cloudflare now uses post-quantum cryptography to talk to your origin server\n\n](https://blog.cloudflare.com/post-quantum-to-origins/)[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [Post-Quantum](https://blog.cloudflare.com/tag/post-quantum/) [Research](https://blog.cloudflare.com/tag/research/) [Cryptography](https://blog.cloudflare.com/tag/cryptography/) [Security](https://blog.cloudflare.com/tag/security/)\n\nStarting today, you can secure the connection between Cloudflare and your origin server with post-quantum cryptography...\n\n*   [![Suleman Ahmad](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/09/profile_pic.png)](https://blog.cloudflare.com/author/suleman/)\n*   [![Luke Valenta](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2019/03/profile.jpg)](https://blog.cloudflare.com/author/luke/)\n*   [![Bas Westerbaan](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/11/me.png)](https://blog.cloudflare.com/author/bas/)\n\nSeptember 29, 2023 2:00PM\n\n[\n\n## Privacy-preserving measurement and machine learning\n\n](https://blog.cloudflare.com/deep-dive-privacy-preserving-measurement/)[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [Privacy](https://blog.cloudflare.com/tag/privacy/) [Machine Learning](https://blog.cloudflare.com/tag/machine-learning/) [Research](https://blog.cloudflare.com/tag/research/)\n\nCloudflare is implementing DAP (Distributed Aggregation Protocol) – a way of aggregating data without exposing individual measurements that uses multi-party computation...\n\n*   [![Christopher Patton](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2018/09/0-8.jpg)](https://blog.cloudflare.com/author/christopher-patton/)\n*   [![Mari Galicer](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/02/Screen-Shot-2022-01-07-at-1.06.44-PM.png)](https://blog.cloudflare.com/author/mari/)\n\nSeptember 29, 2023 2:00PM\n\n[\n\n## Network performance update: Birthday Week 2023\n\n](https://blog.cloudflare.com/network-performance-update-birthday-week-2023/)[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [Cloudflare Radar](https://blog.cloudflare.com/tag/cloudflare-radar/) [Network Performance Update](https://blog.cloudflare.com/tag/network-performance-update/)\n\nIn this post we are going to share the most recent updates since our last post in June, and tell you about our tools and processes that we use to monitor and improve our network performance...\n\n*   [![David Tuber](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/05/kutiya-face-1.png)](https://blog.cloudflare.com/author/tubes/)\n*   [![Onur Karaagaoglu](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/06/Onur.jpeg)](https://blog.cloudflare.com/author/onur/)\n\nSeptember 29, 2023 2:00PM\n\n[\n\n## See what threats are lurking in your Office 365 with Cloudflare Email Retro Scan\n\n](https://blog.cloudflare.com/threats-lurking-office-365-cloudflare-email-retro-scan/)[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/)\n\nWe are now announcing the ability for Cloudflare customers to scan old messages within their Office 365 Inboxes for threats. This Retro Scan will let you look back seven days and see what threats your current email security tool has missed...\n\n*   [![Ayush Kumar](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/11/Screen-Shot-2022-01-14-at-2.08.16-PM.png)](https://blog.cloudflare.com/author/ayush/)\n\nSeptember 29, 2023 2:00PM\n\n[\n\n## Detecting zero-days before zero-day\n\n](https://blog.cloudflare.com/detecting-zero-days-before-zero-day/)[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [WAF](https://blog.cloudflare.com/tag/waf/) [Security](https://blog.cloudflare.com/tag/security/) [AI](https://blog.cloudflare.com/tag/ai/)\n\nIn this blog post we talk about our approach and ongoing research into detecting novel web attack vectors in our WAF before they are seen by a security researcher....\n\n*   [![Michael Tremante](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2017/03/url-3.jpg)](https://blog.cloudflare.com/author/michael-tremante/)\n\nSeptember 29, 2023 2:00PM\n\n[\n\n## Cloudflare is free of CAPTCHAs; Turnstile is free for everyone\n\n](https://blog.cloudflare.com/turnstile-ga/)[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [Product News](https://blog.cloudflare.com/tag/product-news/) [Turnstile](https://blog.cloudflare.com/tag/turnstile/) [CAPTCHA](https://blog.cloudflare.com/tag/captcha/) [Speed & Reliability](https://blog.cloudflare.com/tag/speed-and-reliability/) [Security](https://blog.cloudflare.com/tag/security/) [Bots](https://blog.cloudflare.com/tag/bots/)\n\nNow that we’ve eliminated CAPTCHAs at Cloudflare, we want to hasten the demise of CAPTCHAs across the internet. We’re thrilled to announce that Turnstile is generally available, and Turnstile’s ‘Managed’ mode is now completely free to everyone for unlimited use....\n\n*   [![Benedikt Wolters](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/04/Screenshot-2020-10-27-165713.png)](https://blog.cloudflare.com/author/benedikt/)\n*   [![Maxime Guerreiro](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2019/06/unnamed.jpg)](https://blog.cloudflare.com/author/maxime/)\n*   [![Adam Martinetti](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/12/_tmp_uploaded20220909-4-1lugsmy.jpg)](https://blog.cloudflare.com/author/adam-martinetti/)\n\nSeptember 29, 2023 2:00PM\n\n[\n\n## Easily manage AI crawlers with our new bot categories\n\n](https://blog.cloudflare.com/ai-bots/)[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/)\n\nManage AI crawlers, out of the box with Cloudflare...\n\n*   [![Reid Tatoris](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/03/reid-headshot.jpeg)](https://blog.cloudflare.com/author/reid/)\n*   [![Pawel Klimek](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/09/received_3612573035734026-7.jpeg)](https://blog.cloudflare.com/author/pawel/)\n\nSeptember 28, 2023 2:02PM\n\n[\n\n## Hyperdrive: making databases feel like they’re global\n\n](https://blog.cloudflare.com/hyperdrive-making-regional-databases-feel-distributed/)[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [Product News](https://blog.cloudflare.com/tag/product-news/) [Database](https://blog.cloudflare.com/tag/database/) [Developer Platform](https://blog.cloudflare.com/tag/developer-platform/)\n\nHyperdrive makes accessing your existing databases from Cloudflare Workers, wherever they are running, hyper fast...\n\n*   [![Matt Silverlock](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/08/profile-1500px-square.jpeg)](https://blog.cloudflare.com/author/silverlock/)\n*   [![Alex Robinson](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/09/headshot.jpeg)](https://blog.cloudflare.com/author/alex-robinson/)\n\nSeptember 28, 2023 2:00PM\n\n[\n\n## Race ahead with Cloudflare Pages build caching\n\n](https://blog.cloudflare.com/race-ahead-with-build-caching/)[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [Cloudflare Pages](https://blog.cloudflare.com/tag/cloudflare-pages/) [Beta](https://blog.cloudflare.com/tag/beta/) [Speed](https://blog.cloudflare.com/tag/speed/)\n\nUnleash the fast & furious in your builds with Cloudflare Pages' build caching. Reduce build times by caching previously computed project components. Now in Beta for select frameworks and package managers....\n\n*   [![Anni Wang](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/08/Image_20230721_085153_520--1--copy_cropped.png)](https://blog.cloudflare.com/author/anni/)\n*   [![Jacob Hands](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/11/twitter-profile-photo-2.jpeg)](https://blog.cloudflare.com/author/jacob-hands/)\n*   [![John Fawcett](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2017/03/767070.jpg)](https://blog.cloudflare.com/author/john-fawcett/)\n\nSeptember 28, 2023 2:00PM\n\n[\n\n## Re-introducing the Cloudflare Workers Playground\n\n](https://blog.cloudflare.com/workers-playground/)[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [Developers](https://blog.cloudflare.com/tag/developers/) [Cloudflare Workers](https://blog.cloudflare.com/tag/workers/) [Dashboard](https://blog.cloudflare.com/tag/dashboard-tag/)\n\nToday, we’re excited to announce an updated Cloudflare Workers playground, where users can develop and test Workers before sharing or deploying them...\n\n*   [![Adam Murray](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/04/adam_headshot-1.jpg)](https://blog.cloudflare.com/author/adam-murray/)\n*   [![Samuel Macleod](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/04/IMG_5133.jpg)](https://blog.cloudflare.com/author/samuel/)\n\nSeptember 28, 2023 2:00PM\n\n[\n\n## Running Serverless Puppeteer with Workers and Durable Objects\n\n](https://blog.cloudflare.com/running-serverless-puppeteer-workers-durable-objects/)[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [Product News](https://blog.cloudflare.com/tag/product-news/) [Durable Objects](https://blog.cloudflare.com/tag/durable-objects/) [JavaScript](https://blog.cloudflare.com/tag/javascript/) [Developers](https://blog.cloudflare.com/tag/developers/)\n\nWe’ve heard from developers that configuring and maintaining their own serverless browser automation systems can be quite painful. The Workers Browser Rendering API solves this...\n\n*   [![Tanushree Sharma](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/09/IMG_1328-1.jpg)](https://blog.cloudflare.com/author/tanushree/)\n\nSeptember 28, 2023 2:00PM\n\n[\n\n## A Socket API that works across JavaScript runtimes — announcing a WinterCG spec and Node.js implementation of connect()\n\n](https://blog.cloudflare.com/socket-api-works-javascript-runtimes-wintercg-polyfill-connect/)[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [Product News](https://blog.cloudflare.com/tag/product-news/) [Cloudflare Workers](https://blog.cloudflare.com/tag/workers/) [Developers](https://blog.cloudflare.com/tag/developers/) [TCP](https://blog.cloudflare.com/tag/tcp/) [Node.js](https://blog.cloudflare.com/tag/node-js/)\n\nEngineers from Cloudflare and Vercel have published a specification of the connect() sockets API for review by the community, along with a Node.js compatible implementation of connect() that developers can start using today...\n\n*   [![Dominik Picheta](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/09/_tmp_mini_magick20230403-40-vmc77s.jpg)](https://blog.cloudflare.com/author/dominik/)\n*   [![James M Snell](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/11/mecoffee.jpg)](https://blog.cloudflare.com/author/jasnell/)\n*   [![Ethan Arrowood (Guest author)](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/09/Ethan-Arrowood.jpeg)](https://blog.cloudflare.com/author/ethan-arrowood/)\n\nSeptember 28, 2023 2:00PM\n\n[\n\n## Cloudflare Integrations Marketplace introduces three new partners: Sentry, Momento and Turso\n\n](https://blog.cloudflare.com/cloudflare-integrations-marketplace-new-partners-sentry-momento-turso/)[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [Product News](https://blog.cloudflare.com/tag/product-news/) [Developers](https://blog.cloudflare.com/tag/developers/) [Cloudflare Workers](https://blog.cloudflare.com/tag/workers/) [Observability](https://blog.cloudflare.com/tag/observability/) [Edge Database](https://blog.cloudflare.com/tag/edge-database/) [Cache](https://blog.cloudflare.com/tag/cache/) [Partners](https://blog.cloudflare.com/tag/partners/)\n\nEarlier this year, we introduced integrations with Supabase, PlanetScale, Neon and Upstash. Today, we are thrilled to introduce our newest additions to Cloudflare’s Integrations Marketplace – Sentry, Turso and Momento...\n\n*   [![Tanushree Sharma](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/09/IMG_1328-1.jpg)](https://blog.cloudflare.com/author/tanushree/)\n\nSeptember 28, 2023 2:00PM\n\n[\n\n## Cloudflare is now powering Microsoft Edge Secure Network\n\n](https://blog.cloudflare.com/cloudflare-now-powering-microsoft-edge-secure-network/)[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [Privacy](https://blog.cloudflare.com/tag/privacy/)\n\nThis integration will allow Microsoft Edge users who opt in to browse the Internet more privately, without being tracked across websites...\n\n*   [![Mari Galicer](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/02/Screen-Shot-2022-01-07-at-1.06.44-PM.png)](https://blog.cloudflare.com/author/mari/)\n\nSeptember 28, 2023 2:00PM\n\n[\n\n## D1: open beta is here\n\n](https://blog.cloudflare.com/d1-open-beta-is-here/)[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [Developer Platform](https://blog.cloudflare.com/tag/developer-platform/) [Database](https://blog.cloudflare.com/tag/database/) [D1](https://blog.cloudflare.com/tag/d1/)\n\nD1 is now in open beta, and the theme is “scale”: with higher per-database storage limits and the ability to create more databases, we’re unlocking the ability for developers to build production-scale applications on D1...\n\n*   [![Matt Silverlock](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/08/profile-1500px-square.jpeg)](https://blog.cloudflare.com/author/silverlock/)\n*   [![Ben Yule](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/09/headshot.png)](https://blog.cloudflare.com/author/ben-yule/)\n\nSeptember 28, 2023 2:00PM\n\n[\n\n## New Workers pricing — never pay to wait on I/O again\n\n](https://blog.cloudflare.com/workers-pricing-scale-to-zero/)[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [Developers](https://blog.cloudflare.com/tag/developers/) [Cloudflare Workers](https://blog.cloudflare.com/tag/workers/) [Product News](https://blog.cloudflare.com/tag/product-news/)\n\nAnnouncing new pricing for Cloudflare Workers, where you are billed based on CPU time, and never for the idle time that your Worker spends waiting on network requests and other I/O....\n\n*   [![Rita Kozlov](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/05/Rita-Kozlov.jpeg)](https://blog.cloudflare.com/author/rita/)\n*   [![Brendan Irvine-Broque](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/09/IMG_2312.JPG)](https://blog.cloudflare.com/author/brendan-irvine-broque/)\n\nSeptember 27, 2023 2:02PM\n\n[\n\n## The best place on Region: Earth for inference\n\n](https://blog.cloudflare.com/best-place-region-earth-inference/)[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/)\n\nToday, we’re excited to make a series of announcements that we believe will make a similar impact as Workers did in the future of computing...\n\n*   [![Rita Kozlov](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/05/Rita-Kozlov.jpeg)](https://blog.cloudflare.com/author/rita/)\n*   [![James Allworth](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2020/10/jamesallworth-20171002-024-bw-centered-square.jpg)](https://blog.cloudflare.com/author/james-allworth/)\n*   [![Seph Zdarko](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/08/_tmp_mini_magick20230113-42-cjr1w4.jpg)](https://blog.cloudflare.com/author/seph/)\n\nSeptember 27, 2023 2:00PM\n\n[\n\n## You can now use WebGPU in Cloudflare Workers\n\n](https://blog.cloudflare.com/webgpu-in-workers/)[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [Cloudflare Workers](https://blog.cloudflare.com/tag/workers/) [Standards](https://blog.cloudflare.com/tag/standards/)\n\nToday, we are introducing WebGPU support to Cloudflare Workers. This blog will explain why it's important, why we did it, how you can use it, and what comes next...\n\n*   [![André Cruz](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/02/andre2.jpg)](https://blog.cloudflare.com/author/andre-cruz/)\n*   [![Celso Martinho](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/Celso-Martinho.png)](https://blog.cloudflare.com/author/celso/)"
    },
    {
      "url": "https://blog.cloudflare.com/measuring-hyper-threading-and-turbo-boost/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/measuring-hyper-threading-and-turbo-boost/",
        "loadedTime": "2023-12-05T02:30:21.845Z",
        "referrerUrl": "https://blog.cloudflare.com/cloudflare-gen-12-server-bigger-better-cooler-in-a-2u1n-form-factor/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/measuring-hyper-threading-and-turbo-boost/",
        "title": "Measuring Hyper-Threading and Turbo Boost",
        "description": "Contemporary x86 processors implement some variants of Hyper-Threading and Turbo Boost. We decided to learn about the implications of these two technologies.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/05/2021\n6 min read\nWe often put together experiments that measure hardware performance to improve our understanding and provide insights to our hardware partners. We recently wanted to know more about Hyper-Threading and Turbo Boost. The last time we assessed these two technologies was when we were still deploying the Intel Xeons (Skylake/Purley), but beginning with our Gen X servers we switched over to the AMD EPYC (Zen 2/Rome). This blog is about our latest attempt at quantifying the performance impact of Hyper-Threading and Turbo Boost on our AMD-based servers running our software stack.\nIntel briefly introduced Hyper-Threading with NetBurst (Northwood) back in 2002, then reintroduced Hyper-Threading six years later with Nehalem along with Turbo Boost. AMD presented their own implementation of these technologies with Zen in 2017, but AMD’s version of Turbo Boost actually dates back to AMD K10 (Thuban), in 2010, when it used to be called Turbo Core. Since Zen, Hyper-Threading and Turbo Boost are known as simultaneous multithreading (SMT) and Core Performance Boost (CPB), respectively. The underlying implementation of Hyper-Threading and Turbo Boost differs between the two vendors, but the high-level concept remains the same.\nHyper-Threading or simultaneous multithreading creates a second hardware thread within a processor’s core, also known as a logical core, by duplicating various parts of the core to support the context of a second application thread. The two hardware threads execute simultaneously within the core, across their dedicated and remaining shared resources. If neither hardware threads contend over a particular shared resource, then the throughput can be drastically increased.\nTurbo Boost or Core Performance Boost opportunistically allows the processor to operate beyond its rated base frequency as long as the processor operates within guidelines set by Intel or AMD. Generally speaking, the higher the frequency, the faster the processor finishes a task.\nSimulated Environment\nCPU Specification\nOur Gen X or 10th generation servers are powered by the AMD EPYC 7642, based on the Zen 2 microarchitecture. The vast majority of the Zen 2-based processors along with its successor Zen 3 that our Gen 11 servers are based on, supports simultaneous multithreading and Core Performance Boost.\nSimilar to Intel’s Hyper-Threading, AMD implemented 2-way simultaneous multithreading. The AMD EPYC 7642 has 48 cores, and with simultaneous multithreading enabled it can simultaneously execute 96 hardware threads. Core Performance Boost allows the AMD EPYC 7642 to operate anywhere between 2.3 to 3.3 GHz, depending on the workload and limitations imposed on the processor. With Core Performance Boost disabled, the processor will operate at 2.3 GHz, the rated base frequency on the AMD EPYC 7642. We took our usual simulated traffic pattern of 10 KiB cached assets over HTTPS, provided by our performance team, to generate a sustained workload that saturated the processor to 100% CPU utilization.\nResults\nAfter establishing a baseline with simultaneous multithreading and Core Performance Boost disabled, we started enabling one feature at a time. When we enabled Core Performance Boost, the processor operated near its peak turbo frequency, hovering between 3.2 to 3.3 GHz which is more than 39% higher than the base frequency. Higher operating frequency directly translated into 40% additional requests per second. We then disabled Core Performance Boost and enabled simultaneous multithreading. Similar to Core Performance Boost, simultaneous multithreading alone improved requests per second by 43%. Lastly, by enabling both features, we observed an 86% improvement in requests per second.\nLatencies were generally lowered by either or both Core Performance Boost and simultaneous multithreading. While Core Performance Boost consistently maintained a lower latency than the baseline, simultaneous multithreading gradually took longer to process a request as it reached tail latencies. Though not depicted in the figure below, when we examined beyond p9999 or 99.99th percentile, simultaneous multithreading, even with the help of Core Performance Boost, exponentially increased in latency by more than 150% over the baseline, presumably due to the two hardware threads contending over a shared resource within the core.\nProduction Environment\nMoving into production, since our traffic fluctuates throughout the day, we took four identical Gen X servers and measured in parallel during peak hours. The only changes we made to the servers were enabling and disabling simultaneous multithreading and Core Performance Boost to create a comprehensive test matrix. We conducted the experiment in two different regions to identify any anomalies and mismatching trends. All trends were alike.\nBefore diving into the results, we should preface that the baseline server operated at a higher CPU utilization than others. Every generation, our servers deliver a noticeable improvement in performance. So our load balancer, named Unimog, sends a different number of connections to the target server based on its generation to balance out the CPU utilization. When we disabled simultaneous multithreading and Core Performance Boost, the baseline server’s performance degraded to the point where Unimog encountered a “guard rail” or the lower limit on the requests sent to the server, and so its CPU utilization rose instead. Given that the baseline server operated at a higher CPU utilization, the baseline server processed more requests per second to meet the minimum performance threshold.\nResults\nDue to the skewed baseline, when core performance boost was enabled, we only observed 7% additional requests per second. Next, simultaneous multithreading alone improved requests per second by 41%. Lastly, with both features enabled, we saw an 86% improvement in requests per second.\nThough we lack concrete baseline data, we can normalize requests per second by CPU utilization to approximate the improvement for each scenario. Once normalized, the estimated improvement in requests per second from core performance boost and simultaneous multithreading were 36% and 80%, respectively. With both features enabled, requests per second improved by 136%.\nLatency was not as interesting since the baseline server operated at a higher CPU utilization, and in turn, it produced a higher tail latency than we would have otherwise expected. All other servers maintained a lower latency due to their lower CPU utilization in conjunction with Core Performance Boost, simultaneous multithreading, or both.\nAt this point, our experiment did not go as we had planned. Our baseline is skewed, and we only got half useful answers. However, we find experimenting to be important because we usually end up finding other helpful insights as well.\nLet’s add power data. Since our baseline server was operating at a higher CPU utilization, we knew it was serving more requests and therefore, consumed more power than it needed to. Enabling Core Performance Boost allowed the processor to run up to its peak turbo frequency, increasing power consumption by 35% over the skewed baseline. More interestingly, enabling simultaneous multithreading increased power consumption by only 7%. Combining Core Performance Boost with simultaneous multithreading resulted in 58% increase in power consumption.\nAMD’s implementation of simultaneous multithreading appears to be power efficient as it achieves 41% additional requests per second while consuming only 7% more power compared to the skewed baseline. For completeness, using the data we have, we bridged performance and power together to obtain performance per watt to summarize power efficiency. We divided the non-normalized requests per second by power consumption to produce the requests per watt figure below. Our Gen X servers attained the best performance per watt by enabling just simultaneous multithreading.\nConclusion\nIn our assessment of AMD’s implementation of Hyper-Threading and Turbo Boost, the original experiment we designed to measure requests per second and latency did not pan out as expected. As soon as we entered production, our baseline measurement was skewed due to the imbalance in CPU utilization and only partially reproduced our lab results.\nWe added power to the experiment and found other meaningful insights. By analyzing the performance and power characteristics of simultaneous multithreading and Core Performance Boost, we concluded that simultaneous multithreading could be a power-efficient mechanism to attain additional requests per second. Drawbacks of simultaneous multithreading include long tail latency that is currently curtailed by enabling Core Performance Boost. While the higher frequency enabled by Core Performance Boost provides latency reduction and more requests per second, we are more mindful that the increase in power consumption is quite significant.\nDo you want to help shape the Cloudflare network? This blog was a glimpse of the work we do at Cloudflare. Come join us and help complete the feedback loop for our developers and hardware partners.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nHardware AMD EPYC Partners Performance",
      "markdown": "10/05/2021\n\n*   [![Sung Park](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2019/11/_tmp_uploaded20190827-140-5glea4.jpg)](https://blog.cloudflare.com/author/sung/)\n\n6 min read\n\n![](https://blog.cloudflare.com/content/images/2021/10/unnamed-6.png)\n\nWe often put together experiments that measure hardware performance to improve our understanding and provide insights to our hardware partners. We recently wanted to know more about Hyper-Threading and Turbo Boost. The last time we assessed these two technologies was when we were still [deploying the Intel Xeons](https://blog.cloudflare.com/a-tour-inside-cloudflares-g9-servers/) (Skylake/Purley), but beginning with our Gen X servers we [switched over to the AMD EPYC](https://blog.cloudflare.com/technical-details-of-why-cloudflare-chose-amd-epyc-for-gen-x-servers/) (Zen 2/Rome). This blog is about our latest attempt at quantifying the performance impact of Hyper-Threading and Turbo Boost on our AMD-based servers running our software stack.\n\nIntel briefly introduced Hyper-Threading with NetBurst (Northwood) back in 2002, then reintroduced Hyper-Threading six years later with Nehalem along with Turbo Boost. AMD presented their own implementation of these technologies with Zen in 2017, but AMD’s version of Turbo Boost actually dates back to AMD K10 (Thuban), in 2010, when it used to be called Turbo Core. Since Zen, Hyper-Threading and Turbo Boost are known as simultaneous multithreading (SMT) and Core Performance Boost (CPB), respectively. The underlying implementation of Hyper-Threading and Turbo Boost differs between the two vendors, but the high-level concept remains the same.\n\nHyper-Threading or simultaneous multithreading creates a second hardware thread within a processor’s core, also known as a logical core, by duplicating various parts of the core to support the context of a second application thread. The two hardware threads execute simultaneously within the core, across their dedicated and remaining shared resources. If neither hardware threads contend over a particular shared resource, then the throughput can be drastically increased.\n\nTurbo Boost or Core Performance Boost opportunistically allows the processor to operate beyond its rated base frequency as long as the processor operates within guidelines set by Intel or AMD. Generally speaking, the higher the frequency, the faster the processor finishes a task.\n\n## Simulated Environment\n\n### CPU Specification\n\n![](https://blog.cloudflare.com/content/images/2021/10/image2-12.png)\n\nOur [Gen X or 10th generation servers](https://blog.cloudflare.com/cloudflares-gen-x-servers-for-an-accelerated-future/) are powered by the [AMD EPYC 7642](https://www.amd.com/en/products/cpu/amd-epyc-7642), based on the Zen 2 microarchitecture. The vast majority of the Zen 2-based processors along with its successor Zen 3 that our [Gen 11 servers are based on](https://blog.cloudflare.com/the-epyc-journey-continues-to-milan-in-cloudflares-11th-generation-edge-server/), supports simultaneous multithreading and Core Performance Boost.\n\nSimilar to Intel’s Hyper-Threading, AMD implemented 2-way simultaneous multithreading. The AMD EPYC 7642 has 48 cores, and with simultaneous multithreading enabled it can simultaneously execute 96 hardware threads. Core Performance Boost allows the AMD EPYC 7642 to operate anywhere between 2.3 to 3.3 GHz, depending on the workload and limitations imposed on the processor. With Core Performance Boost disabled, the processor will operate at 2.3 GHz, the rated base frequency on the AMD EPYC 7642. We took our usual simulated traffic pattern of 10 KiB cached assets over HTTPS, [provided by our performance team](https://blog.cloudflare.com/keepalives-considered-harmful/), to generate a sustained workload that saturated the processor to 100% CPU utilization.\n\n### Results\n\nAfter establishing a baseline with simultaneous multithreading and Core Performance Boost disabled, we started enabling one feature at a time. When we enabled Core Performance Boost, the processor operated near its peak turbo frequency, hovering between 3.2 to 3.3 GHz which is more than 39% higher than the base frequency. Higher operating frequency directly translated into 40% additional requests per second. We then disabled Core Performance Boost and enabled simultaneous multithreading. Similar to Core Performance Boost, simultaneous multithreading alone improved requests per second by 43%. Lastly, by enabling both features, we observed an 86% improvement in requests per second.\n\n![](https://blog.cloudflare.com/content/images/2021/10/image4-8.png)\n\nLatencies were generally lowered by either or both Core Performance Boost and simultaneous multithreading. While Core Performance Boost consistently maintained a lower latency than the baseline, simultaneous multithreading gradually took longer to process a request as it reached tail latencies. Though not depicted in the figure below, when we examined beyond p9999 or 99.99th percentile, simultaneous multithreading, even with the help of Core Performance Boost, exponentially increased in latency by more than 150% over the baseline, presumably due to the two hardware threads contending over a shared resource within the core.\n\n![](https://blog.cloudflare.com/content/images/2021/10/image7-4.png)\n\n## Production Environment\n\nMoving into production, since our [traffic fluctuates throughout the day](https://radar.cloudflare.com/), we took four identical Gen X servers and measured in parallel during peak hours. The only changes we made to the servers were enabling and disabling simultaneous multithreading and Core Performance Boost to create a comprehensive test matrix. We conducted the experiment in two different regions to identify any anomalies and mismatching trends. All trends were alike.\n\nBefore diving into the results, we should preface that the baseline server operated at a higher CPU utilization than others. Every generation, our servers deliver a noticeable improvement in performance. So our load balancer, named Unimog, [sends a different number of connections to the target server based on its generation](https://blog.cloudflare.com/unimog-cloudflares-edge-load-balancer/) to balance out the CPU utilization. When we disabled simultaneous multithreading and Core Performance Boost, the baseline server’s performance degraded to the point where Unimog encountered a “guard rail” or the lower limit on the requests sent to the server, and so its CPU utilization rose instead. Given that the baseline server operated at a higher CPU utilization, the baseline server processed more requests per second to meet the minimum performance threshold.\n\n![](https://blog.cloudflare.com/content/images/2021/10/image8-5.png)\n\n### Results\n\nDue to the skewed baseline, when core performance boost was enabled, we only observed 7% additional requests per second. Next, simultaneous multithreading alone improved requests per second by 41%. Lastly, with both features enabled, we saw an 86% improvement in requests per second.\n\n![](https://blog.cloudflare.com/content/images/2021/10/image6-6.png)\n\nThough we lack concrete baseline data, we can normalize requests per second by CPU utilization to approximate the improvement for each scenario. Once normalized, the estimated improvement in requests per second from core performance boost and simultaneous multithreading were 36% and 80%, respectively. With both features enabled, requests per second improved by 136%.\n\n![](https://blog.cloudflare.com/content/images/2021/10/image5-6.png)\n\nLatency was not as interesting since the baseline server operated at a higher CPU utilization, and in turn, it produced a higher tail latency than we would have otherwise expected. All other servers maintained a lower latency due to their lower CPU utilization in conjunction with Core Performance Boost, simultaneous multithreading, or both.\n\n![](https://blog.cloudflare.com/content/images/2021/10/image1-10.png)\n\nAt this point, our experiment did not go as we had planned. Our baseline is skewed, and we only got half useful answers. However, we find experimenting to be important because we usually end up finding other helpful insights as well.\n\nLet’s add power data. Since our baseline server was operating at a higher CPU utilization, we knew it was serving more requests and therefore, consumed more power than it needed to. Enabling Core Performance Boost allowed the processor to run up to its peak turbo frequency, increasing power consumption by 35% over the skewed baseline. More interestingly, enabling simultaneous multithreading increased power consumption by only 7%. Combining Core Performance Boost with simultaneous multithreading resulted in 58% increase in power consumption.\n\n![](https://blog.cloudflare.com/content/images/2021/10/image3-6.png)\n\nAMD’s implementation of simultaneous multithreading appears to be power efficient as it achieves 41% additional requests per second while consuming only 7% more power compared to the skewed baseline. For completeness, using the data we have, we bridged performance and power together to obtain performance per watt to summarize power efficiency. We divided the non-normalized requests per second by power consumption to produce the requests per watt figure below. Our Gen X servers attained the best performance per watt by enabling just simultaneous multithreading.\n\n![](https://blog.cloudflare.com/content/images/2021/10/image9-4.png)\n\n## Conclusion\n\nIn our assessment of AMD’s implementation of Hyper-Threading and Turbo Boost, the original experiment we designed to measure requests per second and latency did not pan out as expected. As soon as we entered production, our baseline measurement was skewed due to the imbalance in CPU utilization and only partially reproduced our lab results.\n\nWe added power to the experiment and found other meaningful insights. By analyzing the performance and power characteristics of simultaneous multithreading and Core Performance Boost, we concluded that simultaneous multithreading could be a power-efficient mechanism to attain additional requests per second. Drawbacks of simultaneous multithreading include long tail latency that is currently curtailed by enabling Core Performance Boost. While the higher frequency enabled by Core Performance Boost provides latency reduction and more requests per second, we are more mindful that the increase in power consumption is quite significant.\n\nDo you want to help shape the Cloudflare network? This blog was a glimpse of the work we do at Cloudflare. [Come join us](https://www.cloudflare.com/careers/) and help complete the feedback loop for our developers and hardware partners.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Hardware](https://blog.cloudflare.com/tag/hardware/) [AMD](https://blog.cloudflare.com/tag/amd/) [EPYC](https://blog.cloudflare.com/tag/epyc/) [Partners](https://blog.cloudflare.com/tag/partners/) [Performance](https://blog.cloudflare.com/tag/performance/)"
    },
    {
      "url": "https://blog.cloudflare.com/branch-predictor/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/branch-predictor/",
        "loadedTime": "2023-12-05T02:30:21.246Z",
        "referrerUrl": "https://blog.cloudflare.com/cloudflare-gen-12-server-bigger-better-cooler-in-a-2u1n-form-factor/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/branch-predictor/",
        "title": "Branch predictor: How many \"if\"s are too many? Including x86 and M1 benchmarks!",
        "description": "Is it ok to have if clauses that will basically never be run? Surely, there must be some performance cost to that...",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "05/06/2021\n16 min read\nSome time ago I was looking at a hot section in our code and I saw this:\nif (debug) { log(\"...\"); } \nThis got me thinking. This code is in a performance critical loop and it looks like a waste - we never run with the \"debug\" flag enabled[1]. Is it ok to have if clauses that will basically never be run? Surely, there must be some performance cost to that...\nJust how bad is peppering the code with avoidable if statements?\nBack in the days the general rule was: a fully predictable branch has close to zero CPU cost.\nTo what extent is this true? If one branch is fine, then how about ten? A hundred? A thousand? When does adding one more if statement become a bad idea?\nAt some point the negligible cost of simple branch instructions surely adds up to a significant amount. As another example, a colleague of mine found this snippet in our production code:\nconst char *getCountry(int cc) { if(cc == 1) return \"A1\"; if(cc == 2) return \"A2\"; if(cc == 3) return \"O1\"; if(cc == 4) return \"AD\"; if(cc == 5) return \"AE\"; if(cc == 6) return \"AF\"; if(cc == 7) return \"AG\"; if(cc == 8) return \"AI\"; ... if(cc == 252) return \"YT\"; if(cc == 253) return \"ZA\"; if(cc == 254) return \"ZM\"; if(cc == 255) return \"ZW\"; if(cc == 256) return \"XK\"; if(cc == 257) return \"T1\"; return \"UNKNOWN\"; } \nObviously, this code could be improved[2]. But when I thought about it more: should it be improved? Is there an actual performance hit of a code that consists of a series of simple branches?\nUnderstanding the cost of jump\nWe must start our journey with a bit of theory. We want to figure out if the CPU cost of a branch increases as we add more of them. As it turns out, assessing the cost of a branch is not trivial. On modern processors it takes between one and twenty CPU cycles. There are at least four categories of control flow instructions[3]: unconditional branch (jmp on x86), call/return, conditional branch (e.g. je on x86) taken and conditional branch not taken. The taken branches are especially problematic: without special care they are inherently costly - we'll explain this in the following section. To bring down the cost, modern CPU's try to predict the future and figure out the branch target before the branch is actually fully executed! This is done in a special part of the processor called the branch predictor unit (BPU).\nThe branch predictor attempts to figure out a destination of a branching instruction very early and with very little context. This magic happens before the \"decoder\" pipeline stage and the predictor has very limited data available. It only has some past history and the address of the current instruction. If you think about it - this is super powerful. Given only current instruction pointer it can assess, with very high confidence, where the target of the jump will be.\nSource: https://en.wikipedia.org/wiki/Branch_predictor\nThe BPU maintains a couple of data structures, but today we'll focus on Branch Target Buffer (BTB). It's a place where the BPU remembers the target instruction pointer of previously taken branches. The whole mechanism is much more complex, take a look a the Vladimir Uzelac's Master thesis for details about branch prediction on CPU's from 2008 era:\nFor the scope of this article we'll simplify and focus on the BTB only. We'll try to show how large it is and how it behaves under different conditions.\nWhy is branch prediction needed?\nBut first, why is branch prediction used at all? In order to get the best performance, the CPU pipeline must feed a constant flow of instructions. Consider what happens to the multi-stage CPU pipeline on a branch instruction. To illustrate let's consider the following ARM program:\nBR label_a; X1 ... label_a: Y1 \nAssuming a simplistic CPU model, the operations would flow through the pipeline like this:\nIn the first cycle the BR instruction is fetched. This is an unconditional branch instruction changing the execution flow of the CPU. At this point it's not yet decoded, but the CPU would like to fetch another instruction already! Without a branch predictor in cycle 2 the fetch unit either has to wait or simply continues to the next instruction in memory, hoping it will be the right one. \nIn our example, instruction X1 is fetched even though this isn't the correct instruction to run. In cycle 4, when the branch instruction finishes the execute stage, the CPU will be able to understand the mistake, and roll back the speculated instructions before they have any effect. At this point the fetch unit is updated to correctly get the right instruction - Y1 in our case.\nThis situation of losing a number of cycles due to fetching code from an incorrect place is called a \"frontend bubble\". Our theoretical CPU has a two-cycle frontend bubble when a branch target wasn’t predicted right.\nIn this example we see that, although the CPU does the right thing in the end, without good branch prediction it wasted effort on bad instructions. In the past, various techniques have been used to reduce this problem, such as static branch prediction and branch delay slots. But the dominant CPU designs today rely on dynamic branch prediction. This technique is able to mostly avoid the frontend bubble problem, by predicting the correct address of the next instruction even for branches that aren’t fully decoded and executed yet.\nPlaying with the BTB\nToday we're focusing on the BTB - a data structure managed by the branch predictor responsible for figuring out a target of a branch. It's important to note that the BTB is distinct from and independent of the system assessing if the branch was taken or not taken. Remember, we want to figure out if a cost of a branch increases as we run more of them. \nPreparing an experiment to stress only the BTB is relatively simple (based on Matt Godbolt's work). It turns out a sequence of unconditional jmps is totally sufficient. Consider this x86 code:\nThis code stresses the BTB to an extreme - it just consists of a chain of jmp +2 statements (i.e. literally jumping to the next instruction). In order to avoid wasting cycles on frontend pipeline bubbles, each taken jump needs a BTB hit. This branch prediction must happen very early in the CPU pipeline, before instruction decode is finished. This same mechanism is needed for any taken branch, whether it's unconditional, conditional or a function call.\nThe code above was run inside a test harness that measures how many CPU cycles elapse for each instruction. For example, in this run we're measuring times of dense - every two bytes - 1024 jmp instructions one after another:\nWe’ll look at the results of experiments like this for a few different CPUs. But in this instance, it was run on a machine with an AMD EPYC 7642. Here, the cold run took 10.5 cycles per jmp, and then all subsequent runs took ~3.5 cycles per jmp. The code is prepared in such a way to make sure it's the BTB that is slowing down the first run. Take a look at the full code, there is quite some magic to warm up the L1 cache and iTLB without priming the BTB.\nTop tip 1. On this CPU a branch instruction that is taken but not predicted, costs ~7 cycles more than one that is taken and predicted. Even if the branch was unconditional.\nDensity matters\nTo get a full picture we also need to think about the density of jmp instructions in the code. The code above did eight jmps per 16-byte code block. This is a lot. For example, the code below contains one jmp instruction in each block of 16 bytes. Notice that the nop opcodes are jumped over. The block size doesn't change the number of executed instructions, only the code density:\nVarying the jmp block size might be important. It allows us to control the placement of the jmp opcodes. Remember the BTB is indexed by instruction pointer address. Its value and its alignment might influence the placement in the BTB and help us reveal the BTB layout. Increasing the alignment will cause more nop padding to be added. The sequence of a single measured instruction - jmp in this case - and zero or more nops, I will call \"block\", and its size \"block size\". Notice that the larger the block size, the larger the working code size for the CPU. At larger values we might see some performance drop due to exhausting L1 cache space.\nThe experiment\nOur experiment is crafted to show the performance drop depending on the number of branches, across different working code sizes. Hopefully, we will be able to prove the performance is mostly dependent on the number of blocks - and therefore the BTB size, and not the working code size.\nSee the code on GitHub. If you want to see the generated machine code, though, you need to run a special command. It's created procedurally by the code, customized by passed parameters. Here's an example gdb incantation:\nLet's bring this experiment forward, what if we took the best times of each run - with a fully primed BTB - for varying values of jmp block sizes and number of blocks - working set size? Here you go:\nThis is an astonishing chart. First, it's obvious something happens at the 4096 jmp mark[4] regardless of how large the jmp block sizes - how many nop's we skip over. Reading it aloud:\nOn the far left, we see that if the amount of code is small enough - less than 2048 bytes (256 times a block of 8 bytes) - it's possible to hit some kind of uop/L1 cache and get ~1.5 cycles per fully predicted branch. This is amazing.\nOtherwise, if you keep your hot loop to 4096 branches then, no matter how dense your code is you are likely to see ~3.4 cycles per fully predicted branch\nAbove 4096 branches the branch predictor gives up and the cost of each branch shoots to ~10.5 cycles per jmp. This is consistent with what we saw above - unpredicted branch on flushed BTB took ~10.5 cycles.\nGreat, so what does it mean? Well, you should avoid branch instructions if you want to avoid branch misses because you have at most 4096 of fast BTB slots. This is not a very pragmatic advice though - it's not like we deliberately put many unconditional jmps in real code!\nThere are a couple of takeaways for the discussed CPU. I repeated the experiment with an always-taken conditional branch sequence and the resulting chart looks almost identical. The only difference being the predicted taken conditional-je instruction being 2 cycles slower than unconditional jmp.\nAn entry to BTB is added wherever a branch is \"taken\" - that is, the jump actually happens. An unconditional \"jmp\" or always taken conditional branch, will cost a BTB slot. To get best performance make sure to not have more than 4096 taken branches in the hot loop. The good news is that branches never-taken don't take space in the BTB. We can illustrate this with another experiment:\nThis boring code is going over not-taken jne followed by two nops (block size=4). Aimed with this test (jne never-taken), the previous one (jmp always-taken) and a conditional branch je always-taken, we can draw this chart:\nFirst, without any surprise we can see the conditional 'je always-taken' is getting slightly more costly than the simple unconditional jmp, but only after the 4096 branches mark. This makes sense, the conditional branch is resolved later in the pipeline so the frontend bubble is longer. Then take a look at the blue line hovering near zero. This is the \"jne never-taken\" line flat at 0.3 clocks / block, no matter how many blocks we run in sequence. The takeaway is clear - you can have as many never-taken branches as you want, without incurring any cost. There isn't any spike at 4096 mark, meaning BTB is not used in this case. It seems the conditional jump not seen before is guessed to be not-taken.\nTop tip 2: conditional branches never-taken are basically free - at least on this CPU.\nSo far we established that branches always-taken occupy BTB, branches never taken do not. How about other control flow instructions, like the call?\nI haven't been able to find this in the literature, but it seems call/ret also need the BTB entry for best performance. I was able to illustrate this on our AMD EPYC. Let's take a look at this test:\nThis time we'll issue a number of callq instructions followed by ret - both of which should be fully predicted. The experiment is crafted so that each callq calls a unique function, to allow for retq prediction - each one returns to exactly one caller.\nThis chart confirms the theory: no matter the code density - with the exception of 64-byte block size being notably slower - the cost of predicted call/ret starts to deteriorate after the 2048 mark. At this point the BTB is filled with call and ret predictions and can't handle any more data. This leads to an important conclusion:\nTop tip 3. In the hot code you want to have less than 2K function calls - on this CPU.\nIn our test CPU a sequence of fully predicted call/ret takes about 7 cycles, which is about the same as two unconditional predicted jmp opcodes. It's consistent with our results above.\nSo far we thoroughly checked AMD EPYC 7642. We started with this CPU because the branch predictor is relatively simple and the charts were easy to read. It turns out more recent CPUs are less clear.\nAMD EPYC 7713 \nNewer AMD is more complex than the previous generations. Let's run the two most important experiments. First, the jmp one:\nFor the always-taken branches case we can see a very good, sub 1 cycle, timings when the number of branches doesn't exceed 1024 and the code isn't too dense. \nTop tip 4. On this CPU it's possible to get <1 cycle per predicted jmp when the hot loop fits in ~32KiB.\nThen there is some noise starting after the 4096 jmps mark. This is followed by a complete drop of speed at about 6000 branches. This is in line with the theory that BTB is 4096 entries long. We can speculate that some other prediction mechanism is successfully kicking in beyond that, and keeps up the performance up the ~6k mark.\nThe call/ret chart shows a similar tale, the timings start breaking after 2048 mark, and completely fail to be predicted beyond ~3000.\nXeon Gold 6262\nThe Intel Xeon looks different from the AMD:\nOur test shows the predicted taken branch costs 2 cycles. Intel has documented a clock penalty for very dense branching code - this explains the 4-byte block size line hovering at ~3 cycles. The branch cost breaks at the 4096 jmp mark, confirming the theory that the Intel BTB can hold 4096 entries. The 64-byte block size chart looks confusing, but really isn't. The branch cost stays at flat 2 cycles up till the 512 jmp count. Then it increases. This is caused by the internal layout of the BTB which is said to be 8-way associative. It seems with the 64-byte block size we can utilize at most half of the 4096 BTB slots.\nTop tip 5. On Intel avoid placing your jmp/call/ret instructions at regular 64-byte intervals.\nThen the call/ret chart:\nSimilarly, we can see the branch predictions failing after the 2048 jmp mark - in this experiment one block uses two flow control instructions: call and ret. This again confirms the BTB size of 4K entries. The 64-byte block size is generally slower due to the nop padding but also breaks faster due to the instructions alignment issue. Notice, we haven't seen this effect on AMD.\nApple Silicon M1\nSo far we saw examples of AMD and Intel server grade CPUs. How does an Apple Silicon M1 fit in this picture?\nWe expect it to be very different - it's designed for mobile and it's using ARM64 architecture. Let's see our two experiments:\nThe predicted jmp test shows an interesting story. First, when the code fits 4096 bytes (1024*4 or 512*8, etc) you can expect a predicted jmp to cost 1 clock cycle. This is an excellent score.\nBeyond that, generally, you can expect a cost of 3 clock cycles per predicted jmp. This is also very good. This starts to deteriorate when the working code grows beyond ~200KiB. This is visible with block size 64 breaking at 3072 mark 3072*64=196K, and for block 32 at 6144: 6144*32=196K. At this point the prediction seems to stop working. The documentation indicates that the M1 CPU has 192 KB L1 of instruction cache - our experiment matches that.\nLet's compare the \"predicted jmp\" with the \"unpredicted jmp\" chart. Take this chart with a grain of salt, because flushing the branch predictor is notoriously difficult.\nHowever, even if we don't trust the flush-bpu code (adapted from Matt Godbolt), this chart reveals two things. First, the \"unpredicted\" branch cost seems to be correlated with the branch distance. The longer the branch the costlier it is. We haven't seen such behaviour on x86 CPUs.\nThen there is the cost itself. We saw a predicted sequence of branches cost, and what a supposedly-unpredicted jmp costs. In the first chart we saw that beyond ~192KiB working code, the branch predictor seems to become ineffective. The supposedly-flushed BPU seems to show the same cost. For example, the cost of a 64-byte block size jmp with a small working set size is 3 cycles. A miss is ~8 cycles. For a large working set size both times are ~8 cycles. It seems that the BTB is linked to the L1 cache state. Paul A. Clayton suggested a possibility of such a design back in 2016.\nTop tip 6. on M1 the predicted-taken branch generally takes 3 cycles and unpredicted but taken has varying cost, depending on jmp length. BTB is likely linked with L1 cache.\nThe call/ret chart is funny:\nLike in the chart before, we can see a big benefit if hot code fits within 4096 bytes (512*4 or 256*8). Otherwise, you can count on 4-6 cycles per call/ret sequence (or, bl/ret as it's known in ARM). The chart shows funny alignment issues. It's unclear what they are caused by. Beware, comparing the numbers in this chart with x86 is unfair, since ARM call operation differs substantially from the x86 variant.\nM1 seems pretty fast, with predicted branches usually at 3 clock cycles. Even unpredicted branches never cost more than 8 ticks in our benchmark. Call+ret sequence for dense code should fit under 5 cycles.\nSummary\nWe started our journey from a piece of trivial code, and asked a basic question: how costly is adding a never-taken if branch in the hot portion of code?\nThen we quickly dived in very low level CPU features. By the end of this article, hopefully, an astute reader might get better intuition how a modern branch predictors works. \nOn x86 the hot code needs to split the BTB budget between function calls and taken branches. The BTB has only a size of 4096 entries. There are strong benefits in keeping the hot code under 16KiB.\nOn the other hand on M1 the BTB seems to be limited by L1 instruction cache. If you're writing super hot code, ideally it should fit 4KiB.\nFinally, can you add this one more if statement? If it's never-taken, it's probably ok. I found no evidence that such branches incur any extra cost. But do avoid always-taken branches and function calls.\nSources\nI'm not the first person to investigate how BTB works. I based my experiments on:\nVladimir Uzelac thesis\nMatt Godbolt work. The series has 5 articles.\nTravis Downs BTB questions on Real World Tech\nvarious stackoverflow discussions. Especially this one and this\nAgner Fog microarchitecture guide has a good section on branch predictions.\nAcknowledgements\nThanks to David Wragg and Dan Luu for technical expertise and proofreading help.\nPS\nOh, oh. But this is not the whole story! Similar research was the base to the Spectre v2 attack. The attack was exploiting the little known fact that the BPU state was not cleared between context switches. With the correct technique it was possible to train the BPU - in the case of Spectre it was iBTB - and force a privileged piece of code to be speculatively executed. This, combined with a cache side-channel data leak, allowed an attacker to steal secrets from the privileged kernel. Powerful stuff.\nA proposed solution was to avoid using shared BTB. This can be done in two ways: make the indirect jumps to always fail to predict, or fix the CPU to avoid sharing BTB state across isolation domains. This is a long story, maybe for another time...\nFootnotes\n1. One historical solution to this specific 'if debug' problem is called \"runtime nop'ing\". The idea is to modify the code in runtime and patch the never-taken branch instruction with a nop. For example, see the \"ISENABLED\" discussion on https://bugzilla.mozilla.org/showbug.cgi?id=370906.\n2. Fun fact: modern compilers are pretty smart. New gcc (>=11) and older clang (>=3.7) are able to actually optimize it quite a lot. See for yourself. But, let's not get distracted by that. This article is about low level machine code branch instructions!\n3. This is a simplification. There are of course more control flow instructions, like: software interrupts, syscalls, VMENTER/VMEXIT.\n4. Ok, I'm slightly overinterpreting the chart. Maybe the 4096 jmp mark is due to the 4096 uop cache or some instruction decoder artifact? To prove this spike is indeed BTB related I looked at Intel BPUCLEARS.EARLY and BACLEAR.CLEAR performance counters. Its value is small for block count under 4096 and large for block count greater than 5378. This is strong evidence that the performance drop is indeed caused by the BPU and likely BTB.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nDeep Dive Programming AMD EPYC Speed & Reliability",
      "markdown": "05/06/2021\n\n*   [![Marek Majkowski](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2017/03/b5967d6c687939594adb6992723d0529.jpeg)](https://blog.cloudflare.com/author/marek-majkowski/)\n\n16 min read\n\n![](https://blog.cloudflare.com/content/images/2021/05/Branch-Prediction.png)\n\nSome time ago I was looking at a hot section in our code and I saw this:\n\n```\n\n\tif (debug) {\n    \t  log(\"...\");\n    }\n    \n```\n\nThis got me thinking. This code is in a performance critical loop and it looks like a waste - we never run with the \"debug\" flag enabled\\[[1](#footnotes)\\]. Is it ok to have `if` clauses that will basically never be run? Surely, there must be some performance cost to that...\n\n### Just how bad is peppering the code with avoidable `if` statements?\n\nBack in the days the general rule was: a fully predictable branch has close to zero CPU cost.\n\nTo what extent is this true? If one branch is fine, then how about ten? A hundred? A thousand? When does adding one more `if` statement become a bad idea?\n\nAt some point the negligible cost of simple branch instructions surely adds up to a significant amount. As another example, a colleague of mine found this snippet in our production code:\n\n```\n\nconst char *getCountry(int cc) {\n\t\tif(cc == 1) return \"A1\";\n        if(cc == 2) return \"A2\";\n        if(cc == 3) return \"O1\";\n        if(cc == 4) return \"AD\";\n        if(cc == 5) return \"AE\";\n        if(cc == 6) return \"AF\";\n        if(cc == 7) return \"AG\";\n        if(cc == 8) return \"AI\";\n        ...\n        if(cc == 252) return \"YT\";\n        if(cc == 253) return \"ZA\";\n        if(cc == 254) return \"ZM\";\n        if(cc == 255) return \"ZW\";\n        if(cc == 256) return \"XK\";\n        if(cc == 257) return \"T1\";\n        return \"UNKNOWN\";\n}\n        \n```\n\nObviously, this code could be improved\\[[2](#footnotes)\\]. But when I thought about it more: _should_ it be improved? Is there an actual performance hit of a code that consists of a series of simple branches?\n\n### Understanding the cost of jump\n\nWe must start our journey with a bit of theory. We want to figure out if the CPU cost of a branch increases as we add more of them. As it turns out, assessing the cost of a branch is not trivial. On modern processors it takes between one and twenty CPU cycles. There are at least four categories of control flow instructions\\[[3](#footnotes)\\]: unconditional branch (jmp on x86), call/return, conditional branch (e.g. je on x86) taken and conditional branch not taken. The taken branches are especially problematic: without special care they are inherently costly - we'll explain this in the following section. To bring down the cost, modern CPU's try to predict the future and figure out the branch **target** before the branch is actually fully executed! This is done in a special part of the processor called the branch predictor unit (BPU).\n\nThe branch predictor attempts to figure out a destination of a branching instruction very early and with very little context. This magic happens **before** the \"decoder\" pipeline stage and the predictor has very limited data available. It only has some past history and the address of the current instruction. If you think about it - this is super powerful. Given only current instruction pointer it can assess, with very high confidence, where the target of the jump will be.\n\n![](https://blog.cloudflare.com/content/images/2021/05/pasted-image-0-1.png)\n\nSource: [https://en.wikipedia.org/wiki/Branch\\_predictor](https://en.wikipedia.org/wiki/Branch_predictor)\n\nThe BPU maintains a couple of data structures, but today we'll focus on Branch Target Buffer (BTB). It's a place where the BPU remembers the target instruction pointer of previously taken branches. The whole mechanism is much more complex, take a look a the [Vladimir Uzelac's Master thesis](http://www.ece.uah.edu/~milenka/docs/VladimirUzelac.thesis.pdf) for details about branch prediction on CPU's from 2008 era:\n\n![](https://blog.cloudflare.com/content/images/2021/05/pasted-image-0--1-.png)\n\nFor the scope of this article we'll simplify and focus on the BTB only. We'll try to show how large it is and how it behaves under different conditions.\n\n### Why is branch prediction needed?\n\nBut first, why is branch prediction used at all? In order to get the best performance, the CPU pipeline must feed a constant flow of instructions. Consider what happens to the multi-stage CPU pipeline on a branch instruction. To illustrate let's consider the following ARM program:\n\n```\n\n\tBR label_a;\n    X1\n    ...\nlabel_a:\n \tY1\n    \n```\n\nAssuming a simplistic CPU model, the operations would flow through the pipeline like this:\n\n![](https://lh3.googleusercontent.com/3P6PIWN6gPAdYzP8oDgrsaOMKgUmG51zIiFhbm071cZKM276S7vRb5atpTwlKrM1lFHRYsobw8P4e-Z9t1Vb9TGeutpBe2CkMrNGruWO8yb5Qz0vZ6Qn6RbOi5Tp8JmzXyU6TzS2)\n\nIn the first cycle the BR instruction is fetched. This is an unconditional branch instruction changing the execution flow of the CPU. At this point it's not yet decoded, but the CPU would like to fetch another instruction already! Without a branch predictor in cycle 2 the fetch unit either has to wait or simply continues to the next instruction in memory, hoping it will be the right one.\n\nIn our example, instruction X1 is fetched even though this isn't the correct instruction to run. In cycle 4, when the branch instruction finishes the execute stage, the CPU will be able to understand the mistake, and roll back the speculated instructions before they have any effect. At this point the fetch unit is updated to correctly get the right instruction - Y1 in our case.\n\nThis situation of losing a number of cycles due to fetching code from an incorrect place is called a \"frontend bubble\". Our theoretical CPU has a two-cycle frontend bubble when a branch target wasn’t predicted right.\n\nIn this example we see that, although the CPU does the right thing in the end, without good branch prediction it wasted effort on bad instructions. In the past, various techniques have been used to reduce this problem, such as static branch prediction and branch delay slots. But the dominant CPU designs today rely [on _dynamic branch prediction_](https://danluu.com/branch-prediction/#one-bit). This technique is able to mostly avoid the frontend bubble problem, by predicting the correct address of the next instruction even for branches that aren’t fully decoded and executed yet.\n\n### Playing with the BTB\n\nToday we're focusing on the BTB - a data structure managed by the branch predictor responsible for figuring out a target of a branch. It's important to note that the BTB is distinct from and independent of the system assessing if the branch was taken or not taken. Remember, we want to figure out if a cost of a branch increases as we run more of them.\n\nPreparing an experiment to stress only the BTB is relatively simple ([based on Matt Godbolt's work](https://xania.org/201602/bpu-part-three)). It turns out a sequence of unconditional `jmps` is totally sufficient. Consider this x86 code:\n\n![](https://blog.cloudflare.com/content/images/2021/05/pasted-image-0--2-.png)\n\nThis code stresses the BTB to an extreme - it just consists of a chain of `jmp +2` statements (i.e. literally jumping to the next instruction). In order to avoid wasting cycles on frontend pipeline bubbles, each taken jump needs a BTB hit. This branch prediction must happen very early in the CPU pipeline, before instruction decode is finished. This same mechanism is needed for any taken branch, whether it's unconditional, conditional or a function call.\n\nThe code above was run inside a test harness that measures how many CPU cycles elapse for each instruction. For example, in this run we're measuring times of dense - every two bytes - 1024 jmp instructions one after another:\n\n![](https://blog.cloudflare.com/content/images/2021/05/pasted-image-0--3-.png)\n\nWe’ll look at the results of experiments like this for a few different CPUs. But in this instance, it was run on a machine with an AMD EPYC 7642. Here, the cold run took 10.5 cycles per jmp, and then all subsequent runs took ~3.5 cycles per jmp. The code is prepared in such a way to make sure it's the BTB that is slowing down the first run. Take a look at the full code, there is quite some magic to warm up the L1 cache and iTLB without priming the BTB.\n\n**Top tip 1. On this CPU a branch instruction that is taken but not predicted, costs ~7 cycles more than one that is taken and predicted.** Even if the branch was unconditional.\n\n### Density matters\n\nTo get a full picture we also need to think about the density of jmp instructions in the code. The code above did eight jmps per 16-byte code block. This is a lot. For example, the code below contains one jmp instruction in each block of 16 bytes. Notice that the `nop` opcodes are jumped over. The block size doesn't change the number of executed instructions, only the code density:\n\n![](https://blog.cloudflare.com/content/images/2021/05/pasted-image-0--4-.png)\n\nVarying the jmp block size might be important. It allows us to control the placement of the jmp opcodes. Remember the BTB is indexed by instruction pointer address. Its value and its alignment might influence the placement in the BTB and help us reveal the BTB layout. Increasing the alignment will cause more nop padding to be added. The sequence of a single measured instruction - jmp in this case - and zero or more nops, I will call \"block\", and its size \"block size\". Notice that the larger the block size, the larger the working code size for the CPU. At larger values we might see some performance drop due to exhausting L1 cache space.\n\n### The experiment\n\nOur experiment is crafted to show the performance drop depending on the number of branches, across different working code sizes. Hopefully, we will be able to prove the performance is mostly dependent on the number of blocks - and therefore the BTB size, and not the working code size.\n\nSee the [code on GitHub](https://github.com/cloudflare/cloudflare-blog/tree/master/2021-05-branch-prediction). If you want to see the generated machine code, though, you need to run a special command. It's created procedurally by the code, customized by passed parameters. Here's an example `gdb` incantation:\n\n![](https://blog.cloudflare.com/content/images/2021/05/pasted-image-0--5-.png)\n\nLet's bring this experiment forward, what if we took the best times of each run - with a fully primed BTB - for varying values of jmp block sizes and number of blocks - working set size? Here you go:\n\n![](https://blog.cloudflare.com/content/images/2021/05/pasted-image-0--6-.png)\n\nThis is an astonishing chart. First, it's obvious something happens at the 4096 jmp mark\\[[4](#footnotes)\\] regardless of how large the jmp block sizes - how many nop's we skip over. Reading it aloud:\n\n*   On the far left, we see that if the amount of code is small enough - less than 2048 bytes (256 times a block of 8 bytes) - it's possible to hit some kind of uop/L1 cache and get ~1.5 cycles per fully predicted branch. This is amazing.\n*   Otherwise, if you keep your hot loop to 4096 branches then, no matter how dense your code is you are likely to see ~3.4 cycles per fully predicted branch\n*   Above 4096 branches the branch predictor gives up and the cost of each branch shoots to ~10.5 cycles per jmp. This is consistent with what we saw above - unpredicted branch on flushed BTB took ~10.5 cycles.\n\nGreat, so what does it mean? Well, you should avoid branch instructions if you want to avoid branch misses because you have at most 4096 of fast BTB slots. This is not a very pragmatic advice though - it's not like we deliberately put many unconditional `jmp`s in real code!\n\nThere are a couple of takeaways for the discussed CPU. I repeated the experiment with an always-taken conditional branch sequence and the resulting chart looks almost identical. The only difference being the predicted taken conditional-je instruction being 2 cycles slower than unconditional jmp.\n\nAn entry to BTB is added wherever a branch is \"taken\" - that is, the jump actually happens. An unconditional \"jmp\" or always taken conditional branch, will cost a BTB slot. To get best performance make sure to not have more than 4096 taken branches in the hot loop. The good news is that branches never-taken don't take space in the BTB. We can illustrate this with another experiment:\n\n![](https://blog.cloudflare.com/content/images/2021/05/pasted-image-0--7-.png)\n\nThis boring code is going over not-taken `jne` followed by two nops (block size=4). Aimed with this test (jne never-taken), the previous one (jmp always-taken) and a conditional branch `je` always-taken, we can draw this chart:\n\n![](https://blog.cloudflare.com/content/images/2021/05/pasted-image-0--8-.png)\n\nFirst, without any surprise we can see the conditional 'je always-taken' is getting slightly more costly than the simple unconditional `jmp`, but only after the 4096 branches mark. This makes sense, the conditional branch is resolved later in the pipeline so the frontend bubble is longer. Then take a look at the blue line hovering near zero. This is the \"jne never-taken\" line flat at 0.3 clocks / block, no matter how many blocks we run in sequence. The takeaway is clear - you can have as many never-taken branches as you want, without incurring any cost. There isn't any spike at 4096 mark, meaning BTB is not used in this case. It seems the conditional jump not seen before is guessed to be not-taken.\n\n**Top tip 2: conditional branches never-taken are basically free** \\- at least on this CPU.\n\nSo far we established that branches always-taken occupy BTB, branches never taken do not. How about other control flow instructions, like the `call`?\n\nI haven't been able to find this in the literature, but it seems call/ret also need the BTB entry for best performance. I was able to illustrate this on our AMD EPYC. Let's take a look at this test:\n\n![](https://blog.cloudflare.com/content/images/2021/05/pasted-image-0--9-.png)\n\nThis time we'll issue a number of `callq` instructions followed by `ret` - both of which should be fully predicted. The experiment is crafted so that each callq calls a unique function, to allow for retq prediction - each one returns to exactly one caller.\n\n![](https://blog.cloudflare.com/content/images/2021/05/pasted-image-0--10-.png)\n\nThis chart confirms the theory: no matter the code density - with the exception of 64-byte block size being notably slower -  the cost of predicted call/ret starts to deteriorate after the 2048 mark. At this point the BTB is filled with call and ret predictions and can't handle any more data. This leads to an important conclusion:\n\n**Top tip 3. In the hot code you want to have less than 2K function calls** \\- on this CPU.\n\nIn our test CPU a sequence of fully predicted call/ret takes about 7 cycles, which is about the same as two unconditional predicted `jmp` opcodes. It's consistent with our results above.\n\nSo far we thoroughly checked AMD EPYC 7642. We started with this CPU because the branch predictor is relatively simple and the charts were easy to read. It turns out more recent CPUs are less clear.\n\n### AMD EPYC 7713\n\nNewer AMD is more complex than the previous generations. Let's run the two most important experiments. First, the `jmp` one:\n\n![](https://blog.cloudflare.com/content/images/2021/05/pasted-image-0--11-.png)\n\nFor the always-taken branches case we can see a very good, sub 1 cycle, timings when the number of branches doesn't exceed 1024 and the code isn't too dense.\n\n**Top tip 4. On this CPU it's possible to get <1 cycle per predicted jmp when the hot loop fits in ~32KiB.**\n\nThen there is some noise starting after the 4096 jmps mark. This is followed by a complete drop of speed at about 6000 branches. This is in line with the theory that BTB is 4096 entries long. We can speculate that some other prediction mechanism is successfully kicking in beyond that, and keeps up the performance up the ~6k mark.\n\n![](https://blog.cloudflare.com/content/images/2021/05/pasted-image-0--12-.png)\n\nThe call/ret chart shows a similar tale, the timings start breaking after 2048 mark, and completely fail to be predicted beyond ~3000.\n\n### Xeon Gold 6262\n\nThe Intel Xeon looks different from the AMD:\n\n![](https://blog.cloudflare.com/content/images/2021/05/pasted-image-0--13-.png)\n\nOur test shows the predicted taken branch costs 2 cycles. Intel has documented a clock penalty for very dense branching code - this explains the 4-byte block size line hovering at ~3 cycles. The branch cost breaks at the 4096 jmp mark, confirming the theory that the Intel BTB can hold 4096 entries. The 64-byte block size chart looks confusing, but really isn't. The branch cost stays at flat 2 cycles up till the 512 jmp count. Then it increases. This is caused by the internal layout of the BTB which is said to be 8-way associative. It seems with the 64-byte block size we can utilize at most half of the 4096 BTB slots.\n\n**Top tip 5. On Intel avoid placing your jmp/call/ret instructions at regular 64-byte intervals.**\n\nThen the call/ret chart:\n\n![](https://blog.cloudflare.com/content/images/2021/05/pasted-image-0--14-.png)\n\nSimilarly, we can see the branch predictions failing after the 2048 jmp mark - in this experiment one block uses two flow control instructions: call and ret. This again confirms the BTB size of 4K entries. The 64-byte block size is generally slower due to the nop padding but also breaks faster due to the instructions alignment issue. Notice, we haven't seen this effect on AMD.\n\n### Apple Silicon M1\n\nSo far we saw examples of AMD and Intel server grade CPUs. How does an Apple Silicon M1 fit in this picture?\n\nWe expect it to be very different - it's designed for mobile and it's using ARM64 architecture. Let's see our two experiments:\n\n![](https://blog.cloudflare.com/content/images/2021/05/pasted-image-0--15-.png)\n\nThe predicted `jmp` test shows an interesting story. First, when the code fits 4096 bytes (1024\\*4 or 512\\*8, etc) you can expect a predicted `jmp` to cost 1 clock cycle. This is an excellent score.\n\nBeyond that, generally, you can expect a cost of 3 clock cycles per predicted jmp. This is also very good. This starts to deteriorate when the working code grows beyond ~200KiB. This is visible with block size 64 breaking at 3072 mark 3072\\*64=196K, and for block 32 at 6144: 6144\\*32=196K. At this point the prediction seems to stop working. The documentation indicates that the M1 CPU has 192 KB L1 of instruction cache - our experiment matches that.\n\nLet's compare the \"predicted jmp\" with the \"unpredicted jmp\" chart. Take this chart with a grain of salt, because flushing the branch predictor is notoriously difficult.\n\n![](https://blog.cloudflare.com/content/images/2021/05/pasted-image-0--16-.png)\n\nHowever, even if we don't trust the flush-bpu code ([adapted from Matt Godbolt](https://xania.org/201602/bpu-part-three)), this chart reveals two things. First, the \"unpredicted\" branch cost seems to be correlated with the branch distance. The longer the branch the costlier it is. We haven't seen such behaviour on x86 CPUs.\n\nThen there is the cost itself. We saw a predicted sequence of branches cost, and what a supposedly-unpredicted jmp costs. In the first chart we saw that beyond ~192KiB working code, the branch predictor seems to become ineffective. The supposedly-flushed BPU seems to show the same cost. For example, the cost of a 64-byte block size jmp with a small working set size is 3 cycles. A miss is ~8 cycles. For a large working set size both times are ~8 cycles. It seems that the BTB is linked to the L1 cache state. [Paul A. Clayton suggested](https://www.realworldtech.com/forum/?threadid=159985&curpostid=160001) a possibility of such a design back in 2016.\n\n**Top tip 6. on M1 the predicted-taken branch generally takes 3 cycles and unpredicted but taken has varying cost, depending on jmp length. BTB is likely linked with L1 cache.**\n\nThe call/ret chart is funny:\n\n![](https://blog.cloudflare.com/content/images/2021/05/pasted-image-0--17-.png)\n\nLike in the chart before, we can see a big benefit if hot code fits within 4096 bytes (512\\*4 or 256\\*8). Otherwise, you can count on 4-6 cycles per call/ret sequence (or, bl/ret as it's known in ARM). The chart shows funny alignment issues. It's unclear what they are caused by. Beware, comparing the numbers in this chart with x86 is unfair, since ARM `call` operation differs substantially from the x86 variant.\n\nM1 seems pretty fast, with predicted branches usually at 3 clock cycles. Even unpredicted branches never cost more than 8 ticks in our benchmark. Call+ret sequence for dense code should fit under 5 cycles.\n\n### Summary\n\nWe started our journey from a piece of trivial code, and asked a basic question: how costly is  adding a never-taken `if` branch in the hot portion of code?\n\nThen we quickly dived in very low level CPU features. By the end of this article, hopefully, an astute reader might get better intuition how a modern branch predictors works.\n\nOn x86 the hot code needs to split the BTB budget between function calls and taken branches. The BTB has only a size of 4096 entries. There are strong benefits in keeping the hot code under 16KiB.\n\nOn the other hand on M1 the BTB seems to be limited by L1 instruction cache. If you're writing super hot code, ideally it should fit 4KiB.\n\nFinally, can you add this one more `if` statement? If it's never-taken, it's probably ok. I found no evidence that such branches incur any extra cost. But do avoid always-taken branches and function calls.\n\n**Sources**\n\nI'm not the first person to investigate how BTB works. I based my experiments on:\n\n*   [Vladimir Uzelac thesis](http://www.ece.uah.edu/~milenka/docs/VladimirUzelac.thesis.pdf)\n*   [Matt Godbolt work](https://xania.org/201602/bpu-part-three). The series has 5 articles.\n*   [Travis Downs BTB questions](https://www.realworldtech.com/forum/?threadid=159985&curpostid=159985) on Real World Tech\n*   [various](https://stackoverflow.com/questions/38811901/slow-jmp-instruction) [stackoverflow](https://stackoverflow.com/questions/51822731/why-did-intel-change-the-static-branch-prediction-mechanism-over-these-years) [discussions](https://stackoverflow.com/questions/38512886/btb-size-for-haswell-sandy-bridge-ivy-bridge-and-skylake). Especially [this one](https://stackoverflow.com/questions/31280817/what-branch-misprediction-does-the-branch-target-buffer-detect) and [this](https://stackoverflow.com/questions/31642902/intel-cpus-instruction-queue-provides-static-branch-prediction)\n*   [Agner Fog](https://www.agner.org/optimize/microarchitecture.pdf) microarchitecture guide has a good section on branch predictions.\n\n### Acknowledgements\n\nThanks to [David Wragg](https://blog.cloudflare.com/author/david-wragg/) and [Dan Luu](https://twitter.com/danluu) for technical expertise and proofreading help.\n\n### PS\n\nOh, oh. But this is not the whole story! Similar research was the base to the [Spectre v2](https://spectreattack.com/spectre.pdf) attack. The attack was exploiting the little known fact that the BPU state was not cleared between context switches. With the correct technique it was possible to train the BPU - in the case of Spectre it was iBTB - and force a privileged piece of code to be speculatively executed. This, combined with a cache side-channel data leak, allowed an attacker to steal secrets from the privileged kernel. Powerful stuff.\n\nA proposed solution was to avoid using shared BTB. This can be done in two ways: make the indirect jumps to always fail to predict, or fix the CPU to avoid sharing BTB state across isolation domains. This is a long story, maybe for another time...\n\n* * *\n\nFootnotes\n\n1\\. One historical solution to this specific 'if debug' problem is called \"runtime nop'ing\". The idea is to modify the code in runtime and patch the never-taken branch instruction with a `nop`. For example, see the \"ISENABLED\" discussion on [https://bugzilla.mozilla.org/showbug.cgi?id=370906.](https://bugzilla.mozilla.org/showbug.cgi?id=370906.)\n\n2\\. Fun fact: modern compilers are pretty smart. New gcc (>=11) and older clang (>=3.7) are able to actually optimize it quite a lot. [See for yourself](https://godbolt.org/z/KWYEW3d9s). But, let's not get distracted by that. This article is about low level machine code branch instructions!\n\n3\\. This is a simplification. There are of course more control flow instructions, like: software interrupts, syscalls, VMENTER/VMEXIT.\n\n4\\. Ok, I'm slightly overinterpreting the chart. Maybe the 4096 jmp mark is due to the 4096 uop cache or some instruction decoder artifact? To prove this spike is indeed BTB related I looked at Intel BPUCLEARS.EARLY and BACLEAR.CLEAR performance counters. Its value is small for block count under 4096 and large for block count greater than 5378. This is strong evidence that the performance drop is indeed caused by the BPU and likely BTB.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Deep Dive](https://blog.cloudflare.com/tag/deep-dive/) [Programming](https://blog.cloudflare.com/tag/programming/) [AMD](https://blog.cloudflare.com/tag/amd/) [EPYC](https://blog.cloudflare.com/tag/epyc/) [Speed & Reliability](https://blog.cloudflare.com/tag/speed-and-reliability/)"
    },
    {
      "url": "https://blog.cloudflare.com/cache-rules-go-ga/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/cache-rules-go-ga/",
        "loadedTime": "2023-12-05T02:30:22.454Z",
        "referrerUrl": "https://blog.cloudflare.com/page/2/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/cache-rules-go-ga/",
        "title": "Cache Rules are now GA: precision control over every part of your cache",
        "description": "Today, we're thrilled to announce that Cache Rules, along with several other Rules products, are generally available (GA). But that’s not all — we're also introducing new configuration options for Cache Rules",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/24/2023\n7 min read\nOne year ago we introduced Cache Rules, a new way to customize cache settings on Cloudflare. Cache Rules provide greater flexibility for how users cache content, offering precise controls, a user-friendly API, and seamless Terraform integrations. Since it was released in late September 2022, over 100,000 websites have used Cache Rules to fine-tune their cache settings.\nToday, we're thrilled to announce that Cache Rules, along with several other Rules products, are generally available (GA). But that’s not all — we're also introducing new configuration options for Cache Rules that provide even more options to customize how you cache on Cloudflare. These include functionality to define what resources are eligible for Cache Reserve, what timeout values should be respected when receiving data from your origin server, which custom ports we should use when we cache content, and whether we should bypass Cloudflare’s cache in the absence of a cache-control header.\nCache Rules give users full control and the ability to tailor their content delivery strategy for almost any use case, without needing to write code. As Cache Rules go GA, we are incredibly excited to see how fast customers can achieve their perfect cache strategy.\nHistory of Customizing Cache on Cloudflare\nThe journey of cache customization on Cloudflare began more than a decade ago, right at the beginning of the company. From the outset, one of the most frequent requests from our customers involved simplifying their configurations. Customers wanted to easily implement precise cache policies, apply robust security measures, manipulate headers, set up redirects, and more for any page on their website. Using Cloudflare to set these controls was especially crucial for customers utilizing origin servers that only provided convoluted configuration options to add headers or policies to responses, which could later be applied downstream by CDNs.\nIn response to this demand, we introduced Page Rules, a product that has since witnessed remarkable growth in both its popularity and functionality. Page Rules became the preferred choice for customers seeking granular control over how Cloudflare caches their content. Currently, there are over 5 million active cache-related Page Rules, assisting websites in tailoring their content delivery strategies.\nHowever, behind the scenes, Page Rules encountered a scalability issue.\nWhenever a Page Rule is encountered by Cloudflare we must transform all rule conditions for a customer into a single regex pattern. This pattern is then applied to requests for the website to achieve the desired cache configuration. When thinking about how all the regexes from all customers are then compared against tens of millions of requests per second, spanning across more than 300 data centers worldwide, it’s easy to see that the computational demands for applying Page Rules can be immense. This pressure is directly tied to the number of rules we could offer our users. For example, Page Rules would only allow for 125 rules to be deployed on a given website.\nTo address this challenge, we rebuilt all the Page Rule functionality on the new Rulesets Engine. Not only do ruleset engine-based products give users more rules to play with, they also offer greater flexibility on when these rules should run. Part of the magic of the Rulesets engine is that rather than combine all of a page's rules into a single regular expression, rule logic can be evaluated on a conditional basis. For example, if subdomain A and B have different caching policies, a request from subdomain A can be evaluated using regex logic specific to A (while omitting any logic that applies to B). This yields meaningful benefits to performance, and reduces the computational demands of applying Page Rules across Cloudflare's network.\nOver the past year, Cache Rules, along with Origin Rules, Configuration Rules, and Single Redirect Rules, have been in beta. Thanks to the invaluable support of our early adopters, we have successfully fine-tuned our product, reaching a stage where it is ready to transition from beta to GA. These products can now accomplish everything that Page Rules could and more. This also marks the beginning of the EOL process for Page Rules. In the coming months we will announce timelines and information regarding how customers will replace their Page Rules with specific Rules products. We will automate this as much as possible and provide simple steps to ensure a smooth transition away from Page Rules for everyone.\nHow to use Cache Rules and What’s New\nThose that have used Cache Rules know that they are intuitive and work similarly to our other ruleset engine products. User-defined criteria like URLs or request headers are evaluated, and if matching a specified value, the Cloudflare caching configuration is obeyed. Each Cache Rule depends on fields, operators, and values. For all the different options available, you should see our Cache Rules documentation.\nBelow are two examples of how to deploy different strategies to customize your cache. These examples only show the tip-of-the-iceberg of what’s possible with Cache Rules, so we encourage you to try them out and let us know what you think.\nExample: Cached content is updated at a regular cadence\nAs an example, let’s say that Acme Corp wants to update their caching strategy. They want to customize their cache to take advantage of certain request headers and use the presence of those request headers to be the criteria that decides when to apply different cache rules. The first thing they’d need to decide is what information should be used to trigger the specific rule. This is defined in the expression.\nOnce the triggering criteria is defined Acme Corp should next determine how they want to customize their cache.\nContent changing quickly\nThe most common cache strategy is to update the Edge Cache TTL. If Acme Corp thinks a particular piece of content on their website might change quickly, they can alter the time Cloudflare should consider a resource eligible to be served from cache to be shorter. This way Cloudflare would go back to the origin more frequently to revalidate and update the content. The Edge Cache TTL section is also where Acme Corp can define a resource’s TTL based on the status code Cloudflare gets back from their origin, and what Cloudflare should cache if there aren’t any cache-control instructions sent from Acme’s origin server.\nContent changing slowly\nOn the other hand, if Acme Corp had a lot of content that did not change very frequently (like a favicon or logo) and they preferred to serve that from Cloudflare’s cache instead of their origin, they can define which content should be eligible for Cache Reserve with a new Cache Rule. Cache Reserve reduces egress fees by storing assets persistently in Cloudflare's cache for an extended period of time.\nTraditionally when a user would enable Cache Reserve, their entire zone would be eligible to be written to Cache Reserve. For customers that care about saving origin egress fees on all resources on their website, this is still the best path forward. But for customers that want to have additional control over precisely what assets should be part of their Cache Reserve or even what size of assets should be eligible, the Cache Reserve Eligibility Rule provides additional knobs so that customers can precisely increase their cache hits and reduce origin egress in a customized manner. Note that this rule requires a Cache Reserve subscription.\nExample: Origin is slow\nLet’s consider a hypothetical example. Recently, Acme Corp has been seeing an increase in errors in their Cloudflare logs. These errors are related to a new report that Acme is providing its users based on Acme’s proprietary data. This report requires that their origin access several databases, perform some calculations and generate the report based on these calculations. The origin generating this report needs to wait to respond until all of this background work is completed. Acme’s report is a success, generating an influx of traffic from visitors wanting to see it. But their origin is struggling to keep up. A lot of the errors they are seeing are 524s which correlate to Cloudflare not seeing an origin response before a timeout occurred.\nAcme has plans to improve this by scaling their origin infrastructure but it’s taking a long time to deploy. In the meantime, they can turn to Cache Rules to configure a timeout to be longer. Historically the timeout value between Cloudflare and two successive origin reads was 100 seconds, which meant that if an origin didn't successfully send a response for a period lasting longer than 100 seconds, it could lead to a 524 error. By using a Cache Rule to extend this timeout, Acme Corp can rely more heavily on Cloudflare's cache.\nThe above cache strategies focus on how often a resource is changed on an origin, and the origin’s performance. But there are numerous other rules that allow for other strategies, like custom cache keys which allow for customers to determine how their cache should be defined on Cloudflare, respecting strong ETags which help customers determine when Cloudflare should revalidate particular cached assets, and custom ports which allow for customers to define non-standard ports that Cloudflare should use when making caching decisions about content.\nThe full list of Cache Rules can be found here.\nTry Cache Rules today!\nWe will continue to build and release additional rules that provide powerful, easy to enable control for anyone using Cloudflare’s cache. If you have feature requests for additional Cache Rules, please let us know in the Cloudflare Community.\nGo to the dashboard and try Cache Rules out today!\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nGeneral Availability Cache Rules Product News Application Services Performance",
      "markdown": "10/24/2023\n\n*   [![Alex Krivit](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2018/08/RE0BQ6EF_400x400.jpg)](https://blog.cloudflare.com/author/alex/)\n\n7 min read\n\n![Cache Rules go GA: precision control over every part of your cache](https://blog.cloudflare.com/content/images/2023/10/Cache-Rules-GA-1.png)\n\nOne year ago we introduced Cache Rules, a new way to customize cache settings on Cloudflare. Cache Rules provide greater flexibility for how users cache content, offering precise controls, a user-friendly API, and seamless Terraform integrations. Since it was released in late September 2022, over 100,000 websites have used Cache Rules to fine-tune their cache settings.\n\nToday, we're thrilled to announce that Cache Rules, along with several other [Rules products](https://developers.cloudflare.com/rules/), are **generally available (GA)**. But that’s not all — we're also introducing new configuration options for Cache Rules that provide even more options to customize how you cache on Cloudflare. These include functionality to define what resources are eligible for [Cache Reserve](https://developers.cloudflare.com/cache/advanced-configuration/cache-reserve/), what [timeout values](https://developers.cloudflare.com/support/troubleshooting/cloudflare-errors/troubleshooting-cloudflare-5xx-errors/#error-524-a-timeout-occurred) should be respected when receiving data from your origin server, which [custom ports](https://developers.cloudflare.com/fundamentals/reference/network-ports/#network-ports-compatible-with-cloudflares-proxy) we should use when we cache content, and whether we should bypass Cloudflare’s cache in the absence of a [cache-control](https://developers.cloudflare.com/cache/concepts/cache-control/#cache-control-directives) header.\n\nCache Rules give users full control and the ability to tailor their content delivery strategy for almost any use case, without needing to write code. As Cache Rules go GA, we are incredibly excited to see how fast customers can achieve their perfect cache strategy.\n\n### History of Customizing Cache on Cloudflare\n\nThe journey of [cache](https://www.cloudflare.com/learning/cdn/what-is-caching/) customization on Cloudflare began more than a decade ago, right at the beginning of the company. From the outset, one of the most frequent requests from our customers involved simplifying their configurations. Customers wanted to easily implement precise cache policies, apply robust security measures, manipulate headers, set up redirects, and more for any page on their website. Using Cloudflare to set these controls was especially crucial for customers utilizing origin servers that only provided convoluted configuration options to add headers or policies to responses, which could later be applied downstream by [CDNs](https://www.cloudflare.com/learning/cdn/what-is-a-cdn/).\n\nIn response to this demand, we introduced Page Rules, a product that has since witnessed remarkable growth in both its popularity and functionality. Page Rules became the preferred choice for customers seeking granular control over how Cloudflare caches their content. Currently, there are over 5 million active cache-related Page Rules, assisting websites in tailoring their content delivery strategies.\n\nHowever, behind the scenes, Page Rules encountered a scalability issue.\n\nWhenever a Page Rule is encountered by Cloudflare we must transform all rule conditions for a customer into a single regex pattern. This pattern is then applied to requests for the website to achieve the desired cache configuration. When thinking about how all the regexes from all customers are then compared against tens of millions of requests per second, spanning across more than 300 data centers worldwide, it’s easy to see that the computational demands for applying Page Rules can be immense. This pressure is directly tied to the number of rules we could offer our users. For example, Page Rules would only allow for 125 rules to be deployed on a given website.\n\nTo address this challenge, we rebuilt all the Page Rule functionality on the new [Rulesets Engine](https://developers.cloudflare.com/ruleset-engine/). Not only do ruleset engine-based products give users more rules to play with, they also offer greater flexibility on when these rules should run. Part of the magic of the Rulesets engine is that rather than combine all of a page's rules into a single regular expression, rule logic can be evaluated on a conditional basis. For example, if [subdomain](https://developers.cloudflare.com/dns/manage-dns-records/how-to/create-subdomain/) A and B have different caching policies, a request from subdomain A can be evaluated using regex logic specific to A (while omitting any logic that applies to B). This yields meaningful benefits to performance, and reduces the computational demands of applying Page Rules across Cloudflare's network.\n\nOver the past year, Cache Rules, along with Origin Rules, Configuration Rules, and Single Redirect Rules, have been in beta. Thanks to the invaluable support of our early adopters, we have successfully fine-tuned our product, reaching a stage where it is ready to transition from beta to GA. These products can now accomplish everything that Page Rules could and more. This also marks the beginning of the [EOL](https://en.wikipedia.org/wiki/End-of-life_product) process for Page Rules. In the coming months we will announce timelines and information regarding how customers will replace their Page Rules with specific Rules products. We will automate this as much as possible and provide simple steps to ensure a smooth transition away from Page Rules for everyone.\n\n### How to use Cache Rules and What’s New\n\nThose that have used Cache Rules know that they are intuitive and work similarly to our other [ruleset engine](https://developers.cloudflare.com/ruleset-engine/) products. User-defined criteria like URLs or request headers are evaluated, and if matching a specified value, the Cloudflare caching configuration is obeyed. Each Cache Rule depends on fields, operators, and values. For all the different options available, you should see our Cache Rules [documentation](https://developers.cloudflare.com/cache/about/cache-rules/).\n\nBelow are two examples of how to deploy different strategies to customize your cache. These examples only show the tip-of-the-iceberg of what’s possible with Cache Rules, so we encourage you to try them out and let us know what you think.\n\n#### Example: Cached content is updated at a regular cadence\n\nAs an example, let’s say that Acme Corp wants to update their caching strategy. They want to customize their cache to take advantage of certain request headers and use the presence of those request headers to be the criteria that decides when to apply different cache rules. The first thing they’d need to decide is what information should be used to trigger the specific rule. This is defined in the [expression](https://developers.cloudflare.com/ruleset-engine/rules-language/expressions/).\n\n![](https://blog.cloudflare.com/content/images/2023/10/Screenshot-2023-10-18-at-6.27.59-PM.png)\n\nOnce the triggering criteria is defined Acme Corp should next determine how they want to customize their cache.\n\n#### Content changing quickly\n\nThe most common cache strategy is to update the [Edge Cache TTL](https://developers.cloudflare.com/cache/how-to/cache-rules/#create-cache-rules-in-the-dashboard). If Acme Corp thinks a particular piece of content on their website might change quickly, they can alter the time Cloudflare should consider a resource eligible to be served from cache to be shorter. This way Cloudflare would go back to the origin more frequently to [revalidate and update the content](https://developers.cloudflare.com/cache/concepts/cache-control/#:~:text=If%20the%20content%20is%20stale%20in%20Cloudflare%E2%80%99s%20cache%2C%20Cloudflare%20attempts%20to%20revalidate%20the%20content%20with%20the%20origin%20before%20serving%20the%20response%20to%20the%20client.). The Edge Cache TTL section is also where Acme Corp can define a resource’s TTL based on the status code Cloudflare gets back from their origin, and what Cloudflare should cache if there aren’t any cache-control instructions sent from Acme’s origin server.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Screenshot-2023-10-08-at-4.14.17-PM.png)\n\n#### Content changing slowly\n\nOn the other hand, if Acme Corp had a lot of content that did not change very frequently (like a favicon or logo) and they preferred to serve that from Cloudflare’s cache instead of their origin, they can define which content should be eligible for [Cache Reserve](https://developers.cloudflare.com/cache/advanced-configuration/cache-reserve/) with a new Cache Rule. Cache Reserve reduces egress fees by storing assets persistently in Cloudflare's cache for an extended period of time.\n\nTraditionally when a user would enable Cache Reserve, their entire zone would be eligible to be written to Cache Reserve. For customers that care about saving origin [egress fees](https://www.cloudflare.com/learning/cloud/what-are-data-egress-fees/) on all resources on their website, this is still the best path forward. But for customers that want to have additional control over precisely what assets should be part of their Cache Reserve or even what size of assets should be eligible, the Cache Reserve Eligibility Rule provides additional knobs so that customers can precisely increase their cache hits and reduce origin egress in a customized manner. Note that this rule requires a Cache Reserve subscription.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Screenshot-2023-10-08-at-4.14.48-PM.png)\n\n### Example: Origin is slow\n\nLet’s consider a hypothetical example. Recently, Acme Corp has been seeing an increase in errors in their Cloudflare logs. These errors are related to a new report that Acme is providing its users based on Acme’s proprietary data. This report requires that their origin access several databases, perform some calculations and generate the report based on these calculations. The origin generating this report needs to wait to respond until all of this background work is completed. Acme’s report is a success, generating an influx of traffic from visitors wanting to see it. But their origin is struggling to keep up. A lot of the errors they are seeing are 524s which correlate to Cloudflare not seeing an origin response before a timeout occurred.\n\nAcme has plans to improve this by scaling their origin infrastructure but it’s taking a long time to deploy. In the meantime, they can turn to Cache Rules to configure a timeout to be longer. Historically the timeout value between Cloudflare and two successive origin reads was 100 seconds, which meant that if an origin didn't successfully send a response for a period lasting longer than 100 seconds, it could lead to a 524 error. By using a Cache Rule to extend this timeout, Acme Corp can rely more heavily on Cloudflare's cache.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Screenshot-2023-10-18-at-6.21.32-PM.png)\n\nThe above cache strategies focus on how often a resource is changed on an origin, and the origin’s performance. But there are numerous other rules that allow for other strategies, like [custom cache keys](https://developers.cloudflare.com/cache/how-to/cache-keys/) which allow for customers to determine how their cache should be defined on Cloudflare, respecting [strong ETags](https://developers.cloudflare.com/cache/reference/etag-headers/) which help customers determine when Cloudflare should revalidate particular cached assets, and custom ports which allow for customers to define [non-standard ports](https://developers.cloudflare.com/fundamentals/reference/network-ports/#network-ports-compatible-with-cloudflares-proxy) that Cloudflare should use when making caching decisions about content.\n\nThe full list of Cache Rules can be found [here](https://developers.cloudflare.com/cache/how-to/cache-rules/).\n\n### Try Cache Rules today!\n\nWe will continue to build and release additional rules that provide powerful, easy to enable control for anyone using Cloudflare’s cache. If you have feature requests for additional Cache Rules, please let us know in the [Cloudflare Community](https://community.cloudflare.com/).\n\nGo to the [dashboard](https://dash.cloudflare.com/caching/cache-rules) and try Cache Rules out today!\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[General Availability](https://blog.cloudflare.com/tag/general-availability/) [Cache Rules](https://blog.cloudflare.com/tag/cache-rules/) [Product News](https://blog.cloudflare.com/tag/product-news/) [Application Services](https://blog.cloudflare.com/tag/application-services/) [Performance](https://blog.cloudflare.com/tag/performance/)"
    },
    {
      "url": "https://blog.cloudflare.com/how-cloudflare-mitigated-yet-another-okta-compromise/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/how-cloudflare-mitigated-yet-another-okta-compromise/",
        "loadedTime": "2023-12-05T02:30:40.534Z",
        "referrerUrl": "https://blog.cloudflare.com/page/2/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/how-cloudflare-mitigated-yet-another-okta-compromise/",
        "title": "How Cloudflare mitigated yet another Okta compromise",
        "description": "On Wednesday, October 18, 2023, we discovered attacks on our system that we were able to trace back to Okta. We have verified that no Cloudflare customer information or systems were impacted by this event because of our rapid response.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/20/2023\n3 min read\nOn Wednesday, October 18, 2023, we discovered attacks on our system that we were able to trace back to Okta – threat actors were able to leverage an authentication token compromised at Okta to pivot into Cloudflare’s Okta instance. While this was a troubling security incident, our Security Incident Response Team’s (SIRT) real-time detection and prompt response enabled containment and minimized the impact to Cloudflare systems and data. We have verified that no Cloudflare customer information or systems were impacted by this event because of our rapid response. Okta has now released a public statement about this incident.\nThis is the second time Cloudflare has been impacted by a breach of Okta’s systems. In March 2022, we blogged about our investigation on how a breach of Okta affected Cloudflare. In that incident, we concluded that there was no access from the threat actor to any of our systems or data – Cloudflare’s use of hard keys for multi-factor authentication stopped this attack. \nThe key to mitigating this week’s incident was our team’s early detection and immediate response. In fact, we contacted Okta about the breach of their systems before they had notified us. The attacker used an open session from Okta, with Administrative privileges, and accessed our Okta instance. We were able to use our Cloudflare Zero Trust Access, Gateway, and Data Loss Prevention and our Cloudforce One threat research to validate the scope of the incident and contain it before the attacker could gain access to customer data, customer systems, or our production network. With this confidence, we were able to quickly mitigate the incident before the threat-actors were able to establish persistence.\nAccording to Okta’s statement, the threat-actor accessed Okta’s customer support system and viewed files uploaded by certain Okta customers as part of recent support cases. It appears that in our case, the threat-actor was able to hijack a session token from a support ticket which was created by a Cloudflare employee. Using the token extracted from Okta, the threat-actor accessed Cloudflare systems on October 18. In this sophisticated attack, we observed that threat-actors compromised two separate Cloudflare employee accounts within the Okta platform. We detected this activity internally more than 24 hours before we were notified of the breach by Okta. Upon detection, our SIRT was able to engage quickly to identify the complete scope of compromise and contain the security incident. Cloudflare’s Zero Trust architecture protects our production environment, which helped prevent any impact to our customers.\nRecommendations for Okta\nWe urge Okta to consider implementing the following best practices, including:\nTake any report of compromise seriously and act immediately to limit damage; in this case Okta was first notified on October 2, 2023 by BeyondTrust but the attacker still had access to their support systems at least until October 18, 2023.\nProvide timely, responsible disclosures to your customers when you identify that a breach of your systems has affected them.\nRequire hardware keys to protect all systems, including third-party support providers.\nFor a critical security service provider like Okta, we believe following these best practices is table stakes.\nRecommendations for Okta’s Customers\nIf you are an Okta customer, we recommend that you reach out to them for further information regarding potential impact to your organization. We also advise the following actions:\nEnable Hardware MFA for all user accounts. Passwords alone do not offer the necessary level of protection against attacks. We strongly recommend the usage of hardware keys, as other methods of MFA can be vulnerable to phishing attacks.\nInvestigate and respond to: \nAll unexpected password and MFA changes for your Okta instances.\nSuspicious support-initiated events.\nEnsure all password resets are valid and force a password reset for any under suspicion.\nAny suspicious MFA-related events, ensuring only valid MFA keys are present in the user's account configuration.\nMonitor for: \nNew Okta users created.\nReactivation of Okta users.\nAll sessions have proper authentication associated with it.\nAll Okta account and permission changes.\nMFA policy overrides, MFA changes, and MFA removal.\nDelegation of sensitive applications.\nSupply chain providers accessing your tenants.\nReview session expiration policies to limit session hijack attacks.\nUtilize tools to validate devices connected to your critical systems, such as Cloudflare Access Device Posture Check.\nPractice defense in depth for your detection and monitoring strategies.\nCloudflare’s Security and IT teams continue to remain vigilant after this compromise. If further information is disclosed by Okta or discovered through additional log analysis, we will publish an update to this post.\nCloudflare's Security Incident Response Team is hiring. \nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nOkta Post Mortem 1.1.1.1",
      "markdown": "10/20/2023\n\n*   [![Sourov Zaman](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/05/Screen-Shot-2022-06-03-at-1.49.26-AM.png)](https://blog.cloudflare.com/author/sourov/)\n*   [![Lucas Ferreira](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/02/IMG_20210718_203048.jpeg)](https://blog.cloudflare.com/author/lucas-ferreira/)\n*   [![Kimberly Hall](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/_tmp_mini_magick20221017-43-3fd60e.jpg)](https://blog.cloudflare.com/author/kimberly/)\n*   [![Grant Bourzikas](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/04/Headshot---GB_LinkedIn.jpg)](https://blog.cloudflare.com/author/grant/)\n\n3 min read\n\nOn Wednesday, October 18, 2023, we discovered attacks on our system that we were able to trace back to Okta – threat actors were able to leverage an authentication token compromised at Okta to pivot into Cloudflare’s Okta instance. While this was a troubling security incident, our Security Incident Response Team’s (SIRT) real-time detection and prompt response enabled containment and minimized the impact to Cloudflare systems and data. We have verified that **no Cloudflare customer information or systems were impacted by this event** because of our rapid response. Okta has now released a [public statement](https://sec.okta.com/harfiles) about this incident.\n\nThis is the second time Cloudflare has been impacted by a breach of Okta’s systems. In [March 2022](https://blog.cloudflare.com/cloudflare-investigation-of-the-january-2022-okta-compromise/), we blogged about our investigation on how a breach of Okta affected Cloudflare. In that incident, we concluded that there was no access from the threat actor to any of our systems or data – Cloudflare’s use of hard keys for multi-factor authentication stopped this attack.  \n\nThe key to mitigating this week’s incident was our team’s early detection and immediate response. In fact, we contacted Okta about the breach of their systems before they had notified us. The attacker used an open session from Okta, with Administrative privileges, and accessed our Okta instance. We were able to use our Cloudflare Zero Trust Access, Gateway, and Data Loss Prevention and our Cloudforce One threat research to validate the scope of the incident and contain it before the attacker could gain access to customer data, customer systems, or our production network. With this confidence, we were able to quickly mitigate the incident before the threat-actors were able to establish persistence.\n\nAccording to Okta’s statement, the threat-actor accessed Okta’s customer support system and viewed files uploaded by certain Okta customers as part of recent support cases. It appears that in our case, the threat-actor was able to hijack a session token from a support ticket which was created by a Cloudflare employee. Using the token extracted from Okta, the threat-actor accessed Cloudflare systems on October 18. In this sophisticated attack, we observed that threat-actors compromised two separate Cloudflare employee accounts within the Okta platform. We detected this activity internally more than 24 hours before we were notified of the breach by Okta. Upon detection, our SIRT was able to engage quickly to identify the complete scope of compromise and contain the security incident. Cloudflare’s [Zero Trust architecture](https://www.cloudflare.com/learning/security/glossary/what-is-zero-trust/) protects our production environment, which helped prevent any impact to our customers.\n\n## Recommendations for Okta\n\nWe urge Okta to consider implementing the following best practices, including:\n\n*   Take any report of compromise seriously and act immediately to limit damage; in this case Okta was first notified on October 2, 2023 by [BeyondTrust](https://www.beyondtrust.com/blog/entry/okta-support-unit-breach) but the attacker still had access to their support systems at least until October 18, 2023.\n*   Provide timely, responsible disclosures to your customers when you identify that a breach of your systems has affected them.\n*   Require hardware keys to protect all systems, including third-party support providers.\n\nFor a critical security service provider like Okta, we believe following these best practices is table stakes.\n\n## Recommendations for Okta’s Customers\n\nIf you are an Okta customer, we recommend that you reach out to them for further information regarding potential impact to your organization. We also advise the following actions:\n\n*   Enable Hardware MFA for all user accounts. Passwords alone do not offer the necessary level of protection against attacks. We strongly recommend the usage of hardware keys, as other methods of MFA can be vulnerable to phishing attacks.\n*   Investigate and respond to:\n    *   All unexpected password and MFA changes for your Okta instances.\n    *   Suspicious support-initiated events.\n    *   Ensure all password resets are valid and force a password reset for any under suspicion.\n    *   Any suspicious MFA-related events, ensuring only valid MFA keys are present in the user's account configuration.\n*   Monitor for:\n    *   New Okta users created.\n    *   Reactivation of Okta users.\n    *   All sessions have proper authentication associated with it.\n    *   All Okta account and permission changes.\n    *   MFA policy overrides, MFA changes, and MFA removal.\n    *   Delegation of sensitive applications.\n    *   Supply chain providers accessing your tenants.\n*   Review session expiration policies to limit session hijack attacks.\n*   Utilize tools to validate devices connected to your critical systems, such as Cloudflare Access Device Posture Check.\n*   Practice defense in depth for your detection and monitoring strategies.\n\nCloudflare’s Security and IT teams continue to remain vigilant after this compromise. If further information is disclosed by Okta or discovered through additional log analysis, we will publish an update to this post.\n\n_Cloudflare's Security Incident Response Team [is hiring](https://cloudflare.com/careers)._\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Okta](https://blog.cloudflare.com/tag/okta/) [Post Mortem](https://blog.cloudflare.com/tag/post-mortem/) [1.1.1.1](https://blog.cloudflare.com/tag/1-1-1-1/)"
    },
    {
      "url": "https://blog.cloudflare.com/cyber-attacks-in-the-israel-hamas-war/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/cyber-attacks-in-the-israel-hamas-war/",
        "loadedTime": "2023-12-05T02:30:37.639Z",
        "referrerUrl": "https://blog.cloudflare.com/page/2/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/cyber-attacks-in-the-israel-hamas-war/",
        "title": "Cyber attacks in the Israel-Hamas war",
        "description": "Since the October 7 Hamas attack, DDoS attackers have been targeting Israeli newspaper and media websites as well as software companies and financial institutions.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/23/2023\n5 min read\nThis post is also available in Deutsch, Français, עברית and عربي.\nOn October 7, 2023, at 03:30 GMT (06:30 AM local time), Hamas attacked Israeli cities and fired thousands of rockets toward populous locations in southern and central Israel, including Tel Aviv and Jerusalem. Air raid sirens began sounding, instructing civilians to take cover.\nApproximately twelve minutes later, Cloudflare systems automatically detected and mitigated DDoS attacks that targeted websites that provide critical information and alerts to civilians on rocket attacks. The initial attack peaked at 100k requests per second (rps) and lasted ten minutes. Forty-five minutes later, a second much larger attack struck and peaked at 1M rps. It lasted six minutes. Additional smaller DDoS attacks continued hitting the websites in the next hours.\nDDoS attacks against Israeli websites that provide civilians information and alerts on rocket attacks\nNot just DDoS attacks\nMultiple Israeli websites and mobile apps have become targets of various pro-Palestinian hacktivist groups. According to Cybernews, one of those groups, AnonGhost, exploited a vulnerability in a mobile app that alerts Israeli civilians of incoming rockets, “Red Alert: Israel”. The exploit allowed them to intercept requests, expose servers and APIs, and send fake alerts to some app users, including a message that a “nuclear bomb is coming”. AnonGhost also claimed to have attacked various other rocket alert apps.\nOn October 14, we revealed the findings of one of our investigations that was conducted by the Cloudforce One Threat Operations team, who identified malicious Android mobile applications impersonating the legitimate RedAlert - Rocket Alerts application. The malicious apps obtained access to sensitive user information such as mobile phone’s contacts list, SMS messages, phone call logs, installed applications, and information about the phone and SIM card themselves. More technical information about our investigation can be found here.\nScreenshot of the malicious site linking to malicious mobile apps\nFurthermore, Cloudflare has identified an Israeli website that was partially defaced by AnonGhost. This website was not using Cloudflare, but we have reached out to the organization to offer support.\n“Death to all Jews” in a part of a website that was hacked and defaced by AnonGhost\nContinued DDoS bombardment\nIn the days following the October 7 attack, Israeli websites have been heavily targeted by DDoS attacks. Cloudflare has been helping onboard and protect many of them.\nHTTP DDoS attacks against Israeli websites using Cloudflare\nSince the October 7, 2023, attack, Newspaper and Media websites have been the main target of DDoS attacks — accounting for 56% of all attacks against Israeli websites. We saw the same trends when Russia attacked Ukraine. Ukrainian media and broadcasting websites were highly targeted. The war on the ground is often accompanied by cyber attacks on websites that provide crucial information for civilians.\nThe second most targeted industry in Israel was the Computer Software industry. Almost 34% of all DDoS attacks targeted computer software companies. In third place, and more significantly, Banking, Financial Services and Insurance (BFSI) companies were attacked. Government Administration websites came in fourth place.\nTop Israeli industries targeted by HTTP DDoS attacks\nWe can also see that Israeli newspaper and media websites were targeted immediately after the October 7 attack.\nHTTP DDoS attacks against Israeli websites using Cloudflare by industry\nSince October 1, 2023, Cloudflare automatically detected and mitigated over 5 billion HTTP requests that were part of DDoS attacks. Before October 7, there were barely any HTTP DDoS attack requests towards Israeli websites using Cloudflare.\nHowever, on the day of the Hamas attack, the percentage of DDoS attack traffic increased. Nearly 1 out of every 100 requests towards Israeli websites using Cloudflare were part of an HTTP DDoS attack. That figure quadrupled on October 8.\nPercentage of DDoS requests out of all requests towards Israeli websites using Cloudflare\nCyber attacks against Palestinian websites\nDuring the same time frame, from October 1, Cloudflare automatically detected and mitigated over 454 million HTTP DDoS attack requests that targeted Palestinian websites using Cloudflare. While that figure is barely a tenth of the amount of attack requests we saw against Israeli websites using Cloudflare, it represented a proportionately larger portion of the overall traffic towards Palestinian websites using Cloudflare.\nOn the days before the Hamas attack, we didn't see any DDoS attacks against Palestinian websites using Cloudflare. That changed on October 7; over 46% of all traffic to Palestinian websites using Cloudflare were part of HTTP DDoS attacks.\nOn October 9, that figure increased to almost 60%. Nearly 6 out of every 10 HTTP requests towards Palestinian websites using Cloudflare were part of DDoS attacks.\nPercentage of DDoS requests out of all requests towards Palestinian websites using Cloudflare\nWe can also see these attacks represented in the spikes in the graph below after the Hamas attack.\nHTTP DDoS attacks against Palestinian websites using Cloudflare\nThere were three Palestinian industries that were attacked in the past weeks. The absolute majority of HTTP DDoS attacks were against Banking websites — nearly 76% of all attacks. The second most attacked industry was the Internet industry with a share of 24% of all DDoS attacks. Another small share targeted Media Production websites.\nHTTP DDoS attacks against Palestinian websites using Cloudflare by industry\nSecuring your applications and preventing DDoS attacks\nAs we’ve seen in recent years, real-world conflicts and wars are always accompanied by cyberattacks. We’ve put together a list of recommendations to optimize your defenses against DDoS attacks. You can also follow our step-by-step wizards to secure your applications and prevent DDoS attacks.\nReaders are also invited to dive in deeper in the Radar dashboard to view traffic and attack insights and trends in Israel and Palestine. You can also read more about the Internet traffic and attack trend in Israel and Palestine following the October 7 attack.\nUnder attack or need additional protection? Click here to get help.\nClick here to protect against malicious mobile apps\nA note about our methodologies\nThe insights that we provide is based on traffic and attacks that we see against websites that are using Cloudflare, unless otherwise stated or referenced to a third party source. More information about our methodologies can be found here.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nDDoS Attacks Israel Cloudflare Radar Insights \nRelated Posts\nApril 12, 2022 2:12PM\nDDoS Attack Trends for 2022 Q1\nWelcome to our first DDoS report of 2022, and the ninth in total so far. This report includes new data points and insights both in the application-layer and network-layer sections — as observed across the global Cloudflare network between January and March 2022...\nBy \nJanuary 10, 2022 1:58PM\nDDoS Attack Trends for Q4 2021\nIn Q4, we observed a 95% increase in L3/4 DDoS attacks and record-breaking levels of Ransom DDoS attacks. The Manufacturing industry was the most targeted alongside a 5,800% increase in SNMP-based DDoS attacks and massive campaigns against VoIP providers around the world...\nBy \nApril 25, 2017 8:45AM\nEcommerce websites on Cloudflare: best practices\nCloudflare provides numerous benefits to ecommerce sites, including advanced DDOS protection and an industry-leading Web Application Firewall (WAF) that helps secure your transactions and protect customers’ private data....\nBy \nNovember 09, 2021 12:59PM\nA Brief History of the Meris Botnet\nOver the past months, we’ve been tracking and analyzing the activity of the Meris botnet....\nBy",
      "markdown": "10/23/2023\n\n*   [![Omer Yoachimik](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/06/Screenshot-2023-06-02-at-9.38.13-AM.png)](https://blog.cloudflare.com/author/omer/)\n*   [![Jorge Pacheco](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/04/CV_Profile.jpeg)](https://blog.cloudflare.com/author/jorge/)\n\n5 min read\n\nThis post is also available in [Deutsch](https://blog.cloudflare.com/de-de/cyber-attacks-in-the-israel-hamas-war-de-de/), [Français](https://blog.cloudflare.com/fr-fr/cyber-attacks-in-the-israel-hamas-war-fr-fr/), [עברית](https://blog.cloudflare.com/he-il/cyber-attacks-in-the-israel-hamas-war-he-il/) and [عربي](https://blog.cloudflare.com/ar-ar/cyber-attacks-in-the-israel-hamas-war-ar-ar/).\n\n![Cyber attacks in the Israel-Hamas war](https://blog.cloudflare.com/content/images/2023/10/image11-1.png)\n\nOn October 7, 2023, at 03:30 GMT (06:30 AM local time), Hamas attacked Israeli cities and fired thousands of rockets toward populous locations in southern and central Israel, including Tel Aviv and Jerusalem. Air raid sirens began sounding, instructing civilians to take cover.\n\nApproximately twelve minutes later, Cloudflare systems automatically detected and mitigated DDoS attacks that targeted websites that provide critical information and alerts to civilians on rocket attacks. The initial attack peaked at 100k requests per second (rps) and lasted ten minutes. Forty-five minutes later, a second much larger attack struck and peaked at 1M rps. It lasted six minutes. Additional smaller DDoS attacks continued hitting the websites in the next hours.\n\n![DDoS attacks against Israeli websites that provide civilians information and alerts on rocket attacks](https://blog.cloudflare.com/content/images/2023/10/image9.png)\n\nDDoS attacks against Israeli websites that provide civilians information and alerts on rocket attacks\n\n### Not just DDoS attacks\n\nMultiple Israeli websites and mobile apps have become targets of various pro-Palestinian hacktivist groups. According to [Cybernews](https://cybernews.com/cyber-war/israel-redalert-breached-anonghost-hamas/), one of those groups, AnonGhost, exploited a vulnerability in a mobile app that alerts Israeli civilians of incoming rockets, “Red Alert: Israel”. The exploit allowed them to intercept requests, expose servers and APIs, and send fake alerts to some app users, including a message that a “[nuclear bomb is coming](https://www.bitdefender.co.uk/blog/hotforsecurity/hacktivists-send-fake-nuclear-attack-warning-via-israeli-red-alert-app/)”. AnonGhost also claimed to have attacked various other rocket alert apps.\n\nOn October 14, we revealed the findings of one of our investigations that was conducted by the [Cloudforce One](https://blog.cloudflare.com/introducing-cloudforce-one-threat-operations-and-threat-research/) Threat Operations team, who identified malicious Android mobile applications impersonating the legitimate RedAlert - Rocket Alerts application. The malicious apps obtained access to sensitive user information such as mobile phone’s contacts list, SMS messages, phone call logs, installed applications, and information about the phone and SIM card themselves. More technical information about our investigation can be found [here](https://blog.cloudflare.com/malicious-redalert-rocket-alerts-application-targets-israeli-phone-calls-sms-and-user-information/).\n\n![Screenshot of the malicious site linking to malicious mobile apps](https://blog.cloudflare.com/content/images/2023/10/image10.png)\n\nScreenshot of the malicious site linking to malicious mobile apps\n\nFurthermore, Cloudflare has identified an Israeli website that was partially defaced by AnonGhost. This website was not using Cloudflare, but we have reached out to the organization to offer support.\n\n![“Death to all Jews” in a part of a website that was hacked and defaced by AnonGhost](https://blog.cloudflare.com/content/images/2023/10/image1-7.png)\n\n“Death to all Jews” in a part of a website that was hacked and defaced by AnonGhost\n\n### Continued DDoS bombardment\n\nIn the days following the October 7 attack, Israeli websites have been heavily targeted by DDoS attacks. Cloudflare has been helping onboard and protect many of them.\n\n![HTTP DDoS attacks against Israeli websites using Cloudflare](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--7-.png)\n\nHTTP DDoS attacks against Israeli websites using Cloudflare\n\nSince the October 7, 2023, attack, Newspaper and Media websites have been the main target of DDoS attacks — accounting for 56% of all attacks against Israeli websites. We saw the same trends when Russia attacked Ukraine. Ukrainian media and broadcasting websites were highly targeted. The war on the ground is often accompanied by cyber attacks on websites that provide crucial information for civilians.\n\nThe second most targeted industry in Israel was the Computer Software industry. Almost 34% of all DDoS attacks targeted computer software companies. In third place, and more significantly, Banking, Financial Services and Insurance (BFSI) companies were attacked. Government Administration websites came in fourth place.\n\n![Top Israeli industries targeted by HTTP DDoS attacks](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0.png)\n\nTop Israeli industries targeted by HTTP DDoS attacks\n\nWe can also see that Israeli newspaper and media websites were targeted immediately after the October 7 attack.\n\n![HTTP DDoS attacks against Israeli websites using Cloudflare by industry](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--1-.png)\n\nHTTP DDoS attacks against Israeli websites using Cloudflare by industry\n\nSince October 1, 2023, Cloudflare automatically detected and mitigated over 5 billion HTTP requests that were part of DDoS attacks. Before October 7, there were barely any HTTP DDoS attack requests towards Israeli websites using Cloudflare.\n\nHowever, on the day of the Hamas attack, the percentage of DDoS attack traffic increased. Nearly 1 out of every 100 requests towards Israeli websites using Cloudflare were part of an HTTP DDoS attack. That figure quadrupled on October 8.\n\n![Percentage of DDoS requests out of all requests towards Israeli websites using Cloudflare](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--2-.png)\n\nPercentage of DDoS requests out of all requests towards Israeli websites using Cloudflare\n\n### Cyber attacks against Palestinian websites\n\nDuring the same time frame, from October 1, Cloudflare automatically detected and mitigated over 454 million HTTP DDoS attack requests that targeted Palestinian websites using Cloudflare. While that figure is barely a tenth of the amount of attack requests we saw against Israeli websites using Cloudflare, it represented a proportionately larger portion of the overall traffic towards Palestinian websites using Cloudflare.\n\nOn the days before the Hamas attack, we didn't see any DDoS attacks against Palestinian websites using Cloudflare. That changed on October 7; over 46% of all traffic to Palestinian websites using Cloudflare were part of HTTP DDoS attacks.\n\nOn October 9, that figure increased to almost 60%. Nearly 6 out of every 10 HTTP requests towards Palestinian websites using Cloudflare were part of DDoS attacks.\n\n![Percentage of DDoS requests out of all requests towards Palestinian websites using Cloudflare](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--3-.png)\n\nPercentage of DDoS requests out of all requests towards Palestinian websites using Cloudflare\n\nWe can also see these attacks represented in the spikes in the graph below after the Hamas attack.\n\n![HTTP DDoS attacks against Palestinian websites using Cloudflare](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--4-.png)\n\nHTTP DDoS attacks against Palestinian websites using Cloudflare\n\nThere were three Palestinian industries that were attacked in the past weeks. The absolute majority of HTTP DDoS attacks were against Banking websites — nearly 76% of all attacks. The second most attacked industry was the Internet industry with a share of 24% of all DDoS attacks. Another small share targeted Media Production websites.\n\n![HTTP DDoS attacks against Palestinian websites using Cloudflare by industry](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--5-.png)\n\nHTTP DDoS attacks against Palestinian websites using Cloudflare by industry\n\n### Securing your applications and preventing DDoS attacks\n\nAs we’ve seen in recent years, real-world conflicts and wars are always accompanied by cyberattacks. We’ve put together a [list of recommendations](https://developers.cloudflare.com/ddos-protection/best-practices/respond-to-ddos-attacks/) to optimize your defenses against DDoS attacks. You can also follow our step-by-step wizards to [secure your applications](https://developers.cloudflare.com/learning-paths/application-security/) and [prevent DDoS attacks](https://developers.cloudflare.com/learning-paths/prevent-ddos-attacks/).\n\nReaders are also invited to dive in deeper in the Radar dashboard to view traffic and attack insights and trends in [Israel](https://radar.cloudflare.com/il?dateRange=28d) and [Palestine](https://radar.cloudflare.com/ps?dateRange=28d). You can also read more about the [Internet traffic and attack trend in Israel and Palestine](https://blog.cloudflare.com/internet-traffic-patterns-in-israel-and-palestine-following-the-october-2023-attacks/) following the October 7 attack.\n\n_**Under attack or need additional protection?**_ [_**Click here to get help**_](https://www.cloudflare.com/under-attack-hotline/)_**.**_\n\n[_**Click here**_](https://1.1.1.1/family/) _**to protect against malicious mobile apps**_\n\n### A note about our methodologies\n\nThe insights that we provide is based on traffic and attacks that we see against websites that are using Cloudflare, unless otherwise stated or referenced to a third party source. More information about our methodologies can be found [here](https://developers.cloudflare.com/radar/reference/quarterly-ddos-reports/#quarterly-ddos-threat-reports).\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[DDoS](https://blog.cloudflare.com/tag/ddos/) [Attacks](https://blog.cloudflare.com/tag/attacks/) [Israel](https://blog.cloudflare.com/tag/israel/) [Cloudflare Radar](https://blog.cloudflare.com/tag/cloudflare-radar/) [Insights](https://blog.cloudflare.com/tag/insights/)\n\nRelated Posts\n\nApril 12, 2022 2:12PM\n\n[\n\n## DDoS Attack Trends for 2022 Q1\n\n](https://blog.cloudflare.com/ddos-attack-trends-for-2022-q1/)\n\nWelcome to our first DDoS report of 2022, and the ninth in total so far. This report includes new data points and insights both in the application-layer and network-layer sections — as observed across the global Cloudflare network between January and March 2022...\n\nBy \n\nJanuary 10, 2022 1:58PM\n\n[\n\n## DDoS Attack Trends for Q4 2021\n\n](https://blog.cloudflare.com/ddos-attack-trends-for-2021-q4/)\n\nIn Q4, we observed a 95% increase in L3/4 DDoS attacks and record-breaking levels of Ransom DDoS attacks. The Manufacturing industry was the most targeted alongside a 5,800% increase in SNMP-based DDoS attacks and massive campaigns against VoIP providers around the world...\n\nBy \n\nApril 25, 2017 8:45AM\n\n[\n\n## Ecommerce websites on Cloudflare: best practices\n\n](https://blog.cloudflare.com/ecommerce-best-practices/)\n\nCloudflare provides numerous benefits to ecommerce sites, including advanced DDOS protection and an industry-leading Web Application Firewall (WAF) that helps secure your transactions and protect customers’ private data....\n\nBy \n\nNovember 09, 2021 12:59PM\n\n[\n\n## A Brief History of the Meris Botnet\n\n](https://blog.cloudflare.com/meris-botnet/)\n\nOver the past months, we’ve been tracking and analyzing the activity of the Meris botnet....\n\nBy"
    },
    {
      "url": "https://blog.cloudflare.com/tenant-platform-ui-ga/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/tenant-platform-ui-ga/",
        "loadedTime": "2023-12-05T02:30:47.025Z",
        "referrerUrl": "https://blog.cloudflare.com/page/2/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/tenant-platform-ui-ga/",
        "title": "Empowering our partners with the new Tenant Platform dashboard",
        "description": "We are proud to announce the general availability of our first dashboard for our Tenant Platform, providing an intuitive user interface for agencies and partners to manage their client accounts",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/20/2023\n4 min read\nItching to get started? Apply to the Self Serve Partner Beta or Enterprise partner programs now.\nCloudflare has always worked closely with partners to help build a better Internet. From our earliest Hosting Partners, to our latest Cloudflare One program and Authorized Service Delivery partners, we are dedicated to supporting our peers across the networking and cybersecurity ecosystem to secure Enterprise networks, mission-critical applications, and remote employees. As part of that commitment, we are proud to announce the general availability of our first dashboard for our Tenant Platform, providing an intuitive user interface for agencies and partners to manage their client accounts.\nTenant Platform introduction\nThe first version of the Tenant Platform was created in 2018 to support one of our large integration partners, IBM Cloud. They needed a secure way to independently provision accounts for their clients, spin up custom subscriptions, invite service users within each new account, and begin to configure the service. This platform, although API only, worked extremely well with our OEM and integration partners that were including our solution within their current platform to support their customers.\nMulti-Tenant Structure\nUser interface overview\nAs Cloudflare has expanded the type of partners and customers it works with, it became clear that a user interface would be required to support agency, service and system integration partners that needed the same administrative controls as our integration partners, but available within our core dashboard. This allows scaled account management to be available to our partners as well as large Enterprise customers managing multiple business units.\nAccessing the platform\nAll of our current partner administrators are now able to access the Tenants link on the left hand navigation within the Cloudflare dashboard. This will place them within the Tenant Overview page. \nTenant Overview\nWithin the Tenant Overview, we surface our documentation and some basic information about the tenant system and the program level of the partner.\nAccount insights\nAfter speaking with our partners, the ability to quickly assess the health of accounts and underlying zones was paramount. To this end, we prioritized surfacing account-level security and performance insights for all child accounts within the Tenant system. This allows partner support teams to immediately dive into a specific Account that may be experiencing an issue or misconfiguration.\nSecurity Insights allow you to quickly confirm that an application is effectively onboarded.Traffic Insights surface error rates across all child accounts. \nManaged accounts\nAn additional improvement is the creation of a Managed Account section under the Tenant category. This section allows you to quickly search for and get a summary of accounts in active management by your Tenant system. This table will include cumulative spend of the account, product activation reviews, and the most recent action from the account audit log. The goal is to provide a single page to understand the health of all of your managed accounts, and jump into the specific account when necessary. \nManaged Accounts provide a summary of all child accounts.\nJust getting started\nCloudflare’s mission is to help build a better Internet. We know we cannot do that alone, and we treasure all of our partners that have worked with us to accomplish that mission across our technical, channel and alliance relationships. Throughout 2023 and 2024, we look to continue to grow our Tenant Platform across a few key areas. We have a robust roadmap to continue to mature our API and UI for tenant user management, centralized subscription management, billing and analytics rollups, and other foundational improvements. Our goal is to make Cloudflare the vendor of choice to seamlessly integrate with our partner’s product and service offerings for CDN, WAF, DDoS, Zero Trust, Workers and more. \nWould you like to learn more?\nApply to become an Enterprise Partner on our Partner Portal\nSign Up for the Self-Serve Partner Program\nReach out to partners@cloudflare.com\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nPartners Product News Cloudflare One \nRelated Posts\nSeptember 28, 2023 2:00PM\nCloudflare Integrations Marketplace introduces three new partners: Sentry, Momento and Turso\nEarlier this year, we introduced integrations with Supabase, PlanetScale, Neon and Upstash. Today, we are thrilled to introduce our newest additions to Cloudflare’s Integrations Marketplace – Sentry, Turso and Momento...\nBy \nMarch 16, 2023 1:00PM\nIBM Cloud works with Cloudflare to help clients modernize and deliver secured cloud infrastructure\nIBM and Cloudflare continue to partner together to help customers meet the unique security, performance, resiliency and compliance needs of their customers through the addition of exciting new product and service offerings....\nBy \nMarch 17, 2022 9:59AM\nCloudflare and CrowdStrike partner to give CISOs secure control across devices, applications, and corporate networks\nWe're very excited to announce multiple new integrations with CrowdStrike. These integrations combine the power of Cloudflare’s expansive network and Zero Trust suite, with CrowdStrike’s Endpoint Detection and Response (EDR) and incident remediation offerings...\nBy \nApril 13, 2021 2:00PM\nExpanding the Cloudflare Workers Observability Ecosystem\nCloudflare adds Data Dog, Honeycomb, New Relic, Sentry, Splunk, and Sumologic as observability partners to the Cloudflare Workers Ecosystem...\nBy",
      "markdown": "10/20/2023\n\n*   [![Dan Hollinger](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2019/06/Tim-King-Session---1977.jpg)](https://blog.cloudflare.com/author/dan/)\n\n4 min read\n\n_Itching to get started? Apply to the [Self Serve Partner Beta](https://www.cloudflare.com/cloudflare-partners-self-serve-program-closed-beta/) or [Enterprise partner programs](https://portal.cloudflarepartners.com/) now._\n\n![Empowering our partners with the new Tenant Platform dashboard](https://blog.cloudflare.com/content/images/2023/10/Cloudflare-Tenant-Platform_2b.png)\n\nCloudflare has always worked closely with partners to help build a better Internet. From our earliest Hosting Partners, to our latest [Cloudflare One program](https://blog.cloudflare.com/cloudflare-one-partner-program/) and [Authorized Service Delivery partners](https://blog.cloudflare.com/cloudflare-one-authorized-services-delivery-partner-track/), we are dedicated to supporting our peers across the networking and cybersecurity ecosystem to secure Enterprise networks, mission-critical applications, and remote employees. As part of that commitment, we are proud to announce the general availability of our first dashboard for our Tenant Platform, providing an intuitive user interface for agencies and partners to manage their client accounts.\n\n## Tenant Platform introduction\n\nThe [first version](https://blog.cloudflare.com/announcing-the-new-cloudflare-partner-platform/) of the [Tenant Platform](https://developers.cloudflare.com/tenant/) was created in 2018 to support one of our large integration partners, [IBM Cloud](https://www.ibm.com/cloud/cloudflare). They needed a secure way to independently provision accounts for their clients, spin up custom subscriptions, invite service users within each new account, and begin to configure the service. This platform, although API only, worked extremely well with our OEM and integration partners that were including our solution within their current platform to support their customers.\n\n![](https://blog.cloudflare.com/content/images/2023/10/image6.png)\n\n[_Multi-Tenant Structure_](https://developers.cloudflare.com/tenant/structure/)\n\n## User interface overview\n\nAs Cloudflare has expanded the type of partners and customers it works with, it became clear that a user interface would be required to support agency, service and system integration partners that needed the same administrative controls as our integration partners, but available within our core [dashboard](https://dash.cloudflare.com/). This allows scaled account management to be available to our partners as well as large Enterprise customers managing multiple business units.\n\n### Accessing the platform\n\n![](https://blog.cloudflare.com/content/images/2023/10/Screenshot-2023-10-20-at-11.53.00.png)\n\nAll of our current partner administrators are now able to access the Tenants link on the left hand navigation within the Cloudflare dashboard. This will place them within the Tenant Overview page.\n\n### Tenant Overview\n\n![](https://blog.cloudflare.com/content/images/2023/10/image4-5.png)\n\nWithin the Tenant Overview, we surface our documentation and some basic information about the tenant system and the program level of the partner.\n\n#### Account insights\n\nAfter speaking with our partners, the ability to quickly assess the health of accounts and underlying zones was paramount. To this end, we prioritized surfacing account-level security and performance insights for all child accounts within the Tenant system. This allows partner support teams to immediately dive into a specific Account that may be experiencing an issue or misconfiguration.\n\n![](https://blog.cloudflare.com/content/images/2023/10/image5-2.png)\n\n[Security Insights](https://developers.cloudflare.com/security-center/security-insights/) allow you to quickly confirm that an application is effectively onboarded.\n\n![](https://blog.cloudflare.com/content/images/2023/10/image2-6.png)\n\n[Traffic Insights](https://developers.cloudflare.com/analytics/account-and-zone-analytics/zone-analytics/) surface error rates across all child accounts. \n\n### Managed accounts\n\nAn additional improvement is the creation of a Managed Account section under the Tenant category. This section allows you to quickly search for and get a summary of accounts in active management by your Tenant system. This table will include cumulative spend of the account, product activation reviews, and the most recent action from the account audit log. The goal is to provide a single page to understand the health of all of your managed accounts, and jump into the specific account when necessary.  \n\n![](https://blog.cloudflare.com/content/images/2023/10/image7.png)\n\nManaged Accounts provide a summary of all child accounts.\n\n## Just getting started\n\nCloudflare’s mission is to help build a better Internet. We know we cannot do that alone, and we treasure all of our partners that have worked with us to accomplish that mission across our technical, channel and alliance relationships. Throughout 2023 and 2024, we look to continue to grow our Tenant Platform across a few key areas. We have a robust roadmap to continue to mature our API and UI for tenant user management, centralized subscription management, billing and analytics rollups, and other foundational improvements. Our goal is to make Cloudflare the vendor of choice to seamlessly integrate with our partner’s product and service offerings for CDN, WAF, DDoS, Zero Trust, Workers and more.\n\n### Would you like to learn more?\n\n*   Apply to become an Enterprise Partner on our [Partner Portal](http://portal.cloudflarepartners.com/)\n*   Sign Up for the [Self-Serve Partner Program](https://www.cloudflare.com/cloudflare-partners-self-serve-program-closed-beta/)\n*   Reach out to [partners@cloudflare.com](mailto:partners@cloudflare.com)\n\n![](https://blog.cloudflare.com/content/images/2023/10/image8.png)\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Partners](https://blog.cloudflare.com/tag/partners/) [Product News](https://blog.cloudflare.com/tag/product-news/) [Cloudflare One](https://blog.cloudflare.com/tag/cloudflare-one/)\n\nRelated Posts\n\nSeptember 28, 2023 2:00PM\n\n[\n\n## Cloudflare Integrations Marketplace introduces three new partners: Sentry, Momento and Turso\n\n](https://blog.cloudflare.com/cloudflare-integrations-marketplace-new-partners-sentry-momento-turso/)\n\nEarlier this year, we introduced integrations with Supabase, PlanetScale, Neon and Upstash. Today, we are thrilled to introduce our newest additions to Cloudflare’s Integrations Marketplace – Sentry, Turso and Momento...\n\nBy \n\nMarch 16, 2023 1:00PM\n\n[\n\n## IBM Cloud works with Cloudflare to help clients modernize and deliver secured cloud infrastructure\n\n](https://blog.cloudflare.com/ibm-keyless-bots/)\n\nIBM and Cloudflare continue to partner together to help customers meet the unique security, performance, resiliency and compliance needs of their customers through the addition of exciting new product and service offerings....\n\nBy \n\nMarch 17, 2022 9:59AM\n\n[\n\n## Cloudflare and CrowdStrike partner to give CISOs secure control across devices, applications, and corporate networks\n\n](https://blog.cloudflare.com/cloudflare-crowdstrike-partnership/)\n\nWe're very excited to announce multiple new integrations with CrowdStrike. These integrations combine the power of Cloudflare’s expansive network and Zero Trust suite, with CrowdStrike’s Endpoint Detection and Response (EDR) and incident remediation offerings...\n\nBy \n\nApril 13, 2021 2:00PM\n\n[\n\n## Expanding the Cloudflare Workers Observability Ecosystem\n\n](https://blog.cloudflare.com/observability-ecosystem/)\n\nCloudflare adds Data Dog, Honeycomb, New Relic, Sentry, Splunk, and Sumologic as observability partners to the Cloudflare Workers Ecosystem...\n\nBy"
    },
    {
      "url": "https://blog.cloudflare.com/network-flow-monitoring-generally-available/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/network-flow-monitoring-generally-available/",
        "loadedTime": "2023-12-05T02:30:51.833Z",
        "referrerUrl": "https://blog.cloudflare.com/page/2/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/network-flow-monitoring-generally-available/",
        "title": "Network flow monitoring is GA, providing end-to-end traffic visibility",
        "description": "Network engineers often need better visibility into their network’s traffic when analyzing DDoS attacks or troubleshooting other traffic anomalies. To solve this problem, Cloudflare offers a network flow monitoring product that gives customers end-to-end traffic visibility across their network.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/18/2023\n8 min read\nThis post is also available in 简体中文, 繁體中文, 日本語, 한국어, Deutsch and Français.\nNetwork engineers often find they need better visibility into their network’s traffic and operations while analyzing DDoS attacks or troubleshooting other traffic anomalies. These engineers typically have some high level metrics about their network traffic, but they struggle to collect essential information on the specific traffic flows that would clarify the issue. To solve this problem, Cloudflare has been piloting a cloud network flow monitoring product called Magic Network Monitoring that gives customers end-to-end visibility into all traffic across their network.\nToday, Cloudflare is excited to announce that Magic Network Monitoring (previously called Flow Based Monitoring) is now generally available to all enterprise customers. Over the last year, the Cloudflare engineering team has significantly improved Magic Network Monitoring; we’re excited to offer a network services product that will help our customers identify threats faster, reduce vulnerabilities, and make their network more secure.\nMagic Network Monitoring is automatically enabled for all Magic Transit and Magic WAN enterprise customers. The product is located at the account level of the Cloudflare dashboard and can be opened by navigating to “Analytics & Logs > Magic Monitoring”. The onboarding process for Magic Network Monitoring is self-serve, and all enterprise customers with access can begin configuring the product today. \nAny enterprise customers without Magic Transit or Magic WAN that are interested in testing Magic Network Monitoring can receive access to the free version (with some limitations on traffic volume) by submitting a request to their Cloudflare account team or filling out this form to talk with an expert.\nWhat is Magic Network Monitoring?\nMagic Network Monitoring is a cloud network flow monitor. Network traffic flow refers to any stream of packets between one source and one destination with the same Internet protocol and set of ports. Customers can send network flow reports from their routers (or any other network flow generator) to a publicly available endpoint on Cloudflare’s anycast network, even if the traffic didn’t originally pass through Cloudflare’s network. Cloudflare analyzes the network flow data, then provides customers visibility into key network traffic metrics via an analytics dashboard. These metrics include: traffic volume (in bits or packets) over time, source IPs, destination IPs, ports, traffic protocols, and router IPs. Customers can also configure alerts to identify DDoS attacks and any other abnormal traffic volume activities.\nSend flow data from your network to Cloudflare for analysis\nEnterprise DDoS attack type detection\nMagic Transit On Demand (MTOD) customers will experience significant traffic visibility benefits when using Magic Network Monitoring. Magic Transit is a network security solution that offers DDoS protection and traffic acceleration from every Cloudflare data center for on-premise, cloud-hosted, and hybrid networks. Magic Transit On Demand customers can activate Magic Transit for protection when a DDoS attack is detected.\nIn general, we noticed that some MTOD customers lacked the network visibility tools to quickly identify DDoS attacks and take the appropriate mitigation action. Now, MTOD customers can use Magic Network Monitoring to analyze their network data and receive an alert if a DDoS attack is detected.\nCloudflare detects a DDoS attack from the customer’s network flow data\nOnce a DDoS attack is detected, Magic Network Monitoring customers can choose to either manually or automatically enable Magic Transit to mitigate any DDoS attacks.\nActivate Magic Transit for DDoS protection\nEnterprise network monitoring\nCloudflare’s Magic WAN and Cloudflare One customers can also benefit from using Magic Network Monitoring. Today, these customers have excellent visibility into the traffic they send through Cloudflare’s network, but sometimes they may lack visibility into traffic that isn’t sent through Cloudflare. This can include traffic that remains on a local network, or network traffic sent in between cloud environments. Magic WAN and Cloudflare One customers can add Magic Network Monitoring into their suite of product solutions to establish end-to-end network visibility across all traffic on their network.\nA deep dive into network flow and network traffic sampling\nMagic Network Monitoring gives customers better visibility into their network traffic by ingesting and analyzing network flow data.\nThe process starts when a router (or other network flow generation device) collects statistical samples of inbound and / or outbound packet data. These samples are collected by examining 1 in every X packets, where X is the sampling rate configured on the router. Typical sampling rates range from 1 in every 1,000 to 1 in every 4,000 packets. The ideal sampling rate depends on the traffic volume, traffic diversity, and the compute / memory power of your router’s hardware. You can read more about the recommended network flow sampling rate in Cloudflare’s MNM Developer Docs.\nThe sampled data is packaged into one of two industry standard formats for network flow data: NetFlow or sFlow. In NetFlow, the sampled packet data is grouped by different packet characteristics such as source / destination IP, port, and protocol. Each group of sampled packet data also includes a traffic volume estimate. In sFlow, the entire packet header is selected as the representative sample, and there isn’t any data summarization. As a result, sFlow is a richer data format and includes more details about network traffic than NetFlow data. Once either the NetFlow or sFlow data samples are collected, they’re sent to Magic Network Monitoring for analysis and alerting.\nWhy simple random sampling didn’t work for Magic Network Monitoring\nMagic Network Monitoring has come a long way from its early access release one year ago. In particular, the Cloudflare engineering team invested significant time in improving the accuracy of the traffic volume estimations in MNM. In the early access version of Magic Network Monitoring, customers were unexpectedly reporting that their network traffic volume estimates were too high and didn’t match the expected value.\nMagic Network Monitoring performs its own sampling of the NetFlow or sFlow data it receives, so it can effectively scale and manage the data ingested across Cloudflare’s global network. Increasing the accuracy of the traffic volume estimations was more difficult than expected, as the NetFlow or sFlow data parsed by MNM is already built on sampled packet data. This introduces multiple distinct layers of data sampling in the product’s analytics.\nThe first version of Magic Network Monitoring used random sampling where a random subset of network flow data with the same timestamp was selected to represent the traffic volume at that point in time. A characteristic of network flow data is that some samples are more significant than others and represent a greater volume of network traffic. In order to account for this significance, we can associate a weight with each sample based on the traffic volume it represents. Network flow data weights are always positive numbers, and they follow a long tail distribution. These data characteristics caused MNM’s random sampling to incorrectly estimate the traffic volume of a customer’s network. Customers would see false spikes in their traffic volume analytics when an outlying data sample from the long tail was randomly selected to be the representative of all traffic at that point in time.\nIncreasing accuracy with VarOpt reservoir sampling\nTo solve this problem, the Cloudflare engineering team implemented an alternative reservoir sampling technique called VarOpt. VarOpt is designed to collect samples from a stream of data when the length of the data stream is unknown (a perfect application for analyzing incoming network flow data). In the MNM implementation of VarOpt, we start with an empty reservoir of a fixed size that is filled with samples of network flow data. When the reservoir is full, and there is still new incoming network flow data, an old sample is randomly discarded from the reservoir and replaced with a new one. \nAfter a certain number of samples have been observed, we calculate the traffic volume across all weighted samples in the reservoir, and that is the estimated traffic volume of a customer’s network flow at that point in time. Finally, the reservoir is emptied, and the VarOpt loop is restarted by filling the reservoir with the next set of the latest network flow samples.\nThe new VarOpt sampling method significantly increased the accuracy of the traffic volume estimations in Magic Network Monitoring, and solved our customer’s problems. These sampling improvements paved the way for general availability, and we’re excited to make accurate network flow analytics available to everyone.\nDeveloper Docs and Discord Community\nThere are detailed Developer Docs for Magic Network Monitoring that explain the product’s features and outlines a step-by-step configuration guide for new customers. As you’re working through the Magic Network Monitoring documentation, please feel free to provide feedback by clicking the “Give Feedback” button in the top right corner of the Developer Docs.\nWe’ve also created a channel in Cloudflare’s Discord community built around debugging configuration problems, testing new features, and providing product feedback. You can follow this link to join the Cloudflare Discord server.\nFree version\nA free version of Magic Network Monitoring is available to all Enterprise customers on request to their Cloudflare account team. The free version is designed to enable Enterprise customers to quickly test and evaluate Magic Network Monitoring before purchasing Magic Transit, Magic WAN, or Cloudflare One. Enterprise customers can fully configure Magic Network Monitoring themselves by following the step-by-step onboarding guide in the product’s documentation. The free version has some limitations on the quantity of traffic that can be processed which are further outlined in the product’s documentation.\nThe free version of Magic Network Monitoring is also available to all Free, Pro, and Business plan Cloudflare customers via a closed beta. Anyone can request access to the free version by reading the free version documentation and filling out this form. Priority access is granted to anyone that joins Cloudflare’s Discord server and sends a message in the Magic Network Monitoring Discord channel.\nNext steps that you can take today\nMagic Network Monitoring is generally available, and all Magic Transit and Magic WAN customers have been automatically granted access to the product today. You can navigate to the product by going to the account level of the Cloudflare dashboard, then selecting “Analytics & Logs > Magic Monitoring”.\nIf you’re an enterprise customer without Magic Transit or Magic WAN, and you want to use Magic Network Monitoring to improve your traffic visibility, you can talk with an MNM expert today.\nIf you’re interested in using Magic Transit and Magic Network Monitoring for DDoS protection, you can request a demo of Magic Transit. If you want to use Magic WAN and Magic Network Monitoring together to establish end-to-end network traffic visibility, you can talk with a Magic WAN expert.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nMagic Network Monitoring Network Services Magic Transit Magic WAN Product News",
      "markdown": "10/18/2023\n\n*   [![Chris Draper](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/06/blog_headshot.jpg)](https://blog.cloudflare.com/author/chris-draper/)\n*   [![Chris J Arges](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/12/chris.jpg)](https://blog.cloudflare.com/author/arges/)\n*   [![Ana Oliveira](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/_tmp_mini_magick20221018-42-1ufmnay.jpg)](https://blog.cloudflare.com/author/ana/)\n*   [![João Santos](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/_tmp_mini_magick20221017-42-2823yd.jpg)](https://blog.cloudflare.com/author/joao-santos/)\n*   [![Luís Franco](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/Profile-Picture.PNG)](https://blog.cloudflare.com/author/luis/)\n*   [![Nadin El-Yabroudi](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2019/11/Screen-Shot-2019-11-21-at-12.26.44-PM.png)](https://blog.cloudflare.com/author/nadin/)\n*   [![Dan Geraghty](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/_tmp_uploaded20220909-4-1fgoh1l.jpg)](https://blog.cloudflare.com/author/dan-geraghty/)\n\n8 min read\n\nThis post is also available in [简体中文](https://blog.cloudflare.com/zh-cn/network-flow-monitoring-generally-available-zh-cn/), [繁體中文](https://blog.cloudflare.com/zh-tw/network-flow-monitoring-generally-available-zh-tw/), [日本語](https://blog.cloudflare.com/ja-jp/network-flow-monitoring-generally-available-ja-jp/), [한국어](https://blog.cloudflare.com/ko-kr/network-flow-monitoring-generally-available-ko-kr/), [Deutsch](https://blog.cloudflare.com/de-de/network-flow-monitoring-generally-available-de-de/) and [Français](https://blog.cloudflare.com/fr-fr/network-flow-monitoring-generally-available-fr-fr/).\n\n![Network flow monitoring is GA, providing end-to-end traffic visibility](https://blog.cloudflare.com/content/images/2023/10/image4-4.png)\n\nNetwork engineers often find they need better visibility into their network’s traffic and operations while analyzing DDoS attacks or troubleshooting other traffic anomalies. These engineers typically have some high level metrics about their network traffic, but they struggle to collect essential information on the specific traffic flows that would clarify the issue. To solve this problem, Cloudflare has been piloting a cloud network flow monitoring product called [Magic Network Monitoring](https://www.cloudflare.com/network-services/products/magic-network-monitoring/) that gives customers end-to-end visibility into all traffic across their network.\n\nToday, Cloudflare is excited to announce that Magic Network Monitoring (previously called [Flow Based Monitoring](https://blog.cloudflare.com/flow-based-monitoring-for-magic-transit/)) is now generally available to all enterprise customers. Over the last year, the Cloudflare engineering team has significantly improved Magic Network Monitoring; we’re excited to offer a network services product that will help our customers identify threats faster, reduce vulnerabilities, and make their network more secure.\n\nMagic Network Monitoring is automatically enabled for all Magic Transit and Magic WAN enterprise customers. The product is located at the account level of the Cloudflare dashboard and can be opened by navigating to “Analytics & Logs > Magic Monitoring”. The onboarding process for Magic Network Monitoring is self-serve, and all enterprise customers with access can begin configuring the product today.\n\nAny enterprise customers without Magic Transit or Magic WAN that are interested in testing Magic Network Monitoring can receive access to the free version (with some [limitations](https://developers.cloudflare.com/magic-network-monitoring/magic-network-monitoring-free/) on traffic volume) by submitting a request to their Cloudflare account team or filling out this form to [talk with an expert](https://cloudflare.com/network-services/products/magic-network-monitoring/).\n\n### What is Magic Network Monitoring?\n\nMagic Network Monitoring is a cloud network flow monitor. [Network traffic flow](https://en.wikipedia.org/wiki/Traffic_flow_(computer_networking)) refers to any stream of packets between one source and one destination with the same Internet protocol and set of ports. Customers can send network flow reports from their routers (or any other network flow generator) to a publicly available endpoint on [Cloudflare’s anycast network](https://www.cloudflare.com/learning/cdn/glossary/anycast-network/), even if the traffic didn’t originally pass through Cloudflare’s network. Cloudflare analyzes the network flow data, then provides customers visibility into key network traffic metrics via an analytics dashboard. These metrics include: traffic volume (in bits or packets) over time, source IPs, destination IPs, ports, traffic protocols, and router IPs. Customers can also configure alerts to identify [DDoS attacks](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/) and any other abnormal traffic volume activities.\n\n![](https://blog.cloudflare.com/content/images/2023/10/1-1.png)\n\nSend flow data from your network to Cloudflare for analysis\n\n### Enterprise DDoS attack type detection\n\n[Magic Transit On Demand](https://developers.cloudflare.com/magic-transit/on-demand/) (MTOD) customers will experience significant traffic visibility benefits when using Magic Network Monitoring. [Magic Transit](https://www.cloudflare.com/network-services/products/magic-transit/) is a [network security solution](https://www.cloudflare.com/network-security/) that offers DDoS protection and traffic acceleration from every Cloudflare data center for on-premise, cloud-hosted, and hybrid networks. Magic Transit On Demand customers can activate Magic Transit for protection when a DDoS attack is detected.\n\nIn general, we noticed that some MTOD customers lacked the network visibility tools to quickly identify DDoS attacks and take the appropriate mitigation action. Now, MTOD customers can use Magic Network Monitoring to analyze their network data and receive an alert if a DDoS attack is detected.\n\n![](https://blog.cloudflare.com/content/images/2023/10/2-1.png)\n\nCloudflare detects a DDoS attack from the customer’s network flow data\n\nOnce a DDoS attack is detected, Magic Network Monitoring customers can choose to either manually or automatically enable Magic Transit to mitigate any DDoS attacks.\n\n![](https://blog.cloudflare.com/content/images/2023/10/3-1.png)\n\nActivate Magic Transit for DDoS protection\n\n### Enterprise network monitoring\n\nCloudflare’s Magic WAN and Cloudflare One customers can also benefit from using Magic Network Monitoring. Today, these customers have excellent visibility into the traffic they send through Cloudflare’s network, but sometimes they may lack visibility into traffic that isn’t sent through Cloudflare. This can include traffic that remains on a local network, or network traffic sent in between cloud environments. Magic WAN and Cloudflare One customers can add Magic Network Monitoring into their suite of product solutions to establish end-to-end network visibility across all traffic on their network.\n\n### A deep dive into network flow and network traffic sampling\n\nMagic Network Monitoring gives customers better visibility into their network traffic by ingesting and analyzing network flow data.\n\nThe process starts when a router (or other network flow generation device) collects [statistical samples](https://en.wikipedia.org/wiki/Sampling_(statistics)) of inbound and / or outbound packet data. These samples are collected by examining 1 in every X packets, where X is the sampling rate configured on the router. Typical sampling rates range from 1 in every 1,000 to 1 in every 4,000 packets. The ideal sampling rate depends on the traffic volume, traffic diversity, and the compute / memory power of your router’s hardware. You can read more about the [recommended network flow sampling rate](https://developers.cloudflare.com/magic-network-monitoring/routers/recommended-sampling-rate/) in Cloudflare’s MNM Developer Docs.\n\nThe sampled data is packaged into one of two industry standard formats for network flow data: NetFlow or sFlow. In NetFlow, the sampled packet data is grouped by different packet characteristics such as source / destination IP, port, and protocol. Each group of sampled packet data also includes a traffic volume estimate. In sFlow, the entire packet header is selected as the representative sample, and there isn’t any data summarization. As a result, sFlow is a richer data format and includes more details about network traffic than NetFlow data. Once either the NetFlow or sFlow data samples are collected, they’re sent to Magic Network Monitoring for analysis and alerting.\n\n![](https://blog.cloudflare.com/content/images/2023/10/4-1.png)\n\n### Why simple random sampling didn’t work for Magic Network Monitoring\n\nMagic Network Monitoring has come a long way from its early access release one year ago. In particular, the Cloudflare engineering team invested significant time in improving the accuracy of the traffic volume estimations in MNM. In the early access version of Magic Network Monitoring, customers were unexpectedly reporting that their network traffic volume estimates were too high and didn’t match the expected value.\n\nMagic Network Monitoring performs its own sampling of the NetFlow or sFlow data it receives, so it can effectively scale and manage the data ingested across Cloudflare’s global network. Increasing the accuracy of the traffic volume estimations was more difficult than expected, as the NetFlow or sFlow data parsed by MNM is already built on sampled packet data. This introduces multiple distinct layers of data sampling in the product’s analytics.\n\nThe first version of Magic Network Monitoring used [random sampling](https://en.wikipedia.org/wiki/Simple_random_sample) where a random subset of network flow data with the same timestamp was selected to represent the traffic volume at that point in time. A characteristic of network flow data is that some samples are more significant than others and represent a greater volume of network traffic. In order to account for this significance, we can associate a [weight](https://en.wikipedia.org/wiki/Weighting) with each sample based on the traffic volume it represents. Network flow data weights are always positive numbers, and they follow a [long tail distribution](https://en.wikipedia.org/wiki/Long_tail). These data characteristics caused MNM’s random sampling to incorrectly estimate the traffic volume of a customer’s network. Customers would see false spikes in their traffic volume analytics when an outlying data sample from the long tail was randomly selected to be the representative of all traffic at that point in time.\n\n![](https://blog.cloudflare.com/content/images/2023/10/5-1.png)\n\n### Increasing accuracy with VarOpt reservoir sampling\n\nTo solve this problem, the Cloudflare engineering team implemented an alternative [reservoir sampling](https://en.wikipedia.org/wiki/Reservoir_sampling) technique called [VarOpt](https://arxiv.org/pdf/0803.0473.pdf). VarOpt is designed to collect samples from a stream of data when the length of the data stream is unknown (a perfect application for analyzing incoming network flow data). In the MNM implementation of VarOpt, we start with an empty reservoir of a fixed size that is filled with samples of network flow data. When the reservoir is full, and there is still new incoming network flow data, an old sample is randomly discarded from the reservoir and replaced with a new one.\n\nAfter a certain number of samples have been observed, we calculate the traffic volume across all weighted samples in the reservoir, and that is the estimated traffic volume of a customer’s network flow at that point in time. Finally, the reservoir is emptied, and the VarOpt loop is restarted by filling the reservoir with the next set of the latest network flow samples.\n\nThe new VarOpt sampling method significantly increased the accuracy of the traffic volume estimations in Magic Network Monitoring, and solved our customer’s problems. These sampling improvements paved the way for general availability, and we’re excited to make accurate network flow analytics available to everyone.\n\n![](https://blog.cloudflare.com/content/images/2023/10/6-1.png)\n\n### Developer Docs and Discord Community\n\nThere are detailed [Developer Docs for Magic Network Monitoring](https://developers.cloudflare.com/magic-network-monitoring/) that explain the product’s features and outlines a step-by-step configuration guide for new customers. As you’re working through the Magic Network Monitoring documentation, please feel free to provide feedback by clicking the “Give Feedback” button in the top right corner of the Developer Docs.\n\nWe’ve also created a channel in Cloudflare’s Discord community built around debugging configuration problems, testing new features, and providing product feedback. You can follow this link to join the [Cloudflare Discord server](https://discord.gg/cloudflaredev).\n\n### Free version\n\nA [free version of Magic Network Monitoring](https://developers.cloudflare.com/magic-network-monitoring/magic-network-monitoring-free/) is available to all Enterprise customers on request to their Cloudflare account team. The free version is designed to enable Enterprise customers to quickly test and evaluate Magic Network Monitoring before purchasing Magic Transit, Magic WAN, or Cloudflare One. Enterprise customers can fully configure Magic Network Monitoring themselves by following the [step-by-step onboarding guide](https://developers.cloudflare.com/magic-network-monitoring/get-started/) in the product’s documentation. The free version has some [limitations](https://developers.cloudflare.com/magic-network-monitoring/magic-network-monitoring-free/) on the quantity of traffic that can be processed which are further outlined in the product’s documentation.\n\nThe free version of Magic Network Monitoring is also available to all Free, Pro, and Business plan Cloudflare customers via a closed beta. Anyone can request access to the free version by [reading the free version documentation](https://developers.cloudflare.com/magic-network-monitoring/magic-network-monitoring-free/) and [filling out this form](https://forms.gle/z93ghpydpKdAFZ7P9). Priority access is granted to anyone that joins [Cloudflare’s Discord server](https://discord.com/invite/cloudflaredev) and sends a message in the Magic Network Monitoring Discord channel.\n\n### Next steps that you can take today\n\nMagic Network Monitoring is generally available, and all Magic Transit and Magic WAN customers have been automatically granted access to the product today. You can navigate to the product by going to the account level of the Cloudflare dashboard, then selecting “Analytics & Logs > Magic Monitoring”.\n\nIf you’re an enterprise customer without Magic Transit or Magic WAN, and you want to use Magic Network Monitoring to improve your traffic visibility, you can [talk with an MNM expert today](https://cloudflare.com/network-services/products/magic-network-monitoring/).\n\nIf you’re interested in using Magic Transit and Magic Network Monitoring for DDoS protection, you can [request a demo of Magic Transit](https://www.cloudflare.com/network-services/products/magic-transit/). If you want to use Magic WAN and Magic Network Monitoring together to establish end-to-end network traffic visibility, you can [talk with a Magic WAN expert](https://www.cloudflare.com/network-services/products/magic-wan/).\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Magic Network Monitoring](https://blog.cloudflare.com/tag/magic-network-monitoring/) [Network Services](https://blog.cloudflare.com/tag/network-services/) [Magic Transit](https://blog.cloudflare.com/tag/magic-transit/) [Magic WAN](https://blog.cloudflare.com/tag/magic-wan/) [Product News](https://blog.cloudflare.com/tag/product-news/)"
    },
    {
      "url": "https://blog.cloudflare.com/malicious-redalert-rocket-alerts-application-targets-israeli-phone-calls-sms-and-user-information/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/malicious-redalert-rocket-alerts-application-targets-israeli-phone-calls-sms-and-user-information/",
        "loadedTime": "2023-12-05T02:31:04.454Z",
        "referrerUrl": "https://blog.cloudflare.com/page/2/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/malicious-redalert-rocket-alerts-application-targets-israeli-phone-calls-sms-and-user-information/",
        "title": "Malicious “RedAlert - Rocket Alerts” Application Targets Israeli Phone Calls, SMS, and User Information",
        "description": "On October 13, 2023, Cloudflare’s Cloudforce One Threat Operations Team became aware of a malicious Google Android application impersonating the real-time rocket alert app, Red Alert, which  provides real-time rocket alerts for Israeli citizens",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/14/2023\n6 min read\nOn October 13, 2023, Cloudflare’s Cloudforce One Threat Operations Team became aware of a website hosting a Google Android Application (APK) impersonating the legitimate RedAlert - Rocket Alerts application (https://play.google.com/store/apps/details?id=com.red.alert&hl=en&pli=1). More than 5,000 rockets have been launched into Israel since the attacks from Hamas began on October 7th 2023. RedAlert - Rocket Alerts developed by Elad Nava allows individuals to receive timely and precise alerts about incoming airstrikes. Many people living in Israel rely on these alerts to seek safety - a service which has become increasingly important given the newest escalations in the region.\nApplications alerting of incoming airstrikes have become targets as only days ago, Pro-Palestinian hacktivist group AnonGhost exploited a vulnerability in another application, “Red Alert: Israel” by Kobi Snir. (https://cybernews.com/cyber-war/israel-redalert-breached-anonghost-hamas/) Their exploit allowed them to intercept requests, expose servers and APIs, and send fake alerts to some app users, including a message that a “nuclear bomb is coming”. AnonGhost also claimed they attacked other rocket alert applications, including RedAlert by Elad Nava. As of October 11, 2023, the RedAlert app was reportedly functioning normally. \nIn the last two days, a new malicious website (hxxps://redalerts[.]me) has advertised the download of well-known open source application RedAlert by Elad Nava (https://github.com/eladnava/redalert-android). Domain impersonation continues to be a popular vector for attackers, as the legitimate website for the application (hxxps://redalert[.]me ) differs from the malicious website by only one letter. Further, threat actors continue to exploit open source code and deploy modified, malicious versions to unsuspecting users.\nThe malicious website hosted links to both the iOS and the Android version of the RedAlert app. But while the link to the Apple App Store referred to the legitimate version of the RedAlert app by Elad Nava, the link supposedly referring to the Android version hosted on the Play Store directly downloads a malicious APK file. This attack demonstrates the danger of sideloading applications directly from the Internet as opposed to installing applications from the approved app store.\nThe malicious RedAlert version imitates the legitimate rocket alert application but simultaneously collects sensitive user data. Additional permissions requested by the malicious app include access to contacts, call logs, SMS, account information, as well as an overview of all installed apps.\nThe website hosting the malicious file was created on October 12, 2023 and has since been taken offline. Only users who installed the Android version of the app from this specific website are impacted and urgently advised to delete the app. Users can determine if they installed the malicious version by reviewing the permissions granted to the RedAlert app. If users are unsure whether they installed the malicious version, they can delete the RedAlert applications and reinstall the legitimate version directly in the Play Store. \nScreenshot of the attacker site https://redalerts[.]me\nMalicious Android Package Kit (APK) Analysis\nThe malicious Android Package Kit (APK) file is installed by a user when they click the Google Play button on the fake RedAlert site. Once clicked, the user downloads the app directly from the fake site at hxxps://redalerts[.]me/app.apk. The SHA-256 hash of the APK is 5087a896360f5d99fbf4eb859c824d19eb6fa358387bf6c2c5e836f7927921c5.\nCapabilities\nA quick analysis of the AndroidManifest.xml file shows several differences compared to the legitimate, open source RedAlert application. Most notable are the additional permissions needed to collect information on the victim. The permissions added are listed below:\nandroid.permission.GET_ACCOUNTS\nandroid.permission.QUERY_ALL_PACKAGES\nandroid.permission.READ_CALL_LOG\nandroid.permission.READ_CONTACTS\nandroid.permission.READ_PHONE_NUMBERS\nandroid.permission.READ_PHONE_STATE\nandroid.permission.READ_PRIVILEGED_PHONE_STATE\nandroid.permission.READ_SMS\nThe application is designed to look and act like RedAlert. However, upon opening the app, a malicious service is started in the background. The startService() call is the only change to the onCreate() method, and this begins the sequence of malicious activity, which the actor has placed in a package called com.company.allinclusive.AI\nThe attacker starts their malicious code within the legitimate RedAlert code com.red.alert.activities: Main.java\nThe service is run to gather data from victims’ phones and upload it to the actor’s secure server. The data is extensive and includes:\nSIM information, including IMEI and IMSI numbers, network type, country, voicemail number, PIN status, and more\nFull Contact list\nAll SMS messages, including content and metadata for all statuses (e.g. received, outgoing, sent, etc.)\nA list of accounts associated with the device\nAll phone calls and conversation details for including incoming, outgoing, missed, rejected, and blocked calls\nLogged-in email and app accounts\nList of installed applications\nThe actor’s code for gathering this information is illustrated below.\ncom.company.allinclusive.AI: AIMain.java contains the data the attacker will capture form the target\nStolen data is uploaded to an HTTP server at a hardcoded IP address. The actor has a Tools class which details the IP address where the data is to be uploaded:\ncom.company.allinclusive.AI: Tools.java stores the attackers command and control for the malware\nAlthough HTTP and port 80 are specified, the actor appears to have the ability to use HTTPS and port 443 if a certificate is found bundled within the application package:\ncom.company.allinclusive.AI: UploadFileAsync.java\nData is uploaded through a Connector class, written by the actor. The Connector is responsible for encrypting the stolen data and uploading it to the HTTP server. In this sample, files are encrypted with AES in CBC mode with PKCS5 Padding. The keys are randomly generated and appended to the packaged data, however the keys are encrypted with RSA using a public key bundled in the malicious app. Because of this, anybody who is able to intercept the stolen data will be unable to decrypt it without the actor’s private key.\nThe encrypted files have names that look like <ID>_<DATE>.final, which contain:\n<ID>_<DATE>.enc (encrypted data)\n<ID>_<DATE>.param (AES encryption parameters, e.g. key and IV)\n<ID>_<DATE>.eparam (RSA parameters, e.g. public key)\nAnti-Analysis Runtime Capabilities\nTo avoid detection the actor included anti-analysis capabilities which can run at the time the app is started. The methods for anti-analysis that the attacker has included were anti-debugging, anti-emulation, and anti-test operations\nAnti-Debugging\nThe application makes a simple call using the builtin android.os.Debug package to see if the application is being debugged.\ncom.company.allinclusive.AI.anti.debugger: FindDebugger.java\nAnti-Emulation\nThe application attempts to locate certain files and identifiers to determine whether it is being run in an emulated environment. A snippet of these indicators are shown below:\ncom.company.allinclusive.AI.anti.emulator: FindEmulator.java checks for common emulators\nAnti-Test\nThe application has utilities to identify whether a test user (“monkey”) is using the application:\ncom.company.allinclusive.AI.anti.monkey: FindMonkey.java\nThese methodologies are all rudimentary checks for whether the application is under runtime analysis. It does not, however, protect the malicious code against static analysis.\nHow To Detect This Malware On Your Device\nIf you have installed RedAlert on your device, the extraneous permissions added by the actor can be used to determine whether you have been compromised. The following permissions appearing on the RedAlert app (whether or not enabled) would indicate compromise:\nCall Logs\nContacts\nPhone\nSMS\nHow To Protect Yourself\nYou can avoid attacks like this by following the guidance below:\nKeep your mobile device up to date on the latest software version at all times\nConsider using Cloudflare Teams (with Cloudflare Gateway)\nAvoid using third party mobile application stores\nNever install applications from Internet URLs or sideload payloads\nConsider using 1.1.1.1 for families to block malicious domains on your network\nIOCs\nType\n\t\nIndicator\n\t\nMalicious RedAlert APK Download URL\n\t\nhxxp://redalerts[.]me/app.apk\n\t\nMalicious RedAlert APK Command and Control\n\t\nhxxp://23.254.228[.]135:80/file.php\n\t\nMalicious RedAlert APK\n\t\n5087a896360f5d99fbf4eb859c824d19eb6fa358387bf6c2c5e836f7927921c5\n\t\nPublic key, RSA/ECB/PKCS1Padding\n\t\nMIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAvBYe8dLw1TVH39EVQEwCr7kgBRtQz2M2vQbgkbr0UiTFm0Tk9KVZ1jn0uVgJ+dh1I7uuIfzFEopFQ35OxRnjmNAJsOYpYA5ZvD2llS+KUyE4TRJZGh+dfGjc98dCGCVW9aPVuyfciFNpzGU+lUV/nIbi8xmHOSzho+GZvrRWNDvJqmX7Xunjr1crAKIpG1kF8bpa9+VkoKnMOqFBTc6aPEmwj4CmeTsTy+j7ubdKc8tsdoCTGfrLzVj4wlGDjtf06dYEtZ6zvdBbzb4UA6Ilxsb12KY03qdlqlFREqCxjtJUYDEYChnpOSkrzpLOu+TTkAlW68+u6JjgE8AAAnjpIGRRNvuj5ZfTS3Ub3xEABBRUuHcesseuaN3wVwvMBIMbWJabVUWUNWYyCewxrtdrc8HStECbS/b05j2lv6Cl1Qv1iQefurL/hvfREmxlHAnkCmzTxlrEStHHnNmhWOccQI+u0VO6klJShNg8XlRsKXnqpPi3aicki+QMo3i1oWOve6aWkAIJvmHaY4Gmz0nX2foxlJ2YxOGQe0rUAqDXa8S6tYSmIyCYJoTmllvwJAEpCtOFxerZIAa/1BaxYFhH/iQUzzayJuc6ooUmKLw7q72pe3tN0cRT3RAJUmRwTcV5hL+UQgakkSzIMFBpM/rpvNC0Qy94mtpNf6iA6gbKm40CAwEAAQ==\n\t\nUnder attack? Contact our hotline to speak with someone immediately.Visit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.To learn more about our mission to help build a better Internet, start here. If you’re looking for a new career direction, check out our open positions.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nCloudforce One Vulnerabilities Internet Traffic Malware Threat Intelligence",
      "markdown": "10/14/2023\n\n*   [![Blake Darché](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/06/30DC176C-CDE6-4271-AD8C-CC27B2A9E730.jpeg)](https://blog.cloudflare.com/author/blake/)\n*   [![Armen Boursalian](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/Screen-Shot-2022-08-18-at-11.51.06-AM-1.png)](https://blog.cloudflare.com/author/armen/)\n*   [![Javier Castro](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/javier-castro-photo.png)](https://blog.cloudflare.com/author/javier/)\n\n6 min read\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--15-.png)\n\nOn October 13, 2023, Cloudflare’s Cloudforce One Threat Operations Team became aware of a website hosting a Google Android Application (APK) impersonating the legitimate RedAlert - Rocket Alerts application (https://play.google.com/store/apps/details?id=com.red.alert&hl=en&pli=1).  More than 5,000 rockets have been launched into Israel since the attacks from Hamas began on October 7th 2023.  RedAlert - Rocket Alerts developed by Elad Nava allows individuals to receive timely and precise alerts about incoming airstrikes. Many people living in Israel rely on these alerts to seek safety - a service which has become increasingly important given the newest escalations in the region.\n\nApplications alerting of incoming airstrikes have become targets as only days ago, Pro-Palestinian hacktivist group AnonGhost exploited a vulnerability in another application, “Red Alert: Israel” by Kobi Snir. ([https://cybernews.com/cyber-war/israel-redalert-breached-anonghost-hamas/](https://cybernews.com/cyber-war/israel-redalert-breached-anonghost-hamas/)) Their exploit allowed them to intercept requests, expose servers and APIs, and send fake alerts to some app users, including a message that a “nuclear bomb is coming”. AnonGhost also claimed they attacked other rocket alert applications, including RedAlert by Elad Nava. As of October 11, 2023, the RedAlert app was reportedly functioning normally.\n\nIn the last two days, a new malicious website (_hxxps://redalerts\\[.\\]me_) has advertised the download of well-known open source application RedAlert by Elad Nava ([https://github.com/eladnava/redalert-android](https://github.com/eladnava/redalert-android)). Domain impersonation continues to be a popular vector for attackers, as the legitimate website for the application (_hxxps://redalert\\[.\\]me_ ) differs from the malicious website by only one letter. Further, threat actors continue to exploit open source code and deploy modified, malicious versions to unsuspecting users.\n\nThe malicious website hosted links to both the iOS and the Android version of the RedAlert app. But while the link to the Apple App Store referred to the legitimate version of the RedAlert app by Elad Nava, the link supposedly referring to the Android version hosted on the Play Store directly downloads a malicious APK file. This attack demonstrates the danger of sideloading applications directly from the Internet as opposed to installing applications from the approved app store.\n\nThe malicious RedAlert version imitates the legitimate rocket alert application but simultaneously collects sensitive user data. Additional permissions requested by the malicious app include access to contacts, call logs, SMS, account information, as well as an overview of all installed apps.\n\nThe website hosting the malicious file was created on October 12, 2023 and has since been taken offline. Only users who installed the Android version of the app from this specific website are impacted and urgently advised to delete the app. Users can determine if they installed the malicious version by reviewing the permissions granted to the RedAlert app. If users are unsure whether they installed the malicious version, they can delete the RedAlert applications and reinstall the legitimate version directly in the Play Store.  \n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--13-.png)\n\n_Screenshot of the attacker site https://redalerts\\[.\\]me_\n\n### Malicious Android Package Kit (APK) Analysis\n\nThe malicious Android Package Kit (APK) file is installed by a user when they click the Google Play button on the fake RedAlert site. Once clicked, the user downloads the app directly from the fake site at _`hxxps://redalerts[.]me/app.apk`_. The SHA-256 hash of the APK is _`5087a896360f5d99fbf4eb859c824d19eb6fa358387bf6c2c5e836f7927921c5`_.\n\n## Capabilities\n\nA quick analysis of the _AndroidManifest.xml_ file shows several differences compared to the legitimate, open source RedAlert application. Most notable are the additional permissions needed to collect information on the victim. The permissions added are listed below:\n\n*   android.permission.GET\\_ACCOUNTS\n*   android.permission.QUERY\\_ALL\\_PACKAGES\n*   android.permission.READ\\_CALL\\_LOG\n*   android.permission.READ\\_CONTACTS\n*   android.permission.READ\\_PHONE\\_NUMBERS\n*   android.permission.READ\\_PHONE\\_STATE\n*   android.permission.READ\\_PRIVILEGED\\_PHONE\\_STATE\n*   android.permission.READ\\_SMS\n\nThe application is designed to look and act like RedAlert. However, upon opening the app, a malicious service is started in the background. The _`startService()`_ call is the only change to the _`onCreate()`_ method, and this begins the sequence of malicious activity, which the actor has placed in a package called _`com.company.allinclusive.AI`_\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--14-.png)\n\n_The attacker starts their malicious code within the legitimate RedAlert code com.red.alert.activities: Main.java_\n\nThe service is run to gather data from victims’ phones and upload it to the actor’s secure server. The data is extensive and includes:\n\n*   SIM information, including IMEI and IMSI numbers, network type, country, voicemail number, PIN status, and more\n*   Full Contact list\n*   All SMS messages, including content and metadata for all statuses (e.g. received, outgoing, sent, etc.)\n*   A list of accounts associated with the device\n*   All phone calls and conversation details for including incoming, outgoing, missed, rejected, and blocked calls\n*   Logged-in email and app accounts\n*   List of installed applications\n\nThe actor’s code for gathering this information is illustrated below.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Screenshot-2023-10-13-at-3.32.27-PM.png)\n\n_com.company.allinclusive.AI: AIMain.java contains the data the attacker will capture form the target_\n\nStolen data is uploaded to an HTTP server at a hardcoded IP address. The actor has a _Tools_ class which details the IP address where the data is to be uploaded:\n\n![](https://blog.cloudflare.com/content/images/2023/10/Screenshot-2023-10-13-at-3.31.42-PM.png)\n\n__com.company.allinclusive.AI: Tools.java stores the attackers command and control for the malware__\n\nAlthough HTTP and port 80 are specified, the actor appears to have the ability to use HTTPS and port 443 if a certificate is found bundled within the application package:\n\n![](https://blog.cloudflare.com/content/images/2023/10/Screenshot-2023-10-13-at-3.30.20-PM.png)\n\n_com.company.allinclusive.AI: UploadFileAsync.java_\n\nData is uploaded through a _Connector_ class, written by the actor. The _Connector_ is responsible for encrypting the stolen data and uploading it to the HTTP server. In this sample, files are encrypted with AES in CBC mode with PKCS5 Padding. The keys are randomly generated and appended to the packaged data, however the keys are encrypted with RSA using a public key bundled in the malicious app. Because of this, anybody who is able to intercept the stolen data will be unable to decrypt it without the actor’s private key.\n\nThe encrypted files have names that look like _<ID>\\_<DATE>.final_, which contain:\n\n*   ___<ID>\\_<DATE>.enc_ (encrypted data)__\n*   ___<ID>\\_<DATE>.param_ (AES encryption parameters, e.g. key and IV)__\n*   ___<ID>\\_<DATE>.eparam_ (RSA parameters, e.g. public key)__\n\n## Anti-Analysis Runtime Capabilities\n\nTo avoid detection the actor included anti-analysis capabilities which can run at the time the app is started. The methods for anti-analysis that the attacker has included were anti-debugging, anti-emulation, and anti-test operations\n\n### Anti-Debugging\n\nThe application makes a simple call using the builtin _android.os.Debug_ package to see if the application is being debugged.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Screenshot-2023-10-13-at-3.29.28-PM.png)\n\n_com.company.allinclusive.AI.anti.debugger: FindDebugger.java_\n\n### Anti-Emulation\n\nThe application attempts to locate certain files and identifiers to determine whether it is being run in an emulated environment. A snippet of these indicators are shown below:\n\n![](https://blog.cloudflare.com/content/images/2023/10/pasted-image-0--12--1.png)\n\n_com.company.allinclusive.AI.anti.emulator: FindEmulator.java checks for common emulators_\n\n### Anti-Test\n\nThe application has utilities to identify whether a test user (“monkey”) is using the application:\n\n![](https://blog.cloudflare.com/content/images/2023/10/Screenshot-2023-10-13-at-3.28.48-PM.png)\n\n_com.company.allinclusive.AI.anti.monkey: FindMonkey.java_\n\nThese methodologies are all rudimentary checks for whether the application is under runtime analysis. It does not, however, protect the malicious code against static analysis.\n\n## How To Detect This Malware On Your Device\n\nIf you have installed RedAlert on your device, the extraneous permissions added by the actor can be used to determine whether you have been compromised. The following permissions appearing on the RedAlert app (whether or not enabled) would indicate compromise:\n\n*   Call Logs\n*   Contacts\n*   Phone\n*   SMS\n\n## How To Protect Yourself\n\nYou can avoid attacks like this by following the guidance below:\n\n*   Keep your mobile device up to date on the latest software version at all times\n*   Consider using Cloudflare Teams (with [Cloudflare Gateway](https://www.cloudflare.com/zero-trust/products/gateway/))\n*   Avoid using third party mobile application stores\n*   Never install applications from Internet URLs or sideload payloads\n*   Consider using [1.1.1.1 for families](https://1.1.1.1/family/) to block malicious domains on your network\n\n## IOCs\n\n|     |     |\n| --- | --- |\n| Type | Indicator |\n| Malicious RedAlert APK Download URL | hxxp://redalerts\\[.\\]me/app.apk |\n| Malicious RedAlert APK Command and Control | hxxp://23.254.228\\[.\\]135:80/file.php |\n| Malicious RedAlert APK | 5087a896360f5d99fbf4eb859c824d19eb6fa358387bf6c2c5e836f7927921c5 |\n| Public key, RSA/ECB/PKCS1Padding | MIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAvBYe8dLw1TVH39EVQEwCr7kgBRtQz2M2vQbgkbr0UiTFm0Tk9KVZ1jn0uVgJ+dh1I7uuIfzFEopFQ35OxRnjmNAJsOYpYA5ZvD2llS+KUyE4TRJZGh+dfGjc98dCGCVW9aPVuyfciFNpzGU+lUV/nIbi8xmHOSzho+GZvrRWNDvJqmX7Xunjr1crAKIpG1kF8bpa9+VkoKnMOqFBTc6aPEmwj4CmeTsTy+j7ubdKc8tsdoCTGfrLzVj4wlGDjtf06dYEtZ6zvdBbzb4UA6Ilxsb12KY03qdlqlFREqCxjtJUYDEYChnpOSkrzpLOu+TTkAlW68+u6JjgE8AAAnjpIGRRNvuj5ZfTS3Ub3xEABBRUuHcesseuaN3wVwvMBIMbWJabVUWUNWYyCewxrtdrc8HStECbS/b05j2lv6Cl1Qv1iQefurL/hvfREmxlHAnkCmzTxlrEStHHnNmhWOccQI+u0VO6klJShNg8XlRsKXnqpPi3aicki+QMo3i1oWOve6aWkAIJvmHaY4Gmz0nX2foxlJ2YxOGQe0rUAqDXa8S6tYSmIyCYJoTmllvwJAEpCtOFxerZIAa/1BaxYFhH/iQUzzayJuc6ooUmKLw7q72pe3tN0cRT3RAJUmRwTcV5hL+UQgakkSzIMFBpM/rpvNC0Qy94mtpNf6iA6gbKm40CAwEAAQ== |\n\n* * *\n\nUnder attack? Contact our [hotline](https://www.cloudflare.com/under-attack-hotline/) to speak with someone immediately._Visit_ [_1.1.1.1_](https://1.1.1.1/) _from any device to get started with our free app that makes your Internet faster and safer.To learn more about our mission to help build a better Internet, start_ [_here_](https://www.cloudflare.com/learning/what-is-cloudflare/)_. If you’re looking for a new career direction, check out_ [_our open positions_](https://cloudflare.com/careers)_._\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Cloudforce One](https://blog.cloudflare.com/tag/cloudforce-one/) [Vulnerabilities](https://blog.cloudflare.com/tag/vulnerabilities/) [Internet Traffic](https://blog.cloudflare.com/tag/internet-traffic/) [Malware](https://blog.cloudflare.com/tag/malware/) [Threat Intelligence](https://blog.cloudflare.com/tag/threat-intelligence/)"
    },
    {
      "url": "https://blog.cloudflare.com/introducing-the-project-argus-datacenter-ready-secure-control-module-design-specification/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/introducing-the-project-argus-datacenter-ready-secure-control-module-design-specification/",
        "loadedTime": "2023-12-05T02:31:03.127Z",
        "referrerUrl": "https://blog.cloudflare.com/page/2/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/introducing-the-project-argus-datacenter-ready-secure-control-module-design-specification/",
        "title": "Introducing the Project Argus Datacenter-ready Secure Control Module design specification",
        "description": "The DC-SCM (Datacenter-ready Secure Control Module) decouples server management from the server motherboard. It provides flexibility to implement multiple server management and security solutions with the same server motherboard design",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/16/2023\n6 min read\nHistorically, data center servers have used motherboards that included all key components on a single circuit board. The DC-SCM (Datacenter-ready Secure Control Module) decouples server management and security functions from a traditional server motherboard, enabling development of server management and security solutions independent of server architecture. It also provides opportunities for reducing server printed circuit board (PCB) material cost, and allows unified firmware images to be developed. \nToday, Cloudflare is announcing that it has partnered with Lenovo to design a DC-SCM for our next-generation servers. The design specification has been published to the OCP (Open Compute Project) contribution database under the name Project Argus.\nA brief introduction to baseboard management controllers\nA baseboard management controller (BMC) is a specialized processor that can be found in virtually every server product. It allows remote access to the server through a network connection, and provides a rich set of server management features. Some of the commonly used BMC features include server power management, device discovery, sensor monitoring, remote firmware update, system event logging, and error reporting.\nIn a typical server design, the BMC resides on the server motherboard, along with other key components such as the processor, memory, CPLD and so on. This was the norm for generations of server products, but that has changed in recent years as motherboards are increasingly optimized for high-speed signal bandwidth, and servers need to support specialized security requirements. This has made it necessary to decouple the BMC and its related components from the server motherboard, and move them to a smaller common form factor module known as the Datacenter Secure Control Module (DC-SCM).\nFigure 1 is a picture of a motherboard used on Cloudflare’s previous generation of edge servers. The BMC and its related circuit components are placed on the same printed circuit board as the host CPU.\nFigure 1: Previous Generation Server Motherboard\nFor Cloudflare’s next generation of edge servers, we are partnering with Lenovo to create a DC-SCM based design. On the left-hand side of Figure 2 is the printed circuit board assembly (PCBA) for the Host Processor Module (HPM). It hosts the CPU, the memory slots, and other components required for the operation and features of the server design. But the BMC and its related circuits have been relocated to a separate PCBA, which is the DC-SCM.\nFigure 2: Next Generation HPM and DC-SCM\nBenefits of DC-SCM based server design\nPCB cost reduction\nAs of today, DDR5 memory runs at 6400MT/s (mega transfers per second). In the future DDR5 speed may even increase to 7200MT/s or 8800MT/s. Meanwhile, PCIe Gen5 is running at 32 GT/s (giga transfers per second), doubling the speed rate of PCIe Gen4. Both DDR5 and PCIE Gen5 are key interfaces for the processors used on our next-generation servers.\nThe increasing rates of high-speed IO signals and memory buses are pushing the next generation of server motherboard designs to transition from low-loss to ultra-low loss dielectric printed circuit board (PCB) materials, and higher layer counts in the PCB. At the same time, the speed of BMC and its related circuitry are not progressing so quickly. For example, the physical layer interface of ASPEED AST2600 BMC is only at PCIe Gen2 (5 GT/s).\nUltra-low loss dielectric PCB material and higher PCB layer count are both driving factors for higher PCB cost. Another driving factor of PCB cost is the size of the PCB. In a traditional server motherboard design, the size of the server motherboard is larger, since the BMC and its related circuits are placed on the same PCB as the host CPU.\nBy decoupling the BMC and its related circuitry from the host processor module (HPM), we can reduce the size of the relatively more expensive PCB for the HPM. BMC and its related circuitry can be placed on relatively cheaper PCB, with reduced layer count and lossier PCB dielectric materials. For example, in the design of Cloudflare’s next generation of servers, the server motherboard PCB needs to be 14 or more layers, whereas the BMC and its related components can be easily routed with 8 or 10 layers of PCB. In addition, the dielectric material used on DC-SCM PCB is low-loss dielectric — another cost saver compared to ultra-low loss dielectric materials used on HPM PCB.\nModularized design enables flexibility\nDC-SCM modularizes server management and security components into a common add-in card form factor, enabling developers to remove customer specific solutions from the more complex components, such as motherboards, to the DC-SCM. This provides flexibility for developers to offer multiple customer-specific solutions, without the need to redesign multiple motherboards for each solution.\nDevelopers are able to reuse the DC-SCM from a previous generation of server design, if the management and security requirements remain the same. This reduces the overall cost of upgrading to a new generation of servers, and has the potential to reduce e-waste when a server is decommissioned.\nLikewise, management and security solution upgrades within a server generation can be carried out separately by modifying or replacing the DC-SCM. The more complex components on the HPM do not need to be redesigned. From a data center perspective, it speeds up the upgrade of management and security hardware across multiple server platforms.\nUnified interoperable OpenBMC firmware development\nData center secure control interface (DC-SCI) is a standardized hardware interface between DC-SCM and the Host Processor Module (HPM). It provides a basis for electrical interoperability between different DC-SCM and host processor module (HPM) designs. \nThis interoperability makes it possible to have a unified firmware image across multiple DC-SCM designs, concentrating development resources on a single firmware rather than an array of them. The publicly-accessible OpenBMC repository provides a perfect platform for firmware developers of different companies to collaborate and develop such unified OpenBMC images. Instead of maintaining a separate BMC firmware image for each platform, we now use a single image that can be applied across multiple server platforms. The device tree specific to each respective server is automatically loaded based on device product information. \nUsing a unified OpenBMC image significantly simplifies the process of releasing BMC firmware to multiple server platforms. Firmware updates and changes are propagated to all supported platforms in a single firmware release.\nProject Argus\nThe DC-SCM specifications have been driven by the Open Compute Project (OCP) Foundation hardware management workstream, as a way to standardize server management, security, and control features.\nCloudflare has partnered with Lenovo on what we call Project Augus, Cloudflare’s first DC-SCM implementation that fully adheres to the DC-SCM 2.0 specification. In the DC-SCM 2.0 specifications, a few design items are left open for implementers to decide on the most suitable architectural choices. With the goal of improving interoperability of Cloudflare DC-SCM designs across server vendors and server designs, Project Argus includes documentation on implementation details and design decisions on form factor, mechanical locking mechanism, faceplate design, DC-SCI pin out, BMC chip, BMC pinout, Hardware Root of Trust (HWRoT), HWRoT pinout, and minimum bootable device tree.\nFigure 3: Project Argus DC-SCM 2.0\nAt the heart of the Project Argus DC-SCM is the ASPEED AST2600 BMC System on Chip (SoC), which when loaded with a compatible OpenBMC firmware, provides a rich set of common features necessary for remote server management. ASPEED AST1060 is used on Project Argus DC-SCM as the HWRoT solution, providing secure firmware authentication, firmware recovery, and firmware update capability. Project Argus DC-SCM 2.0 uses Lattice MachXO3D CPLD with secure boot and dual boot ability as the DC-SCM CPLD to support a variety of IO interfaces including LTPI, SGPIO, UART and GPIOs.\nThe mechanical form factor of Project Argus DC-SCM 2.0 is the horizontal External Form Factor (EFF).\nCloudflare and Lenovo have contributed Project Argus Design Specification and reference design files to the OCP contribution database. Below is a detailed list of our contribution:\nSPI, I2C/I3C, UART, LTPI/SGPIO block diagrams\nDC-SCM PCB stackup\nDC-SCM Board placements (TOP and BOTTOM layers)\nDC-SCM schematic PDF file\nDC-SCI pin definition PDF file\nPower sequence PDF file\nDC-SCM bill of materials Excel spreadsheet\nMinimum bootable device tree requirements\nMechanical Drawings PDF files, including card assembly drawing and interlock rail drawing\nThe security foundation for our Gen 12 hardware\nCloudflare has been innovating around server design for many years, delivering increased performance per watt and reduced carbon footprints. We are excited to integrate Project Argus DC-SCM 2.0 into our next-generation, Cloudflare Gen 12 servers. Stay tuned for more exciting updates on Cloudflare Gen 12 hardware design!\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nHardware Security",
      "markdown": "10/16/2023\n\n*   [![Xiaomin Shen](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/04/mmexport1632277733244_mr1632497208886--1-.jpg)](https://blog.cloudflare.com/author/xiaomin/)\n*   [![JQ Lau](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/_tmp_mini_magick20221017-42-17izwqk.jpg)](https://blog.cloudflare.com/author/jq/)\n\n6 min read\n\n![](https://blog.cloudflare.com/content/images/2023/10/Project-Argus-DC-SCM.png)\n\nHistorically, data center servers have used motherboards that included all key components on a single circuit board. The DC-SCM (Datacenter-ready Secure Control Module) decouples server management and security functions from a traditional server motherboard, enabling development of server management and security solutions independent of server architecture. It also provides opportunities for reducing server printed circuit board (PCB) material cost, and allows unified firmware images to be developed.\n\nToday, Cloudflare is announcing that it has partnered with Lenovo to design a DC-SCM for our next-generation servers. The design specification has been published to the OCP (Open Compute Project) contribution database under the name [Project Argus](https://www.opencompute.org/documents/cloudfare-lenovo-project-argus-dc-scm2-0-module-design-spec-rev1-0-pdf).\n\n## A brief introduction to baseboard management controllers\n\nA baseboard management controller (BMC) is a specialized processor that can be found in virtually every server product. It allows remote access to the server through a network connection, and provides a rich set of server management features. Some of the commonly used BMC features include server power management, device discovery, sensor monitoring, remote firmware update, system event logging, and error reporting.\n\nIn a typical server design, the BMC resides on the server motherboard, along with other key components such as the processor, memory, CPLD and so on. This was the norm for generations of server products, but that has changed in recent years as motherboards are increasingly optimized for high-speed signal bandwidth, and servers need to support specialized security requirements. This has made it necessary to decouple the BMC and its related components from the server motherboard, and move them to a smaller common form factor module known as the Datacenter Secure Control Module (DC-SCM).\n\nFigure 1 is a picture of a motherboard used on Cloudflare’s previous generation of edge servers. The BMC and its related circuit components are placed on the same printed circuit board as the host CPU.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Figure-1.png)\n\nFigure 1: Previous Generation Server Motherboard\n\nFor Cloudflare’s next generation of edge servers, we are partnering with Lenovo to create a DC-SCM based design. On the left-hand side of Figure 2 is the printed circuit board assembly (PCBA) for the Host Processor Module (HPM). It hosts the CPU, the memory slots, and other components required for the operation and features of the server design. But the BMC and its related circuits have been relocated to a separate PCBA, which is the DC-SCM.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Figure-2.png)\n\nFigure 2: Next Generation HPM and DC-SCM\n\n## Benefits of DC-SCM based server design\n\n### PCB cost reduction\n\nAs of today, DDR5 memory runs at 6400MT/s (mega transfers per second). In the future DDR5 speed may even increase to 7200MT/s or 8800MT/s. Meanwhile, PCIe Gen5 is running at 32 GT/s (giga transfers per second), doubling the speed rate of PCIe Gen4. Both DDR5 and PCIE Gen5 are key interfaces for the processors used on our next-generation servers.\n\nThe increasing rates of high-speed IO signals and memory buses are pushing the next generation of server motherboard designs to transition from low-loss to ultra-low loss dielectric printed circuit board (PCB) materials, and higher layer counts in the PCB. At the same time, the speed of BMC and its related circuitry are not progressing so quickly. For example, the physical layer interface of ASPEED AST2600 BMC is only at PCIe Gen2 (5 GT/s).\n\nUltra-low loss dielectric PCB material and higher PCB layer count are both driving factors for higher PCB cost. Another driving factor of PCB cost is the size of the PCB. In a traditional server motherboard design, the size of the server motherboard is larger, since the BMC and its related circuits are placed on the same PCB as the host CPU.\n\nBy decoupling the BMC and its related circuitry from the host processor module (HPM), we can reduce the size of the relatively more expensive PCB for the HPM. BMC and its related circuitry can be placed on relatively cheaper PCB, with reduced layer count and lossier PCB dielectric materials. For example, in the design of Cloudflare’s next generation of servers, the server motherboard PCB needs to be 14 or more layers, whereas the BMC and its related components can be easily routed with 8 or 10 layers of PCB. In addition, the dielectric material used on DC-SCM PCB is low-loss dielectric — another cost saver compared to ultra-low loss dielectric materials used on HPM PCB.\n\n### Modularized design enables flexibility\n\nDC-SCM modularizes server management and security components into a common add-in card form factor, enabling developers to remove customer specific solutions from the more complex components, such as motherboards, to the DC-SCM. This provides flexibility for developers to offer multiple customer-specific solutions, without the need to redesign multiple motherboards for each solution.\n\nDevelopers are able to reuse the DC-SCM from a previous generation of server design, if the management and security requirements remain the same. This reduces the overall cost of upgrading to a new generation of servers, and has the potential to reduce e-waste when a server is decommissioned.\n\nLikewise, management and security solution upgrades within a server generation can be carried out separately by modifying or replacing the DC-SCM. The more complex components on the HPM do not need to be redesigned. From a data center perspective, it speeds up the upgrade of management and security hardware across multiple server platforms.\n\n### Unified interoperable OpenBMC firmware development\n\nData center secure control interface (DC-SCI) is a standardized hardware interface between DC-SCM and the Host Processor Module (HPM). It provides a basis for electrical interoperability between different DC-SCM and host processor module (HPM) designs.\n\nThis interoperability makes it possible to have a unified firmware image across multiple DC-SCM designs, concentrating development resources on a single firmware rather than an array of them. The publicly-accessible [OpenBMC repository](https://github.com/openbmc/openbmc) provides a perfect platform for firmware developers of different companies to collaborate and develop such unified OpenBMC images. Instead of maintaining a separate BMC firmware image for each platform, we now use a single image that can be applied across multiple server platforms. The device tree specific to each respective server is automatically loaded based on device product information.\n\nUsing a unified OpenBMC image significantly simplifies the process of releasing BMC firmware to multiple server platforms. Firmware updates and changes are propagated to all supported platforms in a single firmware release.\n\n## Project Argus\n\nThe DC-SCM specifications have been driven by the Open Compute Project (OCP) Foundation hardware management workstream, as a way to standardize server management, security, and control features.\n\nCloudflare has partnered with Lenovo on what we call Project Augus, Cloudflare’s first DC-SCM implementation that fully adheres to the [DC-SCM 2.0 specification](https://www.opencompute.org/wiki/Hardware_Management/Hardware_Management_Module). In the DC-SCM 2.0 specifications, a few design items are left open for implementers to decide on the most suitable architectural choices. With the goal of improving interoperability of Cloudflare DC-SCM designs across server vendors and server designs, Project Argus includes documentation on implementation details and design decisions on form factor, mechanical locking mechanism, faceplate design, DC-SCI pin out, BMC chip, BMC pinout, Hardware Root of Trust (HWRoT), HWRoT pinout, and minimum bootable device tree.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Figure-3.png)\n\nFigure 3: Project Argus DC-SCM 2.0\n\nAt the heart of the Project Argus DC-SCM is the ASPEED AST2600 BMC System on Chip (SoC), which when loaded with a compatible OpenBMC firmware, provides a rich set of common features necessary for remote server management. ASPEED AST1060 is used on Project Argus DC-SCM as the HWRoT solution, providing secure firmware authentication, firmware recovery, and firmware update capability. Project Argus DC-SCM 2.0 uses Lattice MachXO3D CPLD with secure boot and dual boot ability as the DC-SCM CPLD to support a variety of IO interfaces including LTPI, SGPIO, UART and GPIOs.\n\nThe mechanical form factor of Project Argus DC-SCM 2.0 is the horizontal External Form Factor (EFF).\n\nCloudflare and Lenovo have contributed [Project Argus Design Specification](https://www.opencompute.org/documents/cloudfare-lenovo-project-argus-dc-scm2-0-module-design-spec-rev1-0-pdf) and reference design files to the [OCP contribution database](https://www.opencompute.org/documents/9e2b0d227655c9ea04414626aa958a72a913f315-zip). Below is a detailed list of our contribution:\n\n*   SPI, I2C/I3C, UART, LTPI/SGPIO block diagrams\n*   DC-SCM PCB stackup\n*   DC-SCM Board placements (TOP and BOTTOM layers)\n*   DC-SCM schematic PDF file\n*   DC-SCI pin definition PDF file\n*   Power sequence PDF file\n*   DC-SCM bill of materials Excel spreadsheet\n*   Minimum bootable device tree requirements\n*   Mechanical Drawings PDF files, including card assembly drawing and interlock rail drawing\n\n### The security foundation for our Gen 12 hardware\n\nCloudflare has been innovating around [server design](https://blog.cloudflare.com/tag/hardware/) for many years, delivering increased performance per watt and reduced carbon footprints. We are excited to integrate Project Argus DC-SCM 2.0 into our next-generation, Cloudflare Gen 12 servers. Stay tuned for more exciting updates on Cloudflare Gen 12 hardware design!\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Hardware](https://blog.cloudflare.com/tag/hardware/) [Security](https://blog.cloudflare.com/tag/security/)"
    },
    {
      "url": "https://blog.cloudflare.com/how-prisma-saved-98-percent-on-distribution-costs-with-cloudflare-r2/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/how-prisma-saved-98-percent-on-distribution-costs-with-cloudflare-r2/",
        "loadedTime": "2023-12-05T02:31:10.256Z",
        "referrerUrl": "https://blog.cloudflare.com/page/2/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/how-prisma-saved-98-percent-on-distribution-costs-with-cloudflare-r2/",
        "title": "How Prisma saved 98% on distribution costs with Cloudflare R2",
        "description": "Cloudflare products provide much of the underlying infrastructure for Prisma Accelerate and Prisma Pulse, empowering user-focused product development. This ongoing collaboration extends to enhancing the Prisma ORM.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/12/2023\n7 min read\nThe following is a guest post written by Pierre-Antoine Mills, Miguel Fernández, and Petra Donka of Prisma. Prisma provides a server-side library that helps developers read and write data to the database in an intuitive, efficient and safe way.\nPrisma’s mission is to redefine how developers build data-driven applications. At its core, Prisma provides an open-source, next-generation TypeScript Object-Relational Mapping (ORM) library that unlocks a new level of developer experience thanks to its intuitive data model, migrations, type-safety, and auto-completion.\nPrisma ORM has experienced remarkable growth, engaging a vibrant community of developers. And while it was a great problem to have, this growth was causing an explosion in our AWS infrastructure costs. After investigating a wide range of alternatives, we went with Cloudflare’s R2 storage — and as a result are thrilled that our engine distribution costs have decreased by 98%, while delivering top-notch performance.\nIt was a natural fit: Prisma is already a proud technology partner of Cloudflare’s, offering deep database integration with Cloudflare Workers. And Cloudflare products provide much of the underlying infrastructure for Prisma Accelerate and Prisma Pulse, empowering user-focused product development. In this post, we’ll dig into how we decided to extend our ongoing collaboration with Cloudflare to the Prisma ORM, and how we migrated from AWS S3 + CloudFront to Cloudflare R2, with zero downtime.\nDistributing the Prisma ORM and its engines\nPrisma ORM simplifies data access thanks to its type-safe Prisma Client, and enables efficient database management via the Prisma CLI, so that developers can focus on product development.\nBoth the Prisma Client and the Prisma CLI rely on the Prisma Engines, which are implemented in Rust and distributed as platform-specific compiled binaries. The Prisma Engines perform a variety of tasks ranging from providing information about the schema for type generation, or migrating the database, to transforming Prisma queries into SQL, and executing those queries against the database. Think of the engines as the layer in the Prisma ORM that talks to the database.\nAs a developer, one of the first steps to get started with Prisma is to install Prisma Client and the Prisma CLI from npm. Once installed, these packages need the Prisma Engines to be able to function. These engines have complex target-platform rules and were originally envisioned to be distributed separately from the npm package, so they can be used outside of the Node.js ecosystem. As a result, they are downloaded on demand by the Prisma CLI, only downloading what is strictly required for a given project.\nAs of mid-2023, the engines account for 100 million downloads a month and 250 terabytes of egress data transfer, with a continuous month-over-month increase as our user base grows. This highlights the importance of a highly available, global, and scalable infrastructure that provides low latency engine downloads to Prisma users all around the world.\nOur original solution: AWS S3 & CloudFront\nDuring the early development of the Prisma ORM, our engineering team looked for tools to build the CDN for engine distribution. With extensive AWS experience, we went with the obvious: S3 blob storage for the engine files and CloudFront to cache contents closer to the user.\nA simplified representation of how the Prisma Engines flow from our CI where they are built and uploaded, to the Prisma CLI downloading the correct engine for a given environment when installing Prisma, all the way to the user being able to use it.\nWe were happy with AWS for the most part, and it was able to scale with our demands. However, as our user base continued to grow, so did the costs. At our scale of traffic, data transfer became a considerable cost item that we knew would only continue to grow.\nThe continuously increasing cost of these services prompted us to explore alternative options that could better accommodate our needs while at least maintaining the same level of performance and reliability. Prisma is committed to providing the best products and solutions to our users, and an essential part of that commitment is being intentional about the allocation of our resources, including sensible spending to enable us to serve our growing user base in the best way possible.\nExploring distribution options\nWe extensively explored different technologies and services that provided both reliable and fast engine distribution, while being cost-effective.\nFree solutions: GitHub & npm\nBecause Prisma ORM is an open-source solution, we have explored various ways to distribute the engines through our existing distribution channels, at no cost. In this area, we had both GitHub Releases and npm as candidates to host and distribute our engine files. We dismissed GitHub Releases early on as the quality of service was not guaranteed, which was a requirement for us towards our users, so we can be sure to provide a good developer experience under all circumstances.\nWe also looked at npm, and confirmed that hosting the engine files would be in agreement with their Terms of Service. This made npm a viable option, but also meant we would have to change our engine download and upload logic to accommodate a different system. Additionally, this implied that we would have to update many past Prisma CLI versions, requiring our users to upgrade to take advantage of the new solution.\nPaid solutions: CDNs & Cloudflare\nWe then considered only replacing CloudFront, which accounted for 97% of our distribution costs, while retaining S3 as the origin. When we evaluated different CDNs, we found that alternatives could lead to an estimated 70% cost reduction.\nWe also explored Cloudflare’s offerings and were impressed by Cloudflare R2, an alternative to AWS S3 + CloudFront. It offers robust blob storage compatible with S3 and leverages Cloudflare’s network for global low-latency distribution. Additionally, it has no egress costs, and is solely priced based on the total volume of data stored and operations on that data. Given our reliance on Cloudflare’s product portfolio for our Data Platform, and extensive experience with their Workers platform, we already had high trust in the quality of Cloudflare’s products.\nTo finalize our decision, we implemented a test to confirm our intuitions about Cloudflare’s quality of service. We deployed a script to 50 cities across the globe, representative of our incoming traffic, to measure download latencies for our engine files (~15MB). The test was run multiple times, with latencies for the different cache statuses recorded and compared against our previous AWS-based solution. The results confirmed that Cloudflare R2's reliability and performance were at least on par with AWS S3 + CloudFront. And because R2 is compatible with S3, we wouldn’t need to make substantial changes to our software in order to move over to Cloudflare. These were great results, and we couldn’t wait to switch!\nOur solution: moving to Cloudflare’s R2\nIn order to move our engine file distribution to Cloudflare, we needed to ensure we could make the switch without any disruption or impact to our users.\nWhile R2 URLs match S3's format, Prisma CLI uses a fixed domain to point to the engine file distribution. This fixed domain enabled us to transition without making any changes to the code of older Prisma versions, and simply point the existing URLs to R2. However, to make the transition, we needed to change our DNS configuration to point to Cloudflare. While this seems trivial, potential issues like unexpected DNS propagation challenges, or certificate validation problems when connecting via TLS, required us to plan ahead in order to proceed confidently and safely.\nWe modified the Prisma ORM release pipeline to upload assets to both S3 and R2, and used the R2 Super Slurper for migrating past engine versions to R2. This ensured all Prisma releases, past and future, existed in both places. We also established Grafana monitoring checks to pull engine files from R2, using a DNS and TLS configuration similar to our desired production setup, but via an experimental domain. Those monitoring checks were later reused during the final traffic cutover to ensure that there was no service disruption.\nAs ensuring no impact or disruption to our users was of utmost importance, we proceeded with a gradual rollout of the DNS changes using DNS load balancing, a method where a group of alias records assigned to a domain are weighted differently. This meant that the DNS resolver directed more traffic to heavier-weighted records. We began with a load balancing configuration simulating our old setup, with one record (the control) pointing to AWS CloudFront, and the other (the candidate) pointing to R2. Initially, all weight was on the control, effectively preserving the old routing to CloudFront. We also set the lowest TTL possible, so changes in the record weights took effect as soon as possible, creating more control over DNS propagation. Additionally, we implemented a health check that would redirect all traffic to the control if download latencies were significantly higher, or if errors were detected, ensuring a stable fallback.\nAt this point, everything was in place and we could start the rollout.\nOur DNS load balancing setup during the rollout. We assigned increasing weights to route traffic to Cloudflare R2. The health check that would fail over to AWS CloudFront never fired.\nThe rollout began with a gradual increase in R2's DNS weight, and our monitoring dashboards showed that Cloudflare downloads were proportional to the weight assigned to R2. With as little as 5% traffic routed to Cloudflare, cache hit ratios neared 100%, as expected. Latencies matched the control, so the health checks were all good, and our fallback never activated. Over the duration of an hour, we gradually increased R2's DNS weight to manage 25%, 50%, and finally 100% of traffic, without any issues. The cutover could not have gone any smoother.\nAfter monitoring for an additional two days, we simplified the DNS topology and routed to Cloudflare exclusively. We were extremely satisfied with the change, and started seeing our infrastructure costs drop considerably, as expected, not to mention the zero downtime and zero reported issues from users.\nA success\nTransitioning to Cloudflare R2 was easy thanks to their great product and tooling, intuitive platform and supportive team. We've had an excellent experience with their service, with consistently great uptime, performance and latency. Cloudflare proved once again to be a valuable partner to help us scale.\nWe are thrilled that our engine distribution costs have decreased by 98%. Cloudflare's cost-effective solution has not only delivered top-notch performance but has also brought significant savings to our operations. An all around success!\nTo learn more about how Prisma is building Data DX solutions with Cloudflare, take a look at Developer Experience Redefined: Prisma & Cloudflare Lead the Way to Data DX.\nAnd if you want to see Prisma in action, get started with the Quickstart guide.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nPartners R2 Storage Developers",
      "markdown": "10/12/2023\n\n*   [![Pierre-Antoine Mills (Guest Author)](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/09/Pierre-Antoine-Mills.jpeg)](https://blog.cloudflare.com/author/pierre-antoine-mills/)\n*   [![Miguel Fernández (Guest Author)](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/09/Miguel-Ferna-ndez.jpeg)](https://blog.cloudflare.com/author/miguel-fernandez-guest-author/)\n*   [![Petra Donka (Guest Author)](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/09/Petra-Donka.jpeg)](https://blog.cloudflare.com/author/petra-donka/)\n\n7 min read\n\n![How Prisma saved 98% on distribution costs with Cloudflare R2](https://blog.cloudflare.com/content/images/2023/09/Prisma-Partnership-1.png)\n\n_The following is a guest post written by Pierre-Antoine Mills, Miguel Fernández, and Petra Donka of Prisma. Prisma provides a server-side library that helps developers read and write data to the database in an intuitive, efficient and safe way._\n\nPrisma’s mission is to redefine how developers build data-driven applications. At its core, Prisma provides an open-source, next-generation TypeScript Object-Relational Mapping (ORM) library that unlocks a new level of developer experience thanks to its intuitive data model, migrations, type-safety, and auto-completion.\n\nPrisma ORM has experienced remarkable growth, engaging a vibrant community of developers. And while it was a great problem to have, this growth was causing an explosion in our AWS infrastructure costs. After investigating a wide range of alternatives, we went with Cloudflare’s [R2 storage](https://www.cloudflare.com/developer-platform/r2/) — and as a result are thrilled that our engine distribution costs have decreased by _98%,_ while delivering top-notch performance.\n\nIt was a natural fit: Prisma is already a proud [technology partner of Cloudflare’s](https://www.cloudflare.com/partners/technology-partners/prisma/), offering deep database integration with Cloudflare Workers. And Cloudflare products provide much of the underlying infrastructure for [Prisma Accelerate](https://www.prisma.io/accelerate) and [Prisma Pulse](http://prisma.io/pulse), empowering user-focused product development. In this post, we’ll dig into how we decided to extend our ongoing collaboration with Cloudflare to the Prisma ORM, and how we migrated from AWS S3 + CloudFront to Cloudflare R2, with zero downtime.\n\n## Distributing the Prisma ORM and its engines\n\nPrisma ORM simplifies data access thanks to its type-safe [Prisma Client](https://www.prisma.io/docs/concepts/components/prisma-client), and enables efficient database management via the [Prisma CLI](https://www.prisma.io/docs/concepts/components/prisma-cli), so that developers can focus on product development.\n\nBoth the Prisma Client and the Prisma CLI rely on the [Prisma Engines](https://www.prisma.io/docs/concepts/components/prisma-engines#prisma-engines), which are implemented in Rust and distributed as platform-specific compiled binaries. The Prisma Engines perform a variety of tasks ranging from providing information about the schema for type generation, or migrating the database, to transforming Prisma queries into SQL, and executing those queries against the database. Think of the engines as the layer in the Prisma ORM that talks to the database.\n\n![](https://blog.cloudflare.com/content/images/2023/10/1-.png)\n\nAs a developer, one of the first steps to [get started](https://www.prisma.io/docs/getting-started/quickstart) with Prisma is to install [Prisma Client](https://www.prisma.io/docs/concepts/components/prisma-client) and the [Prisma CLI](https://www.prisma.io/docs/concepts/components/prisma-cli) from npm. Once installed, these packages need the Prisma Engines to be able to function. These engines have complex target-platform rules and were originally envisioned to be distributed separately from the npm package, so they can be used outside of the Node.js ecosystem. As a result, they are downloaded on demand by the Prisma CLI, only downloading what is strictly required for a given project.\n\nAs of mid-2023, the engines account for **100 million downloads a month and 250 terabytes of egress data transfer**, with a continuous month-over-month increase as our user base grows. This highlights the importance of a highly available, global, and scalable infrastructure that provides low latency engine downloads to Prisma users all around the world.\n\n## Our original solution: AWS S3 & CloudFront\n\nDuring the early development of the Prisma ORM, our engineering team looked for tools to build the CDN for engine distribution. With extensive AWS experience, we went with the obvious: S3 blob storage for the engine files and CloudFront to cache contents closer to the user.\n\n![](https://blog.cloudflare.com/content/images/2023/10/2-.png)\n\n_A simplified representation of how the Prisma Engines flow from our CI where they are built and uploaded, to the Prisma CLI downloading the correct engine for a given environment when installing Prisma, all the way to the user being able to use it._\n\nWe were happy with AWS for the most part, and it was able to scale with our demands. However, as our user base continued to grow, so did the costs. At our scale of traffic, data transfer became a considerable cost item that we knew would only continue to grow.\n\nThe continuously increasing cost of these services prompted us to explore alternative options that could better accommodate our needs while at least maintaining the same level of performance and reliability. Prisma is committed to providing the best products and solutions to our users, and an essential part of that commitment is being intentional about the allocation of our resources, including sensible spending to enable us to serve our growing user base in the best way possible.\n\n## Exploring distribution options\n\nWe extensively explored different technologies and services that provided both reliable and fast engine distribution, while being cost-effective.\n\n### Free solutions: GitHub & npm\n\nBecause Prisma ORM is an open-source solution, we have explored various ways to distribute the engines through our existing distribution channels, at no cost. In this area, we had both GitHub Releases and npm as candidates to host and distribute our engine files. We dismissed GitHub Releases early on as the quality of service was not guaranteed, which was a requirement for us towards our users, so we can be sure to provide a good developer experience under all circumstances.\n\nWe also looked at npm, and confirmed that hosting the engine files would be in agreement with their Terms of Service. This made npm a viable option, but also meant we would have to change our engine download and upload logic to accommodate a different system. Additionally, this implied that we would have to update many past Prisma CLI versions, requiring our users to upgrade to take advantage of the new solution.\n\n### Paid solutions: CDNs & Cloudflare\n\nWe then considered only replacing CloudFront, which accounted for 97% of our distribution costs, while retaining S3 as the origin. When we evaluated different CDNs, we found that alternatives could lead to an estimated 70% cost reduction.\n\nWe also explored Cloudflare’s offerings and were impressed by [**Cloudflare R2**](https://www.cloudflare.com/developer-platform/r2/), an alternative to AWS S3 + CloudFront. It offers robust blob storage compatible with S3 and leverages Cloudflare’s network for global low-latency distribution. Additionally, it has no egress costs, and is solely priced based on the total volume of data stored and operations on that data. Given our reliance on Cloudflare’s [**product portfolio**](https://www.cloudflare.com/cloudflare-product-portfolio/) for our [**Data Platform**](https://console.prisma.io/), and extensive experience with their Workers platform, we already had high trust in the quality of Cloudflare’s products.\n\nTo finalize our decision, we implemented a test to confirm our intuitions about Cloudflare’s quality of service. We deployed a script to 50 cities across the globe, representative of our incoming traffic, to measure download latencies for our engine files (~15MB). The test was run multiple times, with latencies for the different [cache statuses](https://developers.cloudflare.com/cache/concepts/default-cache-behavior/#cloudflare-cache-responses) recorded and compared against our previous AWS-based solution. The results confirmed that Cloudflare R2's reliability and performance were at least on par with AWS S3 + CloudFront. And because R2 is compatible with S3, we wouldn’t need to make substantial changes to our software in order to move over to Cloudflare. These were great results, and we couldn’t wait to switch!\n\n## Our solution: moving to Cloudflare’s R2\n\nIn order to move our engine file distribution to Cloudflare, we needed to ensure we could make the switch without any disruption or impact to our users.\n\nWhile R2 URLs match S3's format, Prisma CLI uses a fixed domain to point to the engine file distribution. This fixed domain enabled us to transition without making any changes to the code of older Prisma versions, and simply point the existing URLs to R2. However, to make the transition, we needed to change our DNS configuration to point to Cloudflare. While this seems trivial, potential issues like unexpected DNS propagation challenges, or certificate validation problems when connecting via TLS, required us to plan ahead in order to proceed confidently and safely.\n\nWe modified the Prisma ORM release pipeline to upload assets to both S3 and R2, and used the [R2 Super Slurper](https://blog.cloudflare.com/r2-super-slurper-ga/) for migrating past engine versions to R2. This ensured all Prisma releases, past and future, existed in both places. We also established Grafana monitoring checks to pull engine files from R2, using a DNS and TLS configuration similar to our desired production setup, but via an experimental domain. Those monitoring checks were later reused during the final traffic cutover to ensure that there was no service disruption.\n\nAs ensuring no impact or disruption to our users was of utmost importance, we proceeded with a gradual rollout of the DNS changes using DNS load balancing, a method where a group of alias records assigned to a domain are weighted differently. This meant that the DNS resolver directed more traffic to heavier-weighted records. We began with a load balancing configuration simulating our old setup, with one record (the _control_) pointing to AWS CloudFront, and the other (the _candidate_) pointing to R2. Initially, all weight was on the _control_, effectively preserving the old routing to CloudFront. We also set the lowest TTL possible, so changes in the record weights took effect as soon as possible, creating more control over DNS propagation. Additionally, we implemented a health check that would redirect all traffic to the _control_ if download latencies were significantly higher, or if errors were detected, ensuring a stable fallback.\n\nAt this point, everything was in place and we could start the rollout.\n\n![](https://blog.cloudflare.com/content/images/2023/10/3-.png)\n\n__Our DNS load balancing setup during the rollout. We assigned increasing weights to route traffic to Cloudflare R2. The health check that would fail over to AWS CloudFront never fired.__\n\nThe rollout began with a gradual increase in R2's DNS weight, and our monitoring dashboards showed that Cloudflare downloads were proportional to the weight assigned to R2. With as little as 5% traffic routed to Cloudflare, cache hit ratios neared 100%, as expected. Latencies matched the _control_, so the health checks were all good, and our fallback never activated. Over the duration of an hour, we gradually increased R2's DNS weight to manage 25%, 50%, and finally 100% of traffic, without any issues. The cutover could not have gone any smoother.\n\nAfter monitoring for an additional two days, we simplified the DNS topology and routed to Cloudflare exclusively. We were extremely satisfied with the change, and started seeing our infrastructure costs drop considerably, as expected, not to mention the zero downtime and zero reported issues from users.\n\n## A success\n\nTransitioning to Cloudflare R2 was easy thanks to their great product and tooling, intuitive platform and supportive team. We've had an excellent experience with their service, with consistently great uptime, performance and latency. Cloudflare proved once again to be a valuable partner to help us scale.\n\nWe are thrilled that our engine distribution costs have decreased by 98%. Cloudflare's cost-effective solution has not only delivered top-notch performance but has also brought significant savings to our operations. An all around success!\n\nTo learn more about how Prisma is building [Data DX](https://datadx.io/) solutions with Cloudflare, take a look at [**Developer Experience Redefined: Prisma & Cloudflare Lead the Way to Data DX**](https://www.prisma.io/blog/cloudflare-partnership-qerefgvwirjq).\n\nAnd if you want to see Prisma in action, get started with the [Quickstart guide](https://www.prisma.io/docs/getting-started/quickstart).\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Partners](https://blog.cloudflare.com/tag/partners/) [R2 Storage](https://blog.cloudflare.com/tag/cloudflare-r2/) [Developers](https://blog.cloudflare.com/tag/developers/)"
    },
    {
      "url": "https://blog.cloudflare.com/technical-breakdown-http2-rapid-reset-ddos-attack/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/technical-breakdown-http2-rapid-reset-ddos-attack/",
        "loadedTime": "2023-12-05T02:31:14.782Z",
        "referrerUrl": "https://blog.cloudflare.com/page/2/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/technical-breakdown-http2-rapid-reset-ddos-attack/",
        "title": "HTTP/2 Rapid Reset: deconstructing the record-breaking attack",
        "description": "This post dives into the details of the HTTP/2 protocol, the feature that attackers exploited to generate the massive Rapid Reset attacks, and the mitigation strategies we took to ensure all our customers are protected",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/10/2023\n16 min read\nThis post is also available in 简体中文, 繁體中文, 日本語, 한국어, Deutsch, Français and Español.\nStarting on Aug 25, 2023, we started to notice some unusually big HTTP attacks hitting many of our customers. These attacks were detected and mitigated by our automated DDoS system. It was not long however, before they started to reach record breaking sizes — and eventually peaked just above 201 million requests per second. This was nearly 3x bigger than our previous biggest attack on record.\nUnder attack or need additional protection? Click here to get help.\nConcerning is the fact that the attacker was able to generate such an attack with a botnet of merely 20,000 machines. There are botnets today that are made up of hundreds of thousands or millions of machines. Given that the entire web typically sees only between 1–3 billion requests per second, it's not inconceivable that using this method could focus an entire web’s worth of requests on a small number of targets.\nDetecting and Mitigating\nThis was a novel attack vector at an unprecedented scale, but Cloudflare's existing protections were largely able to absorb the brunt of the attacks. While initially we saw some impact to customer traffic — affecting roughly 1% of requests during the initial wave of attacks — today we’ve been able to refine our mitigation methods to stop the attack for any Cloudflare customer without it impacting our systems.\nWe noticed these attacks at the same time two other major industry players — Google and AWS — were seeing the same. We worked to harden Cloudflare’s systems to ensure that, today, all our customers are protected from this new DDoS attack method without any customer impact. We’ve also participated with Google and AWS in a coordinated disclosure of the attack to impacted vendors and critical infrastructure providers.\nThis attack was made possible by abusing some features of the HTTP/2 protocol and server implementation details (see CVE-2023-44487 for details). Because the attack abuses an underlying weakness in the HTTP/2 protocol, we believe any vendor that has implemented HTTP/2 will be subject to the attack. This included every modern web server. We, along with Google and AWS, have disclosed the attack method to web server vendors who we expect will implement patches. In the meantime, the best defense is using a DDoS mitigation service like Cloudflare’s in front of any web-facing web or API server. \nThis post dives into the details of the HTTP/2 protocol, the feature that attackers exploited to generate these massive attacks, and the mitigation strategies we took to ensure all our customers are protected. Our hope is that by publishing these details other impacted web servers and services will have the information they need to implement mitigation strategies. And, moreover, the HTTP/2 protocol standards team, as well as teams working on future web standards, can better design them to prevent such attacks.\nRST attack details\nHTTP is the application protocol that powers the Web. HTTP Semantics are common to all versions of HTTP — the overall architecture, terminology, and protocol aspects such as request and response messages, methods, status codes, header and trailer fields, message content, and much more. Each individual HTTP version defines how semantics are transformed into a \"wire format\" for exchange over the Internet. For example, a client has to serialize a request message into binary data and send it, then the server parses that back into a message it can process.\nHTTP/1.1 uses a textual form of serialization. Request and response messages are exchanged as a stream of ASCII characters, sent over a reliable transport layer like TCP, using the following format (where CRLF means carriage-return and linefeed):\nHTTP-message = start-line CRLF *( field-line CRLF ) CRLF [ message-body ] \nFor example, a very simple GET request for https://blog.cloudflare.com/ would look like this on the wire:\nGET / HTTP/1.1 CRLFHost: blog.cloudflare.comCRLFCRLF\nAnd the response would look like:\nHTTP/1.1 200 OK CRLFServer: cloudflareCRLFContent-Length: 100CRLFtext/html; charset=UTF-8CRLFCRLF<100 bytes of data>\nThis format frames messages on the wire, meaning that it is possible to use a single TCP connection to exchange multiple requests and responses. However, the format requires that each message is sent whole. Furthermore, in order to correctly correlate requests with responses, strict ordering is required; meaning that messages are exchanged serially and can not be multiplexed. Two GET requests, for https://blog.cloudflare.com/ and https://blog.cloudflare.com/page/2/, would be:\nGET / HTTP/1.1 CRLFHost: blog.cloudflare.comCRLFCRLFGET /page/2/ HTTP/1.1 CRLFHost: blog.cloudflare.comCRLFCRLF\nWith the responses:\nHTTP/1.1 200 OK CRLFServer: cloudflareCRLFContent-Length: 100CRLFtext/html; charset=UTF-8CRLFCRLF<100 bytes of data>CRLFHTTP/1.1 200 OK CRLFServer: cloudflareCRLFContent-Length: 100CRLFtext/html; charset=UTF-8CRLFCRLF<100 bytes of data>\nWeb pages require more complicated HTTP interactions than these examples. When visiting the Cloudflare blog, your browser will load multiple scripts, styles and media assets. If you visit the front page using HTTP/1.1 and decide quickly to navigate to page 2, your browser can pick from two options. Either wait for all of the queued up responses for the page that you no longer want before page 2 can even start, or cancel in-flight requests by closing the TCP connection and opening a new connection. Neither of these is very practical. Browsers tend to work around these limitations by managing a pool of TCP connections (up to 6 per host) and implementing complex request dispatch logic over the pool.\nHTTP/2 addresses many of the issues with HTTP/1.1. Each HTTP message is serialized into a set of HTTP/2 frames that have type, length, flags, stream identifier (ID) and payload. The stream ID makes it clear which bytes on the wire apply to which message, allowing safe multiplexing and concurrency. Streams are bidirectional. Clients send frames and servers reply with frames using the same ID.\nIn HTTP/2 our GET request for https://blog.cloudflare.com would be exchanged across stream ID 1, with the client sending one HEADERS frame, and the server responding with one HEADERS frame, followed by one or more DATA frames. Client requests always use odd-numbered stream IDs, so subsequent requests would use stream ID 3, 5, and so on. Responses can be served in any order, and frames from different streams can be interleaved. \nStream multiplexing and concurrency are powerful features of HTTP/2. They enable more efficient usage of a single TCP connection. HTTP/2 optimizes resources fetching especially when coupled with prioritization. On the flip side, making it easy for clients to launch large amounts of parallel work can increase the peak demand for server resources when compared to HTTP/1.1. This is an obvious vector for denial-of-service.\nIn order to provide some guardrails, HTTP/2 provides a notion of maximum active concurrent streams. The SETTINGS_MAX_CONCURRENT_STREAMS parameter allows a server to advertise its limit of concurrency. For example, if the server states a limit of 100, then only 100 requests can be active at any time. If a client attempts to open a stream above this limit, it must be rejected by the server using a RST_STREAM frame. Stream rejection does not affect the other in-flight streams on the connection.\nThe true story is a little more complicated. Streams have a lifecycle. Below is a diagram of the HTTP/2 stream state machine. Client and server manage their own views of the state of a stream. HEADERS, DATA and RST_STREAM frames trigger transitions when they are sent or received. Although the views of the stream state are independent, they are synchronized.\nHEADERS and DATA frames include an END_STREAM flag, that when set to the value 1 (true), can trigger a state transition.\nLet's work through this with an example of a GET request that has no message content. The client sends the request as a HEADERS frame with the END_STREAM flag set to 1. The client first transitions the stream from idle to open state, then immediately transitions into half-closed state. The client half-closed state means that it can no longer send HEADERS or DATA, only WINDOW_UPDATE, PRIORITY or RST_STREAM frames. It can receive any frame however.\nOnce the server receives and parses the HEADERS frame, it transitions the stream state from idle to open and then half-closed, so it matches the client. The server half-closed state means it can send any frame but receive only WINDOW_UPDATE, PRIORITY or RST_STREAM frames. \nThe response to the GET contains message content, so the server sends HEADERS with END_STREAM flag set to 0, then DATA with END_STREAM flag set to 1. The DATA frame triggers the transition of the stream from half-closed to closed on the server. When the client receives it, it also transitions to closed. Once a stream is closed, no frames can be sent or received.\nApplying this lifecycle back into the context of concurrency, HTTP/2 states: \nStreams that are in the \"open\" state or in either of the \"half-closed\" states count toward the maximum number of streams that an endpoint is permitted to open. Streams in any of these three states count toward the limit advertised in the SETTINGS_MAX_CONCURRENT_STREAMS setting.\nIn theory, the concurrency limit is useful. However, there are practical factors that hamper its effectiveness— which we will cover later in the blog. \nHTTP/2 request cancellation\nEarlier, we talked about client cancellation of in-flight requests. HTTP/2 supports this in a much more efficient way than HTTP/1.1. Rather than needing to tear down the whole connection, a client can send a RST_STREAM frame for a single stream. This instructs the server to stop processing the request and to abort the response, which frees up server resources and avoids wasting bandwidth. \nLet's consider our previous example of 3 requests. This time the client cancels the request on stream 1 after all of the HEADERS have been sent. The server parses this RST_STREAM frame before it is ready to serve the response and instead only responds to stream 3 and 5:\nRequest cancellation is a useful feature. For example, when scrolling a webpage with multiple images, a web browser can cancel images that fall outside the viewport, meaning that images entering it can load faster. HTTP/2 makes this behaviour a lot more efficient compared to HTTP/1.1.\nA request stream that is canceled, rapidly transitions through the stream lifecycle. The client's HEADERS with END_STREAM flag set to 1 transitions the state from idle to open to half-closed, then RST_STREAM immediately causes a transition from half-closed to closed. \nRecall that only streams that are in the open or half-closed state contribute to the stream concurrency limit. When a client cancels a stream, it instantly gets the ability to open another stream in its place and can send another request immediately. This is the crux of what makes CVE-2023-44487 work.\nRapid resets leading to denial of service\nHTTP/2 request cancellation can be abused to rapidly reset an unbounded number of streams. When an HTTP/2 server is able to process client-sent RST_STREAM frames and tear down state quickly enough, such rapid resets do not cause a problem. Where issues start to crop up is when there is any kind of delay or lag in tidying up. The client can churn through so many requests that a backlog of work accumulates, resulting in excess consumption of resources on the server. \nA common HTTP deployment architecture is to run an HTTP/2 proxy or load-balancer in front of other components. When a client request arrives it is quickly dispatched and the actual work is done as an asynchronous activity somewhere else. This allows the proxy to handle client traffic very efficiently. However, this separation of concerns can make it hard for the proxy to tidy up the in-process jobs. Therefore, these deployments are more likely to encounter issues from rapid resets.\nWhen Cloudflare's reverse proxies process incoming HTTP/2 client traffic, they copy the data from the connection’s socket into a buffer and process that buffered data in order. As each request is read (HEADERS and DATA frames) it is dispatched to an upstream service. When RST_STREAM frames are read, the local state for the request is torn down and the upstream is notified that the request has been canceled. Rinse and repeat until the entire buffer is consumed. However this logic can be abused: when a malicious client started sending an enormous chain of requests and resets at the start of a connection, our servers would eagerly read them all and create stress on the upstream servers to the point of being unable to process any new incoming request.\nSomething that is important to highlight is that stream concurrency on its own cannot mitigate rapid reset. The client can churn requests to create high request rates no matter the server's chosen value of SETTINGS_MAX_CONCURRENT_STREAMS.\nRapid Reset dissected\nHere's an example of rapid reset reproduced using a proof-of-concept client attempting to make a total of 1000 requests. I've used an off-the-shelf server without any mitigations; listening on port 443 in a test environment. The traffic is dissected using Wireshark and filtered to show only HTTP/2 traffic for clarity. Download the pcap to follow along.\nIt's a bit difficult to see, because there are a lot of frames. We can get a quick summary via Wireshark's Statistics > HTTP2 tool:\nThe first frame in this trace, in packet 14, is the server's SETTINGS frame, which advertises a maximum stream concurrency of 100. In packet 15, the client sends a few control frames and then starts making requests that are rapidly reset. The first HEADERS frame is 26 bytes long, all subsequent HEADERS are only 9 bytes. This size difference is due to a compression technology called HPACK. In total, packet 15 contains 525 requests, going up to stream 1051.\nInterestingly, the RST_STREAM for stream 1051 doesn't fit in packet 15, so in packet 16 we see the server respond with a 404 response. Then in packet 17 the client does send the RST_STREAM, before moving on to sending the remaining 475 requests.\nNote that although the server advertised 100 concurrent streams, both packets sent by the client sent a lot more HEADERS frames than that. The client did not have to wait for any return traffic from the server, it was only limited by the size of the packets it could send. No server RST_STREAM frames are seen in this trace, indicating that the server did not observe a concurrent stream violation. \nImpact on customers\nAs mentioned above, as requests are canceled, upstream services are notified and can abort requests before wasting too many resources on it. This was the case with this attack, where most malicious requests were never forwarded to the origin servers. However, the sheer size of these attacks did cause some impact.\nFirst, as the rate of incoming requests reached peaks never seen before, we had reports of increased levels of 502 errors seen by clients. This happened on our most impacted data centers as they were struggling to process all the requests. While our network is meant to deal with large attacks, this particular vulnerability exposed a weakness in our infrastructure. Let's dig a little deeper into the details, focusing on how incoming requests are handled when they hit one of our data centers:\nWe can see that our infrastructure is composed of a chain of different proxy servers with different responsibilities. In particular, when a client connects to Cloudflare to send HTTPS traffic, it first hits our TLS decryption proxy: it decrypts TLS traffic, processes HTTP 1, 2 or 3 traffic, then forwards it to our \"business logic\" proxy. This one is responsible for loading all the settings for each customer, then routing the requests correctly to other upstream services — and more importantly in our case, it is also responsible for security features. This is where L7 attack mitigation is processed.\nThe problem with this attack vector is that it manages to send a lot of requests very quickly in every single connection. Each of them had to be forwarded to the business logic proxy before we had a chance to block it. As the request throughput became higher than our proxy capacity, the pipe connecting these two services reached its saturation level in some of our servers.\nWhen this happens, the TLS proxy cannot connect anymore to its upstream proxy, this is why some clients saw a bare \"502 Bad Gateway\" error during the most serious attacks. It is important to note that, as of today, the logs used to create HTTP analytics are also emitted by our business logic proxy. The consequence of that is that these errors are not visible in the Cloudflare dashboard. Our internal dashboards show that about 1% of requests were impacted during the initial wave of attacks (before we implemented mitigations), with peaks at around 12% for a few seconds during the most serious one on August 29th. The following graph shows the ratio of these errors over a two hours while this was happening:\nWe worked to reduce this number dramatically in the following days, as detailed later on in this post. Both thanks to changes in our stack and to our mitigation that reduce the size of these attacks considerably, this number today is effectively zero.\n499 errors and the challenges for HTTP/2 stream concurrency\nAnother symptom reported by some customers is an increase in 499 errors. The reason for this is a bit different and is related to the maximum stream concurrency in a HTTP/2 connection detailed earlier in this post.\nHTTP/2 settings are exchanged at the start of a connection using SETTINGS frames. In the absence of receiving an explicit parameter, default values apply. Once a client establishes an HTTP/2 connection, it can wait for a server's SETTINGS (slow) or it can assume the default values and start making requests (fast). For SETTINGS_MAX_CONCURRENT_STREAMS, the default is effectively unlimited (stream IDs use a 31-bit number space, and requests use odd numbers, so the actual limit is 1073741824). The specification recommends that a server offer no fewer than 100 streams. Clients are generally biased towards speed, so don't tend to wait for server settings, which creates a bit of a race condition. Clients are taking a gamble on what limit the server might pick; if they pick wrong the request will be rejected and will have to be retried. Gambling on 1073741824 streams is a bit silly. Instead, a lot of clients decide to limit themselves to issuing 100 concurrent streams, with the hope that servers followed the specification recommendation. Where servers pick something below 100, this client gamble fails and streams are reset. \nThere are many reasons a server might reset a stream beyond concurrency limit overstepping. HTTP/2 is strict and requires a stream to be closed when there are parsing or logic errors. In 2019, Cloudflare developed several mitigations in response to HTTP/2 DoS vulnerabilities. Several of those vulnerabilities were caused by a client misbehaving, leading the server to reset a stream. A very effective strategy to clamp down on such clients is to count the number of server resets during a connection, and when that exceeds some threshold value, close the connection with a GOAWAY frame. Legitimate clients might make one or two mistakes in a connection and that is acceptable. A client that makes too many mistakes is probably either broken or malicious and closing the connection addresses both cases.\nWhile responding to DoS attacks enabled by CVE-2023-44487, Cloudflare reduced maximum stream concurrency to 64. Before making this change, we were unaware that clients don't wait for SETTINGS and instead assume a concurrency of 100. Some web pages, such as an image gallery, do indeed cause a browser to send 100 requests immediately at the start of a connection. Unfortunately, the 36 streams above our limit all needed to be reset, which triggered our counting mitigations. This meant that we closed connections on legitimate clients, leading to a complete page load failure. As soon as we realized this interoperability issue, we changed the maximum stream concurrency to 100.\nActions from the Cloudflare side\nIn 2019 several DoS vulnerabilities were uncovered related to implementations of HTTP/2. Cloudflare developed and deployed a series of detections and mitigations in response. CVE-2023-44487 is a different manifestation of HTTP/2 vulnerability. However, to mitigate it we were able to extend the existing protections to monitor client-sent RST_STREAM frames and close connections when they are being used for abuse. Legitimate client uses for RST_STREAM are unaffected.\nIn addition to a direct fix, we have implemented several improvements to the server's HTTP/2 frame processing and request dispatch code. Furthermore, the business logic server has received improvements to queuing and scheduling that reduce unnecessary work and improve cancellation responsiveness. Together these lessen the impact of various potential abuse patterns as well as giving more room to the server to process requests before saturating.\nMitigate attacks earlier\nCloudflare already had systems in place to efficiently mitigate very large attacks with less expensive methods. One of them is named \"IP Jail\". For hyper volumetric attacks, this system collects the client IPs participating in the attack and stops them from connecting to the attacked property, either at the IP level, or in our TLS proxy. This system however needs a few seconds to be fully effective; during these precious seconds, the origins are already protected but our infrastructure still needs to absorb all HTTP requests. As this new botnet has effectively no ramp-up period, we need to be able to neutralize attacks before they can become a problem.\nTo achieve this we expanded the IP Jail system to protect our entire infrastructure: once an IP is \"jailed\", not only it is blocked from connecting to the attacked property, we also forbid the corresponding IPs from using HTTP/2 to any other domain on Cloudflare for some time. As such protocol abuses are not possible using HTTP/1.x, this limits the attacker's ability to run large attacks, while any legitimate client sharing the same IP would only see a very small performance decrease during that time. IP based mitigations are a very blunt tool — this is why we have to be extremely careful when using them at that scale and seek to avoid false positives as much as possible. Moreover, the lifespan of a given IP in a botnet is usually short so any long term mitigation is likely to do more harm than good. The following graph shows the churn of IPs in the attacks we witnessed:\nAs we can see, many new IPs spotted on a given day disappear very quickly afterwards.\nAs all these actions happen in our TLS proxy at the beginning of our HTTPS pipeline, this saves considerable resources compared to our regular L7 mitigation system. This allowed us to weather these attacks much more smoothly and now the number of random 502 errors caused by these botnets is down to zero.\nObservability improvements\nAnother front on which we are making change is observability. Returning errors to clients without being visible in customer analytics is unsatisfactory. Fortunately, a project has been underway to overhaul these systems since long before the recent attacks. It will eventually allow each service within our infrastructure to log its own data, instead of relying on our business logic proxy to consolidate and emit log data. This incident underscored the importance of this work, and we are redoubling our efforts.\nWe are also working on better connection-level logging, allowing us to spot such protocol abuses much more quickly to improve our DDoS mitigation capabilities.\nConclusion\nWhile this was the latest record-breaking attack, we know it won’t be the last. As attacks continue to become more sophisticated, Cloudflare works relentlessly to proactively identify new threats — deploying countermeasures to our global network so that our millions of customers are immediately and automatically protected. \nCloudflare has provided free, unmetered and unlimited DDoS protection to all of our customers since 2017. In addition, we offer a range of additional security features to suit the needs of organizations of all sizes. Contact us if you’re unsure whether you’re protected or want to understand how you can be.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nDDoS Vulnerabilities Trends Attacks Security",
      "markdown": "10/10/2023\n\n*   [![Lucas Pardue](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2018/12/Lucas---KIN01111.jpg)](https://blog.cloudflare.com/author/lucas/)\n*   [![Julien Desgats](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2017/06/profile-square-small.jpg)](https://blog.cloudflare.com/author/julien-desgats/)\n\n16 min read\n\nThis post is also available in [简体中文](https://blog.cloudflare.com/zh-cn/technical-breakdown-http2-rapid-reset-ddos-attack-zh-cn/), [繁體中文](https://blog.cloudflare.com/zh-tw/technical-breakdown-http2-rapid-reset-ddos-attack-zh-tw/), [日本語](https://blog.cloudflare.com/ja-jp/technical-breakdown-http2-rapid-reset-ddos-attack-ja-jp/), [한국어](https://blog.cloudflare.com/ko-kr/technical-breakdown-http2-rapid-reset-ddos-attack-ko-kr/), [Deutsch](https://blog.cloudflare.com/de-de/technical-breakdown-http2-rapid-reset-ddos-attack-de-de/), [Français](https://blog.cloudflare.com/fr-fr/technical-breakdown-http2-rapid-reset-ddos-attack-fr-fr/) and [Español](https://blog.cloudflare.com/es-es/technical-breakdown-http2-rapid-reset-ddos-attack-es-es/).\n\n![](https://blog.cloudflare.com/content/images/2023/10/Investigating-Rapid-Reset-Vulnerability.png)\n\nStarting on Aug 25, 2023, we started to notice some unusually big HTTP attacks hitting many of our customers. These attacks were detected and mitigated by our automated DDoS system. It was not long however, before they started to reach record breaking sizes — and eventually peaked just above 201 million requests per second. This was nearly 3x bigger than our [previous biggest attack on record](https://blog.cloudflare.com/cloudflare-mitigates-record-breaking-71-million-request-per-second-ddos-attack/).\n\n_Under attack or need additional protection? [Click here to get help](https://www.cloudflare.com/h2/)._\n\n  \n\nConcerning is the fact that the attacker was able to generate such an attack with a botnet of merely 20,000 machines. There are botnets today that are made up of hundreds of thousands or millions of machines. Given that the entire web typically sees only between 1–3 billion requests per second, it's not inconceivable that using this method could focus an entire web’s worth of requests on a small number of targets.\n\n## Detecting and Mitigating\n\nThis was a novel attack vector at an unprecedented scale, but Cloudflare's existing protections were largely able to absorb the brunt of the attacks. While initially we saw some impact to customer traffic — affecting roughly 1% of requests during the initial wave of attacks — today we’ve been able to refine our mitigation methods to stop the attack for any Cloudflare customer without it impacting our systems.\n\nWe noticed these attacks at the same time two other major industry players — Google and AWS — were seeing the same. We worked to harden Cloudflare’s systems to ensure that, today, all our customers are protected from this new DDoS attack method without any customer impact. We’ve also participated with Google and AWS in a coordinated disclosure of the attack to impacted vendors and critical infrastructure providers.\n\nThis attack was made possible by abusing some features of the HTTP/2 protocol and server implementation details (see  [CVE-2023-44487](https://www.cve.org/CVERecord?id=CVE-2023-44487) for details). Because the attack abuses an underlying weakness in the HTTP/2 protocol, we believe any vendor that has implemented HTTP/2 will be subject to the attack. This included every modern web server. We, along with Google and AWS, have disclosed the attack method to web server vendors who we expect will implement patches. In the meantime, the best defense is using a DDoS mitigation service like Cloudflare’s in front of any web-facing web or API server.\n\nThis post dives into the details of the HTTP/2 protocol, the feature that attackers exploited to generate these massive attacks, and the mitigation strategies we took to ensure all our customers are protected. Our hope is that by publishing these details other impacted web servers and services will have the information they need to implement mitigation strategies. And, moreover, the HTTP/2 protocol standards team, as well as teams working on future web standards, can better design them to [prevent such attacks](https://www.cloudflare.com/learning/ddos/how-to-prevent-ddos-attacks/).\n\n## RST attack details\n\nHTTP is the application protocol that powers the Web. [HTTP Semantics](https://www.rfc-editor.org/rfc/rfc9110.html) are common to all versions of HTTP — the overall architecture, terminology, and protocol aspects such as request and response messages, methods, status codes, header and trailer fields, message content, and much more. Each individual HTTP version defines how semantics are transformed into a \"wire format\" for exchange over the Internet. For example, a client has to serialize a request message into binary data and send it, then the server parses that back into a message it can process.\n\n[HTTP/1.1](https://www.rfc-editor.org/rfc/rfc9112.html) uses a textual form of serialization. Request and response messages are exchanged as a stream of ASCII characters, sent over a reliable transport layer like TCP, using the following [format](https://www.rfc-editor.org/rfc/rfc9112.html#section-2.1) (where CRLF means carriage-return and linefeed):\n\n```\n HTTP-message   = start-line CRLF\n                   *( field-line CRLF )\n                   CRLF\n                   [ message-body ]\n```\n\nFor example, a very simple GET request for `https://blog.cloudflare.com/` would look like this on the wire:\n\n`GET / HTTP/1.1 CRLFHost: blog.cloudflare.comCRLFCRLF`\n\nAnd the response would look like:\n\n`HTTP/1.1 200 OK CRLFServer: cloudflareCRLFContent-Length: 100CRLFtext/html; charset=UTF-8CRLFCRLF<100 bytes of data>`\n\nThis format **frames** messages on the wire, meaning that it is possible to use a single TCP connection to exchange multiple requests and responses. However, the format requires that each message is sent whole. Furthermore, in order to correctly correlate requests with responses, strict ordering is required; meaning that messages are exchanged serially and can not be multiplexed. Two GET requests, for `https://blog.cloudflare.com/` and `https://blog.cloudflare.com/page/2/`, would be:\n\n`GET / HTTP/1.1 CRLFHost: blog.cloudflare.comCRLFCRLFGET /page/2/ HTTP/1.1 CRLFHost: blog.cloudflare.comCRLFCRLF`\n\nWith the responses:\n\n`HTTP/1.1 200 OK CRLFServer: cloudflareCRLFContent-Length: 100CRLFtext/html; charset=UTF-8CRLFCRLF<100 bytes of data>CRLFHTTP/1.1 200 OK CRLFServer: cloudflareCRLFContent-Length: 100CRLFtext/html; charset=UTF-8CRLFCRLF<100 bytes of data>`\n\nWeb pages require more complicated HTTP interactions than these examples. When visiting the Cloudflare blog, your browser will load multiple scripts, styles and media assets. If you visit the front page using HTTP/1.1 and decide quickly to navigate to page 2, your browser can pick from two options. Either wait for all of the queued up responses for the page that you no longer want before page 2 can even start, or cancel in-flight requests by closing the TCP connection and opening a new connection. Neither of these is very practical. Browsers tend to work around these limitations by managing a pool of TCP connections (up to 6 per host) and implementing complex request dispatch logic over the pool.\n\n[HTTP/2](https://www.rfc-editor.org/rfc/rfc9113) addresses many of the issues with HTTP/1.1. Each HTTP message is serialized into a set of **HTTP/2 frames** that have type, length, flags, stream identifier (ID) and payload. The stream ID makes it clear which bytes on the wire apply to which message, allowing safe multiplexing and concurrency. Streams are bidirectional. Clients send frames and servers reply with frames using the same ID.\n\nIn HTTP/2 our GET request for `https://blog.cloudflare.com` would be exchanged across stream ID 1, with the client sending one [HEADERS](https://www.rfc-editor.org/rfc/rfc9113#name-headers) frame, and the server responding with one HEADERS frame, followed by one or more [DATA](https://www.rfc-editor.org/rfc/rfc9113#name-data) frames. Client requests always use odd-numbered stream IDs, so subsequent requests would use stream ID 3, 5, and so on. Responses can be served in any order, and frames from different streams can be interleaved.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Screenshot-2023-10-09-at-2.13.29-PM.png)\n\nStream multiplexing and concurrency are powerful features of HTTP/2. They enable more efficient usage of a single TCP connection. HTTP/2 optimizes resources fetching especially when coupled with [prioritization](https://blog.cloudflare.com/better-http-2-prioritization-for-a-faster-web/). On the flip side, making it easy for clients to launch large amounts of parallel work can increase the peak demand for server resources when compared to HTTP/1.1. This is an obvious vector for denial-of-service.\n\nIn order to provide some guardrails, HTTP/2 provides a notion of maximum active [concurrent streams](https://www.rfc-editor.org/rfc/rfc9113#section-5.1.2). The [SETTINGS\\_MAX\\_CONCURRENT\\_STREAMS](https://www.rfc-editor.org/rfc/rfc9113#SETTINGS_MAX_FRAME_SIZE) parameter allows a server to advertise its limit of concurrency. For example, if the server states a limit of 100, then only 100 requests can be active at any time. If a client attempts to open a stream above this limit, it must be rejected by the server using a [RST\\_STREAM](https://www.rfc-editor.org/rfc/rfc9113#section-6.4) frame. Stream rejection does not affect the other in-flight streams on the connection.\n\nThe true story is a little more complicated. Streams have a [lifecycle](https://www.rfc-editor.org/rfc/rfc9113#section-5.1). Below is a diagram of the HTTP/2 stream state machine. Client and server manage their own views of the state of a stream. HEADERS, DATA and RST\\_STREAM frames trigger transitions when they are sent or received. Although the views of the stream state are independent, they are synchronized.\n\nHEADERS and DATA frames include an END\\_STREAM flag, that when set to the value 1 (true), can trigger a state transition.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Request-stream-states.png)\n\nLet's work through this with an example of a GET request that has no message content. The client sends the request as a HEADERS frame with the END\\_STREAM flag set to 1. The client first transitions the stream from **idle** to **open** state, then immediately transitions into **half-closed** state. The client half-closed state means that it can no longer send HEADERS or DATA, only [WINDOW\\_UPDATE](https://www.rfc-editor.org/rfc/rfc9113.html#section-6.9), [PRIORITY](https://www.rfc-editor.org/rfc/rfc9113.html#section-6.3) or RST\\_STREAM frames. It can receive any frame however.\n\nOnce the server receives and parses the HEADERS frame, it transitions the stream state from idle to open and then half-closed, so it matches the client. The server half-closed state means it can send any frame but receive only WINDOW\\_UPDATE, PRIORITY or RST\\_STREAM frames.\n\nThe response to the GET contains message content, so the server sends HEADERS with END\\_STREAM flag set to 0, then DATA with END\\_STREAM flag set to 1. The DATA frame triggers the transition of the stream from **half-closed** to **closed** on the server. When the client receives it, it also transitions to closed. Once a stream is closed, no frames can be sent or received.\n\nApplying this lifecycle back into the context of concurrency, HTTP/2 [states](https://www.rfc-editor.org/rfc/rfc9113#section-5.1.2-2):\n\n_Streams that are in the \"open\" state or in either of the \"half-closed\" states count toward the maximum number of streams that an endpoint is permitted to open. Streams in any of these three states count toward the limit advertised in the_ [_SETTINGS\\_MAX\\_CONCURRENT\\_STREAMS_](https://www.rfc-editor.org/rfc/rfc9113#SETTINGS_MAX_CONCURRENT_STREAMS) _setting._\n\nIn theory, the concurrency limit is useful. However, there are practical factors that hamper its effectiveness— which we will cover later in the blog.\n\n### HTTP/2 request cancellation\n\nEarlier, we talked about client cancellation of in-flight requests. HTTP/2 supports this in a much more efficient way than HTTP/1.1. Rather than needing to tear down the whole connection, a client can send a RST\\_STREAM frame for a single stream. This instructs the server to stop processing the request and to abort the response, which frees up server resources and avoids wasting bandwidth.\n\nLet's consider our previous example of 3 requests. This time the client cancels the request on stream 1 after all of the HEADERS have been sent. The server parses this RST\\_STREAM frame before it is ready to serve the response and instead only responds to stream 3 and 5:\n\n![](https://blog.cloudflare.com/content/images/2023/10/Screenshot-2023-10-09-at-2.12.04-PM.png)\n\nRequest cancellation is a useful feature. For example, when scrolling a webpage with multiple images, a web browser can cancel images that fall outside the viewport, meaning that images entering it can load faster. HTTP/2 makes this behaviour a lot more efficient compared to HTTP/1.1.\n\nA request stream that is canceled, rapidly transitions through the stream lifecycle. The client's HEADERS with END\\_STREAM flag set to 1 transitions the state from **idle** to **open** to **half-closed**, then RST\\_STREAM immediately causes a transition from **half-closed** to **closed.**\n\n![](https://blog.cloudflare.com/content/images/2023/10/Request-stream-states-reset.png)\n\nRecall that only streams that are in the open or half-closed state contribute to the stream concurrency limit. When a client cancels a stream, it instantly gets the ability to open another stream in its place and can send another request immediately. This is the crux of what makes [CVE-2023-44487](https://www.cve.org/CVERecord?id=CVE-2023-44487) work.\n\n### Rapid resets leading to denial of service\n\nHTTP/2 request cancellation can be abused to rapidly reset an unbounded number of streams. When an HTTP/2 server is able to process client-sent RST\\_STREAM frames and tear down state quickly enough, such rapid resets do not cause a problem. Where issues start to crop up is when there is any kind of delay or lag in tidying up. The client can churn through so many requests that a backlog of work accumulates, resulting in excess consumption of resources on the server.\n\nA common HTTP deployment architecture is to run an HTTP/2 proxy or load-balancer in front of other components. When a client request arrives it is quickly dispatched and the actual work is done as an asynchronous activity somewhere else. This allows the proxy to handle client traffic very efficiently. However, this separation of concerns can make it hard for the proxy to tidy up the in-process jobs. Therefore, these deployments are more likely to encounter issues from rapid resets.\n\nWhen Cloudflare's [reverse proxies](https://www.rfc-editor.org/rfc/rfc9110#section-3.7-6) process incoming HTTP/2 client traffic, they copy the data from the connection’s socket into a buffer and process that buffered data in order. As each request is read (HEADERS and DATA frames) it is dispatched to an upstream service. When RST\\_STREAM frames are read, the local state for the request is torn down and the upstream is notified that the request has been canceled. Rinse and repeat until the entire buffer is consumed. However this logic can be abused: when a malicious client started sending an enormous chain of requests and resets at the start of a connection, our servers would eagerly read them all and create stress on the upstream servers to the point of being unable to process any new incoming request.\n\nSomething that is important to highlight is that stream concurrency on its own cannot mitigate rapid reset. The client can churn requests to create high request rates no matter the server's chosen value of [SETTINGS\\_MAX\\_CONCURRENT\\_STREAMS](https://www.rfc-editor.org/rfc/rfc9113#SETTINGS_MAX_CONCURRENT_STREAMS).\n\n### Rapid Reset dissected\n\nHere's an example of rapid reset reproduced using a proof-of-concept client attempting to make a total of 1000 requests. I've used an off-the-shelf server without any mitigations; listening on port 443 in a test environment. The traffic is dissected using Wireshark and filtered to show only HTTP/2 traffic for clarity. [Download the pcap](https://blog.cloudflare.com/content/images/rapidreset.pcapng) to follow along.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Untitled--2-.png)\n\nIt's a bit difficult to see, because there are a lot of frames. We can get a quick summary via Wireshark's Statistics > HTTP2 tool:\n\n![](https://blog.cloudflare.com/content/images/2023/10/Screenshot-2023-10-09-at-10.50.42-PM.png)\n\nThe first frame in this trace, in packet 14, is the server's SETTINGS frame, which advertises a maximum stream concurrency of 100. In packet 15, the client sends a few control frames and then starts making requests that are rapidly reset. The first HEADERS frame is 26 bytes long, all subsequent HEADERS are only 9 bytes. This size difference is due to a compression technology called [HPACK](https://blog.cloudflare.com/hpack-the-silent-killer-feature-of-http-2/). In total, packet 15 contains 525 requests, going up to stream 1051.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Untitled--3-.png)\n\nInterestingly, the RST\\_STREAM for stream 1051 doesn't fit in packet 15, so in packet 16 we see the server respond with a 404 response.  Then in packet 17 the client does send the RST\\_STREAM, before moving on to sending the remaining 475 requests.\n\nNote that although the server advertised 100 concurrent streams, both packets sent by the client sent a lot more HEADERS frames than that. The client did not have to wait for any return traffic from the server, it was only limited by the size of the packets it could send. No server RST\\_STREAM frames are seen in this trace, indicating that the server did not observe a concurrent stream violation.\n\n## Impact on customers\n\nAs mentioned above, as requests are canceled, upstream services are notified and can abort requests before wasting too many resources on it. This was the case with this attack, where most malicious requests were never forwarded to the origin servers. However, the sheer size of these attacks did cause some impact.\n\nFirst, as the rate of incoming requests reached peaks never seen before, we had reports of increased levels of 502 errors seen by clients. This happened on our most impacted data centers as they were struggling to process all the requests. While our network is meant to deal with large attacks, this particular vulnerability exposed a weakness in our infrastructure. Let's dig a little deeper into the details, focusing on how incoming requests are handled when they hit one of our data centers:\n\n![](https://blog.cloudflare.com/content/images/2023/10/Untitled-2023-10-04-1953.png)\n\nWe can see that our infrastructure is composed of a chain of different proxy servers with different responsibilities. In particular, when a client connects to Cloudflare to send HTTPS traffic, it first hits our TLS decryption proxy: it decrypts TLS traffic, processes HTTP 1, 2 or 3 traffic, then forwards it to our \"business logic\" proxy. This one is responsible for loading all the settings for each customer, then routing the requests correctly to other upstream services — and more importantly in our case, **it is also responsible for security features**. This is where L7 attack mitigation is processed.\n\nThe problem with this attack vector is that it manages to send a lot of requests very quickly in every single connection. Each of them had to be forwarded to the business logic proxy before we had a chance to block it. As the request throughput became higher than our proxy capacity, the pipe connecting these two services reached its saturation level in some of our servers.\n\nWhen this happens, the TLS proxy cannot connect anymore to its upstream proxy, this is why some clients saw a bare \"502 Bad Gateway\" error during the most serious attacks. It is important to note that, as of today, the logs used to create HTTP analytics are also emitted by our business logic proxy. The consequence of that is that these errors are not visible in the Cloudflare dashboard. Our internal dashboards show that about 1% of requests were impacted during the initial wave of attacks (before we implemented mitigations), with peaks at around 12% for a few seconds during the most serious one on August 29th. The following graph shows the ratio of these errors over a two hours while this was happening:\n\n![](https://blog.cloudflare.com/content/images/2023/10/imageLikeEmbed.png \"Chart\")\n\nWe worked to reduce this number dramatically in the following days, as detailed later on in this post. Both thanks to changes in our stack and to our mitigation that reduce the size of these attacks considerably, this number today is effectively zero.\n\n![](https://blog.cloudflare.com/content/images/2023/10/imageLikeEmbed--2-.png \"Chart\")\n\n### 499 errors and the challenges for HTTP/2 stream concurrency\n\nAnother symptom reported by some customers is an increase in 499 errors. The reason for this is a bit different and is related to the maximum stream concurrency in a HTTP/2 connection detailed earlier in this post.\n\nHTTP/2 settings are exchanged at the start of a connection using SETTINGS frames. In the absence of receiving an explicit parameter, default values apply. Once a client establishes an HTTP/2 connection, it can wait for a server's SETTINGS (slow) or it can assume the default values and start making requests (fast). For SETTINGS\\_MAX\\_CONCURRENT\\_STREAMS, the default is effectively unlimited (stream IDs use a 31-bit number space, and requests use odd numbers, so the actual limit is 1073741824). The specification recommends that a server offer no fewer than 100 streams. Clients are generally biased towards speed, so don't tend to wait for server settings, which creates a bit of a race condition. Clients are taking a gamble on what limit the server might pick; if they pick wrong the request will be rejected and will have to be retried. Gambling on 1073741824 streams is a bit silly. Instead, a lot of clients decide to limit themselves to issuing 100 concurrent streams, with the hope that servers followed the specification recommendation. Where servers pick something below 100, this client gamble fails and streams are reset.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Untitled-2023-10-04-1953--1-.png)\n\nThere are many reasons a server might reset a stream beyond concurrency limit overstepping. HTTP/2 is strict and requires a stream to be closed when there are parsing or logic errors. In 2019, Cloudflare developed several mitigations in response to [HTTP/2 DoS vulnerabilities](https://blog.cloudflare.com/on-the-recent-http-2-dos-attacks/). Several of those vulnerabilities were caused by a client misbehaving, leading the server to reset a stream. A very effective strategy to clamp down on such clients is to count the number of server resets during a connection, and when that exceeds some threshold value, close the connection with a [GOAWAY](https://www.rfc-editor.org/rfc/rfc9113#section-6.8) frame. Legitimate clients might make one or two mistakes in a connection and that is acceptable. A client that makes too many mistakes is probably either broken or malicious and closing the connection addresses both cases.\n\nWhile responding to DoS attacks enabled by [CVE-2023-44487](https://www.cve.org/CVERecord?id=CVE-2023-44487), Cloudflare reduced maximum stream concurrency to 64. Before making this change, we were unaware that clients don't wait for SETTINGS and instead assume a concurrency of 100. Some web pages, such as an image gallery, do indeed cause a browser to send 100 requests immediately at the start of a connection. Unfortunately, the 36 streams above our limit all needed to be reset, which triggered our counting mitigations. This meant that we closed connections on legitimate clients, leading to a complete page load failure. As soon as we realized this interoperability issue, we changed the maximum stream concurrency to 100.\n\n## Actions from the Cloudflare side\n\nIn 2019 several [DoS vulnerabilities](https://blog.cloudflare.com/on-the-recent-http-2-dos-attacks/) were uncovered related to implementations of HTTP/2. Cloudflare developed and deployed a series of detections and mitigations in response.  [CVE-2023-44487](https://www.cve.org/CVERecord?id=CVE-2023-44487) is a different manifestation of HTTP/2 vulnerability. However, to mitigate it we were able to extend the existing protections to monitor client-sent RST\\_STREAM frames and close connections when they are being used for abuse. Legitimate client uses for RST\\_STREAM are unaffected.\n\nIn addition to a direct fix, we have implemented several improvements to the server's HTTP/2 frame processing and request dispatch code. Furthermore, the business logic server has received improvements to queuing and scheduling that reduce unnecessary work and improve cancellation responsiveness. Together these lessen the impact of various potential abuse patterns as well as giving more room to the server to process requests before saturating.\n\n### Mitigate attacks earlier\n\nCloudflare already had systems in place to efficiently mitigate very large attacks with less expensive methods. One of them is named \"IP Jail\". For hyper volumetric attacks, this system collects the client IPs participating in the attack and stops them from connecting to the attacked property, either at the IP level, or in our TLS proxy. This system however needs a few seconds to be fully effective; during these precious seconds, the origins are already protected but our infrastructure still needs to absorb all HTTP requests. As this new botnet has effectively no ramp-up period, we need to be able to neutralize attacks before they can become a problem.\n\nTo achieve this we expanded the IP Jail system to protect our entire infrastructure: once an IP is \"jailed\", not only it is blocked from connecting to the attacked property, we also forbid the corresponding IPs from using HTTP/2 to any other domain on Cloudflare for some time. As such protocol abuses are not possible using HTTP/1.x, this limits the attacker's ability to run large attacks, while any legitimate client sharing the same IP would only see a very small performance decrease during that time. IP based mitigations are a very blunt tool — this is why we have to be extremely careful when using them at that scale and seek to avoid false positives as much as possible. Moreover, the lifespan of a given IP in a botnet is usually short so any long term mitigation is likely to do more harm than good. The following graph shows the churn of IPs in the attacks we witnessed:\n\n![](https://blog.cloudflare.com/content/images/2023/10/ip-churn.png)\n\nAs we can see, many new IPs spotted on a given day disappear very quickly afterwards.\n\nAs all these actions happen in our TLS proxy at the beginning of our HTTPS pipeline, this saves considerable resources compared to our regular L7 mitigation system. This allowed us to weather these attacks much more smoothly and now the number of random 502 errors caused by these botnets is down to zero.\n\n### Observability improvements\n\nAnother front on which we are making change is observability. Returning errors to clients without being visible in customer analytics is unsatisfactory. Fortunately, a project has been underway to overhaul these systems since long before the recent attacks. It will eventually allow each service within our infrastructure to log its own data, instead of relying on our business logic proxy to consolidate and emit log data. This incident underscored the importance of this work, and we are redoubling our efforts.\n\nWe are also working on better connection-level logging, allowing us to spot such protocol abuses much more quickly to improve our DDoS mitigation capabilities.\n\n## Conclusion\n\nWhile this was the latest record-breaking attack, we know it won’t be the last. As attacks continue to become more sophisticated, Cloudflare works relentlessly to proactively identify new threats — deploying countermeasures to our global network so that our millions of customers are immediately and automatically protected.\n\nCloudflare has provided free, unmetered and unlimited DDoS protection to all of our customers since 2017. In addition, we offer a range of additional security features to suit the needs of organizations of all sizes. [Contact us](https://www.cloudflare.com/h2) if you’re unsure whether you’re protected or want to understand how you can be.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[DDoS](https://blog.cloudflare.com/tag/ddos/) [Vulnerabilities](https://blog.cloudflare.com/tag/vulnerabilities/) [Trends](https://blog.cloudflare.com/tag/trends/) [Attacks](https://blog.cloudflare.com/tag/attacks/) [Security](https://blog.cloudflare.com/tag/security/)"
    },
    {
      "url": "https://blog.cloudflare.com/zero-day-rapid-reset-http2-record-breaking-ddos-attack/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/zero-day-rapid-reset-http2-record-breaking-ddos-attack/",
        "loadedTime": "2023-12-05T02:31:14.662Z",
        "referrerUrl": "https://blog.cloudflare.com/page/2/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/zero-day-rapid-reset-http2-record-breaking-ddos-attack/",
        "title": "HTTP/2 Zero-Day vulnerability results in record-breaking DDoS attacks",
        "description": "The “HTTP/2 Rapid Reset” attack exploits a weakness in the HTTP/2 protocol to generate enormous, hyper-volumetric DDoS attacks. Cloudflare has mitigated a barrage of these attacks in recent months, including an attack three times larger than any previous attack we’ve observed",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/10/2023\n8 min read\nThis post is also available in 简体中文, 繁體中文, 日本語, 한국어, Deutsch, Français and Español.\nEarlier today, Cloudflare, along with Google and Amazon AWS, disclosed the existence of a novel zero-day vulnerability dubbed the “HTTP/2 Rapid Reset” attack. This attack exploits a weakness in the HTTP/2 protocol to generate enormous, hyper-volumetric Distributed Denial of Service (DDoS) attacks. Cloudflare has mitigated a barrage of these attacks in recent months, including an attack three times larger than any previous attack we’ve observed, which exceeded 201 million requests per second (rps). Since the end of August 2023, Cloudflare has mitigated more than 1,100 other attacks with over 10 million rps — and 184 attacks that were greater than our previous DDoS record of 71 million rps. \nUnder attack or need additional protection? Click here to get help.\nThis zero-day provided threat actors with a critical new tool in their Swiss Army knife of vulnerabilities to exploit and attack their victims at a magnitude that has never been seen before. While at times complex and challenging to combat, these attacks allowed Cloudflare the opportunity to develop purpose-built technology to mitigate the effects of the zero-day vulnerability.\nIf you are using Cloudflare for HTTP DDoS mitigation, you are protected. And below, we’ve included more information on this vulnerability, and resources and recommendations on what you can do to secure yourselves.\nDeconstructing the attack: What every CSO needs to know\nIn late August 2023, our team at Cloudflare noticed a new zero-day vulnerability, developed by an unknown threat actor, that exploits the standard HTTP/2 protocol — a fundamental protocol that is critical to how the Internet and all websites work. This novel zero-day vulnerability attack, dubbed Rapid Reset, leverages HTTP/2’s stream cancellation feature by sending a request and immediately canceling it over and over. \nBy automating this trivial “request, cancel, request, cancel” pattern at scale, threat actors are able to create a denial of service and take down any server or application running the standard implementation of HTTP/2. Furthermore, one crucial thing to note about the record-breaking attack is that it involved a modestly-sized botnet, consisting of roughly 20,000 machines. Cloudflare regularly detects botnets that are orders of magnitude larger than this — comprising hundreds of thousands and even millions of machines. For a relatively small botnet to output such a large volume of requests, with the potential to incapacitate nearly any server or application supporting HTTP/2, underscores how menacing this vulnerability is for unprotected networks.\nThreat actors used botnets in tandem with the HTTP/2 vulnerability to amplify requests at rates we have never seen before. As a result, our team at Cloudflare experienced some intermittent edge instability. While our systems were able to mitigate the overwhelming majority of incoming attacks, the volume overloaded some components in our network, impacting a small number of customers’ performance with intermittent 4xx and 5xx errors — all of which were quickly resolved. \nOnce we successfully mitigated these issues and halted potential attacks for all customers, our team immediately kicked off a responsible disclosure process. We entered into conversations with industry peers to see how we could work together to help move our mission forward and safeguard the large percentage of the Internet that relies on our network prior to releasing this vulnerability to the general public. \nWe cover the technical details of the attack in more detail in a separate blog post: HTTP/2 Rapid Reset: deconstructing the record-breaking attack.\nHow is Cloudflare and the industry thwarting this attack?\nThere is no such thing as a “perfect disclosure.” Thwarting attacks and responding to emerging incidents requires organizations and security teams to live by an assume-breach mindset — because there will always be another zero-day, new evolving threat actor groups, and never-before-seen novel attacks and techniques. \nThis “assume-breach” mindset is a key foundation towards information sharing and ensuring in instances such as this that the Internet remains safe. While Cloudflare was experiencing and mitigating these attacks, we were also working with industry partners to guarantee that the industry at-large could withstand this attack. \nDuring the process of mitigating this attack, our Cloudflare team developed and purpose-built new technology to stop these DDoS attacks and further improve our own mitigations for this and other future attacks of massive scale. These efforts have significantly increased our overall mitigation capabilities and resiliency. If you are using Cloudflare, we are confident that you are protected. \nOur team also alerted web server software partners who are developing patches to ensure this vulnerability cannot be exploited — check their websites for more information.\nDisclosures are never one and done. The lifeblood of Cloudflare is to ensure a better Internet, which stems from instances such as these. When we have the opportunity to work with our industry partners and governments to ensure there are no widespread impacts on the Internet, we are doing our part in increasing the cyber resiliency of every organization no matter the size or vertical.\nTo gain more of an understanding around mitigation tactics and next steps on patching, register for our webinar.\nWhat are the origins of the HTTP/2 Rapid Reset and these record-breaking attacks on Cloudflare?\nIt may seem odd that Cloudflare was one of the first companies to witness these attacks. Why would threat actors attack a company that has some of the most robust defenses against DDoS attacks in the world? \nThe reality is that Cloudflare often sees attacks before they are turned on more vulnerable targets. Threat actors need to develop and test their tools before they deploy them in the wild. Threat actors who possess record-shattering attack methods can have an extremely difficult time testing and understanding how large and effective they are, because they don't have the infrastructure to absorb the attacks they are launching. Because of the transparency that we share on our network performance, and the measurements of attacks they could glean from our public performance charts, this threat actor was likely targeting us to understand the capabilities of the exploit.\nBut that testing, and the ability to see the attack early, helps us develop mitigations for the attack that benefit both our customers and industry as a whole.\nFrom CSO to CSO: What should you do?\nI have been a CSO for over 20 years, on the receiving end of countless disclosures and announcements like this. But whether it was Log4J, Solarwinds, EternalBlue WannaCry/NotPetya, Heartbleed, or Shellshock, all of these security incidents have a commonality. A tremendous explosion that ripples across the world and creates an opportunity to completely disrupt any of the organizations that I have led — regardless of the industry or the size. \nMany of these were attacks or vulnerabilities that we may have not been able to control. But regardless of whether the issue arose from something that was in my control or not, what has set any successful initiative I have led apart from those that did not lean in our favor was the ability to respond when zero-day vulnerabilities and exploits like this are identified. \nWhile I wish I could say that Rapid Reset may be different this time around, it is not. I am calling all CSOs — no matter if you’ve lived through the decades of security incidents that I have, or this is your first day on the job — this is the time to ensure you are protected and stand up your cyber incident response team. \nWe’ve kept the information restricted until today to give as many security vendors as possible the opportunity to react. However, at some point, the responsible thing becomes to publicly disclose zero-day threats like this. Today is that day. That means that after today, threat actors will be largely aware of the HTTP/2 vulnerability; and it will inevitably become trivial to exploit and kickoff the race between defenders and attacks — first to patch vs. first to exploit. Organizations should assume that systems will be tested, and take proactive measures to ensure protection.\nTo me, this is reminiscent of a vulnerability like Log4J, due to the many variants that are emerging daily, and will continue to come to fruition in the weeks, months, and years to come. As more researchers and threat actors experiment with the vulnerability, we may find different variants with even shorter exploit cycles that contain even more advanced bypasses. \nAnd just like Log4J, managing incidents like this isn’t as simple as “run the patch, now you’re done”. You need to turn incident management, patching, and evolving your security protections into ongoing processes — because the patches for each variant of a vulnerability reduce your risk, but they don’t eliminate it.\nI don’t mean to be alarmist, but I will be direct: you must take this seriously. Treat this as a full active incident to ensure nothing happens to your organization.\nRecommendations for a New Standard of Change\nWhile no one security event is ever identical to the next, there are lessons that can be learned. CSOs, here are my recommendations that must be implemented immediately. Not only in this instance, but for years to come:\nUnderstand your external and partner network’s external connectivity to remediate any Internet facing systems with the mitigations below.\nUnderstand your existing security protection and capabilities you have to protect, detect and respond to an attack and immediately remediate any issues you have in your network.\nEnsure your DDoS Protection resides outside of your data center because if the traffic gets to your datacenter, it will be difficult to mitigate the DDoS attack.\nEnsure you have DDoS protection for Applications (Layer 7) and ensure you have Web Application Firewalls. Additionally as a best practice, ensure you have complete DDoS protection for DNS, Network Traffic (Layer 3) and API Firewalls\nEnsure web server and operating system patches are deployed across all Internet Facing Web Servers. Also, ensure all automation like Terraform builds and images are fully patched so older versions of web servers are not deployed into production over the secure images by accident.\nAs a last resort, consider turning off HTTP/2 and HTTP/3 (likely also vulnerable) to mitigate the threat. This is a last resort only, because there will be a significant performance issues if you downgrade to HTTP/1.1\nConsider a secondary, cloud-based DDoS L7 provider at perimeter for resilience.\nCloudflare’s mission is to help build a better Internet. If you are concerned with your current state of DDoS protection, we are more than happy to provide you with our DDoS capabilities and resilience for free to mitigate any attempts of a successful DDoS attack. We know the stress that you are facing as we have fought off these attacks for the last 30 days and made our already best in class systems, even better. \nIf you’re interested in finding out more, view our webinar on the details of the zero-day and how to respond. Contact us if you’re unsure whether you’re protected or want to understand how you can be. We also have more technical details of the attack in more detail in a separate blog post: HTTP/2 Rapid Reset: deconstructing the record-breaking attack. Finally, if you’re being targeted or need immediate protection, please contact your local Cloudflare representative or visit https://www.cloudflare.com/under-attack-hotline/. \nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nSecurity Vulnerabilities Attacks DDoS",
      "markdown": "10/10/2023\n\n*   [![Grant Bourzikas](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/04/Headshot---GB_LinkedIn.jpg)](https://blog.cloudflare.com/author/grant/)\n\n8 min read\n\nThis post is also available in [简体中文](https://blog.cloudflare.com/zh-cn/zero-day-rapid-reset-http2-record-breaking-ddos-attack-zh-cn/), [繁體中文](https://blog.cloudflare.com/zh-tw/zero-day-rapid-reset-http2-record-breaking-ddos-attack-zh-tw/), [日本語](https://blog.cloudflare.com/ja-jp/zero-day-rapid-reset-http2-record-breaking-ddos-attack-ja-jp/), [한국어](https://blog.cloudflare.com/ko-kr/zero-day-rapid-reset-http2-record-breaking-ddos-attack-ko-kr/), [Deutsch](https://blog.cloudflare.com/de-de/zero-day-rapid-reset-http2-record-breaking-ddos-attack-de-de/), [Français](https://blog.cloudflare.com/fr-fr/zero-day-rapid-reset-http2-record-breaking-ddos-attack-fr-fr/) and [Español](https://blog.cloudflare.com/es-es/zero-day-rapid-reset-http2-record-breaking-ddos-attack-es-es/).\n\n![](https://blog.cloudflare.com/content/images/2023/10/Screenshot-2023-10-09-at-10.41.56-PM.png)\n\nEarlier today, Cloudflare, along with Google and Amazon AWS, disclosed the existence of a novel zero-day vulnerability dubbed the “HTTP/2 Rapid Reset” attack. This attack exploits a weakness in the HTTP/2 protocol to generate enormous, hyper-volumetric Distributed Denial of Service (DDoS) attacks. Cloudflare has mitigated a barrage of these attacks in recent months, including an attack three times larger than [any previous attack we’ve observed](https://blog.cloudflare.com/cloudflare-mitigates-record-breaking-71-million-request-per-second-ddos-attack/), which exceeded 201 million requests per second (rps). Since the end of August 2023, Cloudflare has mitigated more than 1,100 other attacks with over 10 million rps — and 184 attacks that were greater than our previous DDoS record of 71 million rps.\n\n_Under attack or need additional protection? [Click here to get help](https://www.cloudflare.com/h2/)._\n\n  \n\nThis zero-day provided threat actors with a critical new tool in their Swiss Army knife of vulnerabilities to exploit and attack their victims at a magnitude that has never been seen before. While at times complex and challenging to combat, these attacks allowed Cloudflare the opportunity to develop purpose-built technology to mitigate the effects of the zero-day vulnerability.\n\nIf you are using Cloudflare for HTTP DDoS mitigation, you are protected. And below, we’ve included more information on this vulnerability, and resources and recommendations on what you can do to secure yourselves.\n\n### Deconstructing the attack: What every CSO needs to know\n\nIn late August 2023, our team at Cloudflare noticed a new zero-day vulnerability, developed by an unknown threat actor, that exploits the standard HTTP/2 protocol — a fundamental protocol that is critical to how the Internet and all websites work. This novel zero-day vulnerability attack, dubbed Rapid Reset, leverages HTTP/2’s stream cancellation feature by sending a request and immediately canceling it over and over.  \n\nBy automating this trivial “request, cancel, request, cancel” pattern at scale, threat actors are able to create a denial of service and take down any server or application running the standard implementation of HTTP/2. Furthermore, one crucial thing to note about the record-breaking attack is that it involved a modestly-sized botnet, consisting of roughly 20,000 machines. Cloudflare regularly detects botnets that are orders of magnitude larger than this — comprising hundreds of thousands and even millions of machines. For a relatively small botnet to output such a large volume of requests, with the potential to incapacitate nearly any server or application supporting HTTP/2, underscores how menacing this vulnerability is for unprotected networks.\n\nThreat actors used botnets in tandem with the HTTP/2 vulnerability to amplify requests at rates we have never seen before. As a result, our team at Cloudflare experienced some intermittent edge instability. While our systems were able to mitigate the overwhelming majority of incoming attacks, the volume overloaded some components in our network, impacting a small number of customers’ performance with intermittent 4xx and 5xx errors — all of which were quickly resolved.\n\nOnce we successfully mitigated these issues and halted potential attacks for all customers, our team immediately kicked off a responsible disclosure process. We entered into conversations with industry peers to see how we could work together to help move our mission forward and safeguard the large percentage of the Internet that relies on our network prior to releasing this vulnerability to the general public.\n\nWe cover the technical details of the attack in more detail in a separate blog post: [HTTP/2 Rapid Reset: deconstructing the record-breaking attack](https://cfl.re/rapid-reset-breakdown).\n\n### How is Cloudflare and the industry thwarting this attack?\n\nThere is no such thing as a “perfect disclosure.” Thwarting attacks and responding to emerging incidents requires organizations and security teams to live by an assume-breach mindset — because there will always be another zero-day, new evolving threat actor groups, and never-before-seen novel attacks and techniques.\n\nThis “assume-breach” mindset is a key foundation towards information sharing and ensuring in instances such as this that the Internet remains safe. While Cloudflare was experiencing and mitigating these attacks, we were also working with industry partners to guarantee that the industry at-large could withstand this attack.  \n\nDuring the process of mitigating this attack, our Cloudflare team developed and purpose-built new technology to stop these DDoS attacks and further improve our own mitigations for this and other future attacks of massive scale. These efforts have significantly increased our overall mitigation capabilities and resiliency. If you are using Cloudflare, we are confident that you are protected.\n\nOur team also alerted web server software partners who are developing patches to ensure this vulnerability cannot be exploited — check their websites for more information.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Zero-Day-protection-4.png)\n\nDisclosures are never one and done. The lifeblood of Cloudflare is to ensure a better Internet, which stems from instances such as these. When we have the opportunity to work with our industry partners and governments to ensure there are no widespread impacts on the Internet, we are doing our part in increasing the cyber resiliency of every organization no matter the size or vertical.\n\nTo gain more of an understanding around mitigation tactics and next steps on patching, [register for our webinar](https://event.on24.com/wcc/r/4378646/EC4EB4A6CE2B363BC6378891E495BEBF).\n\n### What are the origins of the HTTP/2 Rapid Reset and these record-breaking attacks on Cloudflare?\n\nIt may seem odd that Cloudflare was one of the first companies to witness these attacks. Why would threat actors attack a company that has some of the most robust defenses against DDoS attacks in the world?  \n\nThe reality is that Cloudflare often sees attacks before they are turned on more vulnerable targets. Threat actors need to develop and test their tools before they deploy them in the wild. Threat actors who possess record-shattering attack methods can have an extremely difficult time testing and understanding how large and effective they are, because they don't have the infrastructure to absorb the attacks they are launching. Because of the transparency that we share on our network performance, and the measurements of attacks they could glean from our public performance charts, this threat actor was likely targeting us to understand the capabilities of the exploit.\n\nBut that testing, and the ability to see the attack early, helps us develop mitigations for the attack that benefit both our customers and industry as a whole.\n\n### From CSO to CSO: What should you do?\n\nI have been a CSO for over 20 years, on the receiving end of countless disclosures and  announcements like this. But whether it was [Log4J](https://blog.cloudflare.com/exploitation-of-cve-2021-44228-before-public-disclosure-and-evolution-of-waf-evasion-patterns/), [Solarwinds](https://blog.cloudflare.com/solarwinds-orion-compromise-trend-data/), [EternalBlue](https://www.cloudflare.com/learning/security/ransomware/how-to-prevent-ransomware/) [WannaCry/NotPetya](https://www.cloudflare.com/learning/security/ransomware/petya-notpetya-ransomware/), [Heartbleed](https://blog.cloudflare.com/heartbleed-revisited/), or [Shellshock](https://blog.cloudflare.com/inside-shellshock/), all of these security incidents have a commonality. A tremendous explosion that ripples across the world and creates an opportunity to completely disrupt any of the organizations that I have led — regardless of the industry or the size.\n\nMany of these were attacks or vulnerabilities that we may have not been able to control. But regardless of whether the issue arose from something that was in my control or not, what has set any successful initiative I have led apart from those that did not lean in our favor was the ability to respond when zero-day vulnerabilities and exploits like this are identified.    \n\nWhile I wish I could say that Rapid Reset may be different this time around, it is not. I am calling all CSOs — no matter if you’ve lived through the decades of security incidents that I have, or this is your first day on the job — this is the time to ensure you are protected and stand up your cyber incident response team.\n\nWe’ve kept the information restricted until today to give as many security vendors as possible the opportunity to react. However, at some point, the responsible thing becomes to publicly disclose zero-day threats like this. Today is that day. That means that after today, threat actors will be largely aware of the HTTP/2 vulnerability; and it will inevitably become trivial to exploit and kickoff the race between defenders and attacks — first to patch vs. first to exploit. Organizations should assume that systems will be tested, and take proactive measures to ensure protection.\n\nTo me, this is reminiscent of a vulnerability like Log4J, due to the many variants that are emerging daily, and will continue to come to fruition in the weeks, months, and years to come. As more researchers and threat actors experiment with the vulnerability, we may find different variants with even shorter exploit cycles that contain even more advanced bypasses.  \n\nAnd just like Log4J, managing incidents like this isn’t as simple as “run the patch, now you’re done”. You need to turn incident management, patching, and evolving your security protections into ongoing processes — because the patches for each variant of a vulnerability reduce your risk, but they don’t eliminate it.\n\nI don’t mean to be alarmist, but I will be direct: you must take this seriously. Treat this as a full active incident to ensure nothing happens to your organization.\n\n### Recommendations for a New Standard of Change\n\nWhile no one security event is ever identical to the next, there are lessons that can be learned. CSOs, here are my recommendations that must be implemented immediately. Not only in this instance, but for years to come:\n\n*   Understand your external and partner network’s external connectivity to remediate any Internet facing systems with the mitigations below.\n*   Understand your existing security protection and capabilities you have to protect, detect and respond to an attack and immediately remediate any issues you have in your network.\n*   Ensure your DDoS Protection resides outside of your data center because if the traffic gets to your datacenter, it will be difficult to mitigate the DDoS attack.\n*   Ensure you have DDoS protection for Applications (Layer 7) and ensure you have Web Application Firewalls. Additionally as a best practice, ensure you have complete DDoS protection for DNS, Network Traffic (Layer 3) and API Firewalls\n*   Ensure web server and operating system patches are deployed across all Internet Facing Web Servers. Also, ensure all automation like Terraform builds and images are fully patched so older versions of web servers are not deployed into production over the secure images by accident.\n*   As a last resort, consider turning off HTTP/2 and HTTP/3 (likely also vulnerable) to mitigate the threat.  This is a last resort only, because there will be a significant performance issues if you downgrade to HTTP/1.1\n*   Consider a secondary, cloud-based DDoS L7 provider at perimeter for resilience.\n\nCloudflare’s mission is to help build a better Internet. If you are concerned with your current state of DDoS protection, we are more than happy to provide you with our DDoS capabilities and resilience for free to mitigate any attempts of a successful DDoS attack.  We know the stress that you are facing as we have fought off these attacks for the last 30 days and made our already best in class systems, even better.\n\nIf you’re interested in finding out more, [view our webinar](https://event.on24.com/wcc/r/4378646/EC4EB4A6CE2B363BC6378891E495BEBF) on the details of the zero-day and how to respond. [Contact us](https://www.cloudflare.com/h2/) if you’re unsure whether you’re protected or want to understand how you can be. We also have more technical details of the attack in more detail in a separate blog post: [HTTP/2 Rapid Reset: deconstructing the record-breaking attack](https://cfl.re/rapid-reset-breakdown). Finally, if you’re being targeted or need immediate protection, please contact your local Cloudflare representative or visit [https://www.cloudflare.com/under-attack-hotline/](https://www.cloudflare.com/under-attack-hotline/).\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Security](https://blog.cloudflare.com/tag/security/) [Vulnerabilities](https://blog.cloudflare.com/tag/vulnerabilities/) [Attacks](https://blog.cloudflare.com/tag/attacks/) [DDoS](https://blog.cloudflare.com/tag/ddos/)"
    },
    {
      "url": "https://blog.cloudflare.com/uncovering-the-hidden-webp-vulnerability-cve-2023-4863/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/uncovering-the-hidden-webp-vulnerability-cve-2023-4863/",
        "loadedTime": "2023-12-05T02:31:27.936Z",
        "referrerUrl": "https://blog.cloudflare.com/page/2/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/uncovering-the-hidden-webp-vulnerability-cve-2023-4863/",
        "title": "Uncovering the Hidden WebP vulnerability: a tale of a CVE with much bigger implications than it originally seemed",
        "description": "Recently, Google announced a security issue in Google Chrome, titled \"Heap buffer overflow in WebP in Google Chrome.\" Initially, it seemed like just another bug in the popular web browser. However, what we discovered was far more significant and had implications that extended well beyond Chrome",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/05/2023\n6 min read\nAt Cloudflare, we're constantly vigilant when it comes to identifying vulnerabilities that could potentially affect the Internet ecosystem. Recently, on September 12, 2023, Google announced a security issue in Google Chrome, titled \"Heap buffer overflow in WebP in Google Chrome,\" which caught our attention. Initially, it seemed like just another bug in the popular web browser. However, what we discovered was far more significant and had implications that extended well beyond Chrome.\nImpact much wider than suggested\nThe vulnerability, tracked under CVE-2023-4863, was described as a heap buffer overflow in WebP within Google Chrome. While this description might lead one to believe that it's a problem confined solely to Chrome, the reality was quite different. It turned out to be a bug deeply rooted in the libwebp library, which is not only used by Chrome but by virtually every application that handles WebP images.\nDigging deeper, this vulnerability was in fact first reported in an earlier CVE from Apple, CVE-2023-41064, although the connection was not immediately obvious. In early September, Citizen Lab, a research lab based out of the University of Toronto, reported on an apparent exploit that was being used to attempt to install spyware on the iPhone of \"an individual employed by a Washington DC-based civil society organization.\" The advisory from Apple was also incomplete, stating that it was a “buffer overflow issue in ImageIO,” and that they were aware the issue may have been actively exploited. Only after Google released CVE-2023-4863 did it become clear that these two issues were linked, and there was a wider vulnerability in WebP.\nThe vulnerability allows an attacker to create a malformed WebP image file that makes libwebp write data beyond the buffer memory allocated to the image decoder. By writing past the legal bounds of the buffer, it is possible to modify sensitive data in memory, eventually leading to execution of the attacker's code. \nWebP, introduced over a decade ago, has gained widespread adoption in various applications, ranging from web browsers to email clients, chat apps, graphics programs, and even operating systems. This ubiquity meant that this vulnerability had far-reaching consequences, affecting a vast array of software and virtually all users of the WebP format.\nHow the WebP vulnerability is exploited\nUnderstanding the technical details\nSo what exactly was the issue, how could it be exploited, and how was it shut down? We can get our best clues by looking at the patch that was made to libwebp. This patch fixes a potential out-of-buffer (OOB) error in part of the image decoder – the Huffman tables – with two changes: additional validation of the input data, and a modified dynamic memory allocation model. A deeper dive into libwebp and the WebP image format built on top of it reveals what this means.\nWebP is a combination of two different image formats: a lossy format similar to JPEG using VP8 codec, and a lossless format using WebP's custom lossless codec. The bug was in the lossless codec's handling of Huffman coding.\nThe fundamental idea behind Huffman coding is that using a constant number of bits for every basic unit of information in a dataset – like a pixel color – is not the most efficient representation. We can use a variable number of bits, and assign shortest sequences to the most frequently occurring values, and longer ones to the least common values. The sequences of ones and zeros can be represented as a binary tree, with the shorter, more common codes near the root, and longer, less common codes deeper in the tree. Looking up values in the tree bit by bit is relatively slow. Practical implementations build lookup tables that allow matching many bits at a time.\nImage files contain compact information about the shape of the Huffman tree, which the decoder uses to reconstruct the tree, and build lookup tables for the codes. The bug in libwebp was in the code building the lookup tables. A specially crafted WebP file can contain a very unbalanced Huffman tree that contains codes much longer than any normal WebP file would have, and this made the function generating lookup tables write data beyond the buffer allocated for the lookup tables. Libwebp had checks for validity of the Huffman tree, but it would write the invalid lookup tables before the consistency check.\nThe buffer for lookup tables is allocated on the heap. Heap is an area of memory where most of the data of the application is stored. Code that writes data past its buffer allows attackers to modify and corrupt data that happens to be adjacent in memory to the buffer. This can be exploited to make the application misbehave, and eventually start executing code supplied by the attacker. \nThe fixed version of libwebp ensures that the input data will always create a valid internal structure, and if so, allocates more memory if necessary to ensure the buffer is always big enough. \nLibwebp is a mature library, maintained by seasoned professionals. But it's written in the C language, which has very few safeguards against programming errors, especially memory use. Despite the care taken in the library's development, a single erroneous assumption led to a critical vulnerability.\nSwift action\nOn the same day that Google's announcement caught our attention, we filed an internal security ticket, to document and address the vulnerability.\nGoogle was initially perplexed about the true source of the problem. They did not release a patched version of libwebp before announcing the vulnerability. We discovered the yet-unreleased patch for libwebp in its repository, and used it to update libwebp in our services. libwebp officially released the patch a day later. \nOur image processing services are written in Rust. We've submitted patches to Rust packages that contained a copy of libwebp and filed RustSec advisories for them (RUSTSEC-2023-0061 and RUSTSEC-2023-0062). This ensured that the broader Rust ecosystem was informed and could take appropriate action.\nIn an interesting turn of events, GitHub's vulnerability scanner was quick to recognize our RustSec reports as the first case of CVE-2023-4863, even before the issue gained widespread attention. This highlights the importance of having robust security reporting mechanisms in place and the vital role that platforms like GitHub play in keeping the open-source community secure.\nThese quick actions demonstrate how seriously Cloudflare takes this kind of threat. We have a belt-and-suspenders approach to security that limits the binaries we run at our edge to those signed by us, and ensures that all vulnerabilities are identified and remedied as soon as possible. In this case, we have scrutinized our logs, and found no evidence that any attackers attempted to leverage this vulnerability against Cloudflare. We believe this exploit targeted individuals rather than the infrastructure of a company like Cloudflare, but we never take chances with our customers’ data, and so fixed this vulnerability as quickly as possible, before it became well known.\nConclusion\nGoogle has now widened its description of this issue, correctly calling out that all uses of WebP are potentially affected. This widened description was originally filed as yet another new CVE – CVE-2023-5129 – but then that was flagged as a duplicate of the original CVE-2023-4863, and the description of the earlier filing updated. This incident serves as a reminder of the complex and interconnected nature of the Internet ecosystem. What initially seemed like a Chrome-specific problem revealed a much deeper issue that touched nearly every corner of the digital world. The incident also showcased the importance of swift collaboration and the critical role that responsible disclosure plays in mitigating security risks.\nFor each and every user, it demonstrates the need to keep all browsers, apps and operating systems up to date, and to install recommended security patches. All applications supporting WebP images need to be updated. We've updated our services.\nAt Cloudflare, we remain committed to enhancing the security of the Internet, and incidents like these drive us to continually refine our processes and strengthen our partnerships within the global developer community. By working together, we can make the Internet a safer place for everyone.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nVulnerabilities Chrome WebP Security Swift",
      "markdown": "10/05/2023\n\n*   [![Willi Geiger](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/09/selfie-small.jpg)](https://blog.cloudflare.com/author/willi/)\n*   [![Kornel Lesiński](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/09/3a534a3c391d25cb34cb4078cb0c1148.jpg)](https://blog.cloudflare.com/author/kornel/)\n\n6 min read\n\n![](https://blog.cloudflare.com/content/images/2023/10/Uncovering-the-hidden-WebP-vulnerability.png)\n\nAt Cloudflare, we're constantly vigilant when it comes to identifying vulnerabilities that could potentially affect the Internet ecosystem. Recently, on September 12, 2023, Google announced a security issue in Google Chrome, titled \"Heap buffer overflow in WebP in Google Chrome,\" which caught our attention. Initially, it seemed like just another bug in the popular web browser. However, what we discovered was far more significant and had implications that extended well beyond Chrome.\n\n### Impact much wider than suggested\n\nThe vulnerability, tracked under [CVE-2023-4863](https://nvd.nist.gov/vuln/detail/CVE-2023-4863), was described as a heap buffer overflow in WebP within Google Chrome. While this description might lead one to believe that it's a problem confined solely to Chrome, the reality was quite different. It turned out to be a bug deeply rooted in the libwebp library, which is not only used by Chrome but by virtually every application that handles WebP images.\n\nDigging deeper, this vulnerability was in fact first reported in an earlier CVE from Apple, CVE-2023-41064, although the connection was not immediately obvious. In early September, Citizen Lab, a research lab based out of the University of Toronto, reported on an apparent exploit that was being used to attempt to install spyware on the iPhone of \"an individual employed by a Washington DC-based civil society organization.\" The advisory from Apple was also incomplete, stating that it was a “buffer overflow issue in ImageIO,” and that they were aware the issue may have been actively exploited. Only after Google released CVE-2023-4863 did it become clear that these two issues were linked, and there was a wider vulnerability in WebP.\n\nThe vulnerability allows an attacker to create a malformed WebP image file that makes libwebp write data beyond the buffer memory allocated to the image decoder. By writing past the legal bounds of the buffer, it is possible to modify sensitive data in memory, eventually leading to execution of the attacker's code.\n\nWebP, introduced over a decade ago, has gained widespread adoption in various applications, ranging from web browsers to email clients, chat apps, graphics programs, and even operating systems. This ubiquity meant that this vulnerability had far-reaching consequences, affecting a vast array of software and virtually all users of the WebP format.\n\n![](https://blog.cloudflare.com/content/images/2023/10/image2-3.png)\n\n_How the WebP vulnerability is exploited_\n\n### Understanding the technical details\n\nSo what exactly was the issue, how could it be exploited, and how was it shut down? We can get our best clues by looking at [the patch that was made to libwebp](https://chromium.googlesource.com/webm/libwebp/+/902bc9190331343b2017211debcec8d2ab87e17a%5E%21/#F0). This patch fixes a potential out-of-buffer (OOB) error in part of the image decoder – the Huffman tables – with two changes: additional validation of the input data, and a modified dynamic memory allocation model. A deeper dive into libwebp and the WebP image format built on top of it reveals what this means.\n\nWebP is a combination of two different image formats: a lossy format similar to JPEG using VP8 codec, and a lossless format using WebP's custom lossless codec. The bug was in the lossless codec's handling of Huffman coding.\n\nThe fundamental idea behind Huffman coding is that using a constant number of bits for every basic unit of information in a dataset – like a pixel color – is not the most efficient representation. We can use a variable number of bits, and assign shortest sequences to the most frequently occurring values, and longer ones to the least common values. The sequences of ones and zeros can be represented as a binary tree, with the shorter, more common codes near the root, and longer, less common codes deeper in the tree. Looking up values in the tree bit by bit is relatively slow. Practical implementations build lookup tables that allow matching many bits at a time.\n\nImage files contain compact information about the shape of the Huffman tree, which the decoder uses to reconstruct the tree, and build lookup tables for the codes. The bug in libwebp was in the code building the lookup tables. A specially crafted WebP file can contain a very unbalanced Huffman tree that contains codes much longer than any normal WebP file would have, and this made the function generating lookup tables write data beyond the buffer allocated for the lookup tables. Libwebp had checks for validity of the Huffman tree, but it would write the invalid lookup tables before the consistency check.\n\nThe buffer for lookup tables is allocated on the heap. Heap is an area of memory where most of the data of the application is stored. Code that writes data past its buffer allows attackers to modify and corrupt data that happens to be adjacent in memory to the buffer. This can be exploited to make the application misbehave, and eventually start executing code supplied by the attacker.\n\nThe fixed version of libwebp ensures that the input data will always create a valid internal structure, and if so, allocates more memory if necessary to ensure the buffer is always big enough.\n\nLibwebp is a mature library, maintained by seasoned professionals. But it's written in the C language, which has very few safeguards against programming errors, especially memory use. Despite the care taken in the library's development, a single erroneous assumption led to a critical vulnerability.\n\n### Swift action\n\nOn the same day that Google's announcement caught our attention, we filed an internal security ticket, to document and address the vulnerability.\n\nGoogle was initially perplexed about the true source of the problem. They did not release a patched version of libwebp before announcing the vulnerability. We discovered the yet-unreleased patch for libwebp in its repository, and used it to update libwebp in our services. libwebp officially released the patch a day later.\n\nOur image processing services are written in Rust. We've submitted patches to Rust packages that contained a copy of libwebp and filed RustSec advisories for them ([RUSTSEC-2023-0061](https://rustsec.org/advisories/RUSTSEC-2023-0061.html) and [RUSTSEC-2023-0062](https://rustsec.org/advisories/RUSTSEC-2023-0062.html)). This ensured that the broader Rust ecosystem was informed and could take appropriate action.\n\nIn an interesting turn of events, GitHub's vulnerability scanner was quick to recognize our RustSec reports as the first case of CVE-2023-4863, even before the issue gained widespread attention. This highlights the importance of having robust security reporting mechanisms in place and the vital role that platforms like GitHub play in keeping the open-source community secure.\n\nThese quick actions demonstrate how seriously Cloudflare takes this kind of threat. We have a belt-and-suspenders approach to security that limits the binaries we run at our edge to those signed by us, and ensures that all vulnerabilities are identified and remedied as soon as possible. In this case, we have scrutinized our logs, and found no evidence that any attackers attempted to leverage this vulnerability against Cloudflare. We believe this exploit targeted individuals rather than the infrastructure of a company like Cloudflare, but we never take chances with our customers’ data, and so fixed this vulnerability as quickly as possible, before it became well known.\n\n### Conclusion\n\nGoogle has now widened its description of this issue, correctly calling out that all uses of WebP are potentially affected. This widened description was originally filed as yet another new CVE – [CVE-2023-5129](https://nvd.nist.gov/vuln/detail/CVE-2023-5129) – but then that was flagged as a duplicate of the original CVE-2023-4863, and the description of the earlier filing updated. This incident serves as a reminder of the complex and interconnected nature of the Internet ecosystem. What initially seemed like a Chrome-specific problem revealed a much deeper issue that touched nearly every corner of the digital world. The incident also showcased the importance of swift collaboration and the critical role that responsible disclosure plays in mitigating security risks.\n\nFor each and every user, it demonstrates the need to keep all browsers, apps and operating systems up to date, and to install recommended security patches. All applications supporting WebP images need to be updated. We've updated our services.\n\nAt Cloudflare, we remain committed to enhancing the security of the Internet, and incidents like these drive us to continually refine our processes and strengthen our partnerships within the global developer community. By working together, we can make the Internet a safer place for everyone.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Vulnerabilities](https://blog.cloudflare.com/tag/vulnerabilities/) [Chrome](https://blog.cloudflare.com/tag/chrome/) [WebP](https://blog.cloudflare.com/tag/webp/) [Security](https://blog.cloudflare.com/tag/security/) [Swift](https://blog.cloudflare.com/tag/swift/)"
    },
    {
      "url": "https://blog.cloudflare.com/cloudflares-a-top-100-most-loved-workplace-for-2023/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/cloudflares-a-top-100-most-loved-workplace-for-2023/",
        "loadedTime": "2023-12-05T02:31:29.458Z",
        "referrerUrl": "https://blog.cloudflare.com/page/2/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/cloudflares-a-top-100-most-loved-workplace-for-2023/",
        "title": "Cloudflare's a Top 100 Most Loved Workplace for the second consecutive year in 2023",
        "description": "We are proud to share that Cloudflare has been certified and recognized as one of the Top 100 Most Loved Workplaces in 2023 by Newsweek and the Best Practice Institute (BPI) for the second consecutive year.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Cloudflare's a Top 100 Most Loved Workplace for the second consecutive year in 2023\n10/05/2023\n5 min read\nWe have always strived to make Cloudflare somewhere where our entire team feels safe and empowered to bring their whole selves to work. It’s the best way to enable the many incredible people we have working here to be able to do their best work. With that as context, we are proud to share that Cloudflare has been certified and recognized as one of the Top 100 Most Loved Workplaces in 2023 by Newsweek and the Best Practice Institute (BPI) for the second consecutive year. \nCloudflare’s ranking follows surveys of more than 2 million employees at companies with team sizes ranging from 50 to 10,000+, and includes US-based firms and international companies with a strong US presence. As part of the qualification for the certification, Cloudflare participated in a company-wide global employee survey — so this award isn’t a hypothetical, it’s driven by our employees’ sentiment and responses. \nWith this recognition, we wanted to reflect on what’s new, what’s remained the same, and what’s ahead for the team at Cloudflare. There are a few things that especially stand out:\nIt starts with our mission and people\nHelping to build a better Internet.\nIf you speak to any member of the Cloudflare team about why they’re here, they’ll almost certainly talk about our mission. Whether it’s in our careers, or our lives more generally, so many of us have been positively impacted by the Internet. It is an incredible resource for humanity, and being able to contribute back to it is definitely a draw for many Cloudflarians. In an internal survey from September 2023, 92% of our team stated that they are inspired by Cloudflare’s mission to help build a better Internet — and 88% said that their work is important to the company.\nWorking on these kinds of problems, at the scale Cloudflare is at, requires constant innovation and figuring out solutions at the frontier of technology. At the same time, Cloudflare for years has been giving back to the community and society — through programs like Project Galileo, Athenian Project, and Project Cybersafe Schools — that make us especially proud of the work we do.\nCloudflarians get a real sense of our company culture, team, mission, and how we work, right when they start interviewing with us. Candidate feedback paints a picture of what this looks and feels like:\n“The entire process is really different from anything I’ve experienced before — and I’m enjoying it very much. Also, the informal tone makes it much more human and everyone is so approachable.” \n“I’ve never met a group of people at a company that are all consistently excited about the work they are doing.”\nBefore anyone gets an offer, every candidate in the final stage connects with one of our executive leaders including our co-founders. This has been part of our interview process for many years, and is designed to ensure candidates truly understand what they’re signing up for in their role as part of joining the Cloudflare team, and to make our executive team accessible to everyone starting on day one. Transparency is one of our core values, and we want all employees to have a direct line to our leadership.\nTransparency also serves as our guiding light when we engage with the public. We are quick to share detailed reports amid and after incidents, in the hopes that they will benefit the Internet-at-large. And we share regular impact reports detailing our progress around key environmental, social, and governance initiatives.\nWe have continued to receive unmatched interest with close to half a million applicants in the first half of 2023, which is nearly a 300% increase from 2022. Our offer acceptance rate remained equally as impressive at a rate of 90%. We are continuing to hire worldwide with hundreds of open positions across the organization ranging from Sales to Engineering.\nSupported & Flexible\nWe are committed to developing a globally distributed team with a flexible working approach. Individual teams opt into: hybrid, remote, or in-office, depending on what works best for the folks on the team, and the team more broadly. As an example, we've seen less tenured folks on certain teams wanting to go into the office more frequently to work together in-person. For reasons across the board, we give teams the ability to choose what works best for them. We’ve found that this approach gives teams the opportunity to establish their optimal working arrangements that fit with their objectives and enable collaboration. \nOur paid time off policy is on a take-what-you-need basis. We encourage employees to find a comfortable work-life balance by taking as many days off as they need while still being able to perform their jobs satisfactorily.\nWe want to empower and inspire our team members to do their best work every day — and this includes making sure they feel happy, healthy, and fulfilled both inside and outside of our workplace. Aside from comprehensive healthcare benefits, this also includes support for family planning like parental leave and Carrot Fertility, access to mental health and mindfulness programs through Ginger and Headspace apps, and three days of paid time off each year to volunteer in our respective communities.\nDiversity, equity, and inclusion is a priority at Cloudflare to help ensure a sense of belonging and community for all of our employees and to best propel our business forward. Our 15+ Employee Resource Groups (ERGs) are employee-led and have designated executive sponsors of each group. These communities come together to support each other, celebrate their cultures, and help with initiatives to educate, support professional development, and more. We have a robust number of ERGs and this continues to grow as the diversity at Cloudflare expands.\nConsistently growing our team\nOur employees believe that Cloudflare is a special place, with an especially meaningful mission. They believe there is no other place where a team so small is having such a large impact on the Internet. To keep up, it's critical that we have great people across the company to help us continue our mission. To learn more about the Cloudflare career opportunities, please check out Cloudflare Careers!\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nLife @ Cloudflare Careers People Employee Resource Groups \nRelated Posts\nJanuary 30, 2014 2:00PM\nStories from our recent global data center upgrade\nEach day at CloudFlare is full of surprises. As it turns out, it takes a lot of work to stop massive attacks and to help make the web faster. Over the past six months, our entire team has contributed in every way imaginable to more than double the capacity of our global network....\nBy \nJuly 30, 2021 2:00PM\nBuilding a sustainable workforce, through communities\nAt Cloudflare, we place a lot of value on the importance of diversity, equity and inclusion. Diversity, equity, and inclusion lead to better outcomes through improved decision-making, more innovative teams, stronger financial returns and simply a better place to work for everyone....\nBy \nJune 13, 2022 1:00AM\nCloudflare is redefining employee well-being in Japan\nCloudflare Japan is making a few important changes to our employee benefits...\nBy \nMay 20, 2022 2:00AM\nWendy Komadina: No one excited me more than Cloudflare, so I joined.\nWhen I considered joining Cloudflare, I recall consistently reading the message around “Helping to Build a Better Internet”. At first those words didn’t connect with me, but they sounded like an important mission....\nBy",
      "markdown": "## Cloudflare's a Top 100 Most Loved Workplace for the second consecutive year in 2023\n\n10/05/2023\n\n*   [![Scott Tomtania](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/11/Pictures.png)](https://blog.cloudflare.com/author/scott/)\n\n5 min read\n\n![](https://blog.cloudflare.com/content/images/2023/10/top-workplaces-2023-1.png)\n\nWe have always strived to make Cloudflare somewhere where our entire team feels safe and empowered to bring their whole selves to work. It’s the best way to enable the many incredible people we have working here to be able to do their best work. With that as context, we are proud to share that Cloudflare has been certified and recognized as one of the [Top 100 Most Loved Workplaces in 2023](https://www.newsweek.com/rankings/most-loved-workplaces-america-2023) by Newsweek and the Best Practice Institute (BPI) for the [second consecutive year](https://www.newsweek.com/rankings/americas-100-most-loved-workplaces-2022).  \n\n![](https://blog.cloudflare.com/content/images/2023/10/image2-2.png)\n\nCloudflare’s ranking follows surveys of more than 2 million employees at companies with team sizes ranging from 50 to 10,000+, and includes US-based firms and international companies with a strong US presence. As part of the qualification for the certification, Cloudflare participated in a company-wide global employee survey — so this award isn’t a hypothetical, it’s driven by our employees’ sentiment and responses.\n\nWith this recognition, we wanted to reflect on what’s new, what’s remained the same, and what’s ahead for the team at Cloudflare. There are a few things that especially stand out:\n\n### It starts with our mission and people\n\nHelping to build a better Internet.\n\nIf you speak to any member of the Cloudflare team about why they’re here, they’ll almost certainly talk about our mission. Whether it’s in our careers, or our lives more generally, so many of us have been positively impacted by the Internet. It is an incredible resource for humanity, and being able to contribute back to it is definitely a draw for many Cloudflarians. In an internal survey from September 2023, 92% of our team stated that they are inspired by Cloudflare’s mission to help build a better Internet — and 88% said that their work is important to the company.\n\nWorking on these kinds of problems, at the scale Cloudflare is at, requires constant innovation and figuring out solutions at the frontier of technology. At the same time, Cloudflare for years has been giving back to the community and society — through programs like [Project Galileo](https://www.cloudflare.com/galileo/), [Athenian Project](https://www.cloudflare.com/athenian/?cf_target_id=9D705F846ADA8ED51642F593D6DC6DD5), and [Project Cybersafe Schools](https://blog.cloudflare.com/project-cybersafe-schools/) — that make us especially proud of the work we do.\n\nCloudflarians get a real sense of our company culture, team, mission, and how we work, right when they start interviewing with us. Candidate feedback paints a picture of what this looks and feels like:\n\n> “_The entire process is really different from anything I’ve experienced before — and I’m enjoying it very much. Also, the informal tone makes it much more human and everyone is so approachable.”_  \n\n> _“I’ve never met a group of people at a company that are all consistently excited about the work they are doing.”_\n\nBefore anyone gets an offer, every candidate in the final stage connects with one of our executive leaders including our co-founders. This has been part of our interview process for many years, and is designed to ensure candidates truly understand what they’re signing up for in their role as part of joining the Cloudflare team, and to make our executive team accessible to everyone starting on day one. Transparency is one of our core values, and we want all employees to have a direct line to our leadership.\n\nTransparency also serves as our guiding light when we engage with the public. We are quick to share detailed reports amid and after incidents, in the hopes that they will benefit the Internet-at-large. And we share regular [impact reports](https://www.cloudflare.com/impact/) detailing our progress around key environmental, social, and governance initiatives.\n\nWe have continued to receive unmatched interest with close to half a million applicants in the first half of 2023, which is nearly a 300% increase from 2022. Our offer acceptance rate remained equally as impressive at a rate of 90%. We are continuing to hire worldwide with hundreds of [open positions across the organization](https://www.cloudflare.com/careers/jobs/?cf_target_id=50473D8E2531F4C1E6AC38C3C95EC6F0) ranging from Sales to Engineering.\n\n![](https://blog.cloudflare.com/content/images/2023/10/image4-2.png)\n\n### Supported & Flexible\n\nWe are committed to developing a globally distributed team with a flexible working approach. Individual teams opt into: hybrid, remote, or in-office, depending on what works best for the folks on the team, and the team more broadly. As an example, we've seen less tenured folks on certain teams wanting to go into the office more frequently to work together in-person. For reasons across the board, we give teams the ability to choose what works best for them. We’ve found that this approach gives teams the opportunity to establish their optimal working arrangements that fit with their objectives and enable collaboration.\n\nOur paid time off policy is on a take-what-you-need basis. We encourage employees to find a comfortable work-life balance by taking as many days off as they need while still being able to perform their jobs satisfactorily.\n\n![](https://blog.cloudflare.com/content/images/2023/10/image1-4.png)\n\nWe want to empower and inspire our team members to do their best work every day — and this includes making sure they feel happy, healthy, and fulfilled both inside and outside of our workplace. Aside from comprehensive healthcare benefits, this also includes support for family planning like parental leave and Carrot Fertility, access to mental health and mindfulness programs through Ginger and Headspace apps, and three days of paid time off each year to volunteer in our respective communities.\n\nDiversity, equity, and inclusion is a priority at Cloudflare to help ensure a sense of belonging and community for all of our employees and to best propel our business forward. Our 15+ [Employee Resource Groups](https://blog.cloudflare.com/tag/employee-resource-groups/) (ERGs) are employee-led and have designated executive sponsors of each group. These communities come together to support each other, celebrate their cultures, and help with initiatives to educate, support professional development, and more. We have a robust number of ERGs and this continues to grow as the diversity at Cloudflare expands.\n\n![](https://blog.cloudflare.com/content/images/2023/10/image3-2.png)\n\n### Consistently growing our team\n\nOur employees believe that Cloudflare is a special place, with an especially meaningful mission. They believe there is no other place where a team so small is having such a large impact on the Internet. To keep up, it's critical that we have great people across the company to help us continue our mission. To learn more about the Cloudflare career opportunities, please check out [Cloudflare Careers](https://www.cloudflare.com/careers/?cf_target_id=76B74A92829887311C4F43DFBC9A9757)!\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Life @ Cloudflare](https://blog.cloudflare.com/tag/life-at-cloudflare/) [Careers](https://blog.cloudflare.com/tag/careers/) [People](https://blog.cloudflare.com/tag/people/) [Employee Resource Groups](https://blog.cloudflare.com/tag/employee-resource-groups/)\n\nRelated Posts\n\nJanuary 30, 2014 2:00PM\n\n[\n\n## Stories from our recent global data center upgrade\n\n](https://blog.cloudflare.com/stories-from-our-recent-global-data-center-upgrade/)\n\nEach day at CloudFlare is full of surprises. As it turns out, it takes a lot of work to stop massive attacks and to help make the web faster. Over the past six months, our entire team has contributed in every way imaginable to more than double the capacity of our global network....\n\nBy \n\nJuly 30, 2021 2:00PM\n\n[\n\n## Building a sustainable workforce, through communities\n\n](https://blog.cloudflare.com/building-a-sustainable-workforce-through-communities/)\n\nAt Cloudflare, we place a lot of value on the importance of diversity, equity and inclusion. Diversity, equity, and inclusion lead to better outcomes through improved decision-making, more innovative teams, stronger financial returns and simply a better place to work for everyone....\n\nBy \n\nJune 13, 2022 1:00AM\n\n[\n\n## Cloudflare is redefining employee well-being in Japan\n\n](https://blog.cloudflare.com/cloudflare-is-redefining-employee-well-being-in-japan/)\n\nCloudflare Japan is making a few important changes to our employee benefits...\n\nBy \n\nMay 20, 2022 2:00AM\n\n[\n\n## Wendy Komadina: No one excited me more than Cloudflare, so I joined.\n\n](https://blog.cloudflare.com/wendy-komadina-no-one-excited-me-more-than-cloudflare-so-i-joined/)\n\nWhen I considered joining Cloudflare, I recall consistently reading the message around “Helping to Build a Better Internet”. At first those words didn’t connect with me, but they sounded like an important mission....\n\nBy"
    },
    {
      "url": "https://blog.cloudflare.com/virtual-networking-101-understanding-tap/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/virtual-networking-101-understanding-tap/",
        "loadedTime": "2023-12-05T02:31:29.024Z",
        "referrerUrl": "https://blog.cloudflare.com/page/2/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/virtual-networking-101-understanding-tap/",
        "title": "Virtual networking 101: Bridging the gap to understanding TAP",
        "description": "Tasked with optimizing Firecracker network performance, a virtual-machine-manager for \"Micro-VMs\", I decided to focus on understanding tap devices which are used for as a bridge for communication. Tap devices were historically used for VPN clients. Using them for virtual machines is essentially reversing their original purpose - from traffic sinks to traffic sources. In the article I explore the intricacies of tap devices, covering topics like offloads, segmentation, and multi-queue.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/06/2023\n12 min read\nIt's a never-ending effort to improve the performance of our infrastructure. As part of that quest, we wanted to squeeze as much network oomph as possible from our virtual machines. Internally for some projects we use Firecracker, which is a KVM-based virtual machine manager (VMM) that runs light-weight “Micro-VM”s. Each Firecracker instance uses a tap device to communicate with a host system. Not knowing much about tap, I had to up my game, however, it wasn't easy — the documentation is messy and spread across the Internet.\nHere are the notes that I wish someone had passed me when I started out on this journey! \nA tap device is a virtual network interface that looks like an ethernet network card. Instead of having real wires plugged into it, it exposes a nice handy file descriptor to an application willing to send/receive packets. Historically tap devices were mostly used to implement VPN clients. The machine would route traffic towards a tap interface, and a VPN client application would pick them up and process accordingly. For example this is what our Cloudflare WARP Linux client does. Here's how it looks on my laptop:\n$ ip link list ... 18: CloudflareWARP: <POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP> mtu 1280 qdisc mq state UNKNOWN mode DEFAULT group default qlen 500 link/none $ ip tuntap list CloudflareWARP: tun multi_queue \nMore recently tap devices started to be used by virtual machines to enable networking. The VMM (like Qemu, Firecracker, or gVisor) would open the application side of a tap and pass all the packets to the guest VM. The tap network interface would be left for the host kernel to deal with. Typically, a host would behave like a router and firewall, forward or NAT all the packets. This design is somewhat surprising - it's almost reversing the original use case for tap. In the VPN days tap was a traffic destination. With a VM behind, tap looks like a traffic source.\nA Linux tap device is a mean creature. It looks trivial — a virtual network interface, with a file descriptor behind it. However, it's surprisingly hard to get it to perform well. The Linux networking stack is optimized for packets handled by a physical network card, not a userspace application. However, over the years the Linux tap interface grew in features and nowadays, it's possible to get good performance out of it. Later I'll explain how to use the Linux tap API in a modern way.\nSource: DALL-E\nTo tun or to tap?\nThe interface is called \"the universal tun/tap\" in the kernel. The \"tun\" variant, accessible via the IFF_TUN flag, looks like a point-to-point link. There are no L2 Ethernet headers. Since most modern networks are Ethernet, this is a bit less intuitive to set up for a novice user. Most importantly, projects like Firecracker and gVisor do expect L2 headers.\n\"Tap\", with the IFF_TAP flag, is the one which has Ethernet headers, and has been getting all the attention lately. If you are like me and always forget which one is which, you can use this AI-generated rhyme (check out WorkersAI/LLama) to help to remember:\nTap is like a switch,\nEthernet headers it'll hitch.\nTun is like a tunnel,\nVPN connections it'll funnel.\nEthernet headers it won't hold,\nTap uses, tun does not, we're told.\nListing devices\nTun/tap devices are natively supported by iproute2 tooling. Typically, one creates a device with ip tuntap add and lists it with ip tuntap list:\n$ sudo ip tuntap add mode tap user marek group marek name tap0 $ ip tuntap list tap0: tap persist user 1000 group 1000\nAlternatively, it's possible to look for the /sys/devices/virtual/net/<ifr_name>/tun_flags files.\nTap device setup\nTo open or create a new device, you first need to open /dev/net/tun which is called a \"clone device\":\n/* First, whatever you do, the device /dev/net/tun must be * opened read/write. That device is also called the clone * device, because it's used as a starting point for the * creation of any tun/tap virtual interface. */ char *clone_dev_name = \"/dev/net/tun\"; int tap_fd = open(clone_dev_name, O_RDWR | O_CLOEXEC); if (tap_fd < 0) { error(-1, errno, \"open(%s)\", clone_dev_name); }\nWith the clone device file descriptor we can now instantiate a specific tap device by name:\nstruct ifreq ifr = {}; strncpy(ifr.ifr_name, tap_name, IFNAMSIZ); ifr.ifr_flags = IFF_TAP | IFF_NO_PI | IFF_VNET_HDR; int r = ioctl(tap_fd, TUNSETIFF, &ifr); if (r != 0) { error(-1, errno, \"ioctl(TUNSETIFF)\"); }\nIf ifr_name is empty or with a name that doesn't exist, a new tap device is created. Otherwise, an existing device is opened. When opening existing devices, flags like IFF_MULTI_QUEUE must match with the way the device was created, or EINVAL is returned. It's a good idea to try to reopen the device with flipped multi queue setting on EINVAL error.\nThe ifr_flags can have the following bits set:\nIFF_TAP / IFF_TUN\n\t\nAlready discussed.\n\t\nIFF_NO_CARRIER\n\t\nHolding an open tap device file descriptor sets the Ethernet interface CARRIER flag up. In some cases it might be desired to delay that until a TUNSETCARRIER call.\n\t\nIFF_NO_PI\n\t\nHistorically each packet on tap had a \"struct tun_pi\" 4 byte prefix. There are now better alternatives and this option disables this prefix.\n\t\nIFF_TUN_EXCL\n\t\nEnsures a new device is created. Returns EBUSY if the device exists\n\t\nIFF_VNET_HDR\n\t\nPrepend \"struct virtio_net_hdr\" before the RX and TX packets, should be followed by setsockopt(TUNSETVNETHDRSZ).\n\t\nIFF_MULTI_QUEUE\n\t\nUse multi queue tap, see below.\n\t\nIFF_NAPI / IFF_NAPI_FRAGS\n\t\nSee below.\n\t\nYou almost always want IFF_TAP, IFF_NO_PI, IFF_VNET_HDR flags and perhaps sometimes IFF_MULTI_QUEUE.\nThe curious IFF_NAPI\nJudging by the original patchset introducing IFF_NAPI and IFF_NAPI_FRAGS, these flags were introduced to increase code coverage of syzkaller. However, later work indicates there were performance benefits when doing XDP on tap. IFF_NAPI enables a dedicated NAPI instance for packets written from an application into a tap. Besides allowing XDP, it also allows packets to be batched and GRO-ed. Otherwise, a backlog NAPI is used.\nA note on buffer sizes\nInternally, a tap device is just a pair of packet queues. It's exposed as a network interface towards the host, and a file descriptor, a character device, towards the application. The queue in the direction of application (tap TX queue) is of size txqueuelen packets, controlled by an interface parameter:\n$ ip link set dev tap0 txqueuelen 1000 $ ip -s link show dev tap0 26: tap0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 ... qlen 1000 RX: bytes packets errors dropped missed mcast 0 0 0 0 0 0 TX: bytes packets errors dropped carrier collsns 266 3 0 66 0 0 \nIn \"ip link\" statistics the column \"TX dropped\" indicates the tap application was too slow and the queue space exhausted.\nIn the other direction - interface RX queue - from application towards the host, the queue size limit is measured in bytes and controlled by the TUNSETSNDBUF ioctl. The qemu comment discusses this setting, however it's not easy to cause this queue to overflow. See this discussion for details.\nvnethdr size\nAfter the device is opened, a typical scenario is to set up VNET_HDR size and offloads. Typically the VNETHDRSZ should be set to 12:\nlen = 12; r = ioctl(tap_fd, TUNSETVNETHDRSZ, &(int){len}); if (r != 0) { error(-1, errno, \"ioctl(TUNSETVNETHDRSZ)\"); }\nSensible values are {10, 12, 20}, which are derived from virtio spec. 12 bytes makes room for the following header (little endian):\nstruct virtio_net_hdr_v1 { #define VIRTIO_NET_HDR_F_NEEDS_CSUM 1 /* Use csum_start, csum_offset */ #define VIRTIO_NET_HDR_F_DATA_VALID 2 /* Csum is valid */ u8 flags; #define VIRTIO_NET_HDR_GSO_NONE 0 /* Not a GSO frame */ #define VIRTIO_NET_HDR_GSO_TCPV4 1 /* GSO frame, IPv4 TCP (TSO) */ #define VIRTIO_NET_HDR_GSO_UDP 3 /* GSO frame, IPv4 UDP (UFO) */ #define VIRTIO_NET_HDR_GSO_TCPV6 4 /* GSO frame, IPv6 TCP */ #define VIRTIO_NET_HDR_GSO_UDP_L4 5 /* GSO frame, IPv4& IPv6 UDP (USO) */ #define VIRTIO_NET_HDR_GSO_ECN 0x80 /* TCP has ECN set */ u8 gso_type; u16 hdr_len; /* Ethernet + IP + tcp/udp hdrs */ u16 gso_size; /* Bytes to append to hdr_len per frame */ u16 csum_start; u16 csum_offset; u16 num_buffers; }; \noffloads\nTo enable offloads use the ioctl:\nunsigned off_flags = TUN_F_CSUM | TUN_F_TSO4 | TUN_F_TSO6; int r = ioctl(tap_fd, TUNSETOFFLOAD, off_flags); if (r != 0) { error(-1, errno, \"ioctl(TUNSETOFFLOAD)\"); } \nHere are the allowed bit values. They confirm that the userspace application can receive:\nTUN_F_CSUM\n\t\nL4 packet checksum offload\n\t\nTUN_F_TSO4\n\t\nTCP Segmentation Offload - TSO for IPv4 packets\n\t\nTUN_F_TSO6\n\t\nTSO for IPv6 packets\n\t\nTUN_F_TSO_ECN\n\t\nTSO with ECN bits\n\t\nTUN_F_UFO\n\t\nUDP Fragmentation offload - UFO packets. Deprecated\n\t\nTUN_F_USO4\n\t\nUDP Segmentation offload - USO for IPv4 packets\n\t\nTUN_F_USO6\n\t\nUSO for IPv6 packets\n\t\nGenerally, offloads are extra packet features the tap application can deal with. Details of the offloads used by the sender are set on each packet in the vnethdr prefix.\nChecksum offload TUN_F_CSUM\nStructure of a typical UDP packet received over tap.\nLet's start with the checksumming offload. The TUN_F_CSUM offload saves the kernel some work by pushing the checksum processing down the path. Applications which set that flag are indicating they can handle checksum validation. For example with this offload, for UDP IPv4 packet will have:\nvnethdr flags will have VIRTIO_NET_HDR_F_NEEDS_CSUM set\nhdr_len would be 42 (14+20+8)\ncsum_start 34 (14+20)\nand csum_offset 6 (UDP header checksum is 6 bytes into L4)\nThis is illustrated above.\nSupporting checksum offload is needed for further offloads.\nTUN_F_CSUM is a must\nConsider this code:\ns = socket(AF_INET, SOCK_DGRAM) s.setsockopt(SOL_UDP, UDP_SEGMENT, 1400) s.sendto(b\"x\", (\"10.0.0.2\", 5201)) # Would you expect EIO ? \nThis simple code produces a packet. When directed at a tap device, this code will surprisingly yield an EIO \"Input/output error\". This weird behavior happens if the tap is opened without TUN_F_CSUM and the application is sending GSO / UDP_SEGMENT frames. Tough luck. It might be considered a kernel bug, and we're thinking about fixing that. However, in the meantime everyone using tap should just set the TUN_F_CSUM bit.\nSegmentation offloads\nWe wrote about UDP_SEGMENT in the past. In short: on Linux an application can handle many packets with a single send/recv, as long as they have identical length. \nWith UDP_SEGMENT a single send() can transfer multiple packets.\nTap devices support offloading which exposes that very functionality. With TUN_F_TSO4 and TUN_F_TSO6 flags the tap application signals it can deal with long packet trains. Note, that with these features the application must be ready to receive much larger buffers - up to 65507 bytes for IPv4 and 65527 for IPv6. \nTSO4/TSO6 flags are enabling long packet trains for TCP and have been supported for a long time. More recently TUN_F_USO4 and TUN_F_USO6 bits were introduced for UDP. When any of these offloads are used, the gso_type contains the relevant offload type and gso_size holds a segment size within the GRO packet train.\nTUN_F_UFO is a UDP fragmentation offload which is deprecated.\nBy setting TUNSETOFFLOAD, the application is telling the kernel which offloads it's able to handle on the read() side of a tap device. If the ioctl(TUNSETOFFLOAD) succeeds, the application can assume the kernel supports the same offloads for packets in the other direction.\nBug in rx-udp-gro-forwarding - TUN_F_USO4\nWhen working with tap and offloads it's useful to inspect ethtool:\n$ ethtool -k tap0 | egrep -v fixed tx-checksumming: on tx-checksum-ip-generic: on scatter-gather: on tx-scatter-gather: on tx-scatter-gather-fraglist: on tcp-segmentation-offload: on tx-tcp-segmentation: on generic-segmentation-offload: on generic-receive-offload: on tx-udp-segmentation: on rx-gro-list: off rx-udp-gro-forwarding: off \nWith ethtool we can see the enabled offloads and disable them as needed.\nWhile toying with UDP Segmentation Offload (USO) I've noticed that when packet trains from tap are forwarded to a real network interface, sometimes they seem badly packetized. See the netdev discussion, and the proposed fix. In any case - beware of this bug, and maybe consider doing \"ethtool -K tap0 rx-udp-gro-forwarding off\".\nMiscellaneous setsockopts\nTUNGETFEATURES\n\t\nReturn vector of IFF_* constants that the kernel supports. Typically used to detect the host support of: IFF_VNET_HDR, IFF_NAPI and IFF_MULTI_QUEUE.\n\t\nTUNSETIFF\n\t\nTakes \"struct ifreq\", sets up a tap device, fills in the name if empty.\n\t\nTUNGETIFF\n\t\nReturns a \"struct ifreq\" containing the device's current name and flags.\n\t\nTUNSETPERSIST\n\t\nSets TUN_PERSIST flag, if you want the device to remain in the system after the tap_fd is closed.\n\t\nTUNSETOWNER, TUNSETGROUP\n\t\nSet uid and gid that can own the device.\n\t\nTUNSETLINK\n\t\nSet the Ethernet link type for the device. The device must be down. See ARPHRD_* constants. For tap it defaults to ARPHRD_ETHER.\n\t\nTUNSETOFFLOAD\n\t\nAs documented above.\n\t\nTUNGETSNDBUF, TUNSETSNDBUF\n\t\nGet/set send buffer. The default is INT_MAX.\n\t\nTUNGETVNETHDRSZ, TUNSETVNETHDRSZ\n\t\nAlready discussed.\n\t\nTUNSETIFINDEX\n\t\nSet interface index (ifindex), useful in checkpoint-restore.\n\t\nTUNSETCARRIER\n\t\nSet the carrier state of an interface, as discussed earlier, useful with IFF_NO_CARRIER.\n\t\nTUNGETDEVNETNS\n\t\nReturn an fd of a net namespace that the interface belongs to.\n\t\nTUNSETTXFILTER\n\t\nTakes \"struct tun_filter\" which limits the dst mac addresses that can be delivered to the application.\n\t\nTUNATTACHFILTER, TUNDETACHFILTER, TUNGETFILTER\n\t\nAttach/detach/get classic BPF filter for packets going to application. Takes \"struct sock_fprog\".\n\n\t\nTUNSETFILTEREBPF\n\t\nSet an eBPF filter on a tap device. This is independent of the classic BPF above.\n\t\nTUNSETQUEUE\n\t\nUsed to set IFF_DETACH_QUEUE and IFF_ATTACH_QUEUE for multiqueue.\n\t\nTUNSETSTEERINGEBPF\n\t\nSet an eBPF program for selecting a specific tap queue, in the direction towards the application. This is useful if you want to ensure some traffic is sticky to a specific application thread. The eBPF program takes \"struct __sk_buff\" and returns an int. The result queue number is computed from the return value u16 modulo number of queues is the selection.\n\t\nSingle queue speed\nTap devices are quite weird — they aren't network sockets, nor true files. Their semantics are closest to pipes, and unfortunately the API reflects that. To receive or send a packet from a tap device, the application must do a read() or write() syscall, one packet at a time.\nOne might think that some sort of syscall batching would help. Sockets have sendmmsg()/recvmmsg(), but that doesn't work on tap file descriptors. The typical alternatives enabling batching are: an old io_submit AIO interface, or modern io_uring. Io_uring added tap support quite recently. However, it turns out syscall batching doesn't really offer that much of an improvement. Maybe in the range of 10%.\nThe Linux kernel is just not capable of forwarding millions of packets per second for a single flow or on a single CPU. The best possible solution is to scale vertically for elephant flows with TSO/USO (packet trains) offloads, and scale horizontally for multiple concurrent flows with multi queue.\nIn this chart you can see how dramatic the performance gain of offloads is. Without them, a sample \"echo\" tap application can process between 320 and 500 thousand packets per second on a single core. MTU being 1500. When the offloads are enabled it jumps to 2.7Mpps, while keeping the number of received \"packet trains\" to just 56 thousand per second. Of course not every traffic pattern can fully utilize GRO/GSO. However, to get decent performance from tap, and from Linux in general, offloads are absolutely critical.\nMulti queue considerations\nMulti queue is useful when the tap application is handling multiple concurrent flows and needs to utilize more than one CPU. \nTo get a file descriptor of a tap queue, just add the IFF_MULTI_QUEUE flag when opening the tap. It's possible to detach/reattach a queue with TUNSETQUEUE and IFF_DETACH_QUEUE/IFF_ATTACH_QUEUE, but I'm unsure when this is useful.\nWhen a multi queue tap is created, it spreads the load across multiple tap queues, each one having a unique file descriptor. Beware of the algorithm selecting the queue though: it might bite you back.\nBy default, Linux tap driver records a symmetric flow hash of any handled flow in a flow table. It saves on which queue the traffic from the application was transmitted. Then, on the receiving side it follows that selection and sends subsequent packets to that specific queue. For example, if your userspace application is sending some TCP flow over queue #2, then the packets going into the application which are a part of that flow will go to queue #2. This is generally a sensible design as long as the sender is always selecting one specific queue. If the sender changes the TX queue, new packets will immediately shift and packets within one flow might be seen as reordered. Additionally, this queue selection design does not take into account CPU locality and might have minor negative effects on performance for very high throughput applications.\nIt's possible to override the flow hash based queue selection by using tc multiq qdisc and skbedit queue_mapping filter:\ntc qdisc add dev tap0 root handle 1: multiq tc filter add dev tap0 parent 1: protocol ip prio 1 u32 \\ match ip dst 192.168.0.3 \\ action skbedit queue_mapping 0 \ntc is fragile and thus it's not a solution I would recommend. A better way is to customize the queue selection algorithm with a TUNSETSTEERINGEBPF eBPF program. In that case, the flow tracking code is not employed anymore. By smartly using such a steering eBPF program, it's possible to keep the flow processing local to one CPU — useful for best performance.\nSummary\nNow you know everything I wish I had known when I was setting out on this journey!\nTo get the best performance, I recommend:\nenable vnethdr\nenable offloads (TSO and USO)\nconsider spreading the load across multiple queues and CPUs with multi queue\nconsider syscall batching for additional gain of maybe 10%, perhaps try io_uring\nconsider customizing the steering algorithm\nReferences:\nhttps://www.linux-kvm.org/images/6/63/2012-forum-multiqueue-networking-for-kvm.pdf\nhttps://backreference.org/2010/03/26/tuntap-interface-tutorial/\nhttps://ldpreload.com/p/tuntap-notes.txt\nhttps://www.kernel.org/doc/Documentation/networking/tuntap.txt\nhttps://tailscale.com/blog/more-throughput/\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nDeep Dive",
      "markdown": "10/06/2023\n\n*   [![Marek Majkowski](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2017/03/b5967d6c687939594adb6992723d0529.jpeg)](https://blog.cloudflare.com/author/marek-majkowski/)\n\n12 min read\n\n![Tap devices - the missing manual](https://blog.cloudflare.com/content/images/2023/10/image1-5.png)\n\nIt's a never-ending effort to improve the performance of our infrastructure. As part of that quest, we wanted to squeeze as much network oomph as possible from our virtual machines. Internally for some projects we use [Firecracker](https://firecracker-microvm.github.io/), which is a KVM-based virtual machine manager (VMM) that runs light-weight “Micro-VM”s. Each Firecracker instance uses a tap device to communicate with a host system. Not knowing much about tap, I had to up my game, however, it wasn't easy — the documentation is messy and spread across the Internet.\n\nHere are the notes that I wish someone had passed me when I started out on this journey!\n\nA tap device is a virtual **network interface** that looks like an ethernet network card. Instead of having real wires plugged into it, it exposes a nice handy file descriptor to an application willing to send/receive packets. Historically tap devices were mostly used to implement VPN clients. The machine would route traffic towards a **tap interface**, and a VPN client application would pick them up and process accordingly. For example this is what our Cloudflare WARP Linux client does. Here's how it looks on my laptop:\n\n```\n$ ip link list\n...\n18: CloudflareWARP: <POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP> mtu 1280 qdisc mq state UNKNOWN mode DEFAULT group default qlen 500\n\tlink/none\n\n$ ip tuntap list\nCloudflareWARP: tun multi_queue\n```\n\nMore recently tap devices started to be used by [virtual machines](https://www.cloudflare.com/learning/cloud/what-is-a-virtual-machine/) to enable networking. The VMM (like Qemu, Firecracker, or gVisor) would open the application side of a tap and pass all the packets to the guest VM. The tap network interface would be left for the host kernel to deal with. Typically, a host would behave like a router and firewall, forward or NAT all the packets. This design is somewhat surprising - it's almost reversing the original use case for tap. In the VPN days tap was a traffic destination. With a VM behind, tap looks like a traffic source.\n\nA Linux tap device is a **mean creature**. It looks trivial — a virtual network interface, with a file descriptor behind it. However, it's **surprisingly hard** to get it to perform well. The Linux networking stack is optimized for packets handled by a physical network card, not a userspace application. However, over the years the Linux tap interface grew in features and nowadays, it's possible to get good performance out of it. Later I'll explain how to use the Linux tap API in a modern way.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Screenshot-2023-10-05-at-2.42.49-PM.png)\n\nSource: DALL-E\n\n## To tun or to tap?\n\nThe interface is [called \"the universal tun/tap\"](https://docs.kernel.org/networking/tuntap.html) in the kernel. The \"tun\" variant, accessible via the IFF\\_TUN flag, looks like a point-to-point link. There are no L2 Ethernet headers. Since most modern networks are Ethernet, this is a bit less intuitive to set up for a novice user. Most importantly, projects like Firecracker and gVisor do expect L2 headers.\n\n\"Tap\", with the IFF\\_TAP flag, is the one which has Ethernet headers, and has been getting all the attention lately. If you are like me and always forget which one is which, you can use this  AI-generated rhyme (check out [WorkersAI/LLama](https://blog.cloudflare.com/writing-poems-using-llama-2-on-workers-ai/)) to help to remember:\n\n_Tap is like a switch,_  \n_Ethernet headers it'll hitch._  \n_Tun is like a tunnel,_  \n_VPN connections it'll funnel._  \n_Ethernet headers it won't hold,_  \n_Tap uses, tun does not, we're told._\n\n## Listing devices\n\nTun/tap devices are natively supported by iproute2 tooling. Typically, one creates a device with **ip tuntap add** and lists it with **ip tuntap list**:\n\n```\n$ sudo ip tuntap add mode tap user marek group marek name tap0\n$ ip tuntap list\ntap0: tap persist user 1000 group 1000\n```\n\nAlternatively, it's possible to look for the `/sys/devices/virtual/net/<ifr_name>/tun_flags` files.\n\n## Tap device setup\n\nTo open or create a new device, you first need to open `/dev/net/tun` which is called a \"clone device\":\n\n```\n    /* First, whatever you do, the device /dev/net/tun must be\n     * opened read/write. That device is also called the clone\n     * device, because it's used as a starting point for the\n     * creation of any tun/tap virtual interface. */\n    char *clone_dev_name = \"/dev/net/tun\";\n    int tap_fd = open(clone_dev_name, O_RDWR | O_CLOEXEC);\n    if (tap_fd < 0) {\n   \t error(-1, errno, \"open(%s)\", clone_dev_name);\n    }\n```\n\nWith the clone device file descriptor we can now instantiate a specific tap device by name:\n\n```\n    struct ifreq ifr = {};\n    strncpy(ifr.ifr_name, tap_name, IFNAMSIZ);\n    ifr.ifr_flags = IFF_TAP | IFF_NO_PI | IFF_VNET_HDR;\n    int r = ioctl(tap_fd, TUNSETIFF, &ifr);\n    if (r != 0) {\n   \t error(-1, errno, \"ioctl(TUNSETIFF)\");\n    }\n```\n\nIf **ifr\\_name** is empty or with a name that doesn't exist, a new tap device is created. Otherwise, an existing device is opened. When opening existing devices, flags like IFF\\_MULTI\\_QUEUE must match with the way the device was created, or EINVAL is returned. It's a good idea to try to reopen the device with flipped multi queue setting on EINVAL error.\n\nThe **ifr\\_flags** can have the following bits set:\n\n|     |     |\n| --- | --- |\n| IFF\\_TAP / IFF\\_TUN | Already discussed. |\n| IFF\\_NO\\_CARRIER | Holding an open tap device file descriptor sets the Ethernet interface CARRIER flag up. In some cases it might be desired to delay that until a TUNSETCARRIER call. |\n| IFF\\_NO\\_PI | Historically each packet on tap had a \"struct tun\\_pi\" 4 byte prefix. There are now better alternatives and this option disables this prefix. |\n| IFF\\_TUN\\_EXCL | Ensures a new device is created. Returns EBUSY if the device exists |\n| IFF\\_VNET\\_HDR | Prepend \"[struct virtio\\_net\\_hdr](https://elixir.bootlin.com/linux/v6.4.6/source/include/uapi/linux/virtio_net.h#L187)\" before the RX and TX packets, should be followed by setsockopt(TUNSETVNETHDRSZ). |\n| IFF\\_MULTI\\_QUEUE | Use multi queue tap, see below. |\n| IFF\\_NAPI / IFF\\_NAPI\\_FRAGS | See below. |\n\nYou almost always want IFF\\_TAP, IFF\\_NO\\_PI, IFF\\_VNET\\_HDR flags and perhaps sometimes IFF\\_MULTI\\_QUEUE.\n\n## The curious IFF\\_NAPI\n\nJudging by the [original patchset introducing IFF\\_NAPI and IFF\\_NAPI\\_FRAGS](https://www.mail-archive.com/netdev@vger.kernel.org/msg189704.html), these flags were introduced to increase code coverage of syzkaller. However, later work indicates there were [performance benefits when doing XDP on tap](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=fb3f903769e805221eb19209b3d9128d398038a1). IFF\\_NAPI enables a dedicated NAPI instance for packets written from an application into a tap. Besides allowing XDP, it also allows packets to be batched and GRO-ed. Otherwise, a backlog NAPI is used.\n\n## A note on buffer sizes\n\nInternally, a tap device is just a pair of packet queues. It's exposed as a network interface towards the host, and a file descriptor, a character device, towards the application. The queue in the direction of application (tap TX queue) is of size **txqueuelen** packets, controlled by an interface parameter:\n\n```\n$ ip link set dev tap0 txqueuelen 1000\n$ ip -s link show dev tap0\n26: tap0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 ... qlen 1000\n\tRX:  bytes packets errors dropped  missed   mcast      \t \n         \t0   \t0  \t0   \t0   \t0   \t0\n\tTX:  bytes packets errors dropped carrier collsns      \t \n       \t266   \t3  \t0  \t66   \t0   \t0\n```\n\nIn \"ip link\" statistics the column \"**TX dropped**\" indicates the tap application was too slow and the queue space exhausted.\n\nIn the other direction - interface RX queue -  from application towards the host, the queue size limit is measured in bytes and controlled by the TUNSETSNDBUF ioctl. The [qemu comment discusses](https://elixir.bootlin.com/qemu/v8.1.1/source/net/tap-linux.c#L120) this setting, however it's not easy to cause this queue to overflow. See this [discussion for details](https://bugzilla.redhat.com/show_bug.cgi?id=508861).\n\n## vnethdr size\n\nAfter the device is opened, a typical scenario is to set up VNET\\_HDR size and offloads. Typically the VNETHDRSZ should be set to 12:\n\n```\n    len = 12;\n    r = ioctl(tap_fd, TUNSETVNETHDRSZ, &(int){len});\n    if (r != 0) {\n   \t error(-1, errno, \"ioctl(TUNSETVNETHDRSZ)\");\n    }\n```\n\nSensible values are {10, 12, 20}, which are derived from [virtio spec](https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html). 12 bytes makes room for the following header (little endian):\n\n```\nstruct virtio_net_hdr_v1 {\n#define VIRTIO_NET_HDR_F_NEEDS_CSUM  1    /* Use csum_start, csum_offset */\n#define VIRTIO_NET_HDR_F_DATA_VALID  2    /* Csum is valid */\n    u8 flags;\n#define VIRTIO_NET_HDR_GSO_NONE      0    /* Not a GSO frame */\n#define VIRTIO_NET_HDR_GSO_TCPV4     1    /* GSO frame, IPv4 TCP (TSO) */\n#define VIRTIO_NET_HDR_GSO_UDP       3    /* GSO frame, IPv4 UDP (UFO) */\n#define VIRTIO_NET_HDR_GSO_TCPV6     4    /* GSO frame, IPv6 TCP */\n#define VIRTIO_NET_HDR_GSO_UDP_L4    5    /* GSO frame, IPv4& IPv6 UDP (USO) */\n#define VIRTIO_NET_HDR_GSO_ECN       0x80 /* TCP has ECN set */\n    u8 gso_type;\n    u16 hdr_len;     /* Ethernet + IP + tcp/udp hdrs */\n    u16 gso_size;    /* Bytes to append to hdr_len per frame */\n    u16 csum_start;\n    u16 csum_offset;\n    u16 num_buffers;\n};\n```\n\n## offloads\n\nTo enable offloads use the ioctl:\n\n```\n    unsigned off_flags = TUN_F_CSUM | TUN_F_TSO4 | TUN_F_TSO6;\n    int r = ioctl(tap_fd, TUNSETOFFLOAD, off_flags);\n    if (r != 0) {\n   \t error(-1, errno, \"ioctl(TUNSETOFFLOAD)\");\n    }\n```\n\nHere are the allowed bit values. They confirm that the userspace application can receive:\n\n|     |     |\n| --- | --- |\n| TUN\\_F\\_CSUM | L4 packet checksum offload |\n| TUN\\_F\\_TSO4 | TCP Segmentation Offload - TSO for IPv4 packets |\n| TUN\\_F\\_TSO6 | TSO for IPv6 packets |\n| TUN\\_F\\_TSO\\_ECN | TSO with ECN bits |\n| TUN\\_F\\_UFO | UDP Fragmentation offload - UFO packets. Deprecated |\n| TUN\\_F\\_USO4 | UDP Segmentation offload - USO for IPv4 packets |\n| TUN\\_F\\_USO6 | USO for IPv6 packets |\n\nGenerally, offloads are extra packet features the tap application can deal with. Details of the offloads used by the sender are set on each packet in the vnethdr prefix.\n\n## Checksum offload TUN\\_F\\_CSUM\n\n![](https://blog.cloudflare.com/content/images/2023/10/image4.jpg)\n\nStructure of a typical UDP packet received over tap.\n\nLet's start with the checksumming offload. The TUN\\_F\\_CSUM offload saves the kernel some work by pushing the checksum processing down the path. Applications which set that flag are indicating they can handle checksum validation. For example with this offload, for UDP IPv4 packet will have:\n\n*   vnethdr flags will have VIRTIO\\_NET\\_HDR\\_F\\_NEEDS\\_CSUM set\n*   hdr\\_len would be 42 (14+20+8)\n*   csum\\_start 34 (14+20)\n*   and csum\\_offset 6 (UDP header checksum is 6 bytes into L4)\n\nThis is illustrated above.\n\nSupporting checksum offload is needed for further offloads.\n\n## TUN\\_F\\_CSUM is a must\n\nConsider this code:\n\n```\ns = socket(AF_INET, SOCK_DGRAM)\ns.setsockopt(SOL_UDP, UDP_SEGMENT, 1400)\ns.sendto(b\"x\", (\"10.0.0.2\", 5201))     # Would you expect EIO ?\n```\n\nThis simple code produces a packet. When directed at a tap device, this code will surprisingly yield an EIO \"Input/output error\". This weird behavior happens if the tap is opened without TUN\\_F\\_CSUM and the application is sending GSO / UDP\\_SEGMENT frames. Tough luck. It might be considered a kernel bug, and we're thinking about fixing that. However, in the meantime everyone using tap should just set the TUN\\_F\\_CSUM bit.\n\n## Segmentation offloads\n\nWe wrote about [UDP\\_SEGMENT](https://blog.cloudflare.com/accelerating-udp-packet-transmission-for-quic/) in the past. In short: on Linux an application can handle many packets with a single send/recv, as long as they have identical length.\n\n![](https://blog.cloudflare.com/content/images/2023/10/image2-4.png)\n\nWith UDP\\_SEGMENT a single send() can transfer multiple packets.\n\nTap devices support offloading which exposes that very functionality. With TUN\\_F\\_TSO4 and TUN\\_F\\_TSO6 flags the tap application signals it can deal with long packet trains. Note, that with these features the application must be ready to receive much larger buffers - up to 65507 bytes for IPv4 and 65527 for IPv6.\n\nTSO4/TSO6 flags are enabling long packet trains for [TCP](https://www.cloudflare.com/learning/ddos/glossary/tcp-ip/) and have been supported for a long time. More recently TUN\\_F\\_USO4 and TUN\\_F\\_USO6 bits were introduced for [UDP](https://www.cloudflare.com/learning/ddos/glossary/user-datagram-protocol-udp/). When any of these offloads are used, the **gso\\_type** contains the relevant offload type and **gso\\_size** holds a segment size within the GRO packet train.\n\nTUN\\_F\\_UFO is a UDP fragmentation offload which is deprecated.\n\nBy setting TUNSETOFFLOAD, the application is telling the kernel which offloads it's able to handle on the read() side of a tap device. If the ioctl(TUNSETOFFLOAD) succeeds, the application can assume the kernel supports the same offloads for packets in the other direction.\n\n## Bug in rx-udp-gro-forwarding - TUN\\_F\\_USO4\n\nWhen working with tap and offloads it's useful to inspect **ethtool**:\n\n```\n$ ethtool -k tap0 | egrep -v fixed\ntx-checksumming: on\n    tx-checksum-ip-generic: on\nscatter-gather: on\n    tx-scatter-gather: on\n    tx-scatter-gather-fraglist: on\ntcp-segmentation-offload: on\n    tx-tcp-segmentation: on\ngeneric-segmentation-offload: on\ngeneric-receive-offload: on\ntx-udp-segmentation: on\nrx-gro-list: off\nrx-udp-gro-forwarding: off\n```\n\nWith ethtool we can see the enabled offloads and disable them as needed.\n\nWhile toying with UDP Segmentation Offload (USO) I've noticed that when packet trains from tap are forwarded to a real network interface, sometimes they seem badly packetized. See the [netdev discussion](https://lore.kernel.org/all/CAJPywTKDdjtwkLVUW6LRA2FU912qcDmQOQGt2WaDo28KzYDg+A@mail.gmail.com/), and the [proposed fix](https://lore.kernel.org/netdev/ZK9ZiNMsJX8+1F3N@debian.debian/T/#m9f532bb5463b89d997c5d16c78490ceeccb4497f). In any case - beware of this bug, and maybe consider doing \"ethtool -K tap0 rx-udp-gro-forwarding off\".\n\n## Miscellaneous setsockopts\n\n|     |     |\n| --- | --- |\n| TUNGETFEATURES | Return vector of IFF\\_\\* constants that the kernel supports. Typically used to detect the host support of: IFF\\_VNET\\_HDR, IFF\\_NAPI and IFF\\_MULTI\\_QUEUE. |\n| TUNSETIFF | Takes \"struct ifreq\", sets up a tap device, fills in the name if empty. |\n| TUNGETIFF | Returns a \"struct ifreq\" containing the device's current name and flags. |\n| TUNSETPERSIST | Sets TUN\\_PERSIST flag, if you want the device to remain in the system after the tap\\_fd is closed. |\n| TUNSETOWNER, TUNSETGROUP | Set uid and gid that can own the device. |\n| TUNSETLINK | Set the Ethernet link type for the device. The device must be down. See ARPHRD\\_\\* constants. For tap it defaults to ARPHRD\\_ETHER. |\n| TUNSETOFFLOAD | As documented above. |\n| TUNGETSNDBUF, TUNSETSNDBUF | Get/set send buffer. The default is INT\\_MAX. |\n| TUNGETVNETHDRSZ, TUNSETVNETHDRSZ | Already discussed. |\n| TUNSETIFINDEX | Set interface index (ifindex), [useful in checkpoint-restore](https://patchwork.ozlabs.org/project/netdev/patch/51B99946.3000703@parallels.com/). |\n| TUNSETCARRIER | Set the carrier state of an interface, as discussed earlier, useful with IFF\\_NO\\_CARRIER. |\n| TUNGETDEVNETNS | Return an fd of a net namespace that the interface belongs to. |\n\n|     |     |\n| --- | --- |\n| TUNSETTXFILTER | Takes \"struct tun\\_filter\" which limits the dst mac addresses that can be delivered to the application. |\n| TUNATTACHFILTER, TUNDETACHFILTER, TUNGETFILTER | Attach/detach/get classic BPF filter for packets going to application. Takes \"struct sock\\_fprog\". |\n| TUNSETFILTEREBPF | Set an eBPF filter on a tap device. This is independent of the classic BPF above. |\n\n|     |     |\n| --- | --- |\n| TUNSETQUEUE | Used to set IFF\\_DETACH\\_QUEUE and IFF\\_ATTACH\\_QUEUE for multiqueue. |\n| TUNSETSTEERINGEBPF | Set an eBPF program for selecting a specific tap queue, in the direction towards the application. This is useful if you want to ensure some traffic is sticky to a specific application thread. The eBPF program takes \"struct \\_\\_sk\\_buff\" and returns an int. The result queue number is computed from the return value u16 modulo number of queues is the selection. |\n\n## Single queue speed\n\nTap devices are quite weird — they aren't network sockets, nor true files. Their semantics are closest to pipes, and unfortunately the API reflects that. To receive or send a packet from a tap device, the application must do a read() or write() syscall, one packet at a time.\n\nOne might think that some sort of syscall batching would help. Sockets have sendmmsg()/recvmmsg(), but that doesn't work on tap file descriptors. The typical alternatives enabling batching are: an old [io\\_submit AIO interface](https://blog.cloudflare.com/io_submit-the-epoll-alternative-youve-never-heard-about/), or modern io\\_uring. Io\\_uring added tap support quite recently. However, it turns out syscall batching doesn't really offer that much of an improvement. Maybe in the range of 10%.\n\nThe Linux kernel is just not capable of forwarding millions of packets per second for a single flow or on a single CPU. The best possible solution is to scale vertically for elephant flows with TSO/USO (packet trains) offloads, and scale horizontally for multiple concurrent flows with multi queue.\n\n![](https://blog.cloudflare.com/content/images/2023/10/image5-1.png)\n\nIn this chart you can see how dramatic the performance gain of offloads is. Without them, a sample \"echo\" tap application can process between 320 and 500 thousand packets per second on a single core. MTU being 1500. When the offloads are enabled it jumps to 2.7Mpps, while keeping the number of received \"packet trains\" to just 56 thousand per second. Of course not every traffic pattern can fully utilize GRO/GSO. However, to get decent performance from tap, and from Linux in general, offloads are absolutely critical.\n\n## Multi queue considerations\n\nMulti queue is useful when the tap application is handling multiple concurrent flows and needs to utilize more than one CPU.\n\nTo get a file descriptor of a tap queue, just add the IFF\\_MULTI\\_QUEUE flag when opening the tap. It's possible to detach/reattach a queue with TUNSETQUEUE and IFF\\_DETACH\\_QUEUE/IFF\\_ATTACH\\_QUEUE, but I'm unsure when this is useful.\n\nWhen a multi queue tap is created, it spreads the load across multiple tap queues, each one having a unique file descriptor. Beware of the algorithm selecting the queue though: it might bite you back.\n\nBy default, Linux tap driver records a symmetric flow hash of any handled flow in a flow table. It saves on which queue the traffic from the application was transmitted. Then, on the receiving side it follows that selection and sends subsequent packets to that specific queue. For example, if your userspace application is sending some TCP flow over queue #2, then the packets going into the application which are a part of that flow will go to queue #2. This is generally a sensible design as long as the sender is always selecting one specific queue. If the sender changes the TX queue, new packets will immediately shift and packets within one flow might be seen as reordered. Additionally, this queue selection design does not take into account CPU locality and might have minor negative effects on performance for very high throughput applications.\n\nIt's possible to override the flow hash based queue selection by using [tc multiq qdisc and skbedit queue\\_mapping filter](https://www.infradead.org/~mchehab/kernel_docs/networking/multiqueue.html):\n\n```\ntc qdisc add dev tap0 root handle 1: multiq\ntc filter add dev tap0 parent 1: protocol ip prio 1 u32 \\\n        match ip dst 192.168.0.3 \\\n        action skbedit queue_mapping 0\n```\n\n**tc** is fragile and thus it's not a solution I would recommend. A better way is to customize the queue selection algorithm with a TUNSETSTEERINGEBPF eBPF program. In that case, the flow tracking code is not employed anymore. By smartly using such a steering eBPF program, it's possible to keep the flow processing local to one CPU — useful for best performance.\n\n## Summary\n\nNow you know everything I wish I had known when I was setting out on this journey!\n\nTo get the best performance, I recommend:\n\n*   enable vnethdr\n*   enable offloads (TSO and USO)\n*   consider spreading the load across multiple queues and CPUs with multi queue\n*   consider syscall batching for additional gain of maybe 10%, perhaps try io\\_uring\n*   consider customizing the steering algorithm\n\nReferences:\n\n*   [https://www.linux-kvm.org/images/6/63/2012-forum-multiqueue-networking-for-kvm.pdf](https://www.linux-kvm.org/images/6/63/2012-forum-multiqueue-networking-for-kvm.pdf)\n*   [https://backreference.org/2010/03/26/tuntap-interface-tutorial/](https://backreference.org/2010/03/26/tuntap-interface-tutorial/)\n*   [https://ldpreload.com/p/tuntap-notes.txt](https://ldpreload.com/p/tuntap-notes.txt)\n*   [https://www.kernel.org/doc/Documentation/networking/tuntap.txt](https://www.kernel.org/doc/Documentation/networking/tuntap.txt)\n*   [https://tailscale.com/blog/more-throughput/](https://tailscale.com/blog/more-throughput/)\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Deep Dive](https://blog.cloudflare.com/tag/deep-dive/)"
    },
    {
      "url": "https://blog.cloudflare.com/internet-traffic-patterns-in-israel-and-palestine-following-the-october-2023-attacks/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/internet-traffic-patterns-in-israel-and-palestine-following-the-october-2023-attacks/",
        "loadedTime": "2023-12-05T02:31:30.248Z",
        "referrerUrl": "https://blog.cloudflare.com/page/2/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/internet-traffic-patterns-in-israel-and-palestine-following-the-october-2023-attacks/",
        "title": "Internet traffic patterns in Israel and Palestine following the October 2023 attacks",
        "description": "On Saturday, October 7, 2023, attacks from the Palestinian group Hamas launched from the Gaza Strip against the south of Israel started a new conflict in the region. Cloudflare's data shows that Internet traffic was impacted in different ways",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/09/2023\n6 min read\nOn Saturday, October 7, 2023, attacks from the Palestinian group Hamas launched from the Gaza Strip against the south of Israel started a new conflict in the region. Israel officially declared that it is at war the next day. Cloudflare's data shows that Internet traffic was impacted in different ways, both in Israel and Palestine, with two networks (autonomous systems) in the Gaza Strip going offline a few hours after the attacks. Subsequently, on October 9, two additional networks also experienced outages. We also saw an uptick in cyberattacks targeting Israel, including a 1.26 billion HTTP requests DDoS attack, and Palestine.\nStarting with general Internet traffic trends, there was a clear increase in Internet traffic right after the attacks reportedly began (03:30 UTC, 06:30 local time). Traffic spiked at around 03:35 UTC (06:35 local time) in both Israel (~170% growth compared with the previous week) and Palestine (100% growth). \nThat growth is consistent with other situations, where we’ve seen surges in Internet traffic when countrywide events occur and people are going online to check for news, updates, and more information on what is happening, with social media and messaging also playing a role. However, in Palestine, that traffic growth was followed by a clear drop in traffic around 08:00 UTC (11:00 local time).\nThe Palestine uptick in traffic after the Hamas attacks started is more visible when only looking at HTTP requests. Requests in Palestine dropped on Saturday and Sunday, October 7 and 8, as much as 20% and 25%, respectively.\nPalestine's outages and Internet impact\nWhat drove the drop in Internet traffic in Palestine? Our data shows that two Gaza Strip related networks (autonomous systems or ASNs) were offline on that October 7 morning. Fusion (AS42314) was offline from 08:00 UTC, but saw some recovery after 17:00 UTC the next day; this only lasted for a few hours, given that it went back offline after 12:00 UTC this Monday, October 9.\nIt was the same scenario for DCC North (AS203905), but it went offline after 10:00 UTC and with no recovery of traffic observed as of Monday, October 9. These Internet disruptions may be related to power outages in the Gaza Strip.\nDuring the day on October 7, other Palestinian networks saw less traffic than usual. JETNET (AS199046) had around half of the usual traffic after 08:00 UTC, similar to SpeedClick (AS57704), which had around 60% less traffic. After 14:15 on October 9, traffic to those networks dropped sharply (a 95% decrease compared with the previous week), showing only residual traffic.\nWhen looking more closely at the Gaza Strip specifically, we can see that some districts or governorates had a drop in HTTP requests a few hours after the first Hamas attacks. The Gaza Governorate was impacted, with traffic dropping on October 7, 2023, after 09:15 UTC. On October 9, at 18:00 UTC, traffic was 46% lower than in the previous week. (Note: there were spikes in traffic during Friday, October 6, several hours before the attacks, but it is unclear what caused those spikes.)\nThe Deir al-Balah Governorate (on October 9, at 18:00 UTC, traffic was 46% lower than in the previous week) and the Khan Yunis Governorate (50% lower) also both experienced similar drops in traffic:\nIn the Rafah Governorate traffic dropped after 19:00 UTC on October 8 (and on October 9, at 18:00 UTC, traffic was 65% lower than in the previous week).\nOther Palestinian governorates in the West Bank did not experience the same impact to Internet traffic.\nSpikes in Internet traffic in Israel\nIn Israel, Internet traffic surged to ~170% as compared to the previous week right after the Hamas attacks on October 7 at around 03:35 UTC (06:35 local time), and again at around 16:00 UTC (19:00 local time), with ~80% growth compared to the previous week. In both cases, the increase was driven by mobile device traffic.\nThere was also increased traffic, as compared with usual levels, on Sunday, October 8, with notable spikes at around 06:00 (09:00 local time) and 12:00 UTC (15:00 local time), seen in the HTTP requests traffic graph below.\nMobile device traffic drove the Saturday, October 7 spikes in traffic, with the daily mobile device usage percentage reaching its highest in the past two months, reaching 56%.\nLooking at specific Israel districts, traffic looks similar to the nationwide perspective.\nCyber attacks targeting Israel\nCyber attacks are frequent, recurrent, and are not necessarily dependent on actual wars on the ground, as our 2023 attacks landscape clearly showed. However, it is not unusual to see cyberattacks launched in tandem with ground assaults. We saw that in Ukraine, an uptick in cyber attacks started just before war began there on February 24, 2022, and were even more constant, and spread to other countries after that day. \nIn Israel, we saw a clear uptick in cyber attacks earlier this year, with another wave of notable attacks on October 7 and October 8, 2023, after the Hamas attacks. The largest ones were DDoS attacks targeting Israeli newspapers. One attack on October 8, reached 1.26 billion daily requests blocked by Cloudflare as DDoS attacks, and the other reached 346 million daily requests on October 7, and 332 million daily requests the following day.\nLooking at these DDoS attacks in terms of requests per second, one of the impacted sites experienced a peak of 1.1 million requests per second on October 8 at 02:00 UTC, and the other Israeli newspaper saw a peak of 745k requests per second at around 06:00 the same day.\nIn Palestine, we also saw application layer DDoS attacks, but not as big. The main one in the past three months was on October 7, 2023, targeting a Palestine online newspaper, reaching 105 million daily requests.\nLooking at these most notable DDoS attacks targeting Palestine in terms of requests per second (rps), the most impacted site (a Palestinian newspaper) experienced a peak of 214k requests per second at around 17:20 UTC on October 7.\nFollow Cloudflare Radar for up to date information\nWe will continue to monitor trends related to this conflict. You can use Cloudflare Radar to check for up to date Internet traffic patterns, including those related to Israel and Palestine. Follow Cloudflare Radar on social media at @CloudflareRadar (Twitter/X), cloudflare.social/@radar (Mastodon), and radar.cloudflare.com (Bluesky).\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nCloudflare Radar Trends Internet Traffic Outage \nRelated Posts\nApril 12, 2022 2:12PM\nDDoS Attack Trends for 2022 Q1\nWelcome to our first DDoS report of 2022, and the ninth in total so far. This report includes new data points and insights both in the application-layer and network-layer sections — as observed across the global Cloudflare network between January and March 2022...\nBy \nJanuary 10, 2022 1:58PM\nDDoS Attack Trends for Q4 2021\nIn Q4, we observed a 95% increase in L3/4 DDoS attacks and record-breaking levels of Ransom DDoS attacks. The Manufacturing industry was the most targeted alongside a 5,800% increase in SNMP-based DDoS attacks and massive campaigns against VoIP providers around the world...\nBy \nNovember 09, 2021 12:59PM\nA Brief History of the Meris Botnet\nOver the past months, we’ve been tracking and analyzing the activity of the Meris botnet....\nBy \nNovember 04, 2021 12:58PM\nDDoS Attack Trends for Q3 2021\nIn Q3, 2021 we saw and mitigated record-setting HTTP DDoS attacks, terabit-strong network layer attacks, one of the largest botnets ever deployed (Meris), and more recently, ransom attacks on Voice-over-IP (VoIP) service providers....\nBy",
      "markdown": "10/09/2023\n\n*   [![João Tomé](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/V0x3WKfJ_400x400-1.jpeg)](https://blog.cloudflare.com/author/joao-tome/)\n\n6 min read\n\n![](https://blog.cloudflare.com/content/images/2023/10/b.png)\n\nOn Saturday, October 7, 2023, attacks from the Palestinian group Hamas launched from the Gaza Strip against the south of Israel started a new [conflict](https://en.wikipedia.org/wiki/October_2023_Gaza%E2%88%92Israel_conflict) in the region. Israel [officially declared](https://apnews.com/live/israel-hamas-war-live-updates#0000018b-0f83-d2a1-a1df-8fcb0a320000) that it is at war the next day. Cloudflare's data shows that Internet traffic was impacted in different ways, both in Israel and Palestine, with two networks (autonomous systems) in the Gaza Strip going offline a few hours after the attacks. Subsequently, on October 9, two additional networks also experienced outages. We also saw an uptick in cyberattacks targeting Israel, including a 1.26 billion HTTP requests DDoS attack, and Palestine.\n\nStarting with general Internet traffic trends, there was a clear increase in Internet traffic right after the attacks [reportedly began](https://www.aljazeera.com/news/2023/10/7/sirens-warn-of-rockets-launched-towards-israel-from-gaza-news-reports) (03:30 UTC, 06:30 local time). Traffic spiked at around 03:35 UTC (06:35 local time) in both Israel (~170% growth compared with the previous week) and Palestine (100% growth).\n\nThat growth is consistent with other situations, where we’ve seen surges in Internet traffic when countrywide events occur and people are going online to check for news, updates, and more information on what is happening, with social media and messaging also playing a role. However, in Palestine, that traffic growth was followed by a clear drop in traffic around 08:00 UTC (11:00 local time).\n\n![](https://blog.cloudflare.com/content/images/2023/10/1b.png)\n\n![](https://blog.cloudflare.com/content/images/2023/10/2.png)\n\nThe Palestine uptick in traffic after the Hamas attacks started is more visible when only looking at HTTP requests. Requests in Palestine dropped on Saturday and Sunday, October 7 and 8, as much as 20% and 25%, respectively.\n\n![](https://blog.cloudflare.com/content/images/2023/10/3.png)\n\n## Palestine's outages and Internet impact\n\nWhat drove the drop in Internet traffic in Palestine? Our data shows that two Gaza Strip related networks ([autonomous systems or ASNs](https://www.cloudflare.com/learning/network-layer/what-is-an-autonomous-system/)) were offline on that October 7 morning. Fusion ([AS42314](https://radar.cloudflare.com/as42314)) was offline from 08:00 UTC, but saw some recovery after 17:00 UTC the next day; this only lasted for a few hours, given that it went back offline after 12:00 UTC this Monday, October 9.\n\n![](https://blog.cloudflare.com/content/images/2023/10/4.png)\n\nIt was the same scenario for DCC North ([AS203905](https://radar.cloudflare.com/as203905)), but it went offline after 10:00 UTC and with no recovery of traffic observed as of Monday, October 9. These Internet disruptions may be related to [power outages](https://apnews.com/article/israel-palestinians-gaza-hamas-rockets-airstrikes-tel-aviv-11fb98655c256d54ecb5329284fc37d2) in the Gaza Strip.\n\n![](https://blog.cloudflare.com/content/images/2023/10/5.png)\n\nDuring the day on October 7, other Palestinian networks saw less traffic than usual. JETNET ([AS199046](https://radar.cloudflare.com/as199046)) had around half of the usual traffic after 08:00 UTC, similar to SpeedClick ([AS57704](https://radar.cloudflare.com/as57704)), which had around 60% less traffic. After 14:15 on October 9, traffic to those networks dropped sharply (a 95% decrease compared with the previous week), showing only residual traffic.\n\nWhen looking more closely at the Gaza Strip specifically, we can see that some districts or governorates had a drop in HTTP requests a few hours after the first Hamas attacks. The Gaza Governorate was impacted, with traffic dropping on October 7, 2023, after 09:15 UTC. On October 9, at 18:00 UTC, traffic was 46% lower than in the previous week. (Note: there were spikes in traffic during Friday, October 6, several hours before the attacks, but it is unclear what caused those spikes.)\n\n![](https://blog.cloudflare.com/content/images/2023/10/6.png)\n\nThe Deir al-Balah Governorate (on October 9, at 18:00 UTC, traffic was 46% lower than in the previous week) and the Khan Yunis Governorate (50% lower) also both experienced similar drops in traffic:\n\n![](https://blog.cloudflare.com/content/images/2023/10/7.png)\n\n![](https://blog.cloudflare.com/content/images/2023/10/8.png)\n\nIn the Rafah Governorate traffic dropped after 19:00 UTC on October 8 (and on October 9, at 18:00 UTC, traffic was 65% lower than in the previous week).\n\n![](https://blog.cloudflare.com/content/images/2023/10/9.png)\n\nOther Palestinian governorates in the West Bank did not experience the same impact to Internet traffic.\n\n## Spikes in Internet traffic in Israel\n\nIn [Israel](https://radar.cloudflare.com/traffic/il?dateRange=7d), Internet traffic surged to ~170% as compared to the previous week right after the Hamas attacks on October 7 at around 03:35 UTC (06:35 local time), and again at around 16:00 UTC (19:00 local time), with ~80% growth compared to the previous week. In both cases, the increase was driven by mobile device traffic.\n\n![](https://blog.cloudflare.com/content/images/2023/10/10.png)\n\nThere was also increased traffic, as compared with usual levels, on Sunday, October 8, with notable spikes at around 06:00 (09:00 local time) and 12:00 UTC (15:00 local time), seen in the HTTP requests traffic graph below.\n\n![](https://blog.cloudflare.com/content/images/2023/10/11.png)\n\nMobile device traffic drove the Saturday, October 7 spikes in traffic, with the daily mobile device usage percentage reaching its highest in the past two months, reaching 56%.\n\n![](https://blog.cloudflare.com/content/images/2023/10/12-1.png)\n\nLooking at specific Israel districts, traffic looks similar to the nationwide perspective.\n\n## Cyber attacks targeting Israel\n\nCyber attacks are frequent, recurrent, and are not necessarily dependent on actual wars on the ground, as our [2023 attacks landscape](https://blog.cloudflare.com/an-august-reading-list-about-online-security-and-2023-attacks-landscape/) clearly showed. However, it is not unusual to see cyberattacks launched in tandem with ground assaults. We saw that in [Ukraine](https://blog.cloudflare.com/one-year-of-war-in-ukraine/), an uptick in cyber attacks started just before war began there on February 24, 2022, and were even more constant, and spread to other countries after that day.\n\nIn Israel, we saw a [clear uptick](https://blog.cloudflare.com/ddos-threat-report-2023-q1/) in cyber attacks earlier this year, with another wave of notable attacks on October 7 and October 8, 2023, after the Hamas attacks. The largest ones were DDoS attacks targeting Israeli newspapers. One attack on October 8, reached 1.26 billion daily requests blocked by Cloudflare as DDoS attacks, and the other reached 346 million daily requests on October 7, and 332 million daily requests the following day.\n\n![](https://blog.cloudflare.com/content/images/2023/10/13-1.png)\n\nLooking at these DDoS attacks in terms of requests per second, one of the impacted sites experienced a peak of 1.1 million requests per second on October 8 at 02:00 UTC, and the other Israeli newspaper saw a peak of 745k requests per second at around 06:00 the same day.\n\n![](https://blog.cloudflare.com/content/images/2023/10/14-1.png)\n\nIn Palestine, we also saw application layer DDoS attacks, but not as big. The main one in the past three months was on October 7, 2023, targeting a Palestine online newspaper, reaching 105 million daily requests.\n\n![](https://blog.cloudflare.com/content/images/2023/10/15-1.png)\n\nLooking at these most notable DDoS attacks targeting Palestine in terms of requests per second (rps), the most impacted site (a Palestinian newspaper) experienced a peak of 214k requests per second at around 17:20 UTC on October 7.\n\n![](https://blog.cloudflare.com/content/images/2023/10/16-1.png)\n\n## Follow Cloudflare Radar for up to date information\n\nWe will continue to monitor trends related to this conflict. You can use [Cloudflare Radar](https://radar.cloudflare.com/?ref=blog.cloudflare.com) to check for up to date Internet traffic patterns, including those related to [Israel](https://radar.cloudflare.com/il) and [Palestine](https://radar.cloudflare.com/ps). Follow Cloudflare Radar on social media at [@CloudflareRadar](https://twitter.com/CloudflareRadar) (Twitter/X), [cloudflare.social/@radar](https://cloudflare.social/@radar) (Mastodon), and [radar.cloudflare.com](https://bsky.app/profile/radar.cloudflare.com) (Bluesky).\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Cloudflare Radar](https://blog.cloudflare.com/tag/cloudflare-radar/) [Trends](https://blog.cloudflare.com/tag/trends/) [Internet Traffic](https://blog.cloudflare.com/tag/internet-traffic/) [Outage](https://blog.cloudflare.com/tag/outage/)\n\nRelated Posts\n\nApril 12, 2022 2:12PM\n\n[\n\n## DDoS Attack Trends for 2022 Q1\n\n](https://blog.cloudflare.com/ddos-attack-trends-for-2022-q1/)\n\nWelcome to our first DDoS report of 2022, and the ninth in total so far. This report includes new data points and insights both in the application-layer and network-layer sections — as observed across the global Cloudflare network between January and March 2022...\n\nBy \n\nJanuary 10, 2022 1:58PM\n\n[\n\n## DDoS Attack Trends for Q4 2021\n\n](https://blog.cloudflare.com/ddos-attack-trends-for-2021-q4/)\n\nIn Q4, we observed a 95% increase in L3/4 DDoS attacks and record-breaking levels of Ransom DDoS attacks. The Manufacturing industry was the most targeted alongside a 5,800% increase in SNMP-based DDoS attacks and massive campaigns against VoIP providers around the world...\n\nBy \n\nNovember 09, 2021 12:59PM\n\n[\n\n## A Brief History of the Meris Botnet\n\n](https://blog.cloudflare.com/meris-botnet/)\n\nOver the past months, we’ve been tracking and analyzing the activity of the Meris botnet....\n\nBy \n\nNovember 04, 2021 12:58PM\n\n[\n\n## DDoS Attack Trends for Q3 2021\n\n](https://blog.cloudflare.com/ddos-attack-trends-for-2021-q3/)\n\nIn Q3, 2021 we saw and mitigated record-setting HTTP DDoS attacks, terabit-strong network layer attacks, one of the largest botnets ever deployed (Meris), and more recently, ransom attacks on Voice-over-IP (VoIP) service providers....\n\nBy"
    },
    {
      "url": "https://blog.cloudflare.com/1-1-1-1-lookup-failures-on-october-4th-2023/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/1-1-1-1-lookup-failures-on-october-4th-2023/",
        "loadedTime": "2023-12-05T02:31:51.926Z",
        "referrerUrl": "https://blog.cloudflare.com/page/2/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/1-1-1-1-lookup-failures-on-october-4th-2023/",
        "title": "1.1.1.1 lookup failures on  October 4th, 2023",
        "description": "On 4 October 2023, Cloudflare experienced DNS resolution problems. Some users may have received SERVFAIL DNS responses to valid queries. In this blog, we’re going to talk about what the failure was, why it occurred, and what we’re doing to make sure this doesn’t happen again",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/04/2023\n9 min read\nThis post is also available in 简体中文, 繁體中文, 日本語, Deutsch, Français, 한국어 and Español.\nOn 4 October 2023, Cloudflare experienced DNS resolution problems starting at 07:00 UTC and ending at 11:00 UTC. Some users of 1.1.1.1 or products like WARP, Zero Trust, or third party DNS resolvers which use 1.1.1.1 may have received SERVFAIL DNS responses to valid queries. We’re very sorry for this outage. This outage was an internal software error and not the result of an attack. In this blog, we’re going to talk about what the failure was, why it occurred, and what we’re doing to make sure this doesn’t happen again. \nBackground\nIn the Domain Name System (DNS), every domain name exists within a DNS zone. The zone is a collection of domain names and host names that are controlled together. For example, Cloudflare is responsible for the domain name cloudflare.com, which we say is in the “cloudflare.com” zone. The .com top-level domain (TLD) is owned by a third party and is in the “com” zone. It gives directions on how to reach cloudflare.com. Above all of the TLDs is the root zone, which gives directions on how to reach TLDs. This means that the root zone is important in being able to resolve all other domain names. Like other important parts of the DNS, the root zone is signed with DNSSEC, which means the root zone itself contains cryptographic signatures.\nThe root zone is published on the root servers, but it is also common for DNS operators to retrieve and retain a copy of the root zone automatically so that in the event that the root servers cannot be reached, the information in the root zone is still available. Cloudflare’s recursive DNS infrastructure takes this approach as it also makes the resolution process faster. New versions of the root zone are normally published twice a day. 1.1.1.1 has a WebAssembly app called static_zone running on top of the main DNS logic that serves those new versions when they are available.\nWhat happened\nOn 21 September, as part of a known and planned change in root zone management, a new resource record type was included in the root zones for the first time. The new resource record is named ZONEMD, and is in effect a checksum for the contents of the root zone.\nThe root zone is retrieved by software running in Cloudflare’s core network. It is subsequently redistributed to Cloudflare’s data centers around the world. After the change, the root zone containing the ZONEMD record continued to be retrieved and distributed as normal. However, the 1.1.1.1 resolver systems that make use of that data had problems parsing the ZONEMD record. Because zones must be loaded and served in their entirety, the system’s failure to parse ZONEMD meant the new versions of the root zone were not used in Cloudflare’s resolver systems. Some of the servers hosting Cloudflare's resolver infrastructure failed over to querying the DNS root servers directly on a request-by-request basis when they did not receive the new root zone. However, others continued to rely on the known working version of the root zone still available in their memory cache, which was the version pulled on 21 September before the change.\nOn 4 October 2023 at 07:00 UTC, the DNSSEC signatures in the version of the root zone from 21 September expired. Because there was no newer version that the Cloudflare resolver systems were able to use, some of Cloudflare’s resolver systems stopped being able to validate DNSSEC signatures and as a result started sending error responses (SERVFAIL). The rate at which Cloudflare resolvers generated SERVFAIL responses grew by 12%. The diagrams below illustrate the progression of the failure and how it became visible to users.\nIncident timeline and impact\n21 September 6:30 UTC: Last successful pull of the root zone.\n4 October 7:00 UTC: DNSSEC signatures in the root zone obtained on 21 September expired causing an increase in SERVFAIL responses to client queries.\n7:57: First external reports of unexpected SERVFAILs started coming in.\n8:03: Internal Cloudflare incident declared.\n8:50: Initial attempt made at stopping 1.1.1.1 from serving responses using the stale root zone file with an override rule.\n10:30: Stopped 1.1.1.1 from preloading the root zone file entirely.\n10:32: Responses returned to normal.\n11:02: Incident closed.\nThis below chart shows the timeline of impact along with the percentage of DNS queries that returned with a SERVFAIL error:\nWe expect a baseline volume of SERVFAIL errors for regular traffic during normal operation. Usually that percentage sits at around 3%. These SERVFAILs can be caused by legitimate issues in the DNSSEC chain, failures to connect to authoritative servers, authoritative servers taking too long to respond, and many others. During the incident the amount of SERVFAILs peaked at 15% of total queries, although the impact was not evenly distributed around the world and was mainly concentrated in our larger data centers like Ashburn, Virginia; Frankfurt, Germany; and Singapore.\nWhy this incident happened\nWhy parsing the ZONEMD record failed\nDNS has a binary format for storing resource records. In this binary format the type of the resource record (TYPE) is stored as a 16-bit integer. The type of resource record determines how the resource data (RDATA) is parsed. When the record type is 1, this means it is an A record, and the RDATA can be parsed as an IPv4 address. Record type 28 is an AAAA record, whose RDATA can be parsed as an IPv6 address instead. When a parser runs into an unknown resource type it won’t know how to parse its RDATA, but fortunately it doesn’t have to: the RDLENGTH field indicates how long the RDATA field is, allowing the parser to treat it as an opaque data element.\n1 1 1 1 1 1 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+ | | / / / NAME / | | +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+ | TYPE | +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+ | CLASS | +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+ | TTL | | | +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+ | RDLENGTH | +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--| / RDATA / / / +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+ \nRFC 1035\nThe reason static_zone didn’t support the new ZONEMD record is because up until now we had chosen to distribute the root zone internally in its presentation format, rather than in the binary format. When looking at the text representation for a few resource records we can see there is a lot more variation in how different records are presented.\n. 86400 IN SOA a.root-servers.net. nstld.verisign-grs.com. 2023100400 1800 900 604800 86400 . 86400 IN RRSIG SOA 8 0 86400 20231017050000 20231004040000 46780 . J5lVTygIkJHDBt6HHm1QLx7S0EItynbBijgNlcKs/W8FIkPBfCQmw5BsUTZAPVxKj7r2iNLRddwRcM/1sL49jV9Jtctn8OLLc9wtouBmg3LH94M0utW86dKSGEKtzGzWbi5hjVBlkroB8XVQxBphAUqGxNDxdE6AIAvh/eSSb3uSQrarxLnKWvHIHm5PORIOftkIRZ2kcA7Qtou9NqPCSE8fOM5EdXxussKChGthmN5AR5S2EruXIGGRd1vvEYBrRPv55BAWKKRERkaXhgAp7VikYzXesiRLdqVlTQd+fwy2tm/MTw+v3Un48wXPg1lRPlQXmQsuBwqg74Ts5r8w8w== . 518400 IN NS a.root-servers.net. . 86400 IN ZONEMD 2023100400 1 241 E375B158DAEE6141E1F784FDB66620CC4412EDE47C8892B975C90C6A102E97443678CCA4115E27195B468E33ABD9F78C \nExample records taken from https://www.internic.net/domain/root.zone\nWhen we run into an unknown resource record it’s not always easy to know how to handle it. Because of this, the library we use to parse the root zone at the edge does not make an attempt at doing so, and instead returns a parser error.\nWhy a stale version of the root zone was used\nThe static_zone app, tasked with loading and parsing the root zone for the purpose of serving the root zone locally (RFC 7706), stores the latest version in memory. When a new version is published it parses it and, when successfully done so, drops the old version. However, as parsing failed the static_zone app never switched to a newer version, and instead continued using the old version indefinitely. When the 1.1.1.1 service is first started the static_zone app does not have an existing version in memory. When it tries to parse the root zone it fails in doing so, but because it does not have an older version of the root zone to fall back on, it falls back on querying the root servers directly for incoming requests.\nWhy the initial attempt at disabling static_zone didn’t work\nInitially we tried to disable the static_zone app through override rules, a mechanism that allows us to programmatically change some behavior of 1.1.1.1. The rule we deployed was:\nphase = pre-cache set-tag rec_disable_static \nFor any incoming request this rule adds the tag rec_disable_static to the request. Inside the static_zone app we check for this tag and, if it’s set, we do not return a response from the cached, static root zone. However, to improve cache performance queries are sometimes forwarded to another node if the current node can’t find the response in its own cache. Unfortunately, the rec_disable_static tag is not included in the queries being forwarded to other nodes, which caused the static_zone app to continue replying with stale information until we eventually disabled the app entirely.\nWhy the impact was partial\nCloudflare regularly performs rolling reboots of the servers that host our services for tasks like kernel updates that can only take effect after a full system restart. At the time of this outage, resolver server instances that were restarted between the ZONEMD change and the DNSSEC invalidation did not contribute to impact. If they had restarted during this two-week period, they would have failed to load the root zone on startup and fallen back to resolving by sending DNS queries to root servers instead. In addition, the resolver uses a technique called serve stale (RFC 8767) with the purpose of being able to continue to serve popular records from a potentially stale cache to limit the impact. A record is considered to be stale once the TTL amount of seconds has passed since the record was retrieved from upstream. This prevented a total outage; impact was mainly felt in our largest data centers which had many servers that had not restarted the 1.1.1.1 service in that timeframe.\nThis incident had widespread impact, and we take the availability of our services very seriously. We have identified several areas of improvement and will continue to work on uncovering any other gaps that could cause a recurrence.\nHere is what we are working on immediately:\nVisibility: We’re adding alerts to notify when static_zone serves a stale root zone file. It should not have been the case that serving a stale root zone file went unnoticed for as long as it did. If we had been monitoring this better, with the caching that exists, there would have been no impact. It is our goal to protect our customers and their users from upstream changes.\nResilience: We will re-evaluate how we ingest and distribute the root zone internally. Our ingestion and distribution pipelines should handle new RRTYPEs seamlessly, and any brief interruption to the pipeline should be invisible to end users.\nTesting: Despite having tests in place around this problem, including tests related to unreleased changes in parsing the new ZONEMD records, we did not adequately test what happens when the root zone fails to parse. We will improve our test coverage and the related processes.\nArchitecture: We should not use stale copies of the root zone past a certain point. While it’s certainly possible to continue to use stale root zone data for a limited amount of time, past a certain point there are unacceptable operational risks. We will take measures to ensure that the lifetime of cached root zone data is better managed as described in RFC 8806: Running a Root Server Local to a Resolver.\nConclusion\nWe are deeply sorry that this incident happened. There is one clear message from this incident: do not ever assume that something is not going to change! Many modern systems are built with a long chain of libraries that are pulled into the final executable, each one of those may have bugs or may not be updated early enough for programs to operate correctly when changes in input happen. We understand how important it is to have good testing in place that allows detection of regressions and systems and components that fail gracefully on changes to input. We understand that we need to always assume that “format” changes in the most critical systems of the internet (DNS and BGP) are going to have an impact.\nWe have a lot to follow up on internally and are working around the clock to make sure something like this does not happen again.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\n1.1.1.1 Post Mortem Outage",
      "markdown": "10/04/2023\n\n*   [![Ólafur Guðmundsson](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/_tmp_mini_magick20230112-42-p4sd2l.jpg)](https://blog.cloudflare.com/author/olafur-gudmundsson/)\n\n9 min read\n\nThis post is also available in [简体中文](https://blog.cloudflare.com/zh-cn/1-1-1-1-lookup-failures-on-october-4th-2023-zh-cn/), [繁體中文](https://blog.cloudflare.com/zh-tw/1-1-1-1-lookup-failures-on-october-4th-2023-zh-tw/), [日本語](https://blog.cloudflare.com/ja-jp/1-1-1-1-lookup-failures-on-october-4th-2023-ja-jp/), [Deutsch](https://blog.cloudflare.com/de-de/1-1-1-1-lookup-failures-on-october-4th-2023-de-de/), [Français](https://blog.cloudflare.com/fr-fr/1-1-1-1-lookup-failures-on-october-4th-2023-fr-fr/), [한국어](https://blog.cloudflare.com/ko-kr/1-1-1-1-lookup-failures-on-october-4th-2023-ko-kr/) and [Español](https://blog.cloudflare.com/es-es/1-1-1-1-lookup-failures-on-october-4th-2023-es-es/).\n\n![](https://blog.cloudflare.com/content/images/2023/10/DNS-Incident-1.png)\n\nOn 4 October 2023, Cloudflare experienced DNS resolution problems starting at 07:00 UTC and ending at 11:00 UTC. Some users of 1.1.1.1 or products like WARP, Zero Trust, or third party DNS resolvers which use 1.1.1.1 may have received SERVFAIL DNS responses to valid queries. We’re very sorry for this outage. This outage was an internal software error and not the result of an attack. In this blog, we’re going to talk about what the failure was, why it occurred, and what we’re doing to make sure this doesn’t happen again.\n\n## Background\n\nIn the [Domain Name System (DNS)](https://www.cloudflare.com/learning/dns/what-is-dns/), every domain name exists within a DNS zone. The zone is a collection of domain names and host names that are controlled together. For example, Cloudflare is responsible for the domain name cloudflare.com, which we say is in the “cloudflare.com” zone. The .com top-level domain (TLD) is owned by a third party and is in the “com” zone. It gives directions on how to reach cloudflare.com. Above all of the TLDs is [the root zone](https://en.wikipedia.org/wiki/DNS_root_zone), which gives [directions on how to reach TLDs](https://www.iana.org/domains/root/db). This means that the root zone is important in being able to resolve all other domain names. Like other important parts of the DNS, [the root zone is signed with DNSSEC](https://www.iana.org/dnssec/procedures), which means the root zone itself contains cryptographic signatures.\n\nThe root zone is published on [the root servers](https://root-servers.org/), but it is also common for DNS operators to [retrieve and retain a copy of the root zone automatically](https://www.rfc-editor.org/rfc/rfc8806) so that in the event that the root servers cannot be reached, the information in the root zone is still available. Cloudflare’s recursive DNS infrastructure takes this approach as it also makes the resolution process faster. New versions of the root zone are normally published twice a day. 1.1.1.1 has a [WebAssembly app](https://blog.cloudflare.com/big-pineapple-intro/) called static\\_zone running on top of the main DNS logic that serves those new versions when they are available.\n\n![](https://blog.cloudflare.com/content/images/2023/10/image2-1.png)\n\n## What happened\n\nOn 21 September, as part of [a known and planned change in root zone management](https://blog.verisign.com/security/root-zone-zonemd/), a new resource record type was included in the root zones for the first time. The new resource record is named [ZONEMD](https://www.rfc-editor.org/rfc/rfc8976.html), and is in effect a checksum for the contents of the root zone.\n\nThe root zone is retrieved by software running in Cloudflare’s core network. It is subsequently redistributed to Cloudflare’s data centers around the world. After the change, the root zone containing the ZONEMD record continued to be retrieved and distributed as normal. However, the 1.1.1.1 resolver systems that make use of that data had problems parsing the ZONEMD record. Because zones must be loaded and served in their entirety, the system’s failure to parse ZONEMD meant the new versions of the root zone were not used in Cloudflare’s resolver systems. Some of the servers hosting Cloudflare's resolver infrastructure failed over to querying the DNS root servers directly on a request-by-request basis when they did not receive the new root zone. However, others continued to rely on the known working version of the root zone still available in their memory cache, which was the version pulled on 21 September before the change.\n\nOn 4 October 2023 at 07:00 UTC, the DNSSEC signatures in the version of the root zone from 21 September expired. Because there was no newer version that the Cloudflare resolver systems were able to use, some of Cloudflare’s resolver systems stopped being able to validate DNSSEC signatures and as a result started sending error responses (SERVFAIL). The rate at which Cloudflare resolvers generated SERVFAIL responses grew by 12%. The diagrams below illustrate the progression of the failure and how it became visible to users.\n\n![](https://blog.cloudflare.com/content/images/2023/10/image3-1.png)\n\n## Incident timeline and impact\n\n**21 September 6:30 UTC**: Last successful pull of the root zone.  \n**4 October 7:00 UTC**: DNSSEC signatures in the root zone obtained on 21 September expired causing an increase in SERVFAIL responses to client queries.  \n**7:57**: First external reports of unexpected SERVFAILs started coming in.  \n**8:03**: Internal Cloudflare incident declared.  \n**8:50**: Initial attempt made at stopping 1.1.1.1 from serving responses using the stale root zone file with an override rule.  \n**10:30**: Stopped 1.1.1.1 from preloading the root zone file entirely.  \n**10:32**: Responses returned to normal.  \n**11:02**: Incident closed.\n\nThis below chart shows the timeline of impact along with the percentage of DNS queries that returned with a SERVFAIL error:\n\n![](https://blog.cloudflare.com/content/images/2023/10/image1-3.png)\n\nWe expect a baseline volume of SERVFAIL errors for regular traffic during normal operation. Usually that percentage sits at around 3%. These SERVFAILs can be caused by legitimate issues in the DNSSEC chain, failures to connect to authoritative servers, authoritative servers taking too long to respond, [and many others](https://blog.cloudflare.com/unwrap-the-servfail/). During the incident the amount of SERVFAILs peaked at 15% of total queries, although the impact was not evenly distributed around the world and was mainly concentrated in our larger data centers like Ashburn, Virginia; Frankfurt, Germany; and Singapore.\n\n## Why this incident happened\n\n#### Why parsing the ZONEMD record failed\n\nDNS has a binary format for storing resource records. In this binary format the type of the resource record (TYPE)  is stored as a 16-bit integer. The type of resource record determines how the resource data (RDATA) is parsed. When the record type is 1, this means it is an A record, and the RDATA can be parsed as an IPv4 address. Record type 28 is an AAAA record, whose RDATA can be parsed as an IPv6 address instead. When a parser runs into an unknown resource type it won’t know how to parse its RDATA, but fortunately it doesn’t have to: the RDLENGTH field indicates how long the RDATA field is, allowing the parser to treat it as an opaque data element.\n\n```\n                                   1  1  1  1  1  1\n      0  1  2  3  4  5  6  7  8  9  0  1  2  3  4  5\n    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\n    |                                               |\n    /                                               /\n    /                      NAME                     /\n    |                                               |\n    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\n    |                      TYPE                     |\n    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\n    |                     CLASS                     |\n    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\n    |                      TTL                      |\n    |                                               |\n    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\n    |                   RDLENGTH                    |\n    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--|\n    /                     RDATA                     /\n    /                                               /\n    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\n```\n\n[RFC 1035](https://www.ietf.org/rfc/rfc1035.txt)\n\nThe reason static\\_zone didn’t support the new ZONEMD record is because up until now we had chosen to distribute the root zone internally in its presentation format, rather than in the binary format. When looking at the text representation for a few resource records we can see there is a lot more variation in how different records are presented.\n\n```\n.\t\t\t86400\tIN\tSOA\ta.root-servers.net. nstld.verisign-grs.com. 2023100400 1800 900 604800 86400\n.\t\t\t86400\tIN\tRRSIG\tSOA 8 0 86400 20231017050000 20231004040000 46780 . J5lVTygIkJHDBt6HHm1QLx7S0EItynbBijgNlcKs/W8FIkPBfCQmw5BsUTZAPVxKj7r2iNLRddwRcM/1sL49jV9Jtctn8OLLc9wtouBmg3LH94M0utW86dKSGEKtzGzWbi5hjVBlkroB8XVQxBphAUqGxNDxdE6AIAvh/eSSb3uSQrarxLnKWvHIHm5PORIOftkIRZ2kcA7Qtou9NqPCSE8fOM5EdXxussKChGthmN5AR5S2EruXIGGRd1vvEYBrRPv55BAWKKRERkaXhgAp7VikYzXesiRLdqVlTQd+fwy2tm/MTw+v3Un48wXPg1lRPlQXmQsuBwqg74Ts5r8w8w==\n.\t\t\t518400\tIN\tNS\ta.root-servers.net.\n.\t\t\t86400\tIN\tZONEMD\t2023100400 1 241 E375B158DAEE6141E1F784FDB66620CC4412EDE47C8892B975C90C6A102E97443678CCA4115E27195B468E33ABD9F78C\n```\n\nExample records taken from [https://www.internic.net/domain/root.zone](https://www.internic.net/domain/root.zone)\n\nWhen we run into an unknown resource record it’s not always easy to know how to handle it. Because of this, the library we use to parse the root zone at the edge does not make an attempt at doing so, and instead returns a parser error.\n\n### Why a stale version of the root zone was used\n\nThe static\\_zone app, tasked with loading and parsing the root zone for the purpose of serving the root zone locally ([RFC 7706](https://datatracker.ietf.org/doc/html/rfc7706)), stores the latest version in memory. When a new version is published it parses it and, when successfully done so, drops the old version. However, as parsing failed the static\\_zone app never switched to a newer version, and instead continued using the old version indefinitely. When the 1.1.1.1 service is first started the static\\_zone app does not have an existing version in memory. When it tries to parse the root zone it fails in doing so, but because it does not have an older version of the root zone to fall back on, it falls back on querying the root servers directly for incoming requests.\n\n![](https://blog.cloudflare.com/content/images/2023/10/image5.png)\n\n### Why the initial attempt at disabling static\\_zone didn’t work\n\nInitially we tried to disable the static\\_zone app through override rules, a mechanism that allows us to programmatically change some behavior of 1.1.1.1. The rule we deployed was:\n\n```\nphase = pre-cache set-tag rec_disable_static\n```\n\nFor any incoming request this rule adds the tag rec\\_disable\\_static to the request. Inside the static\\_zone app we check for this tag and, if it’s set, we do not return a response from the cached, static root zone. However, [to improve cache performance](https://blog.cloudflare.com/big-pineapple-intro/) queries are sometimes forwarded to another node if the current node can’t find the response in its own cache. Unfortunately, the rec\\_disable\\_static tag is not included in the queries being forwarded to other nodes, which caused the static\\_zone app to continue replying with stale information until we eventually disabled the app entirely.\n\n### Why the impact was partial\n\nCloudflare regularly performs rolling reboots of the servers that host our services for tasks like kernel updates that can only take effect after a full system restart. At the time of this outage, resolver server instances that were restarted between the ZONEMD change and the DNSSEC invalidation did not contribute to impact. If they had restarted during this two-week period, they would have failed to load the root zone on startup and fallen back to resolving by sending DNS queries to root servers instead. In addition, the resolver uses a technique called serve stale ([RFC 8767](https://datatracker.ietf.org/doc/html/rfc8767)) with the purpose of being able to continue to serve popular records from a potentially stale cache to limit the impact. A record is considered to be stale once the TTL amount of seconds has passed since the record was retrieved from upstream.  This prevented a total outage; impact was mainly felt in our largest data centers which had many servers that had not restarted the 1.1.1.1 service in that timeframe.\n\nThis incident had widespread impact, and we take the availability of our services very seriously. We have identified several areas of improvement and will continue to work on uncovering any other gaps that could cause a recurrence.\n\nHere is what we are working on immediately:\n\n**Visibility**: We’re adding alerts to notify when static\\_zone serves a stale root zone file. It should not have been the case that serving a stale root zone file went unnoticed for as long as it did. If we had been monitoring this better, with the caching that exists, there would have been no impact. It is our goal to protect our customers and their users from upstream changes.\n\n**Resilience:** We will re-evaluate how we ingest and distribute the root zone internally. Our ingestion and distribution pipelines should handle new RRTYPEs seamlessly, and any brief interruption to the pipeline should be invisible to end users.\n\n**Testing:** Despite having tests in place around this problem, including tests related to unreleased changes in parsing the new ZONEMD records, we did not adequately test what happens when the root zone fails to parse. We will improve our test coverage and the related processes.\n\n**Architecture**: We should not use stale copies of the root zone past a certain point. While it’s certainly possible to continue to use stale root zone data for a limited amount of time, past a certain point there are unacceptable operational risks. We will take measures to ensure that the lifetime of cached root zone data is better managed as described in [RFC 8806: Running a Root Server Local to a Resolver](https://www.rfc-editor.org/rfc/rfc8806).\n\n## Conclusion\n\nWe are deeply sorry that this incident happened. There is one clear message from this incident: do not ever assume that something is not going to change!  Many modern systems are built with a long chain of libraries that are pulled into the final executable, each one of those may have bugs or may not be updated early enough for programs to operate correctly when changes in input happen. We understand how important it is to have good testing in place that allows detection of regressions and systems and components that fail gracefully on changes to input. We understand that we need to always assume that “format” changes in the most critical systems of the internet (DNS and BGP) are going to have an impact.\n\nWe have a lot to follow up on internally and are working around the clock to make sure something like this does not happen again.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[1.1.1.1](https://blog.cloudflare.com/tag/1-1-1-1/) [Post Mortem](https://blog.cloudflare.com/tag/post-mortem/) [Outage](https://blog.cloudflare.com/tag/outage/)"
    },
    {
      "url": "https://blog.cloudflare.com/all-cloudflare-customers-protected-atlassian-cve-2023-22515/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/all-cloudflare-customers-protected-atlassian-cve-2023-22515/",
        "loadedTime": "2023-12-05T02:31:57.440Z",
        "referrerUrl": "https://blog.cloudflare.com/page/2/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/all-cloudflare-customers-protected-atlassian-cve-2023-22515/",
        "title": "All Cloudflare Customers Protected from Atlassian Confluence CVE-2023-22515",
        "description": "On 2023-10-04 at 13:00 UTC, Atlassian released details of the zero-day vulnerability described as “Privilege Escalation Vulnerability in Confluence Data Center and Server” (CVE-2023-22515), a zero-day vulnerability impacting Confluence Server and Data Center products",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/04/2023\n1 min read\nOn 2023-10-04 at 13:00 UTC, Atlassian released details of the zero-day vulnerability described as “Privilege Escalation Vulnerability in Confluence Data Center and Server” (CVE-2023-22515), a zero-day vulnerability impacting Confluence Server and Data Center products. \nCloudflare was warned about the vulnerability before the advisory was published and worked with Atlassian to proactively apply protective WAF rules for all customers. All Cloudflare customers, including Free, received the protection enabled by default. On 2023-10-03 14:00 UTC Cloudflare WAF team released the following managed rules to protect against the first variant of the vulnerability observed in real traffic.\nRule ID\n\t\nDescription\n\t\nDefault Action\n\t\nNew Managed Rules\n…ec9f34e1\n\t\nAtlassian Confluence - Privilege Escalation - CVE:CVE-2023-22515\n\t\nBlock\n\t\nLegacy Managed Rules\n100604 and 100605\n\t\nAtlassian Confluence - Privilege Escalation - CVE:CVE-2023-22515\n\t\nBlock\n\t\nFree Managed Rule\n…91935fcb\n\t\nAtlassian Confluence - Privilege Escalation - CVE:CVE-2023-22515\n\t\nBlock\n\t\nWhen CVE-2023-22515 is exploited, an attacker could access public Confluence Data Center and Server instances to create unauthorized Confluence administrator accounts to access the instance. According to the advisory the vulnerability is assessed by Atlassian as critical. At the moment of writing a CVSS score is not yet known. More information can be found in the security advisory, including what versions of Confluence Server are affected.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nAtlassian CVE WAF",
      "markdown": "10/04/2023\n\n*   [![Himanshu Anand](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/03/unnamed-4.png)](https://blog.cloudflare.com/author/himanshu/)\n*   [![Daniele Molteni](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2020/10/meProfessional-1-flipped.jpg)](https://blog.cloudflare.com/author/daniele/)\n*   [![Sourov Zaman](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/05/Screen-Shot-2022-06-03-at-1.49.26-AM.png)](https://blog.cloudflare.com/author/sourov/)\n*   [![Vaibhav Singhal](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/06/VS_Photo-Final1.png)](https://blog.cloudflare.com/author/vaibhav/)\n*   [![Ary Widdes](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/_tmp_mini_magick20230419-40-19b08mj.jpg)](https://blog.cloudflare.com/author/ary/)\n*   [![Myles Robinson](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/_tmp_mini_magick20230729-2-1pc5jly.jpg)](https://blog.cloudflare.com/author/myles/)\n\n1 min read\n\n![](https://blog.cloudflare.com/content/images/2023/10/Zero-Day-protection-3.png)\n\nOn 2023-10-04 at 13:00 UTC, Atlassian released details of the zero-day vulnerability described as “Privilege Escalation Vulnerability in Confluence Data Center and Server” (CVE-2023-22515), a zero-day vulnerability impacting Confluence Server and Data Center products.  \n\nCloudflare was warned about the vulnerability before the advisory was published and worked with Atlassian to proactively apply protective WAF rules for all customers. All Cloudflare customers, including Free, received the protection enabled by default. On 2023-10-03 14:00 UTC Cloudflare WAF team [released](https://developers.cloudflare.com/waf/change-log/2023-10-04---emergency-release/) the following managed rules to protect against the first variant of the vulnerability observed in real traffic.\n\n|     |     |     |\n| --- | --- | --- |\n| Rule ID | Description | Default Action |\n| New Managed Rules<br><br>…ec9f34e1 | Atlassian Confluence - Privilege Escalation - CVE:CVE-2023-22515 | Block |\n| Legacy Managed Rules<br><br>100604 and 100605 | Atlassian Confluence - Privilege Escalation - CVE:CVE-2023-22515 | Block |\n| Free Managed Rule<br><br>…91935fcb | Atlassian Confluence - Privilege Escalation - CVE:CVE-2023-22515 | Block |\n\nWhen CVE-2023-22515 is exploited, an attacker could access public Confluence Data Center and Server instances to create unauthorized Confluence administrator accounts to access the instance. According to the advisory the vulnerability is assessed by Atlassian as critical. At the moment of writing a CVSS score is not yet known. More information can be found in the [security advisory](https://confluence.atlassian.com/security/cve-2023-22515-privilege-escalation-vulnerability-in-confluence-data-center-and-server-1295682276.html?subid=1643554570&jobid=106230797&utm_campaign=security-advisory-confluence-sdc_EML-16991&utm_medium=email&utm_source=alert-email), including what versions of Confluence Server are affected.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Atlassian](https://blog.cloudflare.com/tag/atlassian/) [CVE](https://blog.cloudflare.com/tag/cve/) [WAF](https://blog.cloudflare.com/tag/waf/)"
    },
    {
      "url": "https://blog.cloudflare.com/multihost-waiting-room/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/multihost-waiting-room/",
        "loadedTime": "2023-12-05T02:31:57.740Z",
        "referrerUrl": "https://blog.cloudflare.com/page/2/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/multihost-waiting-room/",
        "title": "Waiting Room adds multi-host and path coverage, unlocking broader protection and multilingual setups",
        "description": "Today, we are thrilled to announce that Waiting Room now supports coverage of multiple hostname and path combinations with a single waiting room, giving customers more flexibility and offering broader site coverage without interruptions to end-user flows.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/04/2023\n11 min read\nThis post is also available in 日本語, Français, Español and Deutsch.\nCloudflare Waiting Room protects sites from overwhelming traffic surges by placing excess visitors in a fully customizable virtual waiting room, admitting them dynamically as spots become available. Instead of throwing error pages or delivering poorly-performing site pages, Waiting Room empowers customers to take control of their end-user experience during unmanageable traffic surges.\nA key decision customers make when setting up a waiting room is what pages it will protect. Before now, customers could select one hostname and path combination to determine what pages would be covered by a waiting room. Today, we are thrilled to announce that Waiting Room now supports coverage of multiple hostname and path combinations with a single waiting room, giving customers more flexibility and offering broader site coverage without interruptions to end-user flows. This new capability is available to all Enterprise customers with an Advanced Purchase of Waiting Room.\nWaiting Room site placement\nAs part of the simple, no-coding-necessary process for deploying a waiting room, customers specify a hostname and path combination to indicate which pages are covered by a particular waiting room. When a site visitor makes a preliminary request to that hostname and path or any of its subpaths, they will be issued a waiting room cookie and either be admitted to the site or placed in a waiting room if the site is at capacity.\nLast year, we added Waiting Room Bypass Rules, giving customers many options for creating exceptions to this hostname and path coverage. This unlocked capabilities such as User Agent Bypassing, geo-targeting, URL exclusions, and administrative IP bypassing. It also allowed for some added flexibility regarding which pages a waiting room would apply to on a customer’s site by adding the ability to exclude URLs, paths, and query strings. While this update allowed for greater specificity regarding which traffic should be gated by Waiting Room, it didn’t offer the broader coverage that many customers still needed to protect greater portions of their sites with a single waiting room.\nWhy customers needed broader Waiting Room coverage\nLet’s review some simple yet impactful examples of why this broader coverage option was important for our customers. Imagine you have an online store, example.com, and you want to ensure that a single waiting room covers the entire customer journey — from the homepage, to product browsing, to checkout. Many sites would delineate these steps in the flow using paths: example.com/, example.com/shop/product1, example.com/checkout. Because Waiting Room assumes an implied wildcard at the end of a waiting room’s configured path, this use case was already satisfied for these sites. Thus, placing a waiting room at example.com/ would cover all the URLs associated with every step of this customer journey. In this setup, once a site visitor has passed the waiting room, they would not be re-queued at any step in their user flow because they are still using the same waiting room’s cookie, which indicates to Waiting Room that they are the same user between URLs.\nHowever, many sites delineate different steps of this type of shopping flow using subdomains instead of or as well as paths. For example, many sites will have their checkout page on a different subdomain, such as checkout.example.com. Before now, if a customer had this site structure and wanted to protect their entire site with a single waiting room, they would have needed to deploy a waiting room at example.com/ and another at checkout.example.com/. This option was not ideal for many customers because a site visitor could be queued at two different parts of the same customer journey. This is because the waiting room at checkout.example.com/ would count the same visitor as a different user than the waiting room covering example.com/.\nThat said, there are cases where it is wise to separate waiting rooms on a single site. For example, a ticketing website could place a waiting room at its apex domain (example.com) and distinct waiting rooms with pre-queues on pages for specific events (example.com/popular_artist_tour). The waiting room set at example.com/ ensures that the main point of entry to the site does not get overwhelmed and crash when ticket sales for any one event open. The waiting room placed on the specific event page ensures that traffic for a single event can start queuing ahead of the event without impacting traffic going to other parts of the site.\nUltimately, whether a customer wants one or multiple waiting rooms to protect their site, we wanted to give our customers the flexibility to deploy Waiting Room however was best for their use cases and site structure. We’re thrilled to announce that Waiting Room now supports multi-hostname and path coverage with a single waiting room!\nGetting started with multi-host and path coverage\nCustomers can now configure a waiting room on multiple hostname and path combinations — or routes — belonging to the same zone. To do so, navigate to Traffic > Waiting Room and select Create. The name of your domain will already be populated. To add additional routes to your waiting room configuration, select Add Hostname and Path. You can then enter another hostname and path to be covered by the same waiting room. Remember, there is an implied wildcard after each path. Therefore, creating a waiting room for each URL you want your waiting room to cover is unnecessary. Only create additional routes for URLs that wouldn’t be covered by the other hostname and path combinations you have already entered.\nWhen deploying a waiting room that covers multiple hostname and path combinations, you must create a unique cookie name for this waiting room (more on that later!). Then, proceed with the same workflow as usual to deploy your waiting room.\nDeploying a multi-language waiting room\nA frequent customer request was the ability to cover a multi-language site with a single waiting room — offering different text per language while counting all site traffic toward the same waiting room limits. There are various ways a site can be structured to delineate between different language options, but the two most common are either by subdomain or path. For a site that uses path delineation, this could look like example.com/en and example.com/es for English and Spanish, respectively. For a site using subdomain delineation, this could look like en.example.com/ and es.example.com/. Before multihost Waiting Room coverage, the subdomain variation would not have been able to be covered by a single waiting room.\nWaiting Room’s existing configuration options already supported the path variation. However, this was only true if the customer wanted to gate the entire site, which they could do by placing the waiting room at example.com/. Many e-commerce customers asked for the ability to gate different high-demand product pages selling the same product but with different language options. For instance, consider example.com/en/product_123 and example.com/es/product_123, where the customer wants the same waiting room and traffic limits to cover both URLs. Before now, this would not have been possible without some complex bypass rule logic.\nNow, customers can deploy a waiting room that accommodates either the subdomain or path approach to structuring a multi-language site. The only remaining step is setting up your waiting room to serve different languages when a user is queued in a waiting room. This can be achieved by constructing a template that reads the URL to determine the locale and define the appropriate translations for each of the locales within the template.\nHere is an example of a template that determines the locale from the URL path, and renders the translated text:\n<!DOCTYPE html> <html> <head> <title>Waiting Room powered by Cloudflare</title> </head> <body> <section> <h1 id=\"inline-msg\"> You are now in line. </h1> <h1 id=\"patience-msg\"> Thank you for your patience. </h1> </section> <h2 id=\"waitTime\"></h2> <script> var locale = location.pathname.split(\"/\")[1] || \"en\"; var translations = { \"en\": { \"waittime_60_less\": \"Your estimated wait time is {{waitTime}} minute.\", \"waittime_60_greater\": \"Your estimated wait time is {{waitTimeHours}} hours and {{waitTimeHourMinutes}} minutes.\", \"inline-msg\": \"You are now in line.\", \"patience-msg\": \"Thank you for your patience.\", }, \"es\": { \"waittime_60_less\": \"El tiempo de espera estimado es {{waitTime}} minuto.\", \"waittime_60_greater\": \"El tiempo de espera estimado es {{waitTimeHours}} de horas y {{waitTimeHourMinutes}} minutos.\", \"inline-msg\": \"Ahora se encuentra en la fila de espera previa.\", \"patience-msg\": \"Gracias por su paciencia.\", } }; {{#waitTimeKnown}} var minutes = parseInt( {{waitTime}} , 10); var time = document.getElementById('waitTime'); if ( minutes < 61) { time.innerText = translations[locale][\"waittime_60_less\"] } else { time.innerText = translations[locale][\"waittime_60_greater\"] } {{/waitTimeKnown}} // translate template text for each of the elements with “id” suffixed with “msg” for (const msg of [\"inline-msg\", \"patience-msg\"]) { const el = document.getElementById(msg); if (el == null) continue; el.innerText = translations[locale][msg]; } </script> </body> </html> \nHere’s how the template works: given a site distinguishes different locales with various paths such as /en/product_123 or /es/product_123 in the <script /> body after the rendering the page, the locale is extracted from the location.pathname via locale = location.pathname.split(“/”)[1]. If there isn’t a locale specified within the translations object we default it to “en”. For example, if a user visits example.com/product_123, then by default render the English text template.\nSimilarly, in order to support a multi-language waiting room for sites that delineate site structure via subdomain, it would only require you to update how you extract the locale from the URL. Instead of looking at the pathname we look at the hostname property of the window.location object like locale = location.hostname.split(“.”)[0], given, we have site structure as following: en.example.com, es.example.com.\nThe above code is a pared down example of starter templates we have added to our developer documentation, which we have included to make it easier for you to get started with a multi-language waiting room. You can download these templates and edit them to have the look and feel aligned with your site and with the language options your site offers.\nThis is an example of the starter template provided in dev docs. This waiting room is in Queue-all mode and displays the Spanish text when the user visits example.com/es/product_123.\nThese templates can further be expanded to include other languages by adding translations to the `translations` object for each of the locales. Thus, a single template is able to serve multiple languages depending on whatever the locale the site is being served as either via subdomain or path.\nHow we handle cookies with a multihostname and path waiting room\nThe waiting room assigns a __cfwaitingroom cookie to each user to manage the state of the user that determines the position of the user in line among other properties needed to make the queueing decisions for the user. Traditionally, for a single hostname and path configuration it was trivial to just set the cookie as __cfwaitingroom=[cookie-value]; Domain=example.com; Path=/es/product_123. This ensured that when the page refreshed it would send us the same Waiting Room cookie for us to examine and update. However, this became non-trivial when we wanted to share the same cookie across different subdomain or path combinations, for example, on example.com/en/product_123.\nApproaches to setting multiple cookies\nThere were two approaches that we brainstormed and evaluated to allow the sharing of the cookie for the same waiting room. \nThe first approach we considered was to issue multiple Set-Cookie headers in the HTTP response. For example, in the multi-language example above, we could issue the following in the response header:\nSet-Cookie: __cfwaitingroom=[cookie-value]…Domain=example.com; Path=/en/product_123 … Set-Cookie: __cfwaitingroom=[cookie-value]...Domain=example.com; Path=/es/product_123 … \nThis approach would allow us to handle the multiple hostnames and paths for the same waiting room. However, it does not present itself as a very scalable solution. Per RFC6265, there isn’t a strict limit defined in the specification, browsers have their own implementation-specific limits. For example, Chrome allows the response header to grow up to 256K bytes before throwing the transaction with ERR_RESPONSE_HEADERS_TOO_BIG. Additionally, in this case, the response header would grow proportionally to the number of unique hostname and path combinations, and we would be redundantly repeating the same cookie value N (where N is the number of additional routes) number of times. While this approach would have allowed us to effectively handle a bounded list of hostname and path combinations that need to be set, it was not ideal for cases where we can expect more than a handful routes for a particular site.\nThe second approach that we considered and chose to move forward with was to set the cookie on the apex domain (or the most common subdomain). In other words, we would figure out the most common subdomain from the list of routes and use that as the domain. Similarly, for the paths, this would entail determining the least common prefix from the list of paths and use that as the value to be set on the path attribute. For example, consider a waiting room with the following two routes configured, a.example.com/shoes/product_123 and b.example.com/shoes/product_456.\nTo determine the domain that would be set for the cookie, we can see that example.com is the most common subdomain among the list of domains. Applying the most common subdomain algorithm, we would get example.com as the domain. Applying the least common prefix algorithm on the set of paths, /shoes/product_123 and /shoes/product_456, we would get /shoes as the path. Thus, the set-cookie header would be the following:\nSet-Cookie: … __cfwaitingroom=[cookie-value]; Domain=example.com; Path=/shoes … \nNow, when a visitor accesses any of the pages covered by this waiting room, we can guarantee Waiting Room receives the right cookie and there will only be Set-Cookie included in the response header.\nHowever, we are still not there yet. Although we are able to identify the common subdomain (or apex domain) and common path prefix, there would still be a problem if routes from one waiting room would overlap with another waiting room. For example, say we configure two waiting rooms for the same site where we are selling our special shoes:\nWaiting Room A\na.example.com/shoes/product_123\nb.example.com/shoes/product_456\nWaiting Room B\nc.example.com/shoes/product_789\nd.example.com/shoes/product_012\nIf we apply the same algorithm as described above, we would get the same domain and path properties set for the two cookies. For Waiting Room A, the domain would be example.com and the path would be /shoes. Similarly, for Waiting Room B, the most common subdomain would also be example.com and least common path prefix would be /shoes. This would effectively prevent us from distinguishing the two cookies and route the visitor to the right waiting room. In order to solve the issue of overlapping cookie names, we introduced the requirement of setting a custom suffix that is unique to the customer’s zone. This is why it is required to provide a custom cookie suffix when configuring a waiting room that covers multiple hostnames or paths. Therefore, if Waiting Room A was assigned cookie suffix “a” and Waiting Room B was assigned cookie suffix “b”, the two Set-Cookie definitions would look like the following:\n\nWaiting Room A\nSet-Cookie: __cfwaitingroom_a=[cookie-value]; Domain=example.com; Path=/shoes \nWaiting Room B\nSet-Cookie: __cfwaitingroom_b=[cookie-value]; Domain=example.com; Path=/shoes \nWhen a visitor makes a request to any pages where the two different waiting rooms are configured, Waiting Room can distinguish and select which cookie corresponds to the given request and use this to determine which waiting room’s settings apply to that user depending on where they are on the site.\nWith the addition of multihost and multipath Waiting Room coverage, we’re thrilled to offer more flexible options for incorporating Waiting Room into your site, giving your visitors the best experience possible while protecting your site during times of high traffic. Make sure your site is always protected from traffic surges. Try out Waiting Room today or reach out to us to learn more about the Advanced Waiting Room!\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nWaiting Room Always Online Traffic",
      "markdown": "10/04/2023\n\n*   [![Arielle Olache](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/03/20181111-144427-Original-1-.jpg)](https://blog.cloudflare.com/author/arielle/)\n*   [![Yawar Jamal](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/10/dp.jpg)](https://blog.cloudflare.com/author/yawar/)\n\n11 min read\n\nThis post is also available in [日本語](https://blog.cloudflare.com/ja-jp/multihost-waiting-room-ja-jp/), [Français](https://blog.cloudflare.com/fr-fr/multihost-waiting-room-fr-fr/), [Español](https://blog.cloudflare.com/es-es/multihost-waiting-room-es-es/) and [Deutsch](https://blog.cloudflare.com/de-de/multihost-waiting-room-de-de/).\n\n![](https://blog.cloudflare.com/content/images/2023/10/image4.png)\n\n[Cloudflare Waiting Room](https://www.cloudflare.com/application-services/products/waiting-room/) protects sites from overwhelming traffic surges by placing excess visitors in a fully customizable virtual waiting room, admitting them dynamically as spots become available. Instead of throwing error pages or delivering poorly-performing site pages, Waiting Room empowers customers to take control of their end-user experience during unmanageable traffic surges.\n\n![Customize the look and feel of your waiting room to enhance the end user experience!](https://blog.cloudflare.com/content/images/2023/10/image1.png)\n\nA key decision customers make when setting up a waiting room is what pages it will protect. Before now, customers could select one hostname and path combination to determine what pages would be covered by a waiting room. Today, we are thrilled to announce that Waiting Room now supports coverage of multiple hostname and path combinations with a single waiting room, giving customers more flexibility and offering broader site coverage without interruptions to end-user flows. This new capability is available to all Enterprise customers with an Advanced Purchase of Waiting Room.\n\n### Waiting Room site placement\n\nAs part of the simple, no-coding-necessary process for deploying a waiting room, customers specify a hostname and path combination to indicate which pages are covered by a particular waiting room. When a site visitor makes a preliminary request to that hostname and path or any of its subpaths, they will be issued a waiting room cookie and either be admitted to the site or placed in a waiting room if the site is at capacity.\n\nLast year, we added [Waiting Room Bypass Rules](https://blog.cloudflare.com/waiting-room-bypass-rules/), giving customers many options for creating exceptions to this hostname and path coverage. This unlocked capabilities such as User Agent Bypassing, geo-targeting, URL exclusions, and administrative IP bypassing. It also allowed for some added flexibility regarding which pages a waiting room would apply to on a customer’s site by adding the ability to exclude URLs, paths, and query strings. While this update allowed for greater specificity regarding which traffic should be gated by Waiting Room, it didn’t offer the broader coverage that many customers still needed to protect greater portions of their sites with a single waiting room.\n\n### Why customers needed broader Waiting Room coverage\n\nLet’s review some simple yet impactful examples of why this broader coverage option was important for our customers. Imagine you have an online store, example.com, and you want to ensure that a single waiting room covers the entire customer journey — from the homepage, to product browsing, to checkout. Many sites would delineate these steps in the flow using paths: example.com/, example.com/shop/product1, example.com/checkout. Because Waiting Room assumes an implied wildcard at the end of a waiting room’s configured path, this use case was already satisfied for these sites. Thus, placing a waiting room at example.com/ would cover all the URLs associated with every step of this customer journey. In this setup, once a site visitor has passed the waiting room, they would not be re-queued at any step in their user flow because they are still using the same waiting room’s cookie, which indicates to Waiting Room that they are the same user between URLs.\n\nHowever, many sites delineate different steps of this type of shopping flow using subdomains instead of or as well as paths. For example, many sites will have their checkout page on a different subdomain, such as checkout.example.com. Before now, if a customer had this site structure and wanted to protect their entire site with a single waiting room, they would have needed to deploy a waiting room at example.com/ and another at checkout.example.com/. This option was not ideal for many customers because a site visitor could be queued at two different parts of the same customer journey. This is because the waiting room at checkout.example.com/ would count the same visitor as a different user than the waiting room covering example.com/.\n\nThat said, there are cases where it is wise to separate waiting rooms on a single site. For example, a ticketing website could place a waiting room at its apex domain (example.com) and distinct waiting rooms with pre-queues on pages for specific events (example.com/popular\\_artist\\_tour). The waiting room set at example.com/ ensures that the main point of entry to the site does not get overwhelmed and crash when ticket sales for any one event open. The waiting room placed on the specific event page ensures that traffic for a single event can start queuing ahead of the event without impacting traffic going to other parts of the site.\n\nUltimately, whether a customer wants one or multiple waiting rooms to protect their site, we wanted to give our customers the flexibility to deploy Waiting Room however was best for their use cases and site structure. We’re thrilled to announce that Waiting Room now supports multi-hostname and path coverage with a single waiting room!\n\n### Getting started with multi-host and path coverage\n\nCustomers can now configure a waiting room on multiple hostname and path combinations — or routes — belonging to the same zone. To do so, navigate to Traffic > Waiting Room and select Create. The name of your domain will already be populated. To add additional routes to your waiting room configuration, select Add Hostname and Path. You can then enter another hostname and path to be covered by the same waiting room. Remember, there is an implied wildcard after each path. Therefore, creating a waiting room for each URL you want your waiting room to cover is unnecessary. Only create additional routes for URLs that wouldn’t be covered by the other hostname and path combinations you have already entered.\n\n![Add multiple hostname and path combinations to your waiting room by selecting Add Hostname and Path](https://blog.cloudflare.com/content/images/2023/10/image3.png)\n\nWhen deploying a waiting room that covers multiple hostname and path combinations, you must create a unique cookie name for this waiting room (more on that later!). Then, proceed with the [same workflow](https://developers.cloudflare.com/waiting-room/how-to/create-waiting-room/) as usual to deploy your waiting room.\n\n### Deploying a multi-language waiting room\n\nA frequent customer request was the ability to cover a multi-language site with a single waiting room — offering different text per language while counting all site traffic toward the same waiting room limits. There are various ways a site can be structured to delineate between different language options, but the two most common are either by subdomain or path. For a site that uses path delineation, this could look like example.com/en and example.com/es for English and Spanish, respectively. For a site using subdomain delineation, this could look like en.example.com/ and es.example.com/. Before multihost Waiting Room coverage, the subdomain variation would not have been able to be covered by a single waiting room.\n\nWaiting Room’s existing configuration options already supported the path variation. However, this was only true if the customer wanted to gate the entire site, which they could do by placing the waiting room at example.com/. Many e-commerce customers asked for the ability to gate different high-demand product pages selling the same product but with different language options. For instance, consider example.com/en/product\\_123 and example.com/es/product\\_123, where the customer wants the same waiting room and traffic limits to cover both URLs. Before now, this would not have been possible without some complex bypass rule logic.\n\nNow, customers can deploy a waiting room that accommodates either the subdomain or path approach to structuring a multi-language site. The only remaining step is setting up your waiting room to serve different languages when a user is queued in a waiting room. This can be achieved by constructing a template that reads the URL to determine the locale and define the appropriate translations for each of the locales within the template.\n\nHere is an example of a template that determines the locale from the URL path, and renders the translated text:\n\n```\n<!DOCTYPE html>\n<html>\n  <head>\n    <title>Waiting Room powered by Cloudflare</title>\n  </head>\n  <body>\n    <section>\n      <h1 id=\"inline-msg\">\n        You are now in line.\n      </h1>\n      <h1 id=\"patience-msg\">\n        Thank you for your patience.\n      </h1>\n    </section>\n    <h2 id=\"waitTime\"></h2>\n\n    <script>\n      var locale = location.pathname.split(\"/\")[1] || \"en\";\n      var translations = {\n        \"en\": {\n          \"waittime_60_less\": \"Your estimated wait time is {{waitTime}} minute.\",\n          \"waittime_60_greater\": \"Your estimated wait time is {{waitTimeHours}} hours and {{waitTimeHourMinutes}} minutes.\",\n          \"inline-msg\": \"You are now in line.\",\n          \"patience-msg\": \"Thank you for your patience.\",\n        },\n        \"es\": {\n          \"waittime_60_less\": \"El tiempo de espera estimado es {{waitTime}} minuto.\",\n          \"waittime_60_greater\": \"El tiempo de espera estimado es {{waitTimeHours}} de horas y {{waitTimeHourMinutes}} minutos.\",\n          \"inline-msg\": \"Ahora se encuentra en la fila de espera previa.\",\n          \"patience-msg\": \"Gracias por su paciencia.\",\n        }\n      };\n\n      {{#waitTimeKnown}}\n      var minutes = parseInt( {{waitTime}} , 10);\n      var time = document.getElementById('waitTime');\n\n      if ( minutes < 61) {\n        time.innerText = translations[locale][\"waittime_60_less\"]\n      } else {\n        time.innerText = translations[locale][\"waittime_60_greater\"]\n      }\n      {{/waitTimeKnown}}\n\n      // translate template text for each of the elements with “id” suffixed with “msg”\n      for (const msg of [\"inline-msg\", \"patience-msg\"]) {\n        const el = document.getElementById(msg);\n        if (el == null) continue;\n        el.innerText = translations[locale][msg];\n      }\n    </script>\n  </body>\n</html>\n```\n\nHere’s how the template works: given a site distinguishes different locales with various paths such as `/en/product_123` or `/es/product_123` in the `<script />` body after the rendering the page, the locale is extracted from the `location.pathname` via `locale = location.pathname.split(“/”)[1]`. If there isn’t a locale specified within the `translations` object we default it to “en”. For example, if a user visits example.com/product\\_123, then by default render the English text template.\n\nSimilarly, in order to support a multi-language waiting room for sites that delineate site structure via subdomain, it would only require you to update how you extract the locale from the URL. Instead of looking at the `pathname` we look at the `hostname` property of the `window.location` object like `locale = location.hostname.split(“.”)[0]`, given, we have site structure as following: en.example.com, es.example.com.\n\nThe above code is a pared down example of starter templates we have added to our [developer documentation](https://developers.cloudflare.com/waiting-room/how-to/customize-waiting-room/#multiple-language-support), which we have included to make it easier for you to get started with a multi-language waiting room. You can download these templates and edit them to have the look and feel aligned with your site and with the language options your site offers.\n\n![](https://blog.cloudflare.com/content/images/2023/10/image2.png)\n\nThis is an example of the starter template provided in dev docs. This waiting room is in Queue-all mode and displays the Spanish text when the user visits example.com/es/product\\_123.\n\nThese templates can further be expanded to include other languages by adding translations to the \\`translations\\` object for each of the locales. Thus, a single template is able to serve multiple languages depending on whatever the locale the site is being served as either via subdomain or path.\n\n### How we handle cookies with a multihostname and path waiting room\n\nThe waiting room assigns a `__cfwaitingroom` [cookie](https://developers.cloudflare.com/waiting-room/reference/waiting-room-cookie/) to each user to manage the state of the user that determines the position of the user in line [among other properties](https://blog.cloudflare.com/building-waiting-room-on-workers-and-durable-objects/#:~:text=What%20is%20in%20the%20cookie%20returned%20to%20an%20end%20user%3F) needed to make the queueing decisions for the user. Traditionally, for a single hostname and path configuration it was trivial to just set the cookie as `__cfwaitingroom=[cookie-value]; Domain=example.com; Path=/es/product_123`. This ensured that when the page refreshed it would send us the same Waiting Room cookie for us to examine and update. However, this became non-trivial when we wanted to share the same cookie across different subdomain or path combinations, for example, on `example.com/en/product_123`.\n\n### Approaches to setting multiple cookies\n\nThere were two approaches that we brainstormed and evaluated to allow the sharing of the cookie for the same waiting room.\n\nThe first approach we considered was to issue multiple `Set-Cookie` headers in the HTTP response. For example, in the multi-language example above, we could issue the following in the response header:\n\n```\nSet-Cookie: __cfwaitingroom=[cookie-value]…Domain=example.com; Path=/en/product_123 …\nSet-Cookie: __cfwaitingroom=[cookie-value]...Domain=example.com; Path=/es/product_123 …\n```\n\nThis approach would allow us to handle the multiple hostnames and paths for the same waiting room. However, it does not present itself as a very scalable solution. Per [RFC6265](https://datatracker.ietf.org/doc/html/rfc6265#section-5.2), there isn’t a strict limit defined in the specification, browsers have their own implementation-specific limits. For example, Chrome allows the response header to grow up to [256K bytes](https://source.chromium.org/chromium/chromium/src/+/main:net/http/http_stream_parser.h;l=168;drc=4cc7ba01d3c5dc996ddc98f9d0bd709e3d5bbfd3?q=ERR_RESPONSE_HEADERS_TOO_BIG&ss=chromium) before throwing the transaction with ERR\\_RESPONSE\\_HEADERS\\_TOO\\_BIG. Additionally, in this case, the response header would grow proportionally to the number of unique hostname and path combinations, and we would be redundantly repeating the same cookie value N (where N is the number of additional routes) number of times. While this approach would have allowed us to effectively handle a bounded list of hostname and path combinations that need to be set, it was not ideal for cases where we can expect more than a handful routes for a particular site.\n\nThe second approach that we considered and chose to move forward with was to set the cookie on the apex domain (or the most common subdomain). In other words, we would figure out the most common subdomain from the list of routes and use that as the domain. Similarly, for the paths, this would entail determining the least common prefix from the list of paths and use that as the value to be set on the path attribute. For example, consider a waiting room with the following two routes configured, a.example.com/shoes/product\\_123 and b.example.com/shoes/product\\_456.\n\nTo determine the domain that would be set for the cookie, we can see that `example.com` is the most common subdomain among the list of domains. Applying the most common subdomain algorithm, we would get `example.com` as the domain. Applying the least common prefix algorithm on the set of paths, `/shoes/product_123` and `/shoes/product_456`, we would get `/shoes` as the path. Thus, the set-cookie header would be the following:\n\n```\nSet-Cookie: … __cfwaitingroom=[cookie-value]; Domain=example.com; Path=/shoes …\n```\n\nNow, when a visitor accesses any of the pages covered by this waiting room, we can guarantee Waiting Room receives the right cookie and there will only be Set-Cookie included in the response header.\n\nHowever, we are still not there yet. Although we are able to identify the common subdomain (or apex domain) and common path prefix, there would still be a problem if routes from one waiting room would overlap with another waiting room. For example, say we configure two waiting rooms for the same site where we are selling our special shoes:\n\nWaiting Room A  \n    a.example.com/shoes/product\\_123  \n    b.example.com/shoes/product\\_456\n\nWaiting Room B  \n    c.example.com/shoes/product\\_789  \n    d.example.com/shoes/product\\_012\n\nIf we apply the same algorithm as described above, we would get the same domain and path properties set for the two cookies. For Waiting Room A, the domain would be `example.com` and the path would be `/shoes`. Similarly, for Waiting Room B, the most common subdomain would also be example.com and least common path prefix would be `/shoes`. This would effectively prevent us from distinguishing the two cookies and route the visitor to the right waiting room. In order to solve the issue of overlapping cookie names, we introduced the requirement of setting a custom suffix that is unique to the customer’s zone. This is why it is required to provide a custom cookie suffix when configuring a waiting room that covers multiple hostnames or paths. Therefore, if Waiting Room A was assigned cookie suffix “a” and Waiting Room B was assigned cookie suffix “b”, the two `Set-Cookie` definitions would look like the following:\n\n  \n**Waiting Room A**\n\n```\nSet-Cookie: __cfwaitingroom_a=[cookie-value]; Domain=example.com; Path=/shoes\n```\n\n**Waiting Room B**\n\n```\nSet-Cookie: __cfwaitingroom_b=[cookie-value]; Domain=example.com; Path=/shoes\n```\n\nWhen a visitor makes a request to any pages where the two different waiting rooms are configured, Waiting Room can distinguish and select which cookie corresponds to the given request and use this to determine which waiting room’s settings apply to that user depending on where they are on the site.\n\nWith the addition of multihost and multipath Waiting Room coverage, we’re thrilled to offer more flexible options for incorporating Waiting Room into your site, giving your visitors the best experience possible while protecting your site during times of high traffic. Make sure your site is always protected from traffic surges. Try out [Waiting Room](https://dash.cloudflare.com/?to=/:account/:zone/traffic/waiting-rooms) today or [reach out to us](https://www.cloudflare.com/application-services/products/waiting-room/) to learn more about the Advanced Waiting Room!\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Waiting Room](https://blog.cloudflare.com/tag/waiting-room/) [Always Online](https://blog.cloudflare.com/tag/always-online/) [Traffic](https://blog.cloudflare.com/tag/traffic/)"
    },
    {
      "url": "https://blog.cloudflare.com/magic-wan-connector-general-availability/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/magic-wan-connector-general-availability/",
        "loadedTime": "2023-12-05T02:32:04.264Z",
        "referrerUrl": "https://blog.cloudflare.com/page/2/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/magic-wan-connector-general-availability/",
        "title": "Announcing General Availability for the Magic WAN Connector: the easiest way to jumpstart SASE transformation for your network",
        "description": "We’re announcing the general availability of the Magic WAN Connector, which serves as the glue between your existing network hardware and Cloudflare’s networ",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/03/2023\n6 min read\nToday, we’re announcing the general availability of the Magic WAN Connector, a key component of our SASE platform, Cloudflare One. Magic WAN Connector is the glue between your existing network hardware and Cloudflare’s network — it provides a super simplified software solution that comes pre-installed on Cloudflare-certified hardware, and is entirely managed from the Cloudflare One dashboard. \nIt takes only a few minutes from unboxing to seeing your network traffic automatically routed to the closest Cloudflare location, where it flows through a full stack of Zero Trust security controls before taking an accelerated path to its destination, whether that’s another location on your private network, a SaaS app, or any application on the open Internet. \nSince we announced our beta earlier this year, organizations around the world have deployed the Magic WAN Connector to connect and secure their network locations. We’re excited for the general availability of the Magic WAN Connector to accelerate SASE transformation at scale.\nWhen customers tell us about their journey to embrace SASE, one of the most common stories we hear is: \nWe started with our remote workforce, deploying modern solutions to secure access to internal apps and Internet resources. But now, we’re looking at the broader landscape of our enterprise network connectivity and security, and it’s daunting. We want to shift to a cloud and Internet-centric model for all of our infrastructure, but we’re struggling to figure out how to start. \nThe Magic WAN Connector was created to address this problem.\nZero-touch connectivity to your new corporate WAN\nCloudflare One enables organizations of any size to connect and secure all of their users, devices, applications, networks, and data with a unified platform delivered by our global connectivity cloud. Magic WAN is the network connectivity “glue” of Cloudflare One, allowing our customers to migrate away from legacy private circuits and use our network as an extension of their own. \nPreviously, customers have connected their locations to Magic WAN with Anycast GRE or IPsec tunnels configured on their edge network equipment (usually existing routers or firewalls), or plugged into us directly with CNI. But for the past few years, we’ve heard requests from hundreds of customers asking for a zero-touch approach to connecting their branches: We just want something we can plug in and turn on, and it handles the rest. \nThe Magic WAN Connector is exactly this. Customers receive Cloudflare-certified hardware with our software pre-installed on it, and everything is controlled via the Cloudflare dashboard. What was once a time-consuming, complex process now takes a matter of minutes, enabling robust Zero-Trust protection for all of your traffic. \nIn addition to automatically configuring tunnels and routing policies to direct your network traffic to Cloudflare, the Magic WAN Connector will also handle traffic steering, shaping and failover to make sure your packets always take the best path available to the closest Cloudflare network location — which is likely only milliseconds away. You’ll also get enhanced visibility into all your traffic flows in analytics and logs, providing a unified observability experience across both your branches and the traffic through Cloudflare’s network.\nZero Trust security for all your traffic\nOnce the Magic WAN Connector is deployed at your network location, you have automatic access to enforce Zero Trust security policies across both public and private traffic.\nA secure on-ramp to the Internet\nAn easy first step to improving your organization’s security posture after connecting network locations to Cloudflare is creating Secure Web Gateway policies to defend against ransomware, phishing, and other threats for faster, safer Internet browsing. By default, all Internet traffic from locations with the Magic WAN Connector will route through Cloudflare Gateway, providing a unified management plane for traffic from physical locations and remote employees.\nA more secure private network\nThe Magic WAN Connector also enables routing private traffic between your network locations, with multiple layers of network and Zero Trust security controls in place. Unlike a traditional network architecture, which requires deploying and managing a stack of security hardware and backhauling branch traffic through a central location for filtering, a SASE architecture provides private traffic filtering and control built-in: enforced across a distributed network, but managed from a single dashboard interface or API.\nA simpler approach for hybrid cloud\nCloudflare One enables connectivity for any physical or cloud network with easy on-ramps depending on location type. The Magic WAN Connector provides easy connectivity for branches, but also provides automatic connectivity to other networks including VPCs connected using cloud-native constructs (e.g., VPN Gateways) or direct cloud connectivity (via Cloud CNI). With a unified connectivity and control plane across physical and cloud infrastructure, IT and security teams can reduce overhead and cost of managing multi- and hybrid cloud networks.\nSingle-vendor SASE dramatically reduces cost and complexity\nWith the general availability of the Magic WAN Connector, we’ve put the final piece in place to deliver a unified SASE platform, developed and fully integrated from the ground up. Deploying and managing all the components of SASE with a single vendor, versus piecing together different solutions for networking and security, significantly simplifies deployment and management by reducing complexity and potential integration challenges. Many vendors that market a full SASE solution have actually stitched together separate products through acquisition, leading to an un-integrated experience similar to what you would see deploying and managing multiple separate vendors. In contrast, Cloudflare One (now with the Magic WAN Connector for simplified branch functions) enables organizations to achieve the true promise of SASE: a simplified, efficient, and highly secure network and security infrastructure that reduces your total cost of ownership and adapts to the evolving needs of the modern digital landscape.\nEvolving beyond SD-WAN\nCloudflare One addresses many of the challenges that were left behind as organizations deployed SD-WAN to help simplify networking operations. SD-WAN provides orchestration capabilities to help manage devices and configuration in one place, as well as last mile traffic management to steer and shape traffic based on more sophisticated logic than is possible in traditional routers. But SD-WAN devices generally don't have embedded security controls, leaving teams to stitch together a patchwork of hardware, virtualized and cloud-based tools to keep their networks secure. They can make decisions about the best way to send traffic out from a customer’s branch, but they have no way to influence traffic hops between the last mile and the traffic's destination. And while some SD-WAN providers have surfaced virtualized versions of their appliances that can be deployed in cloud environments, they don't support native cloud connectivity and can complicate rather than ease the transition to cloud.\nCloudflare One represents the next evolution of enterprise networking, and has a fundamentally different architecture from either legacy networking or SD-WAN. It's based on a \"light branch, heavy cloud\" principle: deploy the minimum required hardware within physical locations (or virtual hardware within virtual networks, e.g., cloud VPCs) and use low-cost Internet connectivity to reach the nearest \"service edge\" location. At those locations, traffic can flow through security controls and be optimized on the way to its destination, whether that's another location within the customer's private network or an application on the public Internet. This architecture also enables remote user access to connected networks.\nThis shift — moving most of the \"smarts\" from the branch to a distributed global network edge, and leaving only the functions at the branch that absolutely require local presence, delivered by the Magic WAN Connector — solves our customers’ current problems and sets them up for easier management and a stronger security posture as the connectivity and attack landscape continues to evolve.\nAspect\n\t\nExample\n\t\nMPLS/VPN Service\n\t\nSD-WAN\n\t\nSASE with \nCloudflare One \n\t\nConfiguration\n\t\nNew site setup, configuration and management\n\t\nBy MSP through service request\n\t\nSimplified orchestration and \nmanagement via centralized controller\n\t\nAutomated orchestration via SaaS portal\nSingle Dashboard\n\t\nLast mile \ntraffic control\n\t\nTraffic balancing, QoS, and failover\n\t\nCovered by MPLS SLAs\n\t\nBest Path selection available \nin SD-WAN appliance \n\t\nMinimal on-prem deployment to control local decision making\n\t\nMiddle mile \ntraffic control\n\t\nTraffic steering around middle mile congestion\n\t\nCovered by MPLS SLAs\n\t\n“Tunnel Spaghetti” and still no control over the middle mile\n\t\nIntegrated traffic management & private backbone controls in a unified dashboard\n\t\nCloud integration\n\t\nConnectivity for cloud migration\n\t\nCentralized breakout\n\t\nDecentralized breakout\n\t\nNative connectivity with Cloud Network Interconnect\n\t\nSecurity\n\t\nFilter in & outbound Internet traffic for malware\n\t\nPatchwork of hardware controls\n\t\nPatchwork of hardware \nand/or software controls\n\t\nNative integration with user, data, application & network security tools\n\t\nCost\n\t\nMaximize ROI for network investments\n\t\nHigh cost for hardware and connectivity\n\t\nOptimized connectivity costs at the expense of increased \nhardware and software costs\n\t\nDecreased hardware and connectivity costs for maximized ROI\n\t\nSummary of legacy, SD-WAN based, and SASE architecture considerations\nLove and want to keep your current SD-WAN vendor? No problem - you can still use any appliance that supports IPsec or GRE as an on-ramp for Cloudflare One.\nReady to simplify your SASE journey?\nYou can learn more about the Magic WAN Connector, including device specs, specific feature info, onboarding process details, and more at our dev docs, or contact us to get started today.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nProduct News Security Magic WAN Magic WAN Connector SASE",
      "markdown": "10/03/2023\n\n*   [![Annika Garbers](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2020/04/annika.png)](https://blog.cloudflare.com/author/annika/)\n\n6 min read\n\n![](https://blog.cloudflare.com/content/images/2023/10/Magic-WAN-Connector--buy-our-box-or-BYO-.png)\n\nToday, we’re announcing the general availability of the [Magic WAN Connector](https://www.cloudflare.com/network-services/products/magic-wan/), a key component of our [SASE](https://www.cloudflare.com/learning/access-management/what-is-sase/) platform, Cloudflare One. Magic WAN Connector is the glue between your existing network hardware and Cloudflare’s network — it provides a super simplified software solution that comes pre-installed on Cloudflare-certified hardware, and is entirely managed from the Cloudflare One dashboard.\n\nIt takes only a few minutes from unboxing to seeing your network traffic automatically routed to the closest Cloudflare location, where it flows through a full stack of Zero Trust security controls before taking an accelerated path to its destination, whether that’s another location on your private network, a SaaS app, or any application on the open Internet.\n\nSince we [announced](https://blog.cloudflare.com/magic-wan-connector/) our beta earlier this year, organizations around the world have deployed the Magic WAN Connector to connect and secure their network locations. We’re excited for the general availability of the Magic WAN Connector to accelerate SASE transformation at scale.\n\nWhen customers tell us about their journey to embrace SASE, one of the most common stories we hear is:\n\n> _We started with our remote workforce, deploying modern solutions to secure access to internal apps and Internet resources. But now, we’re looking at the broader landscape of our enterprise network connectivity and security, and it’s daunting. We want to shift to a cloud and Internet-centric model for all of our infrastructure, but we’re struggling to figure out how to start._\n\nThe Magic WAN Connector was created to address this problem.\n\n### Zero-touch connectivity to your new corporate WAN\n\n[Cloudflare One](https://www.cloudflare.com/cloudflare-one/) enables organizations of any size to connect and secure all of their users, devices, applications, networks, and data with a unified platform delivered by our global [connectivity cloud](https://www.cloudflare.com/connectivity-cloud/). [Magic WAN](https://www.cloudflare.com/network-services/products/magic-wan/) is the network connectivity “glue” of Cloudflare One, allowing our customers to migrate away from legacy private circuits and use our network as an extension of their own.\n\nPreviously, customers have connected their locations to Magic WAN with Anycast GRE or IPsec tunnels configured on their edge network equipment (usually existing routers or firewalls), or plugged into us directly with [CNI](https://www.cloudflare.com/network-services/products/network-interconnect/). But for the past few years, we’ve heard requests from hundreds of customers asking for a zero-touch approach to connecting their branches: _We just want something we can plug in and turn on, and it handles the rest._\n\nThe Magic WAN Connector is exactly this. Customers receive Cloudflare-certified hardware with our software pre-installed on it, and everything is controlled via the Cloudflare dashboard. What was once a time-consuming, complex process now takes a matter of minutes, enabling robust Zero-Trust protection for all of your traffic.  \n\nIn addition to automatically configuring tunnels and routing policies to direct your network traffic to Cloudflare, the Magic WAN Connector will also handle traffic steering, shaping and failover to make sure your packets always take the best path available to the closest Cloudflare network location — which is likely only milliseconds away. You’ll also get enhanced visibility into all your traffic flows in analytics and logs, providing a unified observability experience across both your branches and the traffic through Cloudflare’s network.\n\n### Zero Trust security for all your traffic\n\nOnce the Magic WAN Connector is deployed at your network location, you have automatic access to enforce Zero Trust security policies across both public and private traffic.\n\n![](https://blog.cloudflare.com/content/images/2023/10/Branch-Connector-Diagram.png)\n\n#### A secure on-ramp to the Internet\n\nAn easy first step to improving your organization’s security posture after connecting network locations to Cloudflare is creating Secure Web Gateway policies to defend against ransomware, phishing, and other threats for faster, safer Internet browsing. By default, all Internet traffic from locations with the Magic WAN Connector will route through Cloudflare Gateway, providing a unified management plane for traffic from physical locations and remote employees.\n\n#### A more secure private network\n\nThe Magic WAN Connector also enables routing private traffic between your network locations, with multiple layers of network and Zero Trust security controls in place. Unlike a traditional network architecture, which requires deploying and managing a stack of security hardware and backhauling branch traffic through a central location for filtering, a SASE architecture provides private traffic filtering and control built-in: enforced across a distributed network, but managed from a single dashboard interface or API.\n\n#### A simpler approach for hybrid cloud\n\nCloudflare One enables connectivity for any physical or cloud network with easy on-ramps depending on location type. The Magic WAN Connector provides easy connectivity for branches, but also provides automatic connectivity to other networks including VPCs connected using cloud-native constructs (e.g., VPN Gateways) or direct cloud connectivity (via [Cloud CNI](https://blog.cloudflare.com/cloud-cni/)). With a unified connectivity and control plane across physical and cloud infrastructure, IT and security teams can reduce overhead and cost of managing multi- and hybrid cloud networks.\n\n### Single-vendor SASE dramatically reduces cost and complexity\n\nWith the general availability of the Magic WAN Connector, we’ve put the final piece in place to deliver a unified SASE platform, developed and fully integrated from the ground up. Deploying and managing all the components of SASE with a single vendor, versus piecing together different solutions for networking and security, significantly simplifies deployment and management by reducing complexity and potential integration challenges. Many vendors that market a full SASE solution have actually stitched together separate products through acquisition, leading to an un-integrated experience similar to what you would see deploying and managing multiple separate vendors. In contrast, Cloudflare One (now with the Magic WAN Connector for simplified branch functions) enables organizations to achieve the true promise of SASE: a simplified, efficient, and highly secure network and security infrastructure that reduces your total cost of ownership and adapts to the evolving needs of the modern digital landscape.\n\n### Evolving beyond SD-WAN\n\nCloudflare One addresses many of the challenges that were left behind as organizations deployed SD-WAN to help simplify networking operations. SD-WAN provides orchestration capabilities to help manage devices and configuration in one place, as well as last mile traffic management to steer and shape traffic based on more sophisticated logic than is possible in traditional routers. But SD-WAN devices generally don't have embedded security controls, leaving teams to stitch together a patchwork of hardware, virtualized and cloud-based tools to keep their networks secure. They can make decisions about the best way to send traffic out from a customer’s branch, but they have no way to influence traffic hops between the last mile and the traffic's destination. And while some SD-WAN providers have surfaced virtualized versions of their appliances that can be deployed in cloud environments, they don't support native cloud connectivity and can complicate rather than ease the transition to cloud.\n\nCloudflare One represents the next evolution of enterprise networking, and has a fundamentally different architecture from either legacy networking or SD-WAN. It's based on a \"light branch, heavy cloud\" principle: deploy the minimum required hardware within physical locations (or virtual hardware within virtual networks, e.g., cloud VPCs) and use low-cost Internet connectivity to reach the nearest \"service edge\" location. At those locations, traffic can flow through security controls and be optimized on the way to its destination, whether that's another location within the customer's private network or an application on the public Internet. This architecture also enables remote user access to connected networks.\n\nThis shift — moving most of the \"smarts\" from the branch to a distributed global network edge, and leaving only the functions at the branch that absolutely require local presence, delivered by the Magic WAN Connector — solves our customers’ current problems and sets them up for easier management and a stronger security posture as the connectivity and attack landscape continues to evolve.\n\n|     |     |     |     |     |\n| --- | --- | --- | --- | --- |\n| Aspect | Example | MPLS/VPN Service | SD-WAN | SASE with <br><br>Cloudflare One |\n| Configuration | New site setup, configuration and management | By MSP through service request | Simplified orchestration and   <br>management via centralized controller | Automated orchestration via SaaS portal<br><br>Single Dashboard |\n| Last mile <br><br>traffic control | Traffic balancing, QoS, and failover | Covered by MPLS SLAs | Best Path selection available  <br>in SD-WAN appliance | Minimal on-prem deployment to control local decision making |\n| Middle mile <br><br>traffic control | Traffic steering around middle mile congestion | Covered by MPLS SLAs | “Tunnel Spaghetti” and still no control over the middle mile | Integrated traffic management & private backbone controls in a unified dashboard |\n| Cloud integration | Connectivity for cloud migration | Centralized breakout | Decentralized breakout | Native connectivity with Cloud Network Interconnect |\n| Security | Filter in & outbound Internet traffic for malware | Patchwork of hardware controls | Patchwork of hardware  <br>and/or software controls | Native integration with user, data, application & network security tools |\n| Cost | Maximize ROI for network investments | High cost for hardware and connectivity | Optimized connectivity costs at the expense of increased <br><br>hardware and software costs | Decreased hardware and connectivity costs for maximized ROI |\n\n_Summary of legacy, SD-WAN based, and SASE architecture considerations_\n\nLove and want to keep your current SD-WAN vendor? No problem - you can still use any appliance that supports IPsec or GRE as an on-ramp for Cloudflare One.\n\n### Ready to simplify your SASE journey?\n\nYou can learn more about the Magic WAN Connector, including device specs, specific feature info, onboarding process details, and more at our [dev docs](https://developers.cloudflare.com/magic-wan/connector/), or [contact us](https://www.cloudflare.com/products/zero-trust/plans/enterprise/) to get started today.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Product News](https://blog.cloudflare.com/tag/product-news/) [Security](https://blog.cloudflare.com/tag/security/) [Magic WAN](https://blog.cloudflare.com/tag/magic-wan/) [Magic WAN Connector](https://blog.cloudflare.com/tag/magic-wan-connector/) [SASE](https://blog.cloudflare.com/tag/sase/)"
    },
    {
      "url": "https://blog.cloudflare.com/ddos-attack-trends-for-2022-q1/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/ddos-attack-trends-for-2022-q1/",
        "loadedTime": "2023-12-05T02:32:22.021Z",
        "referrerUrl": "https://blog.cloudflare.com/cyber-week-analyzing-internet-traffic-and-e-commerce-trends/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/ddos-attack-trends-for-2022-q1/",
        "title": "DDoS Attack Trends for 2022 Q1",
        "description": "Welcome to our first DDoS report of 2022, and the ninth in total so far. This report includes new data points and insights both in the application-layer and network-layer sections — as observed across the global Cloudflare network between January and March 2022.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "04/12/2022\n15 min read\nThis post is also available in 简体中文, 日本語, Deutsch, Français, Español, 繁體中文, 한국어 and Português.\nWelcome to our first DDoS report of 2022, and the ninth in total so far. This report includes new data points and insights both in the application-layer and network-layer sections — as observed across the global Cloudflare network between January and March 2022.\nThe first quarter of 2022 saw a massive spike in application-layer DDoS attacks, but a decrease in the total number of network-layer DDoS attacks. Despite the decrease, we’ve seen volumetric DDoS attacks surge by up to 645% QoQ, and we mitigated a new zero-day reflection attack with an amplification factor of 220 billion percent.\nIn the Russian and Ukrainian cyberspace, the most targeted industries were Online Media and Broadcast Media. In our Azerbaijan and Palestinian Cloudflare data centers, we’ve seen enormous spikes in DDoS activity — indicating the presence of botnets operating from within.\nThe Highlights\nThe Russian and Ukrainian cyberspace\nRussian Online Media companies were the most targeted industries within Russia in Q1. The next most targeted was the Internet industry, then Cryptocurrency, and then Retail. While many attacks that targeted Russian Cryptocurrency companies originated in Ukraine or the US, another major source of attacks was from within Russia itself.\nThe majority of HTTP DDoS attacks that targeted Russian companies originated from Germany, the US, Singapore, Finland, India, the Netherlands, and Ukraine. It’s important to note that being able to identify where cyber attack traffic originates is not the same as being able to attribute where the attacker is located.\nAttacks on Ukraine targeted Broadcast Media and Publishing websites and seem to have been more distributed, originating from more countries — which may indicate the use of global botnets. Still, most of the attack traffic originated from the US, Russia, Germany, China, the UK, and Thailand.\nRead more about what Cloudflare is doing to keep the Open Internet flowing into Russia and keep attacks from getting out.\nRansom DDoS attacks\nIn January 2022, over 17% of under-attack respondents reported being targeted by ransom DDoS attacks or receiving a threat in advance.\nThat figure drastically dropped to 6% in February, and then to 3% in March.\nWhen compared to previous quarters, we can see that in total, in Q1, only 10% of respondents reported a ransom DDoS attack; a 28% decrease YoY and 52% decrease QoQ.\nApplication-layer DDoS attacks\n2022 Q1 was the busiest quarter in the past 12 months for application-layer attacks. HTTP-layer DDoS attacks increased by 164% YoY and 135% QoQ.\nDiving deeper into the quarter, in March 2022 there were more HTTP DDoS attacks than in all of Q4 combined (and Q3, and Q1).\nAfter four consecutive quarters in a row with China as the top source of HTTP DDoS attacks, the US stepped into the lead this quarter. HTTP DDoS attacks originating from the US increased by a staggering 6,777% QoQ and 2,225% YoY.\nNetwork-layer DDoS attacks\nNetwork-layer attacks in Q1 increased by 71% YoY but decreased 58% QoQ.\nThe Telecommunications industry was the most targeted by network-layer DDoS attacks, followed by Gaming and Gambling companies, and the Information Technology and Services industry.\nVolumetric attacks increased in Q1. Attacks above 10 Mpps (million packets per second) grew by over 300% QoQ, and attacks over 100 Gbps grew by 645% QoQ.\nThis report is based on DDoS attacks that were automatically detected and mitigated by Cloudflare’s DDoS Protection systems. To learn more about how it works, check out this deep-dive blog post.\nA note on how we measure DDoS attacks observed over our network\nTo analyze attack trends, we calculate the “DDoS activity” rate, which is either the percentage of attack traffic out of the total traffic (attack + clean) observed over our global network, or in a specific location, or in a specific category (e.g., industry or billing country). Measuring the percentages allows us to normalize data points and avoid biases reflected in absolute numbers towards, for example, a Cloudflare data center that receives more total traffic and likely, also more attacks. \nTo view an interactive version of this report view it on Cloudflare Radar.\nRansom Attacks\nOur systems constantly analyze traffic and automatically apply mitigation when DDoS attacks are detected. Each DDoS’d customer is prompted with an automated survey to help us better understand the nature of the attack and the success of the mitigation.\nFor over two years now, Cloudflare has been surveying attacked customers — one question on the survey being if they received a threat or a ransom note demanding payment in exchange to stop the DDoS attack. In the last quarter, 2021 Q4, we observed a record-breaking level of reported ransom DDoS attacks (one out of every five customers). This quarter, we’ve witnessed a drop in ransom DDoS attacks with only one out of 10 respondents reporting a ransom DDoS attack; a 28% decrease YoY and 52% decrease QoQ.\nWhen we break it down by month, we can see that January 2022 saw the largest number of respondents reporting receiving a ransom letter in Q1. Almost one out of every five customers (17%).\nApplication-layer DDoS attacks\nApplication-layer DDoS attacks, specifically HTTP DDoS attacks, are attacks that usually aim to disrupt a web server by making it unable to process legitimate user requests. If a server is bombarded with more requests than it can process, the server will drop legitimate requests and — in some cases — crash, resulting in degraded performance or an outage for legitimate users.\nApplication-layer DDoS attacks by month\nIn Q1, application-layer DDoS attacks soared by 164% YoY and 135% QoQ - the busiest quarter within the past year.\nApplication-layer DDoS attacks increased to new heights in the first quarter of 2022. In March alone, there were more HTTP DDoS attacks than in all of 2021 Q4 combined (and Q3, and Q1).\nApplication-layer DDoS attacks by industry\nConsumer Electronics was the most targeted industry in Q1.\nGlobally, the Consumer Electronics industry was the most attacked with an increase of 5,086% QoQ. Second was the Online Media industry with a 2,131% increase in attacks QoQ. Third were Computer Software companies, with an increase of 76% QoQ and 1,472 YoY.\nHowever, if we focus only on Ukraine and Russia, we can see that Broadcast Media, Online Media companies, and Internet companies were the most targeted. Read more about what Cloudflare is doing to keep the Open Internet flowing into Russia and keep attacks from getting out.\nApplication-layer DDoS attacks by source country\nTo understand the origin of the HTTP attacks, we look at the geolocation of the source IP address belonging to the client that generated the attack HTTP requests. Unlike network-layer attacks, source IP addresses cannot be spoofed in HTTP attacks. A high percentage of DDoS activity in a given country usually indicates the presence of botnets operating from within the country's borders.\nAfter four consecutive quarters in a row with China as the top source of HTTP DDoS attacks, the US stepped into the lead this quarter. HTTP DDoS attacks originating from the US increased by a staggering 6,777% QoQ and 2,225% YoY. Following China in second place are India, Germany, Brazil, and Ukraine.\nApplication-layer DDoS attacks by target country\nIn order to identify which countries are targeted by the most HTTP DDoS attacks, we bucket the DDoS attacks by our customers' billing countries and represent it as a percentage out of all DDoS attacks.\nThe US drops to second place, after being first for three consecutive quarters. Organizations in China were targeted the most by HTTP DDoS attacks, followed by the US, Russia, and Cyprus.\nNetwork-layer DDoS attacks\nWhile application-layer attacks target the application (Layer 7 of the OSI model) running the service that end users are trying to access (HTTP/S in our case), network-layer attacks aim to overwhelm network infrastructure (such as in-line routers and servers) and the Internet link itself.\nNetwork-layer DDoS attacks by month\nWhile HTTP DDoS attacks soared in Q1, network-layer DDoS attacks actually decreased by 58% QoQ, but still increased by 71% YoY.\nDiving deeper into Q1, we can see that the amount of network-layer DDoS attacks remained mostly consistent throughout the quarter with about a third of attacks occurring every month.\nCloudflare mitigates zero-day amplification DDoS attack\nAmongst these network-layer DDoS attacks are also zero-day DDoS attacks that Cloudflare automatically detected and mitigated.\nIn the beginning of March, Cloudflare researchers helped investigate and expose a zero-day vulnerability in Mitel business phone systems that amongst other possible exploitations, also enables attackers to launch an amplification DDoS attack. This type of attack reflects traffic off vulnerable Mitel servers to victims, amplifying the amount of traffic sent in the process by an amplification factor of 220 billion percent in this specific case. You can read more about it in our recent blog post.\nWe observed several of these attacks across our network. One of them targeted a North American cloud provider using the Cloudflare Magic Transit service. The attack originated from 100 source IPs mainly from the US, UK, Canada, Netherlands, Australia, and approximately 20 other countries. It peaked above 50 Mpps (~22 Gbps) and was automatically detected and mitigated by Cloudflare systems.\nNetwork-layer DDoS attacks by industry\nMany network-layer DDoS attacks target Cloudflare’s IP ranges directly. These IP ranges serve our WAF/CDN customers, Cloudflare authoritative DNS, Cloudflare public DNS resolver 1.1.1.1, Cloudflare Zero Trust products, and our corporate offices, to name a few. Additionally, we also allocate dedicated IP addresses to customers via our Spectrum product and advertise the IP prefixes of other companies via our Magic Transit, Magic WAN, and Magic Firewall Products for L3/4 DDoS protection.\nIn this report, for the first time, we've begun classifying network-layer DDoS attacks according to the industries of our customers using the Spectrum and Magic products. This classification allows us to understand which industries are targeted the most by network-layer DDoS attacks.\nWhen we look at Q1 statistics, we can see that in terms of attack packets and attack bytes launched towards Cloudflare customers, the Telecommunications industry was targeted the most. More than 8% of all attack bytes and 10% of all attack packets that Cloudflare mitigated targeted Telecommunications companies.\nFollowing not too far behind, in second and third place were the Gaming / Gambling and Information Technology and Services industries.\nNetwork-layer DDoS attacks by target country\nSimilarly to the classification by our customers’ industry, we can also bucket attacks by our customers’ billing country as we do for application-layer DDoS attacks, to identify the top attacked countries.\nLooking at Q1 numbers, we can see that the US was targeted by the highest percentage of DDoS attacks traffic — over 10% of all attack packets and almost 8% of all attack bytes. Following the US is China, Canada, and Singapore.\nNetwork-layer DDoS attacks by ingress country\nWhen trying to understand where network-layer DDoS attacks originate, we cannot use the same method as we use for the application-layer attack analysis. To launch an application-layer DDoS attack, successful handshakes must occur between the client and the server in order to establish an HTTP/S connection. For a successful handshake to occur, the attacker cannot spoof their source IP address. While the attacker may use botnets, proxies, and other methods to obfuscate their identity, the attacking client's source IP location does sufficiently represent the attack source of application-layer DDoS attacks.\nOn the other hand, to launch network-layer DDoS attacks, in most cases, no handshake is needed. Attackers can spoof the source IP address in order to obfuscate the attack source and introduce randomness into the attack properties, which can make it harder for simple DDoS protection systems to block the attack. So if we were to derive the source country based on a spoofed source IP, we would get a ‘spoofed country’.\nFor this reason, when analyzing network-layer DDoS attack sources, we bucket the traffic by the Cloudflare edge data center locations where the traffic was ingested, and not by the (potentially) spoofed source IP to get an understanding of where the attacks originate from. We are able to achieve geographical accuracy in our report because we have data centers in over 270 cities around the world. However, even this method is not 100% accurate, as traffic may be back hauled and routed via various Internet Service Providers and countries for reasons that vary from cost reduction to congestion and failure management.\nIn Q1, the percentage of attacks detected in Cloudflare’s data centers in Azerbaijan increased by 16,624% QoQ and 96,900% YoY, making it the country with the highest percentage of network-layer DDoS activity (48.5%).\nFollowing our Azerbaijanian data center is our Palestinian data center where a staggering 41.9% of all traffic was DDoS traffic. This represents a 10,120% increase QoQ and 46,456% YoY.\nTo view all regions and countries, check out the interactive map.\nAttack vectors\nSYN Floods remain the most popular DDoS attack vector, while use of generic UDP floods drops significantly in Q1.\nAn attack vector is a term used to describe the method that the attacker uses to launch their DDoS attack, i.e., the IP protocol, packet attributes such as TCP flags, flooding method, and other criteria.\nIn Q1, SYN floods accounted for 57% of all network-layer DDoS attacks, representing a 69% increase QoQ and a 13% increase YoY. In second place, attacks over SSDP surged by over 1,100% QoQ. Following were RST floods and attacks over UDP. Last quarter, generic UDP floods took the second place, but this time, generic UDP DDoS attacks plummeted by 87% QoQ from 32% to a mere 3.9%.\nEmerging threats\nIdentifying the top attack vectors helps organizations understand the threat landscape. In turn, this may help them improve their security posture to protect against those threats. Similarly, learning about new emerging threats that may not yet account for a significant portion of attacks, can help mitigate them before they become a significant force.\nWhen we look at new emerging attack vectors in Q1, we can see increases in DDoS attacks reflecting off of Lantronix services (+971% QoQ) and SSDP reflection attacks (+724% QoQ). Additionally, SYN-ACK attacks increased by 437% and attacks by Mirai botnets by 321% QoQ.\nAttacker reflecting traffic off of Lantronix Discovery Service\nLantronix is a US-based software and hardware company that provides solutions for Internet of Things (IoT) management amongst their vast offering. One of the tools that they provide to manage their IoT components is the Lantronix Discovery Protocol. It is a command-line tool that helps to search and find Lantronix devices. The discovery tool is UDP-based, meaning that no handshake is required. The source IP can be spoofed. So an attacker can use the tool to search for publicly exposed Lantronix devices using a 4 byte request, which will then in turn respond with a 30 byte response from port 30718. By spoofing the source IP of the victim, all Lantronix devices will target their responses to the victim — resulting in a reflection/amplification attack.\nSimple Service Discovery Protocol used for reflection DDoS attacks\nThe Simple Service Discovery Protocol (SSDP) protocol works similarly to the Lantronix Discovery protocol, but for Universal Plug and Play (UPnP) devices such as network-connected printers. By abusing the SSDP protocol, attackers can generate a reflection-based DDoS attack overwhelming the target’s infrastructure and taking their Internet properties offline. You can read more about SSDP-based DDoS attacks here.\nNetwork-layer DDoS attacks by attack rate\nIn Q1, we observed a massive uptick in volumetric DDoS attacks — both from the packet rate and bitrate perspective. Attacks over 10 Mpps grew by over 300% QoQ, and attacks over 100 Gbps grew by 645% QoQ. \nThere are different ways of measuring the size of an L3/4 DDoS attack. One is the volume of traffic it delivers, measured as the bit rate (specifically, terabits per second or gigabits per second). Another is the number of packets it delivers, measured as the packet rate (specifically, millions of packets per second).\nAttacks with high bit rates attempt to cause a denial-of-service event by clogging the Internet link, while attacks with high packet rates attempt to overwhelm the servers, routers, or other in-line hardware appliances. These devices dedicate a certain amount of memory and computation power to process each packet. Therefore, by bombarding it with many packets, the appliance can be left with no further processing resources. In such a case, packets are “dropped,” i.e., the appliance is unable to process them. For users, this results in service disruptions and denial of service.\nDistribution by packet rate\nThe majority of network-layer DDoS attacks remain below 50,000 packets per second. While 50 kpps is on the lower side of the spectrum at Cloudflare scale, it can still easily take down unprotected Internet properties and congest even a standard Gigabit Ethernet connection.\nWhen we look at the changes in the attack sizes, we can see that attacks of over 10 Mpps grew by over 300% QoQ. Similarly, attacks of 1-10 Mpps grew by almost 40% QoQ.\nDistribution by bitrate\nIn Q1, most of the network-layer DDoS attacks remain below 500 Mbps. This too is a tiny drop in the water at Cloudflare scale, but can very quickly shut down unprotected Internet properties with less capacity or at the very least congest, even a standard Gigabit Ethernet connection.\nGraph of the distribution of network-layer DDoS attacks by bit rate in 2022 Q1\nSimilarly to the trends observed in the packet-per-second realm, here we can also see large increases. The amount of DDoS attacks that peaked over 100 Gbps increased by 645% QoQ; attacks peaking between 10 Gbps to 100 Gbps increased by 407%; attacks peaking between 1 Gbps to 10 Gbps increased by 88%; and even attacks peaking between 500 Mbps to 1 Gbps increased by almost 20% QoQ.\nNetwork-layer DDoS attacks by duration\nMost attacks remain under one hour in duration, reiterating the need for automated always-on DDoS mitigation solutions.\nWe measure the duration of an attack by recording the difference between when it is first detected by our systems as an attack and the last packet we see with that attack signature towards that specific target.\nIn previous reports, we provided a breakdown of ‘attacks under an hour’, and larger time ranges. However, in most cases over 90 percent of attacks last less than an hour. So starting from this report, we broke down the short attacks and grouped them by shorter time ranges to provide better granularity.\nOne important thing to keep in mind is that even if an attack lasts only a few minutes, if it is successful, the repercussions could last well beyond the initial attack duration. IT personnel responding to a successful attack may spend hours and even days restoring their services.\nIn the first quarter of 2022, more than half of the attacks lasted 10-20 minutes, approximately 40% ended within 10 minutes, another ~5% lasted 20-40 minutes, and the remaining lasted longer than 40 minutes.\nShort attacks can easily go undetected, especially burst attacks that, within seconds, bombard a target with a significant number of packets, bytes, or requests. In this case, DDoS protection services that rely on manual mitigation by security analysis have no chance in mitigating the attack in time. They can only learn from it in their post-attack analysis, then deploy a new rule that filters the attack fingerprint and hope to catch it next time. Similarly, using an “on-demand” service, where the security team will redirect traffic to a DDoS provider during the attack, is also inefficient because the attack will already be over before the traffic routes to the on-demand DDoS provider.\nIt’s recommended that companies use automated, always-on DDoS protection services that analyze traffic and apply real-time fingerprinting fast enough to block short-lived attacks.\nSummary\nCloudflare’s mission is to help build a better Internet. A better Internet is one that is more secure, faster, and reliable for everyone — even in the face of DDoS attacks. As part of our mission, since 2017, we’ve been providing unmetered and unlimited DDoS protection for free to all of our customers. Over the years, it has become increasingly easier for attackers to launch DDoS attacks. But as easy as it has become, we want to make sure that it is even easier — and free — for organizations of all sizes to protect themselves against DDoS attacks of all types.\nNot using Cloudflare yet? Start now with our Free and Pro plans to protect your websites, or contact us for comprehensive DDoS protection for your entire network using Magic Transit.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nDDoS DDoS Reports Attacks Trends Cloudflare Radar",
      "markdown": "04/12/2022\n\n*   [![Omer Yoachimik](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/06/Screenshot-2023-06-02-at-9.38.13-AM.png)](https://blog.cloudflare.com/author/omer/)\n\n15 min read\n\n_This post is also available in [简体中文](https://blog.cloudflare.com/zh-cn/ddos-attack-trends-for-2022-q1-zh-cn/), [日本語](https://blog.cloudflare.com/ja-jp/ddos-attack-trends-for-2022-q1-ja-jp/), [Deutsch](https://blog.cloudflare.com/de-de/ddos-attack-trends-for-2022-q1-de-de/), [Français](https://blog.cloudflare.com/fr-fr/ddos-attack-trends-for-2022-q1-fr-fr/), [Español](https://blog.cloudflare.com/es-es/ddos-attack-trends-for-2022-q1-es-es/), [繁體中文](https://blog.cloudflare.com/zh-tw/ddos-attack-trends-for-2022-q1-zh-tw/), [한국어](https://blog.cloudflare.com/ko-kr/ddos-attack-trends-for-2022-q1-ko-kr/) and [Português](https://blog.cloudflare.com/pt-br/ddos-attack-trends-for-2022-q1-pt-br/)._\n\n![DDoS Attack Trends for 2022 Q1](https://blog.cloudflare.com/content/images/2022/04/image24-1.png)\n\nWelcome to our first DDoS report of 2022, and the ninth in [total](https://blog.cloudflare.com/tag/ddos-reports/) so far. This report includes new data points and insights both in the application-layer and network-layer sections — as observed across the global Cloudflare network between January and March 2022.\n\nThe first quarter of 2022 saw a massive spike in application-layer DDoS attacks, but a decrease in the total number of network-layer DDoS attacks. Despite the decrease, we’ve seen volumetric DDoS attacks surge by up to 645% QoQ, and we mitigated a new zero-day reflection attack with an amplification factor of 220 billion percent.\n\nIn the Russian and Ukrainian cyberspace, the most targeted industries were Online Media and Broadcast Media. In our Azerbaijan and Palestinian Cloudflare data centers, we’ve seen enormous spikes in DDoS activity — indicating the presence of botnets operating from within.\n\n## The Highlights\n\n### The Russian and Ukrainian cyberspace\n\n*   Russian Online Media companies were the most targeted industries within Russia in Q1. The next most targeted was the Internet industry, then Cryptocurrency, and then Retail. While many attacks that targeted Russian Cryptocurrency companies originated in Ukraine or the US, another major source of attacks was from within Russia itself.\n*   The majority of HTTP DDoS attacks that targeted Russian companies originated from Germany, the US, Singapore, Finland, India, the Netherlands, and Ukraine. It’s important to note that being able to identify where cyber attack traffic originates is not the same as being able to attribute where the attacker is located.\n*   Attacks on Ukraine targeted Broadcast Media and Publishing websites and seem to have been more distributed, originating from more countries — which may indicate the use of global botnets. Still, most of the attack traffic originated from the US, Russia, Germany, China, the UK, and Thailand.\n\nRead more about [what Cloudflare is doing to keep the Open Internet flowing into Russia and keep attacks from getting out](https://blog.cloudflare.com/what-cloudflare-is-doing-to-keep-the-open-internet-flowing-into-russia-and-keep-attacks-from-getting-out/).\n\n### Ransom DDoS attacks\n\n*   In January 2022, over 17% of under-attack respondents reported being targeted by ransom DDoS attacks or receiving a threat in advance.\n*   That figure drastically dropped to 6% in February, and then to 3% in March.\n*   When compared to previous quarters, we can see that in total, in Q1, only 10% of respondents reported a ransom DDoS attack; a 28% decrease YoY and 52% decrease QoQ.\n\n### Application-layer DDoS attacks\n\n*   2022 Q1 was the busiest quarter in the past 12 months for application-layer attacks. HTTP-layer DDoS attacks increased by 164% YoY and 135% QoQ.\n*   Diving deeper into the quarter, in March 2022 there were more HTTP DDoS attacks than in all of Q4 combined (and Q3, and Q1).\n*   After four consecutive quarters in a row with China as the top source of HTTP DDoS attacks, the US stepped into the lead this quarter. HTTP DDoS attacks originating from the US increased by a staggering 6,777% QoQ and 2,225% YoY.\n\n### Network-layer DDoS attacks\n\n*   Network-layer attacks in Q1 increased by 71% YoY but decreased 58% QoQ.\n*   The Telecommunications industry was the most targeted by network-layer DDoS attacks, followed by Gaming and Gambling companies, and the Information Technology and Services industry.\n*   Volumetric attacks increased in Q1. Attacks above 10 Mpps (million packets per second) grew by over 300% QoQ, and attacks over 100 Gbps grew by 645% QoQ.\n\nThis report is based on DDoS attacks that were automatically detected and mitigated by [Cloudflare’s DDoS Protection systems](https://www.cloudflare.com/ddos/). To learn more about how it works, check out [this deep-dive blog post](https://blog.cloudflare.com/deep-dive-cloudflare-autonomous-edge-ddos-protection/).\n\n**A note on how we measure DDoS attacks observed over our network**  \nTo analyze attack trends, we calculate the “DDoS activity” rate, which is either the percentage of attack traffic out of the total traffic (attack + clean) observed over our global network, or in a specific location, or in a specific category (e.g., industry or billing country). Measuring the percentages allows us to normalize data points and avoid biases reflected in absolute numbers towards, for example, a Cloudflare data center that receives more total traffic and likely, also more attacks.\n\nTo view an interactive version of this report view it on [Cloudflare Radar](https://radar.cloudflare.com/notebooks/ddos-2022-q1/).\n\n## Ransom Attacks\n\nOur systems constantly analyze traffic and automatically apply mitigation when DDoS attacks are detected. Each DDoS’d customer is prompted with an automated survey to help us better understand the nature of the attack and the success of the mitigation.\n\nFor over two years now, Cloudflare has been surveying attacked customers — one question on the survey being if they received a threat or a ransom note demanding payment in exchange to stop the DDoS attack. In the last quarter, 2021 Q4, we observed a record-breaking level of reported ransom DDoS attacks (one out of every five customers). This quarter, we’ve witnessed a drop in ransom DDoS attacks with only one out of 10 respondents reporting a ransom DDoS attack; a 28% decrease YoY and 52% decrease QoQ.\n\n![The percentage of respondents reported being targeted by a ransom DDoS attack or that have received threats in advance of the attack.](https://blog.cloudflare.com/content/images/2022/04/unnamed.png)\n\nWhen we break it down by month, we can see that January 2022 saw the largest number of respondents reporting receiving a ransom letter in Q1. Almost one out of every five customers (17%).\n\n![Graph of ransom DDoS attacks by month](https://blog.cloudflare.com/content/images/2022/04/unnamed--1-.png)\n\n## Application-layer DDoS attacks\n\n[Application-layer DDoS attacks](https://www.cloudflare.com/learning/ddos/application-layer-ddos-attack/), specifically HTTP DDoS attacks, are attacks that usually aim to disrupt a web server by making it unable to process legitimate user requests. If a server is bombarded with more requests than it can process, the server will drop legitimate requests and — in some cases — crash, resulting in degraded performance or an outage for legitimate users.\n\n![A diagram of a DDoS attack denying service to legitimate users](https://blog.cloudflare.com/content/images/2022/04/unnamed1.png)\n\n### Application-layer DDoS attacks by month\n\n**In Q1, application-layer DDoS attacks soared by 164% YoY and 135% QoQ - the busiest quarter within the past year.**\n\nApplication-layer DDoS attacks increased to new heights in the first quarter of 2022. In March alone, there were more HTTP DDoS attacks than in all of 2021 Q4 combined (and Q3, and Q1).\n\n![Graph of the yearly distribution of application-layer DDoS attacks by month in the past 12 months](https://blog.cloudflare.com/content/images/2022/04/image22-1.png)\n\n![Graph of the quarterly distribution of application-layer DDoS attacks by month in the past 12 months](https://blog.cloudflare.com/content/images/2022/04/image23-1.png)\n\n### Application-layer DDoS attacks by industry\n\n**Consumer Electronics was the most targeted industry in Q1.**\n\nGlobally, the Consumer Electronics industry was the most attacked with an increase of 5,086% QoQ. Second was the Online Media industry with a 2,131% increase in attacks QoQ. Third were Computer Software companies, with an increase of 76% QoQ and 1,472 YoY.\n\n![Graph of the distribution of HTTP DDoS attacks by industry in 2022 Q1](https://blog.cloudflare.com/content/images/2022/04/image9-5.png)\n\nHowever, if we focus only on Ukraine and Russia, we can see that Broadcast Media, Online Media companies, and Internet companies were the most targeted. Read more about [what Cloudflare is doing to keep the Open Internet flowing into Russia and keep attacks from getting out](https://blog.cloudflare.com/what-cloudflare-is-doing-to-keep-the-open-internet-flowing-into-russia-and-keep-attacks-from-getting-out/).\n\n![Graph of the distribution of HTTP DDoS attacks on Russian industries by source country in 2022 Q1](https://blog.cloudflare.com/content/images/2022/04/image14-1.png)\n\n![Graph of the distribution of HTTP DDoS attacks on Ukrainian industries by source country in 2022 Q1](https://blog.cloudflare.com/content/images/2022/04/image3-6.png)\n\n### Application-layer DDoS attacks by source country\n\nTo understand the origin of the HTTP attacks, we look at the geolocation of the source IP address belonging to the client that generated the attack HTTP requests. Unlike network-layer attacks, source IP addresses cannot be [spoofed](https://www.cloudflare.com/learning/ddos/glossary/ip-spoofing/) in HTTP attacks. A high percentage of DDoS activity in a given country usually indicates the presence of botnets operating from within the country's borders.\n\nAfter four consecutive quarters in a row with China as the top source of HTTP DDoS attacks, the US stepped into the lead this quarter. HTTP DDoS attacks originating from the US increased by a staggering 6,777% QoQ and 2,225% YoY. Following China in second place are India, Germany, Brazil, and Ukraine.\n\n![Graph of the distribution of HTTP DDoS attacks by source country in 2022 Q1](https://blog.cloudflare.com/content/images/2022/04/unnamed--2-.png)\n\n### Application-layer DDoS attacks by target country\n\nIn order to identify which countries are targeted by the most HTTP DDoS attacks, we bucket the DDoS attacks by our customers' billing countries and represent it as a percentage out of all DDoS attacks.\n\nThe US drops to second place, after being first for three consecutive quarters. Organizations in China were targeted the most by HTTP DDoS attacks, followed by the US, Russia, and Cyprus.\n\n![Graph of the distribution of HTTP DDoS attacks by target country in 2022 Q1](https://blog.cloudflare.com/content/images/2022/04/image7-4.png)\n\n## Network-layer DDoS attacks\n\nWhile application-layer attacks target the application (Layer 7 of the [OSI model](https://www.cloudflare.com/learning/ddos/glossary/open-systems-interconnection-model-osi/)) running the service that end users are trying to access (HTTP/S in our case), [network-layer attacks](https://www.cloudflare.com/learning/ddos/layer-3-ddos-attacks/) aim to overwhelm network infrastructure (such as in-line routers and servers) and the Internet link itself.\n\n![](https://blog.cloudflare.com/content/images/2022/04/unnamed--1--1.png)\n\n### Network-layer DDoS attacks by month\n\n**While HTTP DDoS attacks soared in Q1, network-layer DDoS attacks actually decreased by 58% QoQ, but still increased by 71% YoY.**\n\nDiving deeper into Q1, we can see that the amount of network-layer DDoS attacks remained mostly consistent throughout the quarter with about a third of attacks occurring every month.\n\n![Graph of the yearly distribution of network-layer DDoS attacks by month in the past 12 months]](https://blog.cloudflare.com/content/images/2022/04/image28.png)\n\n![Graph of the quarterly distribution of network-layer DDoS attacks by month in the past 12 months](https://blog.cloudflare.com/content/images/2022/04/image23-3.png)\n\n![Graph of the distribution of network-layer DDoS attacks in the past 12 months](https://blog.cloudflare.com/content/images/2022/04/unnamed--3-.png)\n\n## Cloudflare mitigates zero-day amplification DDoS attack\n\nAmongst these network-layer DDoS attacks are also zero-day DDoS attacks that Cloudflare automatically detected and mitigated.\n\nIn the beginning of March, Cloudflare researchers helped investigate and expose a zero-day vulnerability in Mitel business phone systems that amongst other possible exploitations, also enables attackers to launch an amplification DDoS attack. This type of attack reflects traffic off vulnerable Mitel servers to victims, amplifying the amount of traffic sent in the process by **an amplification factor of 220 billion percent** in this specific case. You can read more about it in our recent [blog post](https://blog.cloudflare.com/cve-2022-26143-amplification-attack/).\n\nWe observed several of these attacks across our network. One of them targeted a North American cloud provider using the Cloudflare Magic Transit service. The attack originated from 100 source IPs mainly from the US, UK, Canada, Netherlands, Australia, and approximately 20 other countries. It peaked above 50 Mpps (~22 Gbps) and was automatically detected and mitigated by Cloudflare systems.\n\n![Graph of an amplification DDoS attack that was mitigated by Cloudflare](https://blog.cloudflare.com/content/images/2022/04/image1-9.png)\n\n### Network-layer DDoS attacks by industry\n\nMany network-layer DDoS attacks target Cloudflare’s IP ranges directly. These IP ranges serve our [WAF/CDN customers](https://www.cloudflare.com/cdn/), [Cloudflare authoritative DNS](https://www.cloudflare.com/dns/), [Cloudflare public DNS resolver 1.1.1.1](https://www.cloudflare.com/learning/dns/what-is-1.1.1.1/),  [Cloudflare Zero Trust](https://www.cloudflare.com/products/zero-trust/zero-trust-network-access/) products, and our corporate offices, to name a few. Additionally, we also allocate dedicated IP addresses to customers via our [Spectrum](https://www.cloudflare.com/products/cloudflare-spectrum/) product and advertise the IP prefixes of other companies via our [Magic Transit](https://www.cloudflare.com/magic-transit/), [Magic WAN](https://www.cloudflare.com/magic-wan/), and [Magic Firewall](https://www.cloudflare.com/magic-firewall/) Products for L3/4 DDoS protection.\n\nIn this report, for the first time, we've begun classifying network-layer DDoS attacks according to the industries of our customers using the Spectrum and Magic products. This classification allows us to understand which industries are targeted the most by network-layer DDoS attacks.\n\nWhen we look at Q1 statistics, we can see that in terms of attack packets and attack bytes launched towards Cloudflare customers, the Telecommunications industry was targeted the most.  More than 8% of all attack bytes and 10% of all attack packets that Cloudflare mitigated targeted Telecommunications companies.\n\nFollowing not too far behind, in second and third place were the Gaming / Gambling and Information Technology and Services industries.\n\n![Graph of the distribution of network-layer DDoS attack bytes by industry](https://blog.cloudflare.com/content/images/2022/04/image20-1.png)\n\n![Graph of the distribution of network-layer DDoS attack packets by industry](https://blog.cloudflare.com/content/images/2022/04/image5-7.png)\n\n### Network-layer DDoS attacks by target country\n\nSimilarly to the classification by our customers’ industry, we can also bucket attacks by our customers’ billing country as we do for application-layer DDoS attacks, to identify the top attacked countries.\n\nLooking at Q1 numbers, we can see that the US was targeted by the highest percentage of DDoS attacks traffic — over 10% of all attack packets and almost 8% of all attack bytes. Following the US is China, Canada, and Singapore.\n\n![Graph of the distribution of network-layer DDoS attack bytes by target country](https://blog.cloudflare.com/content/images/2022/04/image19-1.png)\n\n![Graph of the distribution of network-layer DDoS attack packets by target country](https://blog.cloudflare.com/content/images/2022/04/image15-2.png)\n\n### Network-layer DDoS attacks by ingress country\n\nWhen trying to understand where network-layer DDoS attacks originate, we cannot use the same method as we use for the application-layer attack analysis. To launch an application-layer DDoS attack, [successful handshakes](https://www.cloudflare.com/learning/ssl/what-happens-in-a-tls-handshake/) must occur between the client and the server in order to establish an HTTP/S connection. For a successful handshake to occur, the attacker cannot [spoof](https://www.cloudflare.com/learning/ddos/glossary/ip-spoofing/) their source IP address. While the attacker may use botnets, proxies, and other methods to obfuscate their identity, the attacking client's source IP location does sufficiently represent the attack source of application-layer DDoS attacks.\n\nOn the other hand, to launch network-layer DDoS attacks, in most cases, no handshake is needed. Attackers can [spoof](https://www.cloudflare.com/learning/ddos/glossary/ip-spoofing/) the source IP address in order to obfuscate the attack source and introduce randomness into the attack properties, which can make it harder for simple DDoS protection systems to block the attack. So if we were to derive the source country based on a spoofed source IP, we would get a ‘spoofed country’.\n\nFor this reason, when analyzing network-layer DDoS attack sources, we bucket the traffic by the Cloudflare edge data center locations where the traffic was ingested, and not by the (potentially) spoofed source IP to get an understanding of where the attacks originate from. We are able to achieve geographical accuracy in our report because we have data centers in [over 270 cities](https://blog.cloudflare.com/mid-2022-new-cities/) around the world. However, even this method is not 100% accurate, as traffic may be back hauled and routed via various Internet Service Providers and countries for reasons that vary from cost reduction to congestion and failure management.\n\nIn Q1, the percentage of attacks detected in Cloudflare’s data centers in Azerbaijan increased by 16,624% QoQ and 96,900% YoY, making it the country with the highest percentage of network-layer DDoS activity (48.5%).\n\nFollowing our Azerbaijanian data center is our Palestinian data center where a staggering 41.9% of all traffic was DDoS traffic. This represents a 10,120% increase QoQ and 46,456% YoY.\n\n![Graph of the distribution of network-layer DDoS attacks by source country in 2022 Q1](https://blog.cloudflare.com/content/images/2022/04/image2-8.png)\n\n![Map of the distribution of network-layer DDoS attacks by source country in 2022 Q1](https://blog.cloudflare.com/content/images/2022/04/image12-1.png)\n\nTo view all regions and countries, check out the [interactive map](https://radar.cloudflare.com/notebooks/ddos-2022-q1/).\n\n### Attack vectors\n\n**SYN Floods remain the most popular DDoS attack vector, while use of generic UDP floods drops significantly in Q1.**\n\nAn attack vector is a term used to describe the method that the attacker uses to launch their DDoS attack, i.e., the IP protocol, packet attributes such as TCP flags, flooding method, and other criteria.\n\nIn Q1, SYN floods accounted for 57% of all network-layer DDoS attacks, representing a 69% increase QoQ and a 13% increase YoY. In second place, attacks over SSDP surged by over 1,100% QoQ. Following were RST floods and attacks over UDP. Last quarter, generic UDP floods took the second place, but this time, generic UDP DDoS attacks plummeted by 87% QoQ from 32% to a mere 3.9%.\n\n![Graph of the top network-layer DDoS attack vectors in 2022 Q1](https://blog.cloudflare.com/content/images/2022/04/image11-1.png)\n\n## Emerging threats\n\nIdentifying the top attack vectors helps organizations understand the threat landscape. In turn, this may help them improve their security posture to protect against those threats. Similarly, learning about new emerging threats that may not yet account for a significant portion of attacks, can help mitigate them before they become a significant force.\n\nWhen we look at new emerging attack vectors in Q1, we can see increases in DDoS attacks reflecting off of Lantronix services (+971% QoQ) and SSDP reflection attacks (+724% QoQ). Additionally, SYN-ACK attacks increased by 437% and attacks by Mirai botnets by 321% QoQ.\n\n### Attacker reflecting traffic off of Lantronix Discovery Service\n\nLantronix is a US-based software and hardware company that provides solutions for Internet of Things (IoT) management amongst their vast offering. One of the tools that they provide to manage their IoT components is the Lantronix Discovery Protocol. It is a command-line tool that helps to search and find Lantronix devices. The discovery tool is UDP-based, meaning that no handshake is required. The source IP can be spoofed. So an attacker can use the tool to search for publicly exposed Lantronix devices using a 4 byte request, which will then in turn respond with a 30 byte response from port 30718. By spoofing the source IP of the victim, all Lantronix devices will target their responses to the victim — resulting in a reflection/amplification attack.\n\n### Simple Service Discovery Protocol used for reflection DDoS attacks\n\nThe Simple Service Discovery Protocol (SSDP) protocol works similarly to the Lantronix Discovery protocol, but for Universal Plug and Play (UPnP) devices such as network-connected printers. By abusing the SSDP protocol, attackers can generate a reflection-based DDoS attack overwhelming the target’s infrastructure and taking their Internet properties offline. You can read more about SSDP-based DDoS attacks [here](https://www.cloudflare.com/learning/ddos/ssdp-ddos-attack/).\n\n![Graph of the top emerging network-layer DDoS attack threats in 2022 Q1](https://blog.cloudflare.com/content/images/2022/04/image21.png)\n\n### Network-layer DDoS attacks by attack rate\n\n**In Q1, we observed a massive uptick in volumetric DDoS attacks — both from the packet rate and bitrate perspective. Attacks over 10 Mpps grew by over 300% QoQ, and attacks over 100 Gbps grew by 645% QoQ.**\n\nThere are different ways of measuring the size of an L3/4 DDoS attack. One is the volume of traffic it delivers, measured as the bit rate (specifically, terabits per second or gigabits per second). Another is the number of packets it delivers, measured as the packet rate (specifically, millions of packets per second).\n\nAttacks with high bit rates attempt to cause a denial-of-service event by clogging the Internet link, while attacks with high packet rates attempt to overwhelm the servers, routers, or other in-line hardware appliances. These devices dedicate a certain amount of memory and computation power to process each packet. Therefore, by bombarding it with many packets, the appliance can be left with no further processing resources. In such a case, packets are “dropped,” i.e., the appliance is unable to process them. For users, this results in service disruptions and denial of service.\n\n### Distribution by packet rate\n\nThe majority of network-layer DDoS attacks remain below 50,000 packets per second. While 50 kpps is on the lower side of the spectrum at Cloudflare scale, it can still easily take down unprotected Internet properties and congest even a standard Gigabit Ethernet connection.\n\n![Graph of the distribution of network-layer DDoS attacks by packet rate in 2022 Q1](https://blog.cloudflare.com/content/images/2022/04/image4-5.png)\n\nWhen we look at the changes in the attack sizes, we can see that attacks of over 10 Mpps grew by over 300% QoQ. Similarly, attacks of 1-10 Mpps grew by almost 40% QoQ.\n\n![Graph of the change in the distribution of network-layer DDoS attacks by packet rate quarter over quarter](https://blog.cloudflare.com/content/images/2022/04/image8-3.png)\n\n### Distribution by bitrate\n\nIn Q1, most of the network-layer DDoS attacks remain below 500 Mbps. This too is a tiny drop in the water at [Cloudflare scale](https://www.cloudflare.com/network/), but can very quickly shut down unprotected Internet properties with less capacity or at the very least congest, even a standard Gigabit Ethernet connection.\n\n![](https://blog.cloudflare.com/content/images/2022/04/image6-4.png)\n\n_Graph of the distribution of network-layer DDoS attacks by bit rate in 2022 Q1_\n\nSimilarly to the trends observed in the packet-per-second realm, here we can also see large increases. The amount of DDoS attacks that peaked over 100 Gbps increased by 645% QoQ; attacks peaking between 10 Gbps to 100 Gbps increased by 407%; attacks peaking between 1 Gbps to 10 Gbps increased by 88%; and even attacks peaking between 500 Mbps to 1 Gbps increased by almost 20% QoQ.\n\n![Graph of the change in the distribution of network-layer DDoS attacks by bit rate quarter over quarter](https://blog.cloudflare.com/content/images/2022/04/image13-1.png)\n\n### Network-layer DDoS attacks by duration\n\n**Most attacks remain under one hour in duration, reiterating the need for automated always-on DDoS mitigation solutions.**\n\nWe measure the duration of an attack by recording the difference between when it is first detected by our systems as an attack and the last packet we see with that attack signature towards that specific target.\n\nIn previous reports, we provided a breakdown of ‘attacks under an hour’, and larger time ranges. However, in most cases over 90 percent of attacks last less than an hour. So starting from this report, we broke down the short attacks and grouped them by shorter time ranges to provide better granularity.\n\nOne important thing to keep in mind is that even if an attack lasts only a few minutes, if it is successful, the repercussions could last well beyond the initial attack duration. IT personnel responding to a successful attack may spend hours and even days restoring their services.\n\nIn the first quarter of 2022, more than half of the attacks lasted 10-20 minutes, approximately 40% ended within 10 minutes, another ~5% lasted 20-40 minutes, and the remaining lasted longer than 40 minutes.\n\n![Graph of the distribution of network-layer DDoS attacks by duration in 2022 Q1](https://blog.cloudflare.com/content/images/2022/04/image27.png)\n\nShort attacks can easily go undetected, especially burst attacks that, within seconds, bombard a target with a significant number of packets, bytes, or requests. In this case, DDoS protection services that rely on manual mitigation by security analysis have no chance in mitigating the attack in time. They can only learn from it in their post-attack analysis, then deploy a new rule that filters the attack fingerprint and hope to catch it next time. Similarly, using an “on-demand” service, where the security team will redirect traffic to a DDoS provider during the attack, is also inefficient because the attack will already be over before the traffic routes to the on-demand DDoS provider.\n\nIt’s recommended that companies use automated, always-on DDoS protection services that analyze traffic and apply real-time fingerprinting fast enough to block short-lived attacks.\n\n## Summary\n\nCloudflare’s mission is to help build a better Internet. A better Internet is one that is more secure, faster, and reliable for everyone — even in the face of DDoS attacks. As part of our mission, since 2017, we’ve been providing [unmetered and unlimited DDoS protection](https://blog.cloudflare.com/unmetered-mitigation/) for free to all of our customers. Over the years, it has become increasingly easier for attackers to launch DDoS attacks. But as easy as it has become, we want to make sure that it is even easier — and free — for organizations of all sizes to protect themselves against DDoS attacks of all types.\n\nNot using Cloudflare yet? [Start now](https://dash.cloudflare.com/sign-up) with our Free and Pro plans to protect your websites, or [contact us](https://www.cloudflare.com/magic-transit/) for comprehensive DDoS protection for your entire network using Magic Transit.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[DDoS](https://blog.cloudflare.com/tag/ddos/) [DDoS Reports](https://blog.cloudflare.com/tag/ddos-reports/) [Attacks](https://blog.cloudflare.com/tag/attacks/) [Trends](https://blog.cloudflare.com/tag/trends/) [Cloudflare Radar](https://blog.cloudflare.com/tag/cloudflare-radar/)"
    },
    {
      "url": "https://blog.cloudflare.com/ddos-attack-trends-for-2021-q4/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/ddos-attack-trends-for-2021-q4/",
        "loadedTime": "2023-12-05T02:32:30.223Z",
        "referrerUrl": "https://blog.cloudflare.com/cyber-week-analyzing-internet-traffic-and-e-commerce-trends/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/ddos-attack-trends-for-2021-q4/",
        "title": "DDoS Attack Trends for Q4 2021",
        "description": "In Q4, we observed a 95% increase in L3/4 DDoS attacks and record-breaking levels of Ransom DDoS attacks. The Manufacturing industry was the most targeted alongside a 5,800% increase in SNMP-based DDoS attacks and massive campaigns against VoIP providers around the world",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "01/10/2022\n12 min read\nThis post is also available in 简体中文, 繁體中文, 日本語, 한국어, Deutsch, Français, Español.\nThe first half of 2021 witnessed massive ransomware and ransom DDoS attack campaigns that interrupted aspects of critical infrastructure around the world (including one of the largest petroleum pipeline system operators in the US) and a vulnerability in IT management software that targeted schools, public sector, travel organizations, and credit unions, to name a few.\nThe second half of the year recorded a growing swarm of one of the most powerful botnets deployed (Meris) and record-breaking HTTP DDoS attacks and network-layer attacks observed over the Cloudflare network. This besides the Log4j2 vulnerability (CVE-2021-44228) discovered in December that allows an attacker to execute code on a remote server — arguably one of the most severe vulnerabilities on the Internet since both Heartbleed and Shellshock.\nProminent attacks such as the ones listed above are but a few examples that demonstrate a trend of intensifying cyber-insecurity that affected everyone, from tech firms and government organizations to wineries and meat processing plants.\nHere are some DDoS attack trends and highlights from 2021 and Q4 ‘21 specifically:\nRansom DDoS attacks\nIn Q4, ransom DDoS attacks increased by 29% YoY and 175% QoQ.\nIn December alone, one out of every three survey respondents reported being targeted by a ransom DDoS attack or threatened by the attacker.\nApplication-layer DDoS attacks\nThe Manufacturing industry was the most attacked in Q4 ’21, recording a whopping 641% increase QoQ in the number of attacks. The Business Services and Gaming/Gambling industries were the second and third most targeted industries by application-layer DDoS attacks.\nFor the fourth time in a row this year, China topped the charts with the highest percentage of attack traffic originating from its networks.\nA new botnet called the Meris botnet emerged in mid-2021 and continued to bombard organizations around the world, launching some of the largest HTTP attacks on record — including a 17.2M rps attack that Cloudflare automatically mitigated.\nNetwork-layer DDoS attacks\nQ4 ’21 was the busiest quarter for attackers in 2021. In December 2021 alone, there were more than all the attacks observed in Q1 and Q2 ’21 separately.\nWhile the majority of attacks were small, terabit-strong attacks became the new norm in the second half of 2021. Cloudflare automatically mitigated dozens of attacks peaking over 1 Tbps, with the largest one peaking just under 2 Tbps — the largest we’ve ever seen.\nQ4 ’21, and November specifically, recorded a persistent ransom DDoS campaign against VoIP providers around the world.\nAttacks originating from Moldova quadrupled in Q4 ’21 QoQ, making it the country with the highest percentage of network-layer DDoS activity.\nSYN floods and UDP floods were the most frequent attack vectors while emerging threats such as SNMP attacks increased by nearly 5,800% QoQ.\nThis report is based on DDoS attacks that were automatically detected and mitigated by Cloudflare’s DDoS Protection systems. To learn more about how it works, check out this deep-dive blog post.\nA note on how we measure DDoS attacks observed over our network\nTo analyze attack trends, we calculate the “DDoS activity” rate, which is the percentage of attack traffic out of the total traffic (attack + clean) observed over our global network. Measuring attack numbers as a percentage of the total traffic observed allows us to normalize data points and avoid biases reflected in absolute numbers towards, for example, a Cloudflare data center that receives more total traffic and likely, also more attacks.\nAn interactive version of this report is available on Cloudflare Radar.\nRansom Attacks\nOur systems constantly analyze traffic and automatically apply mitigation when DDoS attacks are detected. Each DDoS’d customer is prompted with an automated survey to help us better understand the nature of the attack and the success of the mitigation.\nFor over two years now, Cloudflare has been surveying attacked customers — one question on the survey being if they received a ransom note demanding payment in exchange to stop the DDoS attack. Q4 ’21 recorded the highest survey responses ever that indicated ransom threats — ransom attacks increased by 29% YoY and 175% QoQ. More specifically, one out of every 4.5 respondents (22%) reported receiving a ransom letter demanding payment by the attacker.\nThe percentage of respondents reported being targeted by a ransom DDoS attack or that have received threats in advance of the attack.\nWhen we break it down by month, we can see that December 2021 topped the charts with 32% of respondents reporting receiving a ransom letter — that’s nearly one out of every three surveyed respondents.\nApplication-layer DDoS attacks\nApplication-layer DDoS attacks, specifically HTTP DDoS attacks, are attacks that usually aim to disrupt a web server by making it unable to process legitimate user requests. If a server is bombarded with more requests than it can process, the server will drop legitimate requests and — in some cases — crash, resulting in degraded performance or an outage for legitimate users.\nApplication-layer DDoS attacks by industry\nIn Q4, DDoS attacks on Manufacturing companies increased by 641% QoQ, and DDoS attacks on the Business Services industry increased by 97%.\nWhen we break down the application-layer attacks targeted by industry, the Manufacturing, Business Services, and Gaming/Gambling industries were the most targeted industries in Q4 ’21.\nApplication-layer DDoS attacks by source country\nTo understand the origin of the HTTP attacks, we look at the geolocation of the source IP address belonging to the client that generated the attack HTTP requests. Unlike network-layer attacks, source IP addresses cannot be spoofed in HTTP attacks. A high percentage of DDoS activity in a given country usually indicates the presence of botnets operating from within the country's borders.\nFor the fourth quarter in a row, China remains the country with the highest percentage of DDoS attacks originating from within its borders. More than three out of every thousand HTTP requests that originated from Chinese IP addresses were part of an HTTP DDoS attack. The US remained in second place, followed by Brazil and India.\nApplication-layer DDoS attacks by target country\nIn order to identify which countries are targeted by the most HTTP DDoS attacks, we bucket the DDoS attacks by our customers' billing countries and represent it as a percentage out of all DDoS attacks.\nFor the third consecutive time this year, organizations in the United States were targeted by the most HTTP DDoS attacks, followed by Canada and Germany.\nNetwork-layer DDoS attacks\nWhile application-layer attacks target the application (Layer 7 of the OSI model) running the service that end users are trying to access, network-layer attacks aim to overwhelm network infrastructure (such as in-line routers and servers) and the Internet link itself.\nCloudflare thwarts an almost 2 Tbps attack\nIn November, our systems automatically detected and mitigated an almost 2 Tbps DDoS attack. This was a multi-vector attack combining DNS amplification attacks and UDP floods. The entire attack lasted just one minute. The attack was launched from approximately 15,000 bots running a variant of the original Mirai code on IoT devices and unpatched GitLab instances.\nNetwork-layer DDoS attacks by month\nDecember was the busiest month for attackers in 2021.\nQ4 ‘21 was the busiest quarter in 2021 for attackers. Over 43% of all network-layer DDoS attacks took place in the fourth quarter of 2021. While October was a relatively calmer month, in November, the month of the Chinese Singles' Day, the American Thanksgiving holiday, Black Friday, and Cyber Monday, the number of network-layer DDoS attacks nearly doubled. The number of observed attacks increased towards the final days of December ’21 as the world prepared to close out the year. In fact, the total number of attacks in December alone was higher than all the attacks in Q2 ’21 and almost equivalent to all attacks in Q1 ’21.\nNetwork-layer DDoS attacks by attack rate\nWhile most attacks are still relatively ‘small’ in size, terabit-strong attacks are becoming the norm.\nThere are different ways of measuring the size of an L3/4 DDoS attack. One is the volume of traffic it delivers, measured as the bit rate (specifically, terabits per second or gigabits per second). Another is the number of packets it delivers, measured as the packet rate (specifically, millions of packets per second).\nAttacks with high bit rates attempt to cause a denial-of-service event by clogging the Internet link, while attacks with high packet rates attempt to overwhelm the servers, routers, or other in-line hardware appliances. These devices dedicate a certain amount of memory and computation power to process each packet. Therefore, by bombarding it with many packets, the appliance can be left with no further processing resources. In such a case, packets are “dropped,” i.e., the appliance is unable to process them. For users, this results in service disruptions and denial of service.\nThe distribution of attacks by their size (in bit rate) and month is shown below. As seen in the graph above, the majority of attacks took place in December. However, the graph below illustrates that larger attacks, over 300 Gbps in size, took place in November. Most of the attacks between 5-20 Gbps took place in December.\nDistribution by packet rate\nAn interesting correlation Cloudflare has observed is that when the number of attacks increases, their size and duration decrease. In the first two-thirds of 2021, the number of attacks was relatively small, and correspondingly, their rates increased, e.g., in Q3 ’21, attacks ranging from 1-10 million packets per second (mpps) increased by 196%. In Q4 ’21, the number of attacks increased and Cloudflare observed a decrease in the size of attacks. 91% of all attacks peaked below 50,000 packets per second (pps) — easily sufficient to take down unprotected Internet properties.\nLarger attacks of over 1 mpps decreased by 48% to 28% QoQ, while attacks peaking below 50K pps increased by 2.36% QoQ.\nDistribution by bit rate\nSimilar to the trend observed in packet-intensive attacks, the amount of bit-intensive attacks shrunk as well. While attacks over 1 Tbps are becoming the norm, with the largest one we’ve ever seen peak just below 2 Tbps, the majority of attacks are still small and peaked below 500 Mbps (97.2%).\nIn Q4 ’21, larger attacks of all ranges above 500 Mbps saw massive decreases ranging from 35% to 57% for the larger 100+ Gbps attacks.\nNetwork-layer DDoS attacks by duration\nMost attacks remain under one hour in duration, reiterating the need for automated always-on DDoS mitigation solutions.\nWe measure the duration of an attack by recording the difference between when it is first detected by our systems as an attack and the last packet we see with that attack signature towards that specific target. In the last quarter of 2021, 98% of all network-layer attacks lasted less than one hour. This is very common as most of the attacks are short-lived. Even more so, a trend we’ve seen is that when the number of attacks increases, as in this quarter, their rate and duration decreases.\nShort attacks can easily go undetected, especially burst attacks that, within seconds, bombard a target with a significant number of packets, bytes, or requests. In this case, DDoS protection services that rely on manual mitigation by security analysis have no chance in mitigating the attack in time. They can only learn from it in their post-attack analysis, then deploy a new rule that filters the attack fingerprint and hope to catch it next time. Similarly, using an “on-demand” service, where the security team will redirect traffic to a DDoS provider during the attack, is also inefficient because the attack will already be over before the traffic routes to the on-demand DDoS provider.\nIt’s recommended that companies use automated, always-on DDoS protection services that analyze traffic and apply real-time fingerprinting fast enough to block short-lived attacks.\nAttack vectors\nSYN floods remain attackers’ favorite method of attack, while attacks over SNMP saw a massive surge of almost 5,800% QoQ.\nAn attack vector is a term used to describe the method that the attacker uses to launch their DDoS attack, i.e., the IP protocol, packet attributes such as TCP flags, flooding method, and other criteria.\nFor the first time in 2021, the percentage of SYN flood attacks significantly decreased. Throughout 2021, SYN floods accounted for 54% of all network-layer attacks on average. While still grabbing first place as the most frequent vector, its share dropped by 38% QoQ to 34%.\nHowever, it was a close-run for SYN attacks and UDP attacks. A UDP flood is a type of denial-of-service attack in which a large number of User Datagram Protocol (UDP) packets are sent to a targeted server with the aim of overwhelming that device’s ability to process and respond. Oftentimes, the firewall protecting the targeted server can also become exhausted as a result of UDP flooding, resulting in a denial-of-service to legitimate traffic. Attacks over UDP jumped from fourth place in Q3 ’21 to second place in Q4 ’21, with a share of 32% of all network-layer attacks — a 1,198% increase in QoQ.\nIn third place came the SNMP underdog that made a massive leap with its first time 2021 appearance in the top attack vectors.\nEmerging threats\nWhen we look at emerging attack vectors — which helps us understand what new vectors attackers are deploying to launch attacks — we observe a massive spike in SNMP, MSSQL, and generic UDP-based DDoS attacks.\nBoth SNMP and MSSQL attacks are used to reflect and amplify traffic on the target by spoofing the target’s IP address as the source IP in the packets used to trigger the attack.\nSimple Network Management Protocol (SNMP) is a UDP-based protocol that is often used to discover and manage network devices such as printers, switches, routers, and firewalls of a home or enterprise network on UDP well-known port 161. In an SNMP reflection attack, the attacker sends out a large number of SNMP queries while spoofing the source IP address in the packet as the targets to devices on the network that, in turn, reply to that target’s address. Numerous responses from the devices on the network results in the target network being DDoSed.\nSimilar to the SNMP amplification attack, the Microsoft SQL (MSSQL) attack is based on a technique that abuses the Microsoft SQL Server Resolution Protocol for the purpose of launching a reflection-based DDoS attack. The attack occurs when a Microsoft SQL Server responds to a client query or request, attempting to exploit the Microsoft SQL Server Resolution Protocol (MC-SQLR), listening on UDP port 1434.\nNetwork-layer DDoS attacks by country\nAttacks originating from Moldova quadrupled, making it the country with the highest percentage of network-layer DDoS activity.\nWhen analyzing network-layer DDoS attacks, we bucket the traffic by the Cloudflare edge data center locations where the traffic was ingested, and not by the source IP. The reason for this is that, when attackers launch network-layer attacks, they can spoof the source IP address in order to obfuscate the attack source and introduce randomness into the attack properties, which can make it harder for simple DDoS protection systems to block the attack. Hence, if we were to derive the source country based on a spoofed source IP, we would get a spoofed country.\nCloudflare is able to overcome the challenges of spoofed IPs by displaying the attack data by the location of the Cloudflare data center in which the attack was observed. We are able to achieve geographical accuracy in our report because we have data centers in over 250 cities around the world.\nTo view all regions and countries, check out the interactive map.\nSummary\nCloudflare’s mission is to help build a better Internet. A better Internet is one that is more secure, faster, and reliable for everyone — even in the face of DDoS attacks. As part of our mission, since 2017, we’ve been providing unmetered and unlimited DDoS protection for free to all of our customers. Over the years, it has become increasingly easier for attackers to launch DDoS attacks. To counter the attacker’s advantage, we want to make sure that it is also easy and free for organizations of all sizes to protect themselves against DDoS attacks of all types.\nNot using Cloudflare yet? Start now.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nDDoS Attacks Trends Cloudflare Radar RDDoS",
      "markdown": "01/10/2022\n\n*   [![Omer Yoachimik](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/06/Screenshot-2023-06-02-at-9.38.13-AM.png)](https://blog.cloudflare.com/author/omer/)\n*   [![Vivek Ganti](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2020/08/Vivek-Ganti.jpeg)](https://blog.cloudflare.com/author/vivek/)\n\n12 min read\n\n_This post is also available in [简体中文](https://blog.cloudflare.com/zh-cn/ddos-attack-trends-for-2021-q4-zh-cn/), [繁體中文](https://blog.cloudflare.com/zh-tw/ddos-attack-trends-for-2021-q4-zh-tw/), [日本語](https://blog.cloudflare.com/ja-jp/ddos-attack-trends-for-2021-q4-ja-jp/), [한국어](https://blog.cloudflare.com/ko-kr/ddos-attack-trends-for-2021-q4-ko-kr/), [Deutsch](https://blog.cloudflare.com/de-de/ddos-attack-trends-for-2021-q4-de-de/), [Français](https://blog.cloudflare.com/fr-fr/ddos-attack-trends-for-2021-q4-fr-fr/), [Español](https://blog.cloudflare.com/es-es/ddos-attack-trends-for-2021-q4-es-es/)._\n\n![DDoS Attack Trends for Q3 2021](https://blog.cloudflare.com/content/images/2022/01/image3-1.png)\n\nThe first half of 2021 witnessed massive ransomware and ransom DDoS attack campaigns that interrupted aspects of critical infrastructure around the world (including one of the largest petroleum pipeline system operators in the US) and a [vulnerability in IT management software](https://csirt.divd.nl/2021/07/03/Kaseya-Case-Update/) that targeted schools, public sector, travel organizations, and credit unions, to name a few.\n\nThe second half of the year recorded a growing swarm of one of the most powerful botnets deployed ([Meris](https://blog.cloudflare.com/meris-botnet/)) and [record-breaking HTTP DDoS attacks](https://blog.cloudflare.com/cloudflare-thwarts-17-2m-rps-ddos-attack-the-largest-ever-reported/) and [network-layer attacks](https://blog.cloudflare.com/cloudflare-blocks-an-almost-2-tbps-multi-vector-ddos-attack/) observed over the Cloudflare network. This besides the [Log4j2 vulnerability](https://blog.cloudflare.com/inside-the-log4j2-vulnerability-cve-2021-44228/) (CVE-2021-44228) discovered in December that allows an attacker to execute code on a remote server — arguably one of the most severe vulnerabilities on the Internet since both [Heartbleed](https://blog.cloudflare.com/tag/heartbleed/) and [Shellshock](https://blog.cloudflare.com/inside-shellshock/).\n\nProminent attacks such as the ones listed above are but a few examples that demonstrate a trend of intensifying cyber-insecurity that affected everyone, from tech firms and government organizations to wineries and meat processing plants.\n\nHere are some [DDoS attack](https://www.cloudflare.com/en-gb/learning/ddos/what-is-a-ddos-attack/) trends and highlights from 2021 and Q4 ‘21 specifically:\n\n### Ransom DDoS attacks\n\n*   In Q4, [ransom DDoS attacks](https://www.cloudflare.com/en-gb/learning/ddos/ransom-ddos-attack/) increased by 29% YoY and 175% QoQ.\n*   In December alone, one out of every three survey respondents reported being targeted by a ransom DDoS attack or threatened by the attacker.\n\n### Application-layer DDoS attacks\n\n*   The Manufacturing industry was the most attacked in Q4 ’21, recording a whopping 641% increase QoQ in the number of attacks. The Business Services and Gaming/Gambling industries were the second and third most targeted industries by application-layer DDoS attacks.\n*   For the fourth time in a row this year, China topped the charts with the highest percentage of attack traffic originating from its networks.\n*   A new botnet called the [Meris botnet](https://blog.cloudflare.com/meris-botnet/) emerged in mid-2021 and continued to bombard organizations around the world, launching some of the largest HTTP attacks on record — including a [17.2M rps attack that Cloudflare automatically mitigated](https://blog.cloudflare.com/cloudflare-thwarts-17-2m-rps-ddos-attack-the-largest-ever-reported/).\n\n### Network-layer DDoS attacks\n\n*   Q4 ’21 was the busiest quarter for attackers in 2021. In December 2021 alone, there were more than all the attacks observed in Q1 and Q2 ’21 separately.\n*   While the majority of attacks were small, terabit-strong attacks became the new norm in the second half of 2021. Cloudflare automatically mitigated dozens of attacks peaking over 1 Tbps, with the largest one peaking just under [2 Tbps — the largest we’ve ever seen](https://blog.cloudflare.com/cloudflare-blocks-an-almost-2-tbps-multi-vector-ddos-attack/).\n*   Q4 ’21, and November specifically, recorded a persistent ransom [DDoS campaign against VoIP providers](https://blog.cloudflare.com/update-on-voip-attacks/) around the world.\n*   Attacks originating from Moldova quadrupled in Q4 ’21 QoQ, making it the country with the highest percentage of network-layer DDoS activity.\n*   [SYN floods](https://www.cloudflare.com/en-gb/learning/ddos/syn-flood-ddos-attack/) and [UDP floods](https://www.cloudflare.com/en-gb/learning/ddos/udp-flood-ddos-attack/) were the most frequent attack vectors while emerging threats such as SNMP attacks increased by nearly 5,800% QoQ.\n\nThis report is based on DDoS attacks that were automatically detected and mitigated by Cloudflare’s DDoS Protection systems. To learn more about how it works, check out [this deep-dive blog post](https://blog.cloudflare.com/deep-dive-cloudflare-autonomous-edge-ddos-protection/).\n\n### A note on how we measure DDoS attacks observed over our network\n\nTo analyze attack trends, we calculate the “DDoS activity” rate, which is the percentage of attack traffic out of the total traffic (attack + clean) observed over our global network. Measuring attack numbers as a percentage of the total traffic observed allows us to normalize data points and avoid biases reflected in absolute numbers towards, for example, a Cloudflare data center that receives more total traffic and likely, also more attacks.\n\nAn interactive version of this report is available on [Cloudflare Radar](https://radar.cloudflare.com/notebooks/ddos-2021-q4).\n\n## Ransom Attacks\n\nOur systems constantly analyze traffic and automatically apply mitigation when DDoS attacks are detected. Each DDoS’d customer is prompted with an automated survey to help us better understand the nature of the attack and the success of the mitigation.\n\nFor over two years now, Cloudflare has been surveying attacked customers — one question on the survey being if they received a ransom note demanding payment in exchange to stop the DDoS attack. Q4 ’21 recorded the highest survey responses ever that indicated ransom threats — ransom attacks increased by 29% YoY and 175% QoQ. More specifically, one out of every 4.5 respondents (22%) reported receiving a ransom letter demanding payment by the attacker.\n\n![Graph of ransom DDoS attacks by quarter](https://blog.cloudflare.com/content/images/2022/01/image9.png)\n\nThe percentage of respondents reported being targeted by a ransom DDoS attack or that have received threats in advance of the attack.\n\nWhen we break it down by month, we can see that December 2021 topped the charts with 32% of respondents reporting receiving a ransom letter — that’s nearly one out of every three surveyed respondents.\n\n![Graph of ransom DDoS attacks by month](https://blog.cloudflare.com/content/images/2022/01/unnamed.png \"Chart\")\n\n## Application-layer DDoS attacks\n\n[Application-layer DDoS attacks](https://www.cloudflare.com/learning/ddos/application-layer-ddos-attack/), specifically HTTP DDoS attacks, are attacks that usually aim to disrupt a web server by making it unable to process legitimate user requests. If a server is bombarded with more requests than it can process, the server will drop legitimate requests and — in some cases — crash, resulting in degraded performance or an outage for legitimate users.\n\n![](https://blog.cloudflare.com/content/images/2022/01/image13.png)\n\n### Application-layer DDoS attacks by industry\n\n**In Q4, DDoS attacks on Manufacturing companies increased by 641% QoQ, and DDoS attacks on the Business Services industry increased by 97%.**\n\nWhen we break down the application-layer attacks targeted by industry, the Manufacturing, Business Services, and Gaming/Gambling industries were the most targeted industries in Q4 ’21.\n\n![Graph of the distribution of HTTP DDoS attacks by industry in Q4](https://blog.cloudflare.com/content/images/2022/01/image12.png)\n\n### Application-layer DDoS attacks by source country\n\nTo understand the origin of the HTTP attacks, we look at the geolocation of the source IP address belonging to the client that generated the attack HTTP requests. Unlike network-layer attacks, source IP addresses cannot be [spoofed](https://www.cloudflare.com/learning/ddos/glossary/ip-spoofing/) in HTTP attacks. A high percentage of DDoS activity in a given country usually indicates the presence of botnets operating from within the country's borders.\n\nFor the fourth quarter in a row, China remains the country with the highest percentage of DDoS attacks originating from within its borders. More than three out of every thousand HTTP requests that originated from Chinese IP addresses were part of an HTTP DDoS attack. The US remained in second place, followed by Brazil and India.\n\n![Graph of the distribution of HTTP DDoS attacks by source country in Q4](https://blog.cloudflare.com/content/images/2022/01/image18.png)\n\n### Application-layer DDoS attacks by target country\n\nIn order to identify which countries are targeted by the most HTTP DDoS attacks, we bucket the DDoS attacks by our customers' billing countries and represent it as a percentage out of all DDoS attacks.\n\nFor the third consecutive time this year, organizations in the United States were targeted by the most HTTP DDoS attacks, followed by Canada and Germany.\n\n![Graph of the distribution of HTTP DDoS attacks by target country in Q4](https://blog.cloudflare.com/content/images/2022/01/image8.png)\n\n## Network-layer DDoS attacks\n\nWhile application-layer attacks target the application (Layer 7 of the [OSI model](https://www.cloudflare.com/learning/ddos/glossary/open-systems-interconnection-model-osi/)) running the service that end users are trying to access, [network-layer attacks](https://www.cloudflare.com/learning/ddos/layer-3-ddos-attacks/) aim to overwhelm network infrastructure (such as in-line routers and servers) and the Internet link itself.\n\n### Cloudflare thwarts an almost 2 Tbps attack\n\nIn November, our systems automatically detected and mitigated [an almost 2 Tbps DDoS attack](https://blog.cloudflare.com/cloudflare-blocks-an-almost-2-tbps-multi-vector-ddos-attack/). This was a multi-vector attack combining [DNS amplification](https://www.cloudflare.com/en-gb/learning/ddos/dns-amplification-ddos-attack/) attacks and [UDP floods](https://www.cloudflare.com/en-gb/learning/ddos/udp-flood-ddos-attack/). The entire attack lasted just one minute. The attack was launched from approximately 15,000 bots running a variant of the original Mirai code on IoT devices and [unpatched GitLab instances](https://www.rapid7.com/blog/post/2021/11/01/gitlab-unauthenticated-remote-code-execution-cve-2021-22205-exploited-in-the-wild/).\n\n![Graph of a network-layer DDoS attack that peaked at almost 2 Tbps](https://blog.cloudflare.com/content/images/2022/01/image14.jpg)\n\n### Network-layer DDoS attacks by month\n\n**December was the busiest month for attackers in 2021.**\n\nQ4 ‘21 was the busiest quarter in 2021 for attackers. Over 43% of all network-layer DDoS attacks took place in the fourth quarter of 2021. While October was a relatively calmer month, in November, the month of the Chinese Singles' Day, the American Thanksgiving holiday, Black Friday, and Cyber Monday, the number of network-layer DDoS attacks nearly doubled. The number of observed attacks increased towards the final days of December ’21 as the world prepared to close out the year. In fact, the total number of attacks in December alone was higher than all the attacks in Q2 ’21 and almost equivalent to all attacks in Q1 ’21.\n\n![Graph of the distribution of network-layer DDoS attacks by month in 2021](https://blog.cloudflare.com/content/images/2022/01/image5.png)\n\n### Network-layer DDoS attacks by attack rate\n\n**While most attacks are still relatively ‘small’ in size, terabit-strong attacks are becoming the norm.**\n\nThere are different ways of measuring the size of an L3/4 DDoS attack. One is the volume of traffic it delivers, measured as the bit rate (specifically, terabits per second or gigabits per second). Another is the number of packets it delivers, measured as the packet rate (specifically, millions of packets per second).\n\nAttacks with high bit rates attempt to cause a denial-of-service event by clogging the Internet link, while attacks with high packet rates attempt to overwhelm the servers, routers, or other in-line hardware appliances. These devices dedicate a certain amount of memory and computation power to process each packet. Therefore, by bombarding it with many packets, the appliance can be left with no further processing resources. In such a case, packets are “dropped,” i.e., the appliance is unable to process them. For users, this results in service disruptions and denial of service.\n\nThe distribution of attacks by their size (in bit rate) and month is shown below. As seen in the graph above, the majority of attacks took place in December. However, the graph below illustrates that larger attacks, over 300 Gbps in size, took place in November. Most of the attacks between 5-20 Gbps took place in December.\n\n![Graph of the distribution of network-layer DDoS attacks by size by month in Q4](https://blog.cloudflare.com/content/images/2022/01/image10.png)\n\n**Distribution by packet rate**\n\nAn interesting correlation Cloudflare has observed is that when the number of attacks increases, their size and duration decrease. In the first two-thirds of 2021, the number of attacks was relatively small, and correspondingly, their rates increased, e.g., in Q3 ’21, attacks ranging from 1-10 million packets per second (mpps) increased by 196%. In Q4 ’21, the number of attacks increased and Cloudflare observed a decrease in the size of attacks. 91% of all attacks peaked below 50,000 packets per second (pps) — easily sufficient to take down unprotected Internet properties.\n\n![Graph of the distribution of network-layer DDoS attacks by packet rate in Q4](https://blog.cloudflare.com/content/images/2022/01/image4.png)\n\nLarger attacks of over 1 mpps decreased by 48% to 28% QoQ, while attacks peaking below 50K pps increased by 2.36% QoQ.\n\n![Graph of the change in the distribution of network-layer DDoS attacks by packet rate quarter over quarter](https://blog.cloudflare.com/content/images/2022/01/image19.png)\n\n### Distribution by bit rate\n\nSimilar to the trend observed in packet-intensive attacks, the amount of bit-intensive attacks shrunk as well. While attacks over 1 Tbps are becoming the norm, with the largest one we’ve ever seen peak just below 2 Tbps, the majority of attacks are still small and peaked below 500 Mbps (97.2%).\n\n![Graph of the distribution of network-layer DDoS attacks by bit rate in Q4](https://blog.cloudflare.com/content/images/2022/01/image17.png)\n\nIn Q4 ’21, larger attacks of all ranges above 500 Mbps saw massive decreases ranging from 35% to 57% for the larger 100+ Gbps attacks.\n\n![Graph of the change in the distribution of network-layer DDoS attacks by bit rate quarter over quarter](https://blog.cloudflare.com/content/images/2022/01/image1.png)\n\n### Network-layer DDoS attacks by duration\n\n**Most attacks remain under one hour in duration, reiterating the need for automated always-on DDoS mitigation solutions.**\n\nWe measure the duration of an attack by recording the difference between when it is first detected by our systems as an attack and the last packet we see with that attack signature towards that specific target. In the last quarter of 2021, 98% of all network-layer attacks lasted less than one hour. This is very common as most of the attacks are short-lived. Even more so, a trend we’ve seen is that when the number of attacks increases, as in this quarter, their rate and duration decreases.\n\n![Graph of the distribution of network-layer DDoS attacks by duration in Q4](https://blog.cloudflare.com/content/images/2022/01/image2.png)\n\nShort attacks can easily go undetected, especially burst attacks that, within seconds, bombard a target with a significant number of packets, bytes, or requests. In this case, DDoS protection services that rely on manual mitigation by security analysis have no chance in mitigating the attack in time. They can only learn from it in their post-attack analysis, then deploy a new rule that filters the attack fingerprint and hope to catch it next time. Similarly, using an “on-demand” service, where the security team will redirect traffic to a DDoS provider during the attack, is also inefficient because the attack will already be over before the traffic routes to the on-demand DDoS provider.\n\nIt’s recommended that companies use [automated, always-on DDoS protection services](https://www.cloudflare.com/ddos/) that analyze traffic and apply real-time fingerprinting fast enough to block short-lived attacks.\n\n## Attack vectors\n\n**SYN floods remain attackers’ favorite method of attack, while attacks over SNMP saw a massive surge of almost 5,800% QoQ.**\n\nAn attack vector is a term used to describe the method that the attacker uses to launch their DDoS attack, i.e., the IP protocol, packet attributes such as TCP flags, flooding method, and other criteria.\n\nFor the first time in 2021, the percentage of [SYN flood](https://www.cloudflare.com/en-gb/learning/ddos/syn-flood-ddos-attack/) attacks significantly decreased. Throughout 2021, SYN floods accounted for 54% of all network-layer attacks on average. While still grabbing first place as the most frequent vector, its share dropped by 38% QoQ to 34%.\n\nHowever, it was a close-run for SYN attacks and UDP attacks. A [UDP flood](https://www.cloudflare.com/learning/ddos/udp-flood-ddos-attack/) is a type of denial-of-service attack in which a large number of User Datagram Protocol (UDP) packets are sent to a targeted server with the aim of overwhelming that device’s ability to process and respond. Oftentimes, the firewall protecting the targeted server can also become exhausted as a result of UDP flooding, resulting in a denial-of-service to legitimate traffic. Attacks over UDP jumped from fourth place in Q3 ’21 to second place in Q4 ’21, with a share of 32% of all network-layer attacks — a 1,198% increase in QoQ.\n\nIn third place came the SNMP underdog that made a massive leap with its first time 2021 appearance in the top attack vectors.\n\n![Graph of the top network-layer DDoS attack vector in Q4](https://blog.cloudflare.com/content/images/2022/01/image7.png)\n\n## Emerging threats\n\nWhen we look at emerging attack vectors — which helps us understand what new vectors attackers are deploying to launch attacks — we observe a massive spike in SNMP, MSSQL, and generic UDP-based DDoS attacks.\n\nBoth SNMP and MSSQL attacks are used to reflect and amplify traffic on the target by spoofing the target’s IP address as the source IP in the packets used to trigger the attack.\n\nSimple Network Management Protocol (SNMP) is a UDP-based protocol that is often used to discover and manage network devices such as printers, switches, routers, and firewalls of a home or enterprise network on UDP well-known port 161. In an SNMP reflection attack, the attacker sends out a large number of SNMP queries while spoofing the source IP address in the packet as the targets to devices on the network that, in turn, reply to that target’s address. Numerous responses from the devices on the network results in the target network being DDoSed.\n\nSimilar to the SNMP amplification attack, the Microsoft SQL (MSSQL) attack is based on a technique that abuses the Microsoft SQL Server Resolution Protocol for the purpose of launching a reflection-based DDoS attack. The attack occurs when a [Microsoft SQL Server](https://en.wikipedia.org/wiki/Microsoft_SQL_Server) responds to a client query or request, attempting to exploit the Microsoft SQL Server Resolution Protocol (MC-SQLR), listening on UDP port 1434.\n\n![Graph of the top emerging network-layer DDoS attack threats Q4, 2021](https://blog.cloudflare.com/content/images/2022/01/unnamed-6.png)\n\n### Network-layer DDoS attacks by country\n\n**Attacks originating from Moldova quadrupled, making it the country with the highest percentage of network-layer DDoS activity.**\n\nWhen analyzing network-layer DDoS attacks, we bucket the traffic by the Cloudflare edge data center locations where the traffic was ingested, and not by the source IP. The reason for this is that, when attackers launch network-layer attacks, they can [spoof](https://www.cloudflare.com/learning/ddos/glossary/ip-spoofing/) the source IP address in order to obfuscate the attack source and introduce randomness into the attack properties, which can make it harder for simple DDoS protection systems to block the attack. Hence, if we were to derive the source country based on a spoofed source IP, we would get a spoofed country.\n\nCloudflare is able to overcome the challenges of spoofed IPs by displaying the attack data by the location of the Cloudflare data center in which the attack was observed. We are able to achieve geographical accuracy in our report because we have data centers in [over 250 cities](https://www.cloudflare.com/network) around the world.\n\n![Graph of the distribution of network-layer DDoS attacks by source country in Q4, 2021](https://blog.cloudflare.com/content/images/2022/01/image6.png)\n\n![Graph of the distribution of network-layer DDoS attacks by source country in Q4. 2021.](https://blog.cloudflare.com/content/images/2022/01/image16.png)\n\nTo view all regions and countries, check out the [interactive map](https://radar.cloudflare.com/notebooks/ddos-2021-q4#network-layer-ddos-attacks-by-country).\n\n## Summary\n\nCloudflare’s mission is to help build a better Internet. A better Internet is one that is more secure, faster, and reliable for everyone — even in the face of DDoS attacks. As part of our mission, since 2017, we’ve been providing [unmetered and unlimited DDoS protection](https://blog.cloudflare.com/unmetered-mitigation/) for free to all of our customers. Over the years, it has become increasingly easier for attackers to launch DDoS attacks. To counter the attacker’s advantage, we want to make sure that it is also easy and free for organizations of all sizes to protect themselves against DDoS attacks of all types.\n\nNot using Cloudflare yet? [Start now](https://dash.cloudflare.com/sign-up).\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[DDoS](https://blog.cloudflare.com/tag/ddos/) [Attacks](https://blog.cloudflare.com/tag/attacks/) [Trends](https://blog.cloudflare.com/tag/trends/) [Cloudflare Radar](https://blog.cloudflare.com/tag/cloudflare-radar/) [RDDoS](https://blog.cloudflare.com/tag/rddos/)"
    },
    {
      "url": "https://blog.cloudflare.com/birthday-week-2023-wrap-up/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/birthday-week-2023-wrap-up/",
        "loadedTime": "2023-12-05T02:32:27.725Z",
        "referrerUrl": "https://blog.cloudflare.com/page/2/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/birthday-week-2023-wrap-up/",
        "title": "Birthday Week recap: everything we announced — plus an AI-powered opportunity for startups",
        "description": "Need a recap or refresher on all the big Birthday Week news this week? This recap has you covered",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "10/02/2023\n8 min read\nThis post is also available in 简体中文, 日本語, 한국어, Deutsch, Français, Español.\nThis year, Cloudflare officially became a teenager, turning 13 years old. We celebrated this milestone with a series of announcements that benefit both our customers and the Internet community. \nFrom developing applications in the age of AI to securing against the most advanced attacks that are yet to come, Cloudflare is proud to provide the tools that help our customers stay one step ahead. \nWe hope you’ve had a great time following along and for anyone looking for a recap of everything we launched this week, here it is: \nMonday\nWhat\tIn a sentence…\t\nSwitching to Cloudflare can cut emissions by up to 96%\n\t\nSwitching enterprise network services from on-prem to Cloudflare can cut related carbon emissions by up to 96%. \n\t\nCloudflare Trace\n\t\nUse Cloudflare Trace to see which rules and settings are invoked when an HTTP request for your site goes through our network. \n\t\nCloudflare Fonts\n\t\nIntroducing Cloudflare Fonts. Enhance privacy and performance for websites using Google Fonts by loading fonts directly from the Cloudflare network. \n\t\nHow Cloudflare intelligently routes traffic\n\t\nTechnical deep dive that explains how Cloudflare uses machine learning to intelligently route traffic through our vast network. \n\t\nLow Latency Live Streaming\n\t\nCloudflare Stream’s LL-HLS support is now in open beta. You can deliver video to your audience faster, reducing the latency a viewer may experience on their player to as little as 3 seconds. \n\t\nAccount permissions for all\n\t\nCloudflare account permissions are now available to all customers, not just Enterprise. In addition, we’ll show you how you can use them and best practices. \n\t\nIncident Alerts\n\t\nCustomers can subscribe to Cloudflare Incident Alerts and choose when to get notified based on affected products and level of impact. \n\t\nTuesday\nWhat\tIn a sentence…\t\nWelcome to the connectivity cloud\n\t\nCloudflare is the world’s first connectivity cloud — the modern way to connect and protect your cloud, networks, applications and users. \n\t\nAmazon’s $2bn IPv4 tax — and how you can avoid paying it \n\t\nAmazon will begin taxing their customers $43 for IPv4 addresses, so Cloudflare will give those $43 back in the form of credits to bypass that tax. \n\t\nSippy\n\n\t\nMinimize egress fees by using Sippy to incrementally migrate your data from AWS to R2. \n\t\nCloudflare Images\n\t\nAll Image Resizing features will be available under Cloudflare Images and we’re simplifying pricing to make it more predictable and reliable. \n\t\nTraffic anomalies and notifications with Cloudflare Radar\n\t\nCloudflare Radar will be publishing anomalous traffic events for countries and Autonomous Systems (ASes).\n\t\nDetecting Internet outages\n\t\nDeep dive into how Cloudflare detects Internet outages, the challenges that come with it, and our approach to overcome these problems. \n\t\nWednesday\nWhat\tIn a sentence…\t\nThe best place on Region: Earth for inference\n\t\nNow available: Workers AI, a serverless GPU cloud for AI, Vectorize so you can build your own vector databases, and AI Gateway to help manage costs and observability of your AI applications. \nCloudflare delivers the best infrastructure for next-gen AI applications, supported by partnerships with NVIDIA, Microsoft, Hugging Face, Databricks, and Meta.\n\t\nWorkers AI \n\t\nLaunching Workers AI — AI inference as a service platform, empowering developers to run AI models with just a few lines of code, all powered by our global network of GPUs. \n\t\nPartnering with Hugging Face \n\t\nCloudflare is partnering with Hugging Face to make AI models more accessible and affordable to users. \n\t\nVectorize\n\t\nCloudflare’s vector database, designed to allow engineers to build full-stack, AI-powered applications entirely on Cloudflare's global network — available in Beta. \n\t\nAI Gateway\n\t\nAI Gateway helps developers have greater control and visibility in their AI apps, so that you can focus on building without worrying about observability, reliability, and scaling. AI Gateway handles the things that nearly all AI applications need, saving you engineering time so you can focus on what you're building.\n\n\t\nYou can now use WebGPU in Cloudflare Workers\n\t\nDevelopers can now use WebGPU in Cloudflare Workers. Learn more about why WebGPUs are important, why we’re offering them to customers, and what’s next. \n\t\nWhat AI companies are building with Cloudflare\n\t\nMany AI companies are using Cloudflare to build next generation applications. Learn more about what they’re building and how Cloudflare is helping them on their journey. \n\t\nWriting poems using LLama 2 on Workers AI\n\t\nWant to write a poem using AI? Learn how to run your own AI chatbot in 14 lines of code, running on Cloudflare’s global network. \n\t\nThursday\nWhat\tIn a sentence…\t\nHyperdrive\n\t\nCloudflare launches a new product, Hyperdrive, that makes existing regional databases much faster by dramatically speeding up queries that are made from Cloudflare Workers.\n\t\nD1 Open Beta\n\t\nD1 is now in open beta, and the theme is “scale”: with higher per-database storage limits and the ability to create more databases, we’re unlocking the ability for developers to build production-scale applications on D1.\n\t\nPages Build Caching\n\t\nBuild cache is a feature designed to reduce your build times by caching and reusing previously computed project components — now available in Beta. \n\t\nRunning serverless Puppeteer with Workers and Durable Objects\n\t\nIntroducing the Browser Rendering API, which enables developers to utilize the Puppeteer browser automation library within Workers, eliminating the need for serverless browser automation system setup and maintenance\n\t\nCloudflare partners with Microsoft to power their Edge Secure Network\n\t\nWe partnered with Microsoft Edge to provide a fast and secure VPN, right in the browser. Users don’t have to install anything new or understand complex concepts to get the latest in network-level privacy: Edge Secure Network VPN is available on the latest consumer version of Microsoft Edge in most markets, and automatically comes with 5GB of data. \n\t\nRe-introducing the Cloudflare Workers playground\n\t\nWe are revamping the playground that demonstrates the power of Workers, along with new development tooling, and the ability to share your playground code and deploy instantly to Cloudflare’s global network\n\t\nCloudflare integrations marketplace expands\n\t\nIntroducing the newest additions to Cloudflare’s Integration Marketplace. Now available: Sentry, Momento and Turso. \n\t\nA Socket API that works across Javascript runtimes — announcing WinterCG spec and polyfill for connect()\n\t\nEngineers from Cloudflare and Vercel have published a draft specification of the connect() sockets API for review by the community, along with a Node.js compatible polyfill for the connect() API that developers can start using.\n\t\nNew Workers pricing\n\t\nAnnouncing new pricing for Cloudflare Workers, where you are billed based on CPU time, and never for the idle time that your Worker spends waiting on network requests and other I/O.\n\t\nFriday\nWhat\tIn a sentence…\t\nPost Quantum Cryptography goes GA \n\t\nCloudflare is rolling out post-quantum cryptography support to customers, services, and internal systems to proactively protect against advanced attacks. \n\t\nEncrypted Client Hello\n\t\nAnnouncing a contribution that helps improve privacy for everyone on the Internet. Encrypted Client Hello, a new standard that prevents networks from snooping on which websites a user is visiting, is now available on all Cloudflare plans. \n\t\nEmail Retro Scan \n\t\nCloudflare customers can now scan messages within their Office 365 Inboxes for threats. The Retro Scan will let you look back seven days to see what threats your current email security tool has missed. \n\t\nTurnstile is Generally Available\n\t\nTurnstile, Cloudflare’s CAPTCHA replacement, is now generally available and available for free to everyone and includes unlimited use. \n\t\nAI crawler bots\n\t\nAny Cloudflare user, on any plan, can choose specific categories of bots that they want to allow or block, including AI crawlers. We are also recommending a new standard to robots.txt that will make it easier for websites to clearly direct how AI bots can and can’t crawl.\n\n\t\nDetecting zero-days before zero-day\n\t\nDeep dive into Cloudflare’s approach and ongoing research into detecting novel web attack vectors in our WAF before they are seen by a security researcher. \n\t\nPrivacy Preserving Metrics\n\t\nDeep dive into the fundamental concepts behind the Distributed Aggregation Protocol (DAP) protocol with examples on how we’ve implemented it into Daphne, our open source aggregator server. \n\t\nPost-quantum cryptography to origin\n\t\nWe are rolling out post-quantum cryptography support for outbound connections to origins and Cloudflare Workers fetch() calls. Learn more about what we enabled, how we rolled it out in a safe manner, and how you can add support to your origin server today. \n\t\nNetwork performance update\n\t\nCloudflare’s updated benchmark results regarding network performance plus a dive into the tools and processes that we use to monitor and improve our network performance. \n\t\nOne More Thing\nWhen Cloudflare turned 12 last year, we announced the Workers Launchpad Funding Program - you can think of it like a startup accelerator program for companies building on Cloudlare’s Developer Platform, with no restrictions on your size, stage, or geography.\nA refresher on how the Launchpad works: Each quarter, we admit a group of startups who then get access to a wide range of technical advice, mentorship, and fundraising opportunities. That includes our Founders Bootcamp, Open Office Hours with our Solution Architects, and Demo Day. Those who are ready to fundraise will also be connected to our community of 40+ leading global Venture Capital firms. \nIn exchange, we just ask for your honest feedback. We want to know what works, what doesn’t and what you need us to build for you. We don’t ask for a stake in your company, and we don’t ask you to pay to be a part of the program.\nOver the past year, we’ve received applications from nearly 60 different countries. We’ve had a chance to work closely with 50 amazing early and growth-stage startups admitted into the first two cohorts, and have grown our VC partner community to 40+ firms and more than $2 billion in potential investments in startups building on Cloudflare.\nNext up: Cohort #3! Between recently wrapping up Cohort #2 (check out their Demo Day!), celebrating the Launchpad’s 1st birthday, and the heaps of announcements we made last week, we thought that everyone could use a little extra time to catch up on all the news - which is why we are extending the deadline for Cohort #3 a few weeks to October 13, 2023. AND we’re reserving 5 spots in the class for those who are already using any of last Wednesday’s AI announcements. Just be sure to mention what you’re using in your application.\nSo once you’ve had a chance to check out the announcements and pour yourself a cup of coffee, check out the Workers Launchpad. Applying is a breeze — you’ll be done long before your coffee gets cold.\nUntil next time\nThat’s all for Birthday Week 2023. We hope you enjoyed the ride, and we’ll see you at our next innovation week!\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nBirthday Week Product News AI Turnstile CAPTCHA \nRelated Posts\nSeptember 28, 2018 8:40PM\nBirthday Week Wrap-Up: Every day is launch day at Cloudflare\nOur customers are accustomed to us launching new services, features, and functionality at a feverish pace, but recently, we’ve been especially active. This week we celebrated our 8th Birthday Week by announcing new offerings that benefit our customers and the global Internet community....\nBy \nJanuary 07, 2022 3:57PM\nCloudflare Innovation Weeks 2021\nAs we start planning our 2022 Innovation Weeks, we are reflecting back on the highlights from each of these weeks...\nBy \nSeptember 27, 2019 8:00PM\nBirthday Week 2019 Wrap-up\nThis week we celebrated Cloudflare’s 9th birthday by launching a variety of new offerings that support our mission: to help build a better Internet. Below is a summary recap of how we celebrated Birthday Week 2019....\nBy \nSeptember 28, 2023 2:00PM\nCloudflare Integrations Marketplace introduces three new partners: Sentry, Momento and Turso\nEarlier this year, we introduced integrations with Supabase, PlanetScale, Neon and Upstash. Today, we are thrilled to introduce our newest additions to Cloudflare’s Integrations Marketplace – Sentry, Turso and Momento...\nBy",
      "markdown": "10/02/2023\n\n*   [![Dina Kozlov](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2019/06/headshot.jpg)](https://blog.cloudflare.com/author/dina/)\n*   [![Mia Wang](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/09/MW-headshot.JPG)](https://blog.cloudflare.com/author/mia-wang/)\n\n8 min read\n\nThis post is also available in [简体中文](https://blog.cloudflare.com/zh-cn/birthday-week-2023-wrap-up-zh-cn/), [日本語](https://blog.cloudflare.com/ja-jp/birthday-week-2023-wrap-up-ja-jp/), [한국어](https://blog.cloudflare.com/ko-kr/birthday-week-2023-wrap-up-ko-kr/), [Deutsch](https://blog.cloudflare.com/de-de/birthday-week-2023-wrap-up-de-de/), [Français](https://blog.cloudflare.com/fr-fr/birthday-week-2023-wrap-up-fr-fr/), [Español](https://blog.cloudflare.com/es-es/birthday-week-2023-wrap-up-es-es/).\n\n![Birthday Week recap: everything we announced — plus an AI-powered opportunity for startups](https://blog.cloudflare.com/content/images/2023/09/image1-45.png)\n\nThis year, Cloudflare officially became a teenager, turning 13 years old. We celebrated this milestone with a series of announcements that benefit both our customers and the Internet community.\n\nFrom developing applications in the age of AI to securing against the most advanced attacks that are yet to come, Cloudflare is proud to provide the tools that help our customers stay one step ahead.\n\nWe hope you’ve had a great time following along and for anyone looking for a recap of everything we launched this week, here it is:\n\n### Monday\n\n|     |     |\n| --- | --- |\n| What | In a sentence… |\n| [Switching to Cloudflare can cut emissions by up to 96%](https://blog.cloudflare.com/switching-cloudflare-cut-your-network-carbon-emissions-sbti/) | Switching enterprise network services from on-prem to Cloudflare can cut related carbon emissions by up to 96%. |\n| [Cloudflare Trace](https://blog.cloudflare.com/traffic-transparency-unleashing-cloudflare-trace/) | Use Cloudflare Trace to see which rules and settings are invoked when an HTTP request for your site goes through our network. |\n| [Cloudflare Fonts](https://blog.cloudflare.com/cloudflare-fonts-enhancing-website-privacy-speed/) | Introducing Cloudflare Fonts. Enhance privacy and performance for websites using Google Fonts by loading fonts directly from the Cloudflare network. |\n| [How Cloudflare intelligently routes traffic](https://blog.cloudflare.com/meet-traffic-manager/) | Technical deep dive that explains how Cloudflare uses machine learning to intelligently route traffic through our vast network. |\n| [Low Latency Live Streaming](https://blog.cloudflare.com/cloudflare-stream-low-latency-hls-open-beta/) | Cloudflare Stream’s LL-HLS support is now in open beta. You can deliver video to your audience faster, reducing the latency a viewer may experience on their player to as little as 3 seconds. |\n| [Account permissions for all](https://blog.cloudflare.com/permissions-best-practices/) | Cloudflare account permissions are now available to all customers, not just Enterprise. In addition, we’ll show you how you can use them and best practices. |\n| [Incident Alerts](https://blog.cloudflare.com/incident-alerts/) | Customers can subscribe to Cloudflare Incident Alerts and choose when to get notified based on affected products and level of impact. |\n\n### Tuesday\n\n|     |     |\n| --- | --- |\n| What | In a sentence… |\n| [Welcome to the connectivity cloud](https://blog.cloudflare.com/welcome-to-connectivity-cloud/) | Cloudflare is the world’s first connectivity cloud — the modern way to connect and protect your cloud, networks, applications and users. |\n| [Amazon’s $2bn IPv4 tax — and how you can avoid paying it](https://blog.cloudflare.com/amazon-2bn-ipv4-tax-how-avoid-paying/) | Amazon will begin taxing their customers $43 for IPv4 addresses, so Cloudflare will give those $43 back in the form of credits to bypass that tax. |\n| [Sippy](https://blog.cloudflare.com/sippy-incremental-migration-s3-r2/) | Minimize egress fees by using Sippy to incrementally migrate your data from AWS to R2. |\n| [Cloudflare Images](https://blog.cloudflare.com/merging-images-and-image-resizing/) | All Image Resizing features will be available under Cloudflare Images and we’re simplifying pricing to make it more predictable and reliable. |\n| [Traffic anomalies and notifications with Cloudflare Radar](https://blog.cloudflare.com/traffic-anomalies-notifications-radar/) | Cloudflare Radar will be publishing anomalous traffic events for countries and Autonomous Systems (ASes). |\n| [Detecting Internet outages](https://blog.cloudflare.com/detecting-internet-outages/) | Deep dive into how Cloudflare detects Internet outages, the challenges that come with it, and our approach to overcome these problems. |\n\n### Wednesday\n\n|     |     |\n| --- | --- |\n| What | In a sentence… |\n| [The best place on Region: Earth for inference](https://blog.cloudflare.com/best-place-region-earth-inference/) | Now available: Workers AI, a serverless GPU cloud for AI, Vectorize so you can build your own vector databases, and AI Gateway to help manage costs and observability of your AI applications. <br><br>Cloudflare delivers the best infrastructure for next-gen AI applications, supported by partnerships with NVIDIA, Microsoft, Hugging Face, Databricks, and Meta. |\n| [Workers AI](https://blog.cloudflare.com/workers-ai/) | Launching Workers AI — AI inference as a service platform, empowering developers to run AI models with just a few lines of code, all powered by our global network of GPUs. |\n| [Partnering with Hugging Face](https://blog.cloudflare.com/partnering-with-hugging-face-deploying-ai-easier-affordable/) | Cloudflare is partnering with Hugging Face to make AI models more accessible and affordable to users. |\n| [Vectorize](https://blog.cloudflare.com/vectorize-vector-database-open-beta/) | Cloudflare’s vector database, designed to allow engineers to build full-stack, AI-powered applications entirely on Cloudflare's global network — available in Beta. |\n| [AI Gateway](https://blog.cloudflare.com/announcing-ai-gateway/) | AI Gateway helps developers have greater control and visibility in their AI apps, so that you can focus on building without worrying about observability, reliability, and scaling. AI Gateway handles the things that nearly all AI applications need, saving you engineering time so you can focus on what you're building. |\n| [You can now use WebGPU in Cloudflare Workers](https://blog.cloudflare.com/webgpu-in-workers/) | Developers can now use WebGPU in Cloudflare Workers. Learn more about why WebGPUs are important, why we’re offering them to customers, and what’s next. |\n| [What AI companies are building with Cloudflare](https://blog.cloudflare.com/ai-companies-building-cloudflare/) | Many AI companies are using Cloudflare to build next generation applications. Learn more about what they’re building and how Cloudflare is helping them on their journey. |\n| [Writing poems using LLama 2 on Workers AI](https://blog.cloudflare.com/writing-poems-using-llama-2-on-workers-ai/) | Want to write a poem using AI? Learn how to run your own AI chatbot in 14 lines of code, running on Cloudflare’s global network. |\n\n### Thursday\n\n|     |     |\n| --- | --- |\n| What | In a sentence… |\n| [Hyperdrive](https://blog.cloudflare.com/hyperdrive-making-regional-databases-feel-distributed/) | Cloudflare launches a new product, Hyperdrive, that makes existing regional databases much faster by dramatically speeding up queries that are made from Cloudflare Workers. |\n| [D1 Open Beta](https://blog.cloudflare.com/d1-open-beta-is-here/) | D1 is now in open beta, and the theme is “scale”: with higher per-database storage limits and the ability to create more databases, we’re unlocking the ability for developers to build production-scale applications on D1. |\n| [Pages Build Caching](https://blog.cloudflare.com/race-ahead-with-build-caching/) | Build cache is a feature designed to reduce your build times by caching and reusing previously computed project components — now available in Beta. |\n| [Running serverless Puppeteer with Workers and Durable Objects](https://blog.cloudflare.com/running-serverless-puppeteer-workers-durable-objects/) | Introducing the Browser Rendering API, which enables developers to utilize the Puppeteer browser automation library within Workers, eliminating the need for serverless browser automation system setup and maintenance |\n| [Cloudflare partners with Microsoft to power their Edge Secure Network](https://blog.cloudflare.com/cloudflare-now-powering-microsoft-edge-secure-network/) | We partnered with Microsoft Edge to provide a fast and secure VPN, right in the browser. Users don’t have to install anything new or understand complex concepts to get the latest in network-level privacy: Edge Secure Network VPN is available on the latest consumer version of Microsoft Edge in most markets, and automatically comes with 5GB of data. |\n| [Re-introducing the Cloudflare Workers playground](https://blog.cloudflare.com/workers-playground/) | We are revamping the playground that demonstrates the power of Workers, along with new development tooling, and the ability to share your playground code and deploy instantly to Cloudflare’s global network |\n| [Cloudflare integrations marketplace expands](https://blog.cloudflare.com/cloudflare-integrations-marketplace-new-partners-sentry-momento-turso/) | Introducing the newest additions to Cloudflare’s Integration Marketplace. Now available: Sentry, Momento and Turso. |\n| [A Socket API that works across Javascript runtimes — announcing WinterCG spec and polyfill for connect()](https://blog.cloudflare.com/socket-api-works-javascript-runtimes-wintercg-polyfill-connect/) | Engineers from Cloudflare and Vercel have published a draft specification of the connect() sockets API for review by the community, along with a Node.js compatible polyfill for the connect() API that developers can start using. |\n| [New Workers pricing](https://blog.cloudflare.com/workers-pricing-scale-to-zero/) | Announcing new pricing for Cloudflare Workers, where you are billed based on CPU time, and never for the idle time that your Worker spends waiting on network requests and other I/O. |\n\n### Friday\n\n|     |     |\n| --- | --- |\n| What | In a sentence… |\n| [Post Quantum Cryptography goes GA](https://blog.cloudflare.com/post-quantum-cryptography-ga/) | Cloudflare is rolling out post-quantum cryptography support to customers, services, and internal systems to proactively protect against advanced attacks. |\n| [Encrypted Client Hello](https://blog.cloudflare.com/announcing-encrypted-client-hello/) | Announcing a contribution that helps improve privacy for everyone on the Internet. Encrypted Client Hello, a new standard that prevents networks from snooping on which websites a user is visiting, is now available on all Cloudflare plans. |\n| [Email Retro Scan](https://blog.cloudflare.com/threats-lurking-office-365-cloudflare-email-retro-scan/) | Cloudflare customers can now scan messages within their Office 365 Inboxes for threats. The Retro Scan will let you look back seven days to see what threats your current email security tool has missed. |\n| [Turnstile is Generally Available](https://blog.cloudflare.com/turnstile-ga/) | Turnstile, Cloudflare’s CAPTCHA replacement, is now generally available and available for free to everyone and includes unlimited use. |\n| [AI crawler bots](https://blog.cloudflare.com/ai-bots/) | Any Cloudflare user, on any plan, can choose specific categories of bots that they want to allow or block, including AI crawlers. We are also recommending a new standard to robots.txt that will make it easier for websites to clearly direct how AI bots can and can’t crawl. |\n| [Detecting zero-days before zero-day](https://blog.cloudflare.com/detecting-zero-days-before-zero-day/) | Deep dive into Cloudflare’s approach and ongoing research into detecting novel web attack vectors in our WAF before they are seen by a security researcher. |\n| [Privacy Preserving Metrics](https://blog.cloudflare.com/deep-dive-privacy-preserving-measurement/) | Deep dive into the fundamental concepts behind the Distributed Aggregation Protocol (DAP) protocol with examples on how we’ve implemented it into Daphne, our open source aggregator server. |\n| [Post-quantum cryptography to origin](https://blog.cloudflare.com/post-quantum-to-origins/) | We are rolling out post-quantum cryptography support for outbound connections to origins and Cloudflare Workers fetch() calls. Learn more about what we enabled, how we rolled it out in a safe manner, and how you can add support to your origin server today. |\n| [Network performance update](https://blog.cloudflare.com/network-performance-update-birthday-week-2023/) | Cloudflare’s updated benchmark results regarding network performance plus a dive into the tools and processes that we use to monitor and improve our network performance. |\n\n### One More Thing\n\n![](https://blog.cloudflare.com/content/images/2023/09/image1-51.png)\n\nWhen Cloudflare turned 12 last year, we announced the [Workers Launchpad Funding Program](https://www.cloudflare.com/lp/workers-launchpad/) - you can think of it like a startup accelerator program for companies building on Cloudlare’s Developer Platform, with no restrictions on your size, stage, or geography.\n\n**A refresher on how the Launchpad works:** Each quarter, we admit a group of startups who then get access to a wide range of technical advice, mentorship, and fundraising opportunities. That includes our Founders Bootcamp, Open Office Hours with our Solution Architects, and Demo Day. Those who are ready to fundraise will also be connected to our community of 40+ leading global Venture Capital firms.\n\nIn exchange, we just ask for your honest feedback. We want to know what works, what doesn’t and what you need us to build for you. We don’t ask for a stake in your company, and we don’t ask you to pay to be a part of the program.\n\nOver the past year, we’ve received applications from nearly 60 different countries. We’ve had a chance to work closely with 50 amazing early and growth-stage startups admitted into the first two cohorts, and have grown our VC partner community to 40+ firms and more than $2 billion in potential investments in startups building on Cloudflare.\n\n**Next up: Cohort #3!** Between recently wrapping up Cohort #2 (check out their [Demo Day](https://cloudflare.tv/shows/workers-launchpad-demo-day/workers-launchpad-demo-day-cohort-2/3vVqLOgq)!), celebrating the Launchpad’s 1st birthday, and the heaps of announcements we made last week, we thought that everyone could use a little extra time to catch up on all the news - which is why we are extending the deadline for Cohort #3 a few weeks to **October 13, 2023. AND** we’re **reserving 5 spots in the class for those who are already using any of last Wednesday’s AI announcements.** Just be sure to mention what you’re using in your application.\n\nSo once you’ve had a chance to check out the announcements and pour yourself a cup of coffee, check out the [**Workers Launchpad**](https://www.cloudflare.com/lp/workers-launchpad/). Applying is a breeze — you’ll be done long before your coffee gets cold.\n\n### Until next time\n\nThat’s all for Birthday Week 2023. We hope you enjoyed the ride, and we’ll see you at our next innovation week!\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [Product News](https://blog.cloudflare.com/tag/product-news/) [AI](https://blog.cloudflare.com/tag/ai/) [Turnstile](https://blog.cloudflare.com/tag/turnstile/) [CAPTCHA](https://blog.cloudflare.com/tag/captcha/)\n\nRelated Posts\n\nSeptember 28, 2018 8:40PM\n\n[\n\n## Birthday Week Wrap-Up: Every day is launch day at Cloudflare\n\n](https://blog.cloudflare.com/birthday-week-2018-wrap-up/)\n\nOur customers are accustomed to us launching new services, features, and functionality at a feverish pace, but recently, we’ve been especially active. This week we celebrated our 8th Birthday Week by announcing new offerings that benefit our customers and the global Internet community....\n\nBy \n\nJanuary 07, 2022 3:57PM\n\n[\n\n## Cloudflare Innovation Weeks 2021\n\n](https://blog.cloudflare.com/2021-innovations-weeks/)\n\nAs we start planning our 2022 Innovation Weeks, we are reflecting back on the highlights from each of these weeks...\n\nBy \n\nSeptember 27, 2019 8:00PM\n\n[\n\n## Birthday Week 2019 Wrap-up\n\n](https://blog.cloudflare.com/birthday-week-2019-wrap-up/)\n\nThis week we celebrated Cloudflare’s 9th birthday by launching a variety of new offerings that support our mission: to help build a better Internet. Below is a summary recap of how we celebrated Birthday Week 2019....\n\nBy \n\nSeptember 28, 2023 2:00PM\n\n[\n\n## Cloudflare Integrations Marketplace introduces three new partners: Sentry, Momento and Turso\n\n](https://blog.cloudflare.com/cloudflare-integrations-marketplace-new-partners-sentry-momento-turso/)\n\nEarlier this year, we introduced integrations with Supabase, PlanetScale, Neon and Upstash. Today, we are thrilled to introduce our newest additions to Cloudflare’s Integrations Marketplace – Sentry, Turso and Momento...\n\nBy"
    },
    {
      "url": "https://blog.cloudflare.com/meris-botnet/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/meris-botnet/",
        "loadedTime": "2023-12-05T02:32:45.771Z",
        "referrerUrl": "https://blog.cloudflare.com/cyber-week-analyzing-internet-traffic-and-e-commerce-trends/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/meris-botnet/",
        "title": "A Brief History of the Meris Botnet",
        "description": "Over the past months, we’ve been tracking and analyzing the activity of the Meris botnet.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/09/2021\n8 min read\nMeris first got our attention due to an exceptionally large 17.2 million requests per second (rps) DDoS attack that it launched against one of our customers. This attack, along with subsequent attacks originated by the Meris botnet, was automatically detected and mitigated by our DDoS protection systems. Cloudflare customers, even ones on the free plan, are protected against Meris attacks.\nOver the past months, we’ve been tracking and analyzing the activity of the Meris botnet. Some main highlights include:\nMeris targets approximately 50 different websites every single day with a daily average of 104 unique DDoS attacks.\nMore than 33% of all Meris DDoS attack traffic targeted China-based websites.\nMore than 12% of all websites that were attacked by Meris are operated by US-based companies.\nView more Meris attack insights and trends in the interactive Radar dashboard.\nSo what is Meris?\nMeris (Latvian for plague) is the name of an active botnet behind a series of recent DDoS attacks that have targeted thousands of websites around the world. It was originally detected in late June 2021 by QRator in joint research they conducted with Yandex. Their initial research identified 30,000 to 56,000 bots, but they estimated that the numbers are actually much higher, in the ballpark of 250,000 bots.\nThe Meris botnet is formed of infected routers and networking hardware manufactured by the Latvian company MikroTik. According to MikroTik’s blog, the attackers exploited a vulnerability in the router’s operating system (RouterOS) which enabled attackers to gain unauthenticated remote access to read and write arbitrary files (CVE-2018-14847).\nRouterOS is the router operating system that’s used by MikroTik’s routers and the RouterBOARD hardware product family, which can also be used to turn any PC into a router. Administration of RouterOS can be done either via direct SSH connection or by using a configuration utility called WinBox. The vulnerability itself was possible due to a directory traversal vulnerability in the WinBox interface with RouterOS.\nDirectory traversal is a type of exploit that allows attackers to travel to the parent directories to gain access to the operating system’s file system, a method and structure of how data is stored and retrieved in the operating system. Once they gain access to the file system, attackers can then read the existing files that administer the router and write files directly into the file system to administer the routers to their botnet needs.\nWhile the vulnerability was patched after its detection back in 2018, it’s still being exploited in compromised devices that do not use the patched RouterOS versions, or that use the default usernames and passwords. MicroTik has advised its customers to upgrade their devices’ OS version, to only allow access to the devices via secure IPsec, and to inspect for any abnormalities such as unknown SOCKS proxy settings and scripts.\nTo launch volumetric attacks, the botnet uses HTTP pipelining which allows it to send multiple requests over a single connection, thus increasing its total attack throughput. Furthermore, in an attempt to obfuscate the attack source, the botnet uses open SOCKS proxies to proxy their attack traffic to the target.\nCloudflare’s DDoS protection systems automatically detect and mitigate Meris attacks. One of the mitigation actions that the system can choose to use is the ‘Connection Close’ action which eliminates the risk of HTTP pipelining and helps slow down attackers. Additionally, as part of Cloudflare’s threat intelligence suite, we provide a Managed IP List of Open SOCKS Proxies that customers can use as part of their firewall rules — to block, challenge or rate-limit traffic that arrives via SOCKS proxies.\nHow does Meris compare to Mirai?\nAbout five years ago, Mirai (Japanese for future) — the infamous botnet that infected hundreds of thousands of IoT devices — launched record-breaking DDoS attacks against websites.\nThere have been many variants of the Mirai botnet since its source code was leaked. One version of Mirai, called Moobot, was detected last year when it attacked a Cloudflare customer with a 654 Gbps DDoS attack. Another variant recently made a resurgence when it targeted Cloudflare customers with over a dozen UDP and TCP based DDoS attacks that peaked multiple times above 1 Tbps, with a max peak of approximately 1.2 Tbps.\nWhile Mirai infected IoT devices with low computational power, Meris is a swarm of routers that have significantly higher processing power and data transfer capabilities than IoT devices, making them much more potent in causing harm at a larger scale to web properties that are not protected by sophisticated cloud-based DDoS mitigation.\nTracking the Meris botnet attacks\nSince the appearance of Meris, Cloudflare’s systems automatically detected and mitigated Meris attacks using the existing mitigation rules. During our analysis of the Meris botnet attacks, our security experts noticed the attack vectors adapt to try and bypass Cloudflare’s defenses. Needless to say, they were not successful. But we wanted to stay many steps ahead of attackers — and so our engineers deployed additional rules that mitigate Meris attacks even more comprehensively. A side effect of these mitigation rules is that it also provides us with more granular threat intelligence on the Meris attacks.\nSince we deployed the new rules in early August, we’ve seen Meris launch an average of 104 DDoS attacks on Cloudflare customers every day. The highest figure we’ve seen was on September 6, when Meris was used to launch 261 unique attacks against Cloudflare customers.\nView the interactive graph on Cloudflare Radar.\nDuring that same day, on September 6, attacks from Meris accounted for a record-breaking 17.5% of all L7 DDoS attacks that Cloudflare observed.\nView the interactive graph on Cloudflare Radar.\nOverall, Meris targets about 50 different websites and applications every single day. Although the average attack peaked at 106K rps, the median attack size was actually smaller at 17.6K rps. The largest attack we’ve seen was 17.2M rps and that occurred in July. In the graph below, you can see the daily highest requests per second rate after we deployed the new rules. Since then, the largest attack we’ve seen was 16.7M rps, which took place on August 19.\nMeris used to target Banks, Financial Services, and Insurance companies\nOver the past few months, the industry that received the most attack traffic from the Meris botnet was the Banking, Financial Services, and Insurance (BFSI) industry\nView the interactive graph on Cloudflare Radar.\nFollowing the BFSI industry, the most attacked industries were the Publishing, Gaming/Gambling, and IT Services industries. And while BFSI was the number one most attacked industry when considering the Meris DDoS activity rate, it only came in fourth place when considering the percentage of targeted websites.\nIn terms of the percentage of targeted websites, the Computer Software industry came in first place. Almost 4% of all impacted websites were of Computer Software companies protected by Cloudflare, followed by Gaming/Gambling and IT Services with 3% and 2%, respectively.\nView the interactive graph on Cloudflare Radar.\nAttacks on industries over time\nBesides the total breakdowns shown above, we can also view the top industries the botnet attacked over time to understand the changing trends. These trends may be tied to political events, new video game releases, sporting events, or any other global or local public interest events.\nOff the top, we can already see the two largest peaks on August 9 and August 29 — mainly on the Computer Software, Gaming/Gambling, and IT industries. Another interesting peak occurred on August 14 against Cryptocurrency providers.\nIn late August, the botnet was pointed against gambling and casino websites, generating attacks at rates of hundreds of thousands to millions of requests per second. A second significant wave against the same industry was launched in early September.\nView the interactive graph on Cloudflare Radar.\nMeris targets websites in China, Australia, and US\nSimilarly to the analysis of the top industries, we can calculate the Meris DDoS activity rate per target country to identify which countries came under the most attacks. In total, China-based companies saw the largest amount of DDoS attacks. More than 33% of all requests generated by Meris were destined for China-based companies that are protected by Cloudflare. Australia came in second place, and the US in third.\nView the interactive graph on Cloudflare Radar.\nOn the other hand, when we look at the number of websites that were targeted by Meris, the US came in first place. More than 12% of all websites that were targeted by Meris are operated by US-based companies. China came in second place with 5.6% and Russia in third with 4.4%.\nView the interactive graph on Cloudflare Radar.\nAttacks on countries over time\nOver time, we can see how the attacks on the top countries change. Similarly to the per-industry breakdown, we can also see two large peaks. The first one occurred on the same spike as the per-industry breakdown on August 9. However, the second one here occurred on September 1.\nView the interactive graph on Cloudflare Radar.\nLocation of the Meris bots\nAlthough only tens of thousands of bots have been detected per attack, it is estimated that there are roughly 250,000 bots worldwide. As indicated above, the botnet is formed of MikroTik routers. Using the source IP address of the routers, we’re able to identify the origin country of the bots to paint a geographical representation of the bots' presence and growth over time.\nThe change in the location of the bots doesn’t necessarily indicate that the botnet is growing or shrinking. It could also be that different bot groups are activated from time to time to spread the load of the attacks while attempting not to get caught.\nAt the beginning of August, the majority of the bots were located in Brazil. But by the end of August, that number plummeted to a single digit percentage close to zero. Meanwhile, the number of infected devices grew in the United States. From the beginning of September, the number of bots was significantly higher in the US, Russia, India, Indonesia, and China.\nView the interactive graph on Cloudflare Radar.\nCloudflare protects against Meris attacks\nCloudflare operates autonomous DDoS protection systems that automatically detect and mitigate DDoS attacks of all types, including attacks launched by Meris and Mirai. These systems are also customizable, and Cloudflare customers can tweak and tune their DDoS protection settings as needed with the HTTP DDoS Managed Ruleset and the L3/4 DDoS Managed Ruleset.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nDDoS Attacks Trends Cloudflare Radar RDDoS",
      "markdown": "11/09/2021\n\n*   [![Vivek Ganti](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2020/08/Vivek-Ganti.jpeg)](https://blog.cloudflare.com/author/vivek/)\n*   [![Omer Yoachimik](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/06/Screenshot-2023-06-02-at-9.38.13-AM.png)](https://blog.cloudflare.com/author/omer/)\n\n8 min read\n\n![A Brief History of the Meris Botnet](https://blog.cloudflare.com/content/images/2021/11/image12-2.png)\n\nMeris first got our attention due to an exceptionally large [17.2 million requests per second (rps) DDoS attack](https://blog.cloudflare.com/cloudflare-thwarts-17-2m-rps-ddos-attack-the-largest-ever-reported/) that it launched against one of our customers. This attack, along with subsequent attacks originated by the Meris botnet, was automatically detected and mitigated by our [DDoS protection systems](https://www.cloudflare.com/ddos/). Cloudflare customers, even ones on the free plan, are protected against Meris attacks.\n\nOver the past months, we’ve been tracking and analyzing the activity of the Meris botnet. Some main highlights include:\n\n*   Meris targets approximately 50 different websites every single day with a daily average of 104 unique DDoS attacks.\n*   More than 33% of all Meris DDoS attack traffic targeted China-based websites.\n*   More than 12% of all websites that were attacked by Meris are operated by US-based companies.\n\n_View more Meris attack insights and trends in the interactive [Radar dashboard](https://radar.cloudflare.com/notebooks/meris-botnet)._\n\n### So what is Meris?\n\nMeris (Latvian for plague) is the name of an active botnet behind a series of recent DDoS attacks that have targeted thousands of websites around the world. It was originally detected in late June 2021 by [QRator](https://blog.qrator.net/en/meris-botnet-climbing-to-the-record_142/) in joint research they conducted with Yandex. Their initial research identified 30,000 to 56,000 bots, but they estimated that the numbers are actually much higher, in the ballpark of 250,000 bots.\n\nThe Meris botnet is formed of infected routers and networking hardware manufactured by the Latvian company MikroTik. [According to MikroTik’s](https://blog.mikrotik.com/security/meris-botnet.html) blog, the attackers exploited a vulnerability in the router’s operating system (RouterOS) which enabled attackers to gain unauthenticated remote access to read and write arbitrary files ([CVE-2018-14847](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-14847)).\n\n[RouterOS](https://wiki.mikrotik.com/wiki/Manual:RouterOS_FAQ#What_is_MikroTik_RouterOS.E2.84.A2.3F) is the router operating system that’s used by MikroTik’s routers and the RouterBOARD hardware product family, which can also be used to turn any PC into a router. Administration of RouterOS can be done either via direct [SSH connection](https://www.cloudflare.com/learning/access-management/what-is-ssh/) or by using a configuration utility called [WinBox](https://wiki.mikrotik.com/wiki/Manual:Winbox#Summary). The vulnerability itself was possible due to a [directory traversal](https://en.wikipedia.org/wiki/Directory_traversal_attack) vulnerability in the WinBox interface with RouterOS.\n\nDirectory traversal is a type of exploit that allows attackers to travel to the parent directories to gain access to the operating system’s [file system](https://en.wikipedia.org/wiki/File_system), a method and structure of how data is stored and retrieved in the operating system. Once they gain access to the file system, attackers can then read the existing files that administer the router and write files directly into the file system to administer the routers to their botnet needs.\n\nWhile the vulnerability was patched after its detection back in 2018, it’s still being exploited in compromised devices that do not use the patched RouterOS versions, or that use the default usernames and passwords. MicroTik has advised its customers to upgrade their devices’ OS version, to only allow access to the devices via secure IPsec, and to inspect for any abnormalities such as unknown SOCKS proxy settings and scripts.\n\nTo launch volumetric attacks, the botnet uses HTTP pipelining which allows it to send multiple requests over a single connection, thus increasing its total attack throughput. Furthermore, in an attempt to obfuscate the attack source, the botnet uses open SOCKS proxies to proxy their attack traffic to the target.\n\nCloudflare’s DDoS protection systems automatically detect and mitigate Meris attacks. One of the mitigation actions that the system can choose to use is the ‘Connection Close’ action which eliminates the risk of HTTP pipelining and helps slow down attackers. Additionally, as part of Cloudflare’s threat intelligence suite, we provide a Managed IP List of Open SOCKS Proxies that customers can use as part of their firewall rules — to block, challenge or rate-limit traffic that arrives via SOCKS proxies.\n\n### How does Meris compare to Mirai?\n\nAbout five years ago, [Mirai](https://www.cloudflare.com/learning/ddos/glossary/mirai-botnet/) (Japanese for future) — the infamous botnet that infected hundreds of thousands of IoT devices —  [launched record-breaking DDoS attacks](https://blog.cloudflare.com/inside-mirai-the-infamous-iot-botnet-a-retrospective-analysis/) against websites.\n\nThere have been many variants of the Mirai botnet since its source code was leaked. One version of Mirai, called [Moobot](https://blog.cloudflare.com/moobot-vs-gatebot-cloudflare-automatically-blocks-botnet-ddos-attack-topping-at-654-gbps/), was detected last year when it attacked a Cloudflare customer with a 654 Gbps DDoS attack. Another variant [recently made a resurgence](https://blog.cloudflare.com/cloudflare-thwarts-17-2m-rps-ddos-attack-the-largest-ever-reported/#:~:text=Two%20weeks%20before%2C%20a%20Mirai-variant%20botnet%20launched%20over%20a%20dozen%20UDP%20and%20TCP%20based%20DDoS%20attacks%20that%20peaked%20multiple%20times%20above%201%20Tbps%2C%20with%20a%20max%20peak%20of%20approximately%201.2%20Tbps.) when it targeted Cloudflare customers with over a dozen UDP and TCP based DDoS attacks that peaked multiple times above 1 Tbps, with a max peak of approximately 1.2 Tbps.\n\nWhile Mirai infected IoT devices with low computational power, Meris is a swarm of routers that have significantly higher processing power and data transfer capabilities than IoT devices, making them much more potent in causing harm at a larger scale to web properties that are not protected by sophisticated cloud-based DDoS mitigation.\n\n## Tracking the Meris botnet attacks\n\nSince the appearance of Meris, Cloudflare’s systems automatically detected and mitigated Meris attacks using the existing mitigation rules. During our analysis of the Meris botnet attacks, our security experts noticed the attack vectors adapt to try and bypass Cloudflare’s defenses. Needless to say, they were not successful. But we wanted to stay many steps ahead of attackers — and so our engineers deployed additional rules that mitigate Meris attacks even more comprehensively. A side effect of these mitigation rules is that it also provides us with more granular threat intelligence on the Meris attacks.\n\nSince we deployed the new rules in early August, we’ve seen Meris launch an average of 104 DDoS attacks on Cloudflare customers every day. The highest figure we’ve seen was on September 6, when Meris was used to launch 261 unique attacks against Cloudflare customers.\n\n![](https://blog.cloudflare.com/content/images/2021/11/unnamed--8-.png \"Chart\")\n\n_View the interactive graph on_ [_Cloudflare Radar_](https://radar.cloudflare.com/notebooks/meris-botnet#meris_attacks_over_time)_._\n\nDuring that same day, on September 6, attacks from Meris accounted for a record-breaking 17.5% of all L7 DDoS attacks that Cloudflare observed.\n\n![](https://blog.cloudflare.com/content/images/2021/11/unnamed--1--3.png)\n\n_View the interactive graph on [Cloudflare Radar](https://radar.cloudflare.com/notebooks/meris-botnet#share_of_meris_attacks)._\n\nOverall, Meris targets about 50 different websites and applications every single day. Although the average attack peaked at 106K rps, the median attack size was actually smaller at 17.6K rps. The largest attack we’ve seen was 17.2M rps and that occurred in July. In the graph below, you can see the daily highest requests per second rate after we deployed the new rules. Since then, the largest attack we’ve seen was 16.7M rps, which took place on August 19.\n\n![](https://blog.cloudflare.com/content/images/2021/11/unnamed--9-.png \"Chart\")\n\n## Meris used to target Banks, Financial Services, and Insurance companies\n\nOver the past few months, the industry that received the most attack traffic from the Meris botnet was the Banking, Financial Services, and Insurance (BFSI) industry\n\n![](https://blog.cloudflare.com/content/images/2021/11/unnamed-6.png)\n\n_View the interactive graph on [Cloudflare Radar](https://radar.cloudflare.com/notebooks/meris-botnet#top10_industries_by_total_requests)._\n\nFollowing the BFSI industry, the most attacked industries were the Publishing, Gaming/Gambling, and IT Services industries. And while BFSI was the number one most attacked industry when considering the Meris DDoS activity rate, it _only_ came in fourth place when considering the percentage of targeted websites.\n\nIn terms of the percentage of targeted websites, the Computer Software industry came in first place. Almost 4% of all impacted websites were of Computer Software companies protected by Cloudflare, followed by Gaming/Gambling and IT Services with 3% and 2%, respectively.\n\n![](https://blog.cloudflare.com/content/images/2021/11/unnamed--2--3.png)\n\n_View the interactive graph on [Cloudflare Radar](https://radar.cloudflare.com/notebooks/meris-botnet#top10_industries_by_internet_properties)._\n\n### Attacks on industries over time\n\nBesides the total breakdowns shown above, we can also view the top industries the botnet attacked over time to understand the changing trends. These trends may be tied to political events, new video game releases, sporting events, or any other global or local public interest events.\n\nOff the top, we can already see the two largest peaks on August 9 and August 29 — mainly on the Computer Software, Gaming/Gambling, and IT industries. Another interesting peak occurred on August 14 against Cryptocurrency providers.\n\nIn late August, the botnet was pointed against gambling and casino websites, generating attacks at rates of hundreds of thousands to millions of requests per second. A second significant wave against the same industry was launched in early September.\n\n![](https://blog.cloudflare.com/content/images/2021/11/unnamed--3--3.png)\n\n_View the interactive graph on [Cloudflare Radar](https://radar.cloudflare.com/notebooks/meris-botnet#top10_industries_attacked_by_meris)._\n\n## Meris targets websites in China, Australia, and US\n\nSimilarly to the analysis of the top industries, we can calculate the Meris DDoS activity rate per target country to identify which countries came under the most attacks. In total, China-based companies saw the largest amount of DDoS attacks. More than 33% of all requests generated by Meris were destined for China-based companies that are protected by Cloudflare. Australia came in second place, and the US in third.\n\n![](https://blog.cloudflare.com/content/images/2021/11/unnamed--4--3.png)\n\n_View the interactive graph on [Cloudflare Radar](https://radar.cloudflare.com/notebooks/meris-botnet#top10_countries_by_total_requests)._\n\nOn the other hand, when we look at the number of websites that were targeted by Meris, the US came in first place. More than 12% of all websites that were targeted by Meris are operated by US-based companies. China came in second place with 5.6% and Russia in third with 4.4%.\n\n![](https://blog.cloudflare.com/content/images/2021/11/unnamed--5--2.png)\n\n_View the interactive graph on [Cloudflare Radar](https://radar.cloudflare.com/notebooks/meris-botnet#top10_countries_by_internet_properties)._\n\n### Attacks on countries over time\n\nOver time, we can see how the attacks on the top countries change. Similarly to the per-industry breakdown, we can also see two large peaks. The first one occurred on the same spike as the per-industry breakdown on August 9. However, the second one here occurred on September 1.\n\n![](https://blog.cloudflare.com/content/images/2021/11/unnamed--10-.png)\n\n_View the interactive graph on [Cloudflare Radar](https://radar.cloudflare.com/notebooks/meris-botnet#top10_countries_attacked_by_meris)._\n\n## Location of the Meris bots\n\nAlthough only tens of thousands of bots have been detected per attack, it is estimated that there are roughly 250,000 bots worldwide. As indicated above, the botnet is formed of MikroTik routers. Using the source IP address of the routers, we’re able to identify the origin country of the bots to paint a geographical representation of the bots' presence and growth over time.\n\nThe change in the location of the bots doesn’t necessarily indicate that the botnet is growing or shrinking. It could also be that different bot groups are activated from time to time to spread the load of the attacks while attempting not to get caught.\n\nAt the beginning of August, the majority of the bots were located in Brazil. But by the end of August, that number plummeted to a single digit percentage close to zero. Meanwhile, the number of infected devices grew in the United States. From the beginning of September, the number of bots was significantly higher in the US, Russia, India, Indonesia, and China.\n\n_View the interactive graph on [Cloudflare Radar](https://radar.cloudflare.com/notebooks/meris-botnet#location-of-the-meris-bots)._\n\n## Cloudflare protects against Meris attacks\n\nCloudflare operates autonomous DDoS protection systems that automatically detect and mitigate DDoS attacks of all types, including attacks launched by Meris and Mirai. These systems are also customizable, and Cloudflare customers can tweak and tune their DDoS protection settings as needed with the [HTTP DDoS Managed Ruleset](https://blog.cloudflare.com/http-ddos-managed-rules/) and the [L3/4 DDoS Managed Ruleset](https://developers.cloudflare.com/waf/ddos-l34-mitigation).\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[DDoS](https://blog.cloudflare.com/tag/ddos/) [Attacks](https://blog.cloudflare.com/tag/attacks/) [Trends](https://blog.cloudflare.com/tag/trends/) [Cloudflare Radar](https://blog.cloudflare.com/tag/cloudflare-radar/) [RDDoS](https://blog.cloudflare.com/tag/rddos/)"
    },
    {
      "url": "https://blog.cloudflare.com/ddos-attack-trends-for-2021-q3/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/ddos-attack-trends-for-2021-q3/",
        "loadedTime": "2023-12-05T02:32:52.670Z",
        "referrerUrl": "https://blog.cloudflare.com/cyber-week-analyzing-internet-traffic-and-e-commerce-trends/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/ddos-attack-trends-for-2021-q3/",
        "title": "DDoS Attack Trends for Q3 2021",
        "description": "The third quarter of 2021 will go down in Cloudflare history as the quarter with the most DDoS activity observed — we saw and mitigated record-setting HTTP DDoS attacks, terabit-strong network layer attacks, one of the largest botnets ever deployed (Meris), and more recently, ransom attacks on Voice-over-IP (VoIP) service providers and their network infrastructure around the world.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/04/2021\n13 min read\nThis post is also available in 简体中文, 한국어, Deutsch, Français, 繁體中文, 日本語\nThe third quarter of 2021 was a busy quarter for DDoS attackers. Cloudflare observed and mitigated record-setting HTTP DDoS attacks, terabit-strong network-layer attacks, one of the largest botnets ever deployed (Meris), and more recently, ransom DDoS attacks on voice over IP (VoIP) service providers and their network infrastructure around the world.\nHere’s a summary of the trends observed in Q3 ‘21:\nApplication-layer (L7) DDoS attack trends:\nFor the second consecutive quarter in 2021, US-based companies were the most targeted in the world.\nFor the first time in 2021, attacks on UK-based and Canada-based companies skyrocketed, making them the second and third most targeted countries, respectively.\nAttacks on Computer Software, Gaming/ Gambling, IT, and Internet companies increased by an average of 573% compared to the previous quarter.\nMeris, one of the most powerful botnets in history, aided in launching DDoS campaigns across various industries and countries.\nNetwork-layer (L3/4) DDoS attack trends:\nDDoS attacks increased by 44% worldwide compared to the previous quarter.\nThe Middle East and Africa recorded the largest average attack increase of approximately 80%.\nMorocco recorded the highest DDoS activity in the third quarter globally — three out of every 100 packets were part of a DDoS attack.\nWhile SYN and RST attacks remain the dominant attack method used by attackers, Cloudflare observed a surge in DTLS amplification attacks — recording a 3,549% increase QoQ.\nAttackers targeted (and continue to target going into the fourth quarter this year) VoIP service providers with massive DDoS attack campaigns in attempts to bring SIP infrastructure down.\nNote on avoiding data biases: When we analyze attack trends, we calculate the “DDoS activity” rate, which is the percentage of attack traffic of the total traffic (attack + clean). When reporting application- and network-layer DDoS attack trends, we use this metric, which allows us to normalize the data points and avoid biases toward, for example, a larger Cloudflare data center that naturally handles more traffic and therefore also, possibly, more attacks compared to a smaller Cloudflare data center located elsewhere.\nApplication-layer DDoS attacks\nApplication-layer DDoS attacks, specifically HTTP DDoS attacks, are attacks that usually aim to disrupt a web server by making it unable to process legitimate user requests. If a server is bombarded with more requests than it can process, the server will drop legitimate requests and — in some cases — crash, resulting in degraded performance or an outage for legitimate users.\nQ3 ‘21 was the quarter of Meris — one of the most powerful botnets deployed to launch some of the largest HTTP DDoS attacks in history.\nThis past quarter, we observed one of the largest recorded HTTP attacks — 17.2M rps (requests per second) — targeting a customer in the financial services industry. One of the most powerful botnets ever observed, called Meris, is known to be deployed in launching these attacks.\nMeris (Latvian for plague) is a botnet behind recent DDoS attacks that have targeted networks or organizations around the world. The Meris botnet infected routers and other networking equipment manufactured by the Latvian company MikroTik. According to MikroTik’s blog, a vulnerability in the MikroTik RouterOS (that was patched after its detection back in 2018) was exploited in still unpatched devices to build a botnet and launch coordinated DDoS attacks by bad actors.\nSimilar to the Mirai botnet of 2016, Meris is one of the most powerful botnets recorded. While Mirai infected IoT devices with low computational power such as smart cameras, Meris is a growing swarm of networking infrastructure (such as routers and switches) with significantly higher processing power and data transfer capabilities than IoT devices — making them much more potent in causing harm at a larger scale. Be that as it may, Meris is an example of how the attack volume doesn’t necessarily guarantee damage to the target. As far as we know, Meris, despite its strength, was not able to cause significant impact or Internet outages. On the other hand, by tactically targeting the DYN DNS service in 2016, Mirai succeeded in causing significant Internet disruptions.\nApplication-layer DDoS attacks by industry\nThe tech and gaming industries were the most targeted industries in Q3 ‘21.\nWhen we break down the application-layer attacks targeted by industry, Computer Software companies topped the charts. The Gaming/Gambling industry, also known to be regular targets of online attacks, was a close second, followed by the Internet and IT industries.\nApplication-layer DDoS attacks by source country\nTo understand the origin of the HTTP attacks, we look at the geolocation of the source IP address belonging to the client that generated the attack HTTP requests. Unlike network-layer attacks, source IPs cannot be spoofed in HTTP attacks. A high DDoS activity rate in a given country usually indicates the presence of botnets operating from within.\nIn the third quarter of 2021, most attacks originated from devices/servers in China, the United States, and India. While China remains in first place, the number of attacks originating from Chinese IPs actually decreased by 30% compared to the previous quarter. Almost one out of every 200 HTTP requests that originated from China was part of an HTTP DDoS attack.\nAdditionally, attacks from Brazil and Germany shrank by 38% compared to the previous quarter. Attacks originating from the US and Malaysia reduced by 40% and 45%, respectively.\nApplication-layer DDoS attacks by target country\nIn order to identify which countries are targeted the most by L7 attacks, we break down the DDoS activity by our customers’ billing countries.\nFor the second consecutive time this year, organizations in the United States were targeted the most by L7 DDoS attacks in the world, followed by those in the UK and Canada.\nNetwork-layer DDoS attacks\nWhile application-layer attacks target the application (Layer 7 of the OSI model) running the service that end users are trying to access, network-layer attacks aim to overwhelm network infrastructure (such as in-line routers and servers) and the Internet link itself.\nMirai-variant botnet strikes with a force of 1.2 Tbps.\nQ3 ‘21 was also the quarter when the infamous Mirai made a resurgence. A Mirai-variant botnet launched over a dozen UDP- and TCP-based DDoS attacks that peaked multiple times above 1 Tbps, with a max peak of approximately 1.2 Tbps. These network-layer attacks targeted Cloudflare customers on the Magic Transit and Spectrum services. One of these targets was a major APAC-based Internet services, telecommunications, and hosting provider and the other was a gaming company. In all cases, the attacks were automatically detected and mitigated without human intervention.\nNetwork-layer DDoS attacks by month\nSeptember was, by far, the busiest month for attackers this year.\nQ3 ‘21 accounted for more than 38% of all attacks this year. September was the busiest month for attackers so far in 2021 — accounting for over 16% of all attacks this year.\nNetwork-layer DDoS attacks by attack rate\nMost attacks are ‘small’ in size, but the number of larger attacks continues to rise.\nThere are different ways of measuring the size of a L3/4 DDoS attack. One is the volume of traffic it delivers, measured as the bit rate (specifically, terabits per second or gigabits per second). Another is the number of packets it delivers, measured as the packet rate (specifically, millions of packets per second).\nAttacks with high bit rates attempt to cause a denial-of-service event by clogging the Internet link, while attacks with high packet rates attempt to overwhelm the servers, routers, or other in-line hardware appliances. Appliances dedicate a certain amount of memory and computation power to process each packet. Therefore, by bombarding it with many packets, the appliance can be left with no further processing resources. In such a case, packets are “dropped,” i.e., the appliance is unable to process them. For users, this results in service disruptions and denial of service.\nThe distribution of attacks by their size (in bit rate) and month is shown below. Interestingly enough, all attacks over 400 Gbps took place in August, including some of the largest attacks we have seen; multiple attacks peaked above 1 Tbps and reached as high as 1.2 Tbps.\nPacket rate\nAs seen in previous quarters, the majority of attacks observed in Q3 ‘21 were relatively small in size — nearly 89% of all attacks peaked below 50K packets per second (pps). While a majority of attacks are smaller in size, we observed that the number of larger attacks is increasing QoQ — attacks that peaked above 10M pps increased by 142% QoQ.\nAttacks of packet rates ranging from 1-10 million packets per second increased by 196% compared to the previous quarter. This trend is similar to what we observed the last quarter as well, suggesting that larger attacks are increasing.\nBit rate\nFrom the bit rate perspective, a similar trend was observed — a total of 95.4% of all attacks peaked below 500 Mbps.\nQoQ data shows that the number of attacks of sizes ranging from 500 Mbps to 10 Gbps saw massive increases of 126% to 289% compared to the previous quarter. Attacks over 100 Gbps decreased by nearly 14%.\nThe number of larger bitrate attacks increased QoQ (with the one exception being attacks over 100 Gbps, which decreased by nearly 14% QoQ). In particular, attacks ranging from 500 Mbps to 1 Gbps saw a surge of 289% QoQ and those ranging from 1 Gbps to 100 Gbps surged by 126%.\nThis trend once again illustrates that, while (in general) a majority of the attacks are indeed smaller, the number of “larger” attacks is increasing. This suggests that more attackers are garnering more resources to launch larger attacks.\nNetwork-layer DDoS attacks by duration\nMost attacks remain under one hour in duration, reiterating the need for automated always-on DDoS mitigation solutions.\nWe measure the duration of an attack by recording the difference between when it is first detected by our systems as an attack and the last packet we see with that attack signature. As in previous quarters, most of the attacks are short-lived. To be specific, 94.4% of all DDoS attacks lasted less than an hour. On the other end of the axis, attacks over 6 hours accounted for less than 0.4% in Q3 ‘21, and we did see a QoQ increase of 165% in attacks ranging 1-2 hours. Be that as it may, a longer attack does not necessarily mean a more dangerous one.\nShort attacks can easily go undetected, especially burst attacks that, within seconds, bombard a target with a significant number of packets, bytes, or requests. In this case, DDoS protection services that rely on manual mitigation by security analysis have no chance in mitigating the attack in time. They can only learn from it in their post-attack analysis, then deploy a new rule that filters the attack fingerprint and hope to catch it next time. Similarly, using an “on-demand” service, where the security team will redirect traffic to a DDoS provider during the attack, is also inefficient because the attack will already be over before the traffic routes to the on-demand DDoS provider.\nCloudflare recommends that companies use automated, always-on DDoS protection services that analyze traffic and apply real-time fingerprinting fast enough to block the short-lived attacks. Cloudflare analyzes traffic out-of-path, ensuring that DDoS mitigation does not add any latency to legitimate traffic, even in always-on deployments. Once an attack is identified, our autonomous edge DDoS protection system (dosd) generates and applies a dynamically crafted rule with a real-time signature. Pre-configured firewall rules comprising allow/deny lists for known traffic patterns take effect immediately.\nAttack vectors\nSYN floods remain attackers’ favorite method of attack, while attacks over DTLS saw a massive surge — 3,549% QoQ.\nAn attack vector is the term used to describe the method that the attacker utilizes in their attempt to cause a denial-of-service event.\nAs observed in previous quarters, attacks utilizing SYN floods remain the most popular method used by attackers.\nA SYN flood attack is a DDoS attack that works by exploiting the very foundation of the TCP protocol — the stateful TCP connection between a client and a server as a part of the 3-way TCP handshake. As a part of the TCP handshake, the client sends an initial connection request packet with a synchronize flag (SYN). The server responds with a packet that contains a synchronized acknowledgment flag (SYN-ACK). Finally, the client responds with an acknowledgment (ACK) packet. At this point, a connection is established and data can be exchanged until the connection is closed. This stateful process can be abused by attackers to cause denial-of-service events.\nBy repeatedly sending SYN packets, the attacker attempts to overwhelm a server or the router’s connection table that tracks the state of TCP connections. The server replies with a SYN-ACK packet, allocates a certain amount of memory for each given connection, and falsely waits for the client to respond with the final ACK. Given a sufficient number of connections occupying the server’s memory, the server is unable to allocate further memory for legitimate clients, causing the server to crash or preventing it from handling legitimate client connections, i.e., a denial-of-service event.\nMore than half of all attacks observed over our network were SYN floods. This was followed by RST, ACK, and UDP floods.\nEmerging threats\nWhile SYN and RST floods remain popular overall, when we look at emerging attack vectors — which helps us understand what new vectors attackers are deploying to launch attacks — we observed a massive spike in DTLS amplification attacks. DTLS floods increased by 3,549% QoQ.\nDatagram Transport Layer Security (DTLS) is a protocol similar to Transport Layer Security (TLS) designed to provide similar security guarantees to connectionless datagram-based applications to prevent message forgery, eavesdropping, or tampering. DTLS, being connectionless, is specifically useful for establishing VPN connections, without the TCP meltdown problem. The application is responsible for reordering and other connection properties.\nJust as with most UDP-based protocols, DTLS is spoofable and being used by attackers to generate reflection amplification attacks to overwhelm network gateways.\nNetwork-layer DDoS attacks by country\nWhile Morocco topped the charts in terms of the highest network attack rate observed, Asian countries closely followed.\nWhen analyzing network-layer DDoS attacks, we bucket the traffic by the Cloudflare edge data center locations where the traffic was ingested, and not by the source IP. The reason for this is that, when attackers launch network-layer attacks, they can spoof the source IP address in order to obfuscate the attack source and introduce randomness into the attack properties, which may make it harder for simple DDoS protection systems to block the attack. Hence, if we were to derive the source country based on a spoofed source IP, we would get a spoofed country.\nCloudflare is able to overcome the challenges of spoofed IPs by displaying the attack data by the location of the Cloudflare data center in which the attack was observed. We are able to achieve geographical accuracy in our report because we have data centers in over 250 cities around the world.\nWorldwide\nTo view all regions and countries, check out the Radar DDoS Report dashboard’s interactive map.\nA note on recent attacks on voice over-IP service providers — and ransom DDoS attacks\nWe recently reported and provided an update on the surge in DDoS attacks on VoIP service providers — some of who have also received ransom threats. As of early Q4 ‘21, this attack campaign is still ongoing and current. At Cloudflare, we continue to onboard VoIP service providers and shield their applications and networks against attacks.\nHTTP attacks against API gateways and the corporate websites of the providers have been combined with network-layer and transport-layer attacks against VoIP infrastructures.\nExamples include:\nTCP floods targeting stateful firewalls: These are being used in “trial-and-error” type attacks. They are not very effective against telephony infrastructure specifically (because it is mostly UDP) but very effective at overwhelming stateful firewalls.\nUDP floods targeting SIP infrastructure: Floods of UDP traffic that have no well-known fingerprint, aimed at critical VoIP services. Generic floods like this may look like legitimate traffic to unsophisticated filtering systems.\nUDP reflection targeting SIP infrastructure: These methods, when targeted at SIP or RTP services, can easily overwhelm Session Border Controllers (SBCs) and other telephony infrastructure. The attacker seems to learn enough about the target’s infrastructure to target such services with high precision.\nSIP protocol-specific attacks: Attacks at the application layer are of particular concern because of the higher resource cost of generating application errors versus filtering on network devices.\nOrganizations also continue to receive ransom notes that threaten attacks in exchange for bitcoin. Ransomware and ransom DDoS attacks, for the fourth consecutive quarter, continue to be a germane threat to organizations all over the world.\nCloudflare products close off several threat vectors that can lead to a ransomware infection and ransom DDoS attacks:\nCloudflare DNS filtering blocks unsafe websites.\nCloudflare Browser Isolation prevents drive-by downloads and other browser-based attacks.\nA Zero Trust architecture can help prevent ransomware from spreading within a network.\nMagic Transit protects organizations’ networks against DDoS attacks using BGP route redistribution — without impacting latency.\nHelping build a better Internet\nCloudflare was founded on the mission to help build a better Internet. And part of that mission is to build an Internet where the impact of DDoS attacks is a thing of the past. Over the last 10 years, we have been unwavering in our efforts to protect our customers’ Internet properties from DDoS attacks of any size or kind. In 2017, we announced unmetered DDoS protection for free — as part of every Cloudflare service and plan, including the Free plan — to make sure every organization can stay protected and available. Organizations big and small have joined Cloudflare over the past several years to ensure their websites, applications, and networks are secure from DDoS attacks, and remain fast and reliable.\nBut cyberattacks come in various forms, not just DDoS attacks. Malicious bots, ransomware attacks, email phishing, and VPN / remote access hacks are some many attacks that continue to plague organizations of all sizes globally. These attacks target websites, APIs, applications, and entire networks — which form the lifeblood of any online business. That is why the Cloudflare security portfolio accounts for everything and everyone connected to the Internet.\nTo learn more about Cloudflare DDoS or our network services, create an account or reach out to us.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nDDoS Attacks Cloudflare Radar RDDoS REvil",
      "markdown": "11/04/2021\n\n*   [![Vivek Ganti](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2020/08/Vivek-Ganti.jpeg)](https://blog.cloudflare.com/author/vivek/)\n*   [![Omer Yoachimik](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/06/Screenshot-2023-06-02-at-9.38.13-AM.png)](https://blog.cloudflare.com/author/omer/)\n\n13 min read\n\n_This post is also available in [简体中文](https://blog.cloudflare.com/zh-cn/ddos-attack-trends-for-2021-q3-zh-cn/), [한국어](https://blog.cloudflare.com/ko-kr/ddos-attack-trends-for-2021-q3-ko-kr/), [Deutsch](https://blog.cloudflare.com/de-de/ddos-attack-trends-for-2021-q3-de-de/), [Français](https://blog.cloudflare.com/fr-fr/ddos-attack-trends-for-2021-q3-fr-fr/), [繁體中文](https://blog.cloudflare.com/zh-tw/ddos-attack-trends-for-2021-q3-zh-tw/), [日本語](https://blog.cloudflare.com/ja-jp/ddos-attack-trends-for-2021-q3-ja-jp/)_\n\n![DDoS Attack Trends for Q3 2021](https://blog.cloudflare.com/content/images/2021/11/image15-no-text-no-logo-1.png)\n\nThe third quarter of 2021 was a busy quarter for DDoS attackers. Cloudflare observed and mitigated [record-setting HTTP DDoS attacks](https://blog.cloudflare.com/cloudflare-thwarts-17-2m-rps-ddos-attack-the-largest-ever-reported/), [terabit-strong network-layer attacks](https://blog.cloudflare.com/cloudflare-thwarts-17-2m-rps-ddos-attack-the-largest-ever-reported/#:~:text=with%20a%20max%20peak%20of%20approximately%201.2%20Tbps), one of the [largest botnets ever deployed (Meris)](https://blog.cloudflare.com/meris-botnet/), and more recently, [ransom DDoS attacks on voice over IP (VoIP) service providers](https://blog.cloudflare.com/update-on-voip-attacks/) and their [network infrastructure](https://blog.cloudflare.com/attacks-on-voip-providers/) around the world.\n\nHere’s a summary of the trends observed in Q3 ‘21:\n\n#### Application-layer (L7) DDoS attack trends:\n\n*   For the second consecutive quarter in 2021, US-based companies were the most targeted in the world.\n*   For the first time in 2021, attacks on UK-based and Canada-based companies skyrocketed, making them the second and third most targeted countries, respectively.\n*   Attacks on Computer Software, Gaming/ Gambling, IT, and Internet companies increased by an average of 573% compared to the previous quarter.\n*   Meris, one of the most powerful botnets in history, aided in launching DDoS campaigns across various industries and countries.\n\n#### Network-layer (L3/4) DDoS attack trends:\n\n*   DDoS attacks increased by 44% worldwide compared to the previous quarter.\n*   The Middle East and Africa recorded the largest average attack increase of approximately 80%.\n*   Morocco recorded the highest DDoS activity in the third quarter globally — three out of every 100 packets were part of a DDoS attack.\n*   While SYN and RST attacks remain the dominant attack method used by attackers, Cloudflare observed a surge in [DTLS](https://en.wikipedia.org/wiki/Datagram_Transport_Layer_Security) amplification attacks — recording a 3,549% increase QoQ.\n*   Attackers targeted (and continue to target going into the fourth quarter this year) VoIP service providers with massive DDoS attack campaigns in attempts to bring SIP infrastructure down.\n\n**Note on avoiding data biases:** When we analyze attack trends, we calculate the “DDoS activity” rate, which is the percentage of attack traffic of the total traffic (attack + clean). When reporting application- and network-layer DDoS attack trends, we use this metric, which allows us to normalize the data points and avoid biases toward, for example, a larger Cloudflare data center that naturally handles more traffic and therefore also, possibly, more attacks compared to a smaller Cloudflare data center located elsewhere.\n\n## Application-layer DDoS attacks\n\n[Application-layer DDoS attacks](https://www.cloudflare.com/learning/ddos/application-layer-ddos-attack/), specifically HTTP DDoS attacks, are attacks that usually aim to disrupt a web server by making it unable to process legitimate user requests. If a server is bombarded with more requests than it can process, the server will drop legitimate requests and — in some cases — crash, resulting in degraded performance or an outage for legitimate users.\n\n#### Q3 ‘21 was the quarter of Meris — one of the most powerful botnets deployed to launch some of the largest HTTP DDoS attacks in history.\n\nThis past quarter, we observed [one of the largest recorded HTTP attacks](https://blog.cloudflare.com/cloudflare-thwarts-17-2m-rps-ddos-attack-the-largest-ever-reported/) — 17.2M rps (requests per second) — targeting a customer in the financial services industry. One of the most powerful botnets ever observed, called Meris, is known to be deployed in launching these attacks.\n\nMeris (Latvian for plague) is a botnet behind recent DDoS attacks that have targeted networks or organizations around the world. The Meris botnet infected routers and other networking equipment manufactured by the Latvian company MikroTik. According to MikroTik’s blog, a vulnerability in the MikroTik RouterOS (that was patched after its detection back in 2018) was exploited in still unpatched devices to build a botnet and launch coordinated DDoS attacks by bad actors.\n\nSimilar to the [Mirai botnet](https://www.cloudflare.com/learning/ddos/glossary/mirai-botnet/) of 2016, Meris is one of the most powerful botnets recorded. While Mirai infected IoT devices with low computational power such as smart cameras, Meris is a growing swarm of networking infrastructure (such as routers and switches) with significantly higher processing power and data transfer capabilities than IoT devices — making them much more potent in causing harm at a larger scale. Be that as it may, Meris is an example of how the attack volume doesn’t necessarily guarantee damage to the target. As far as we know, Meris, despite its strength, was not able to cause significant impact or Internet outages. On the other hand, by tactically [targeting the DYN DNS service in 2016](https://en.wikipedia.org/wiki/DDoS_attack_on_Dyn), Mirai succeeded in causing significant Internet disruptions.\n\n### Application-layer DDoS attacks by industry\n\n**The tech and gaming industries were the most targeted industries in Q3 ‘21.**\n\nWhen we break down the application-layer attacks targeted by industry, Computer Software companies topped the charts. The Gaming/Gambling industry, also known to be regular targets of online attacks, was a close second, followed by the Internet and IT industries.\n\n![Application-layer DDoS attacks by industry](https://blog.cloudflare.com/content/images/2021/11/image6-5.png)\n\n### Application-layer DDoS attacks by source country\n\nTo understand the origin of the HTTP attacks, we look at the geolocation of the source IP address belonging to the client that generated the attack HTTP requests. Unlike network-layer attacks, source IPs cannot be [spoofed](https://www.cloudflare.com/learning/ddos/glossary/ip-spoofing/) in HTTP attacks. A high DDoS activity rate in a given country usually indicates the presence of botnets operating from within.\n\nIn the third quarter of 2021, most attacks originated from devices/servers in China, the United States, and India. While China remains in first place, the number of attacks originating from Chinese IPs actually decreased by 30% compared to the previous quarter. Almost one out of every 200 HTTP requests that originated from China was part of an HTTP DDoS attack.\n\nAdditionally, attacks from Brazil and Germany shrank by 38% compared to the previous quarter. Attacks originating from the US and Malaysia reduced by 40% and 45%, respectively.\n\n![](https://blog.cloudflare.com/content/images/2021/11/image31.png)\n\n### Application-layer DDoS attacks by target country\n\nIn order to identify which countries are targeted the most by L7 attacks, we break down the DDoS activity by our customers’ billing countries.\n\nFor the second consecutive time this year, organizations in the United States were targeted the most by L7 DDoS attacks in the world, followed by those in the UK and Canada.\n\n![](https://blog.cloudflare.com/content/images/2021/11/image10-7.png)\n\n## Network-layer DDoS attacks\n\nWhile application-layer attacks target the application (Layer 7 of the [OSI model](https://www.cloudflare.com/learning/ddos/glossary/open-systems-interconnection-model-osi/)) running the service that end users are trying to access, [network-layer attacks](https://www.cloudflare.com/learning/ddos/layer-3-ddos-attacks/) aim to overwhelm network infrastructure (such as in-line routers and servers) and the Internet link itself.\n\n#### Mirai-variant botnet strikes with a force of 1.2 Tbps.\n\nQ3 ‘21 was also the quarter when the infamous Mirai made a resurgence. A Mirai-variant botnet launched over a dozen UDP- and TCP-based DDoS attacks that peaked multiple times above 1 Tbps, with a max peak of approximately 1.2 Tbps. These network-layer attacks targeted Cloudflare customers on the [Magic Transit](https://www.cloudflare.com/magic-transit/) and [Spectrum](https://www.cloudflare.com/products/cloudflare-spectrum/) services. One of these targets was a major APAC-based Internet services, telecommunications, and hosting provider and the other was a gaming company. In all cases, the attacks were automatically detected and mitigated without human intervention.\n\n### Network-layer DDoS attacks by month\n\n#### September was, by far, the busiest month for attackers this year.\n\nQ3 ‘21 accounted for more than 38% of all attacks this year. September was the busiest month for attackers so far in 2021 — accounting for over 16% of all attacks this year.\n\n![](https://blog.cloudflare.com/content/images/2021/11/image20.png)\n\n### Network-layer DDoS attacks by attack rate\n\n**Most attacks are ‘small’ in size, but the number of larger attacks continues to rise.**\n\nThere are different ways of measuring the size of a L3/4 DDoS attack. One is the volume of traffic it delivers, measured as the bit rate (specifically, terabits per second or gigabits per second). Another is the number of packets it delivers, measured as the packet rate (specifically, millions of packets per second).\n\nAttacks with high bit rates attempt to cause a denial-of-service event by clogging the Internet link, while attacks with high packet rates attempt to overwhelm the servers, routers, or other in-line hardware appliances. Appliances dedicate a certain amount of memory and computation power to process each packet. Therefore, by bombarding it with many packets, the appliance can be left with no further processing resources. In such a case, packets are “dropped,” i.e., the appliance is unable to process them. For users, this results in service disruptions and denial of service.\n\nThe distribution of attacks by their size (in bit rate) and month is shown below. Interestingly enough, all attacks over 400 Gbps took place in August, including some of the largest attacks we have seen; multiple attacks peaked above 1 Tbps and reached as high as 1.2 Tbps.\n\n![](https://blog.cloudflare.com/content/images/2021/11/image8-6.png)\n\n#### Packet rate\n\nAs seen in previous quarters, the majority of attacks observed in Q3 ‘21 were relatively small in size — nearly 89% of all attacks peaked below 50K packets per second (pps). While a majority of attacks are smaller in size, we observed that the number of larger attacks is increasing QoQ — attacks that peaked above 10M pps increased by 142% QoQ.\n\n![](https://blog.cloudflare.com/content/images/2021/11/image16-1.png)\n\nAttacks of packet rates ranging from 1-10 million packets per second increased by 196% compared to the previous quarter. This trend is similar to what we observed the last quarter as well, suggesting that larger attacks are increasing.\n\n![](https://blog.cloudflare.com/content/images/2021/11/image22-1.png)\n\n#### Bit rate\n\nFrom the bit rate perspective, a similar trend was observed — a total of 95.4% of all attacks peaked below 500 Mbps.\n\n![](https://blog.cloudflare.com/content/images/2021/11/image11-4.png)\n\nQoQ data shows that the number of attacks of sizes ranging from 500 Mbps to 10 Gbps saw massive increases of 126% to 289% compared to the previous quarter. Attacks over 100 Gbps decreased by nearly 14%.\n\nThe number of larger bitrate attacks increased QoQ (with the one exception being attacks over 100 Gbps, which decreased by nearly 14% QoQ). In particular, attacks ranging from 500 Mbps to 1 Gbps saw a surge of 289% QoQ and those ranging from 1 Gbps to 100 Gbps surged by 126%.\n\nThis trend once again illustrates that, while (in general) a majority of the attacks are indeed smaller, the number of “larger” attacks is increasing. This suggests that more attackers are garnering more resources to launch larger attacks.\n\n![](https://blog.cloudflare.com/content/images/2021/11/image2-3.png)\n\n### Network-layer DDoS attacks by duration\n\n#### Most attacks remain under one hour in duration, reiterating the need for automated always-on DDoS mitigation solutions.\n\nWe measure the duration of an attack by recording the difference between when it is first detected by our systems as an attack and the last packet we see with that attack signature. As in previous quarters, most of the attacks are short-lived. To be specific, 94.4% of all DDoS attacks lasted less than an hour. On the other end of the axis, attacks over 6 hours accounted for less than 0.4% in Q3 ‘21, and we did see a QoQ increase of 165% in attacks ranging 1-2 hours. Be that as it may, a longer attack does not necessarily mean a more dangerous one.\n\n![](https://blog.cloudflare.com/content/images/2021/11/image23-1.png)\n\nShort attacks can easily go undetected, especially burst attacks that, within seconds, bombard a target with a significant number of packets, bytes, or requests. In this case, DDoS protection services that rely on manual mitigation by security analysis have no chance in mitigating the attack in time. They can only learn from it in their post-attack analysis, then deploy a new rule that filters the attack fingerprint and hope to catch it next time. Similarly, using an “on-demand” service, where the security team will redirect traffic to a DDoS provider during the attack, is also inefficient because the attack will already be over before the traffic routes to the on-demand DDoS provider.\n\nCloudflare recommends that companies use automated, always-on DDoS protection services that analyze traffic and apply real-time fingerprinting fast enough to block the short-lived attacks. Cloudflare analyzes traffic out-of-path, ensuring that DDoS mitigation does not add any latency to legitimate traffic, even in always-on deployments. Once an attack is identified, our autonomous edge DDoS protection system ([dosd](https://blog.cloudflare.com/deep-dive-cloudflare-autonomous-edge-ddos-protection/)) generates and applies a dynamically crafted rule with a real-time signature. Pre-configured firewall rules comprising allow/deny lists for known traffic patterns take effect immediately.\n\n### Attack vectors\n\n#### SYN floods remain attackers’ favorite method of attack, while attacks over DTLS saw a massive surge — 3,549% QoQ.\n\nAn attack vector is the term used to describe the method that the attacker utilizes in their attempt to cause a denial-of-service event.\n\nAs observed in previous quarters, attacks utilizing SYN floods remain the most popular method used by attackers.\n\nA [SYN flood](https://www.cloudflare.com/learning/ddos/syn-flood-ddos-attack/) attack is a DDoS attack that works by exploiting the very foundation of the TCP protocol — the stateful TCP connection between a client and a server as a part of the 3-way TCP handshake. As a part of the TCP handshake, the client sends an initial connection request packet with a synchronize flag (SYN). The server responds with a packet that contains a synchronized acknowledgment flag (SYN-ACK). Finally, the client responds with an acknowledgment (ACK) packet. At this point, a connection is established and data can be exchanged until the connection is closed. This stateful process can be abused by attackers to cause denial-of-service events.\n\nBy repeatedly sending SYN packets, the attacker attempts to overwhelm a server or the router’s connection table that tracks the state of TCP connections. The server replies with a SYN-ACK packet, allocates a certain amount of memory for each given connection, and falsely waits for the client to respond with the final ACK. Given a sufficient number of connections occupying the server’s memory, the server is unable to allocate further memory for legitimate clients, causing the server to crash or preventing it from handling legitimate client connections, i.e., a denial-of-service event.\n\nMore than half of all attacks observed over our network were SYN floods. This was followed by RST, ACK, and UDP floods.\n\n![](https://blog.cloudflare.com/content/images/2021/11/image4-3.png)\n\n### Emerging threats\n\nWhile SYN and RST floods remain popular overall, when we look at emerging attack vectors — which helps us understand what new vectors attackers are deploying to launch attacks — we observed a massive spike in [DTLS](https://en.wikipedia.org/wiki/Datagram_Transport_Layer_Security) amplification attacks. DTLS floods increased by 3,549% QoQ.\n\nDatagram Transport Layer Security (DTLS) is a protocol similar to Transport Layer Security ([TLS](https://www.cloudflare.com/learning/ssl/transport-layer-security-tls/)) designed to provide similar security guarantees to connectionless datagram-based applications to prevent message forgery, eavesdropping, or tampering. DTLS, being connectionless, is specifically useful for establishing VPN connections, without the [TCP meltdown](https://openvpn.net/faq/what-is-tcp-meltdown/) problem. The application is responsible for reordering and other connection properties.\n\nJust as with most UDP-based protocols, DTLS is spoofable and being used by attackers to generate reflection amplification attacks to overwhelm network gateways.\n\n![](https://blog.cloudflare.com/content/images/2021/11/image18-1.png)\n\n### Network-layer DDoS attacks by country\n\n**While Morocco topped the charts in terms of the highest network attack rate observed, Asian countries closely followed.**\n\nWhen analyzing network-layer DDoS attacks, we bucket the traffic by the Cloudflare edge data center locations where the traffic was ingested, and not by the source IP. The reason for this is that, when attackers launch network-layer attacks, they can [spoof](https://www.cloudflare.com/learning/ddos/glossary/ip-spoofing/) the source IP address in order to obfuscate the attack source and introduce randomness into the attack properties, which may make it harder for simple DDoS protection systems to block the attack. Hence, if we were to derive the source country based on a spoofed source IP, we would get a spoofed country.\n\nCloudflare is able to overcome the challenges of spoofed IPs by displaying the attack data by the location of the Cloudflare data center in which the attack was observed. We are able to achieve geographical accuracy in our report because we have data centers in [over 250 cities](https://www.cloudflare.com/network) around the world.\n\n#### Worldwide\n\n![](https://blog.cloudflare.com/content/images/2021/11/image12-7.png)\n\nTo view all regions and countries, check out the [Radar DDoS Report dashboard’s interactive map](https://radar.cloudflare.com/notebooks/ddos-2021-q3#network-layer-ddos-attacks-by-country).\n\n### A note on recent attacks on voice over-IP service providers — and ransom DDoS attacks\n\n![](https://blog.cloudflare.com/content/images/2021/11/image25-1.png)\n\nWe [recently reported](https://blog.cloudflare.com/attacks-on-voip-providers/) and [provided an update](https://blog.cloudflare.com/update-on-voip-attacks/) on the surge in DDoS attacks on VoIP service providers — some of who have also received ransom threats. As of early Q4 ‘21, this attack campaign is still ongoing and current. At Cloudflare, we continue to onboard VoIP service providers and shield their applications and networks against attacks.\n\nHTTP attacks against [API gateways](https://www.cloudflare.com/learning/security/api/what-is-an-api-gateway/) and the corporate websites of the providers have been combined with network-layer and transport-layer attacks against VoIP infrastructures.\n\nExamples include:\n\n1.  ******TCP floods targeting stateful firewalls:****** These are being used in “trial-and-error” type attacks. They are not very effective against telephony infrastructure specifically (because it is mostly UDP) but very effective at overwhelming stateful firewalls.\n2.  ******UDP floods targeting SIP infrastructure:****** Floods of UDP traffic that have no well-known fingerprint, aimed at critical VoIP services. Generic floods like this may look like legitimate traffic to unsophisticated filtering systems.\n3.  ******UDP reflection targeting SIP infrastructure:****** These methods, when targeted at SIP or RTP services, can easily overwhelm Session Border Controllers (SBCs) and other telephony infrastructure. The attacker seems to learn enough about the target’s infrastructure to target such services with high precision.\n4.  ******SIP protocol-specific attacks:****** Attacks at the application layer are of particular concern because of the higher resource cost of generating application errors versus filtering on network devices.\n\nOrganizations also continue to receive ransom notes that threaten attacks in exchange for bitcoin. [Ransomware](https://www.cloudflare.com/learning/security/ransomware/what-is-ransomware/) and [ransom DDoS](https://www.cloudflare.com/learning/ddos/ransom-ddos-attack/) attacks, for the fourth consecutive quarter, continue to be a germane threat to organizations all over the world.\n\nCloudflare products close off several threat vectors that can lead to a ransomware infection and ransom DDoS attacks:\n\n*   Cloudflare [DNS filtering](https://www.cloudflare.com/learning/access-management/what-is-dns-filtering/) blocks unsafe websites.\n*   Cloudflare [Browser Isolation](https://www.cloudflare.com/teams/browser-isolation/) prevents drive-by downloads and other browser-based attacks.\n*   A [Zero Trust](https://www.cloudflare.com/learning/security/glossary/what-is-zero-trust/) architecture can help [prevent ransomware from spreading](https://www.cloudflare.com/learning/security/ransomware/how-to-prevent-ransomware/) within a network.\n*   [Magic Transit](https://www.cloudflare.com/magic-transit/) protects organizations’ networks against [DDoS attacks](https://www.cloudflare.com/learning/ddos/how-to-prevent-ddos-attacks/) using BGP route redistribution — without impacting latency.\n\n## Helping build a better Internet\n\nCloudflare was founded on the mission to help build a better Internet. And part of that mission is to build an Internet where the impact of DDoS attacks is a thing of the past. Over the last 10 years, we have been unwavering in our efforts to protect our customers’ Internet properties from DDoS attacks of any size or kind. In [2017](https://blog.cloudflare.com/unmetered-mitigation/), we announced [unmetered DDoS protection](https://www.cloudflare.com/ddos/) for free — as part of every Cloudflare service and plan, including the Free plan — to make sure every organization can stay protected and available. Organizations big and small have joined Cloudflare over the past several years to ensure their websites, applications, and networks are secure from DDoS attacks, and remain fast and reliable.\n\nBut cyberattacks come in various forms, not just DDoS attacks. Malicious bots, ransomware attacks, email phishing, and VPN / remote access hacks are some many attacks that continue to plague organizations of all sizes globally. These attacks target websites, APIs, applications, and entire networks — which form the lifeblood of any online business. That is why the [Cloudflare security portfolio](https://www.cloudflare.com/security/) accounts for everything and everyone connected to the Internet.\n\nTo learn more about [Cloudflare DDoS](https://www.cloudflare.com/ddos) or our [network services](https://www.cloudflare.com/network-services/), [create an account](https://dash.cloudflare.com/) or [reach out to us](https://www.cloudflare.com/enterprise).\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[DDoS](https://blog.cloudflare.com/tag/ddos/) [Attacks](https://blog.cloudflare.com/tag/attacks/) [Cloudflare Radar](https://blog.cloudflare.com/tag/cloudflare-radar/) [RDDoS](https://blog.cloudflare.com/tag/rddos/) [REvil](https://blog.cloudflare.com/tag/revil/)"
    },
    {
      "url": "https://blog.cloudflare.com/zh-tw/forrester-wave-edge-development-2023-zh-tw/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/zh-tw/forrester-wave-edge-development-2023-zh-tw/",
        "loadedTime": "2023-12-05T02:33:09.462Z",
        "referrerUrl": "https://blog.cloudflare.com/forrester-wave-edge-development-2023/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/zh-tw/forrester-wave-edge-development-2023-zh-tw/",
        "title": "Cloudflare 被 Forrester Edge Development Platforms Wave 評為 2023 年第四季度領導者",
        "description": "Forrester 將 Cloudflare 評為 2023 年第四季度「Forrester Wave™：邊緣開發平台」的領導者，在當前產品類別中得分最高。",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Forrester 將 Cloudflare 評為 2023 年第四季度「Forrester Wave™：邊緣開發平台」的領導者，在當前產品類別中得分最高。\n根據首席分析師 Devin Dickerson 的報告，「Cloudflare 的邊緣開發平台為企業提供了建立完整堆疊分散式應用程式所需的構建區塊，使開發人員無需成為 CAP 定理的專家即可利用全球分散式運算、儲存和可程式設計安全網路。」\n超過 100 萬名開發人員正在使用開發人員平台產品構建應用程式，其中包括 Workers、Pages、R2、KV、Queues、Durable Objects、D1、Stream、Images等。開發人員可以使用 Cloudflare 的全套運算、儲存和開發人員服務，輕鬆部署高度分散式完整堆疊應用程式。\nWorkers 使Cloudflare 的網路可程式設計\n「該平台的一個關鍵優勢是與 Cloudflare 的可程式設計全球 CDN 的互通性以及利用智慧工作負載放置的部署模型。」– The Forrester Wave™：邊緣開發平台，2023 年第四季度\nWorkers 在Cloudflare 的全球網路上執行，提供 API 來直接讀取和寫入本機快取，並直接在 Worker 接收的請求物件上公開來自 Cloudflare CDN 的上下文。\n與 Cloudflare 網路的緊密整合使開發人員能夠構建、保護和連線全球分散式應用程式，而無需部署到特定區域。Smart Placement 可最佳化 Workers，以在產生最快整體效能的位置執行，可能是距離資料最近的位置，或者是距離使用者最近的位置。Hyperdrive 自動集中資料庫連線，允許在世界各地執行的 Workers 在查詢 PostgreSQL 資料庫時重用它們，從而避免擴展挑戰，導致難以在無伺服器架構中使用傳統資料庫。Cron Triggers 允許長達 15 分鐘的 CPU 時間，允許運算密集型背景工作。\nCloudflare 超越了邊緣運算——它是無處不在的運算。我們利用自己的網路使您的應用程式發揮最佳效能，該網路由真實世界的資料塑造並根據存取模式和程式設計範例進行定制。\n無需成為分散式系統專家即可部署分散式系統\n「推薦客戶一致強調了上線的簡便性，沒有任何背景的開發人員可以在幾分鐘內在全球範圍內交付工作負載，並在一周內生產出高品質的應用程式。」 – The Forrester Wave™：邊緣開發平台，2023 年第四季度\nWorkers 使任何開發人員都能夠部署全球分散式應用程式，而無需成為分散式系統專家或設定雲端基礎結構的專家。\n- 當您部署Worker 時，Cloudflare 會在幕後將其分發到全球範圍內。但對您來說，它是可以在本機執行和測試的單個應用程式，使用與您的 Workers 在生產中執行的相同開放原始碼 JavaScript 執行階段。\n- 當您部署 Durable Object 來協調即時狀態時，您就構建了一個分散式應用程式，但不必學習 RPC 通訊協定和擴展基礎結構，而是使用前端開發人員都知道且每天依賴的 Web 標準 API 在 JavaScript 中為整個應用程式進行了程式設計。\n- 使用 Cloudflare Queues 對訊息進行排隊和批量處理，只需在現有 Worker 中新增幾行 JavaScript 即可。\n- 當您使用 Cloudflare Pages 建立 Web 應用程式時，只需連線到 GitHub 存儲庫，您就可以使用預覽 URL 設定完整的連續構建和部署管道。\n以前只編寫前端程式碼的開發人員現在可以構建後端，使他們的應用程式具有即時性和反應性。因為等待基礎結構專家提供資源而僵持不前的團隊可以立即進行原型設計，而無需等到下周。編寫和部署 Worker 令人熟悉且易於理解，這使得工程團隊能夠更快地行動，並且開銷更少。\n為什麼團隊能夠如此迅速地開始？\nWorkers 使用前端開發人員和構建 Web 應用程式的任何人每天都在使用的 Web 標準 API。Cloudflare 是 Web 互通性執行階段社群群組 (WinterCG) 的創始成員，致力於執行階段之間的互通性。\n開發人員每天使用的工具都是我們平台的原生工具。我們為所有 API 發佈 TypeScript 類型，並支援在透過 Wrangler CLI 或透過 Cloudflare 儀表板（該儀表板本身由流行的 VSCode 編輯器提供支援）中的代碼編輯器創作和部署時編譯 TypeScript。\n支援開發人員喜歡用來構建的開放原始碼框架。Workers 執行階段原生提供了越來越多的來自 Node.js 的 API，允許現有的開放原始碼庫在 Workers 上運作。越來越多開發人員依賴的新開放原始碼專案從設計之初就能在所有 WinterCG 執行階段中運作。每天都有更多的 JavaScript 生態系統在 Workers 上運作。\n利用 GPU、LLM 等技術進軍人工智慧領域\n「其宏偉願景拒絕將未來的足跡限制在邊緣，他們目的明確地在路線圖上構建出各項功能，這表明它將越來越有能力在工作負載方面與公有雲端超級擴展器一較高下。」– The Forrester Wave™：邊緣開發平台，2023 年第四季度\n我們正在為大規模生產應用程式構建一個完整的運算平台。由於每個公司和每個開發人員現在都在構建或試驗 AI，Cloudflare 已將 GPU 整合成為我們開發人員平台的一部分。我們讓 AI 的入門變得和交付全球工作負載一樣簡單。11 月中旬，我們實現了在全球 100 多個城市執行 Workers AI Inference 的目標，到 2024 年底，Workers AI 將在 Cloudflare 開展業務的幾乎每個城市執行。\nWorkers AI 允許開發人員使用最新的開放原始碼 AI 模型構建應用程式，而無需佈建任何基礎結構或為昂貴的未使用容量付費。我們正在對此進行擴展，以支援直接從 Hugging Face 到 Workers AI 部署模型，以獲得更廣泛的 AI 模型集。與在特定資料中心佈建具有 GPU 的虛擬機器不同，我們構建此功能是為了將整個網路視為一個巨大的運算資源，在正確的時間在正確的位置執行模型來滿足開發人員的需求。\n除了模型推理之外，我們還加倍支援 Web 標準 API，並在 Workers 平台內提供 WebGPUAPI。雖然我們因為被認可為領先邊緣平台而感到自豪，但我們不僅如此，我們還是一個用於開發完整堆疊應用程式的平台，甚至是那些需要在一年前還很少使用或需要的運算能力的應用程式。\n我們很高興向您展示後續將推出的內容，包括跨Cloudflare 產品管理機密的新方法、改進的可觀察性以及用於發佈變更的更好工具。每天，我們都會看到在我們的平台上構建了更先進階應用程式，我們致力於將其與工具相匹配，以服務於對任務最關鍵的工作負載——我們自己也使用這些工具來在我們的平台中構建我們自己的產品。\n按一下此處下載報告。",
      "markdown": "Forrester 將 Cloudflare 評為 2023 年第四季度「Forrester Wave™：邊緣開發平台」的領導者，在當前產品類別中得分最高。\n\n根據首席分析師 Devin Dickerson 的[報告](https://www.cloudflare.com/lp/forrester-wave-edge-development-q4-2023/)，「Cloudflare 的邊緣開發平台為企業提供了建立完整堆疊分散式應用程式所需的構建區塊，使開發人員無需成為 CAP 定理的專家即可利用全球分散式運算、儲存和可程式設計安全網路。」\n\n超過 100 萬名開發人員正在使用開發人員平台產品構建應用程式，其中包括 [Workers](https://workers.cloudflare.com/)、[Pages](https://pages.cloudflare.com/)、[R2](https://developers.cloudflare.com/r2/)、[KV](https://developers.cloudflare.com/kv/)、[Queues](https://developers.cloudflare.com/queues/)、[Durable Objects](https://developers.cloudflare.com/durable-objects/)、[D1](https://developers.cloudflare.com/d1/)、[Stream](https://developers.cloudflare.com/stream/)、[Images](https://developers.cloudflare.com/images/)等。開發人員可以使用 Cloudflare 的全套運算、儲存和開發人員服務，輕鬆部署高度分散式完整堆疊應用程式。\n\n### Workers 使Cloudflare 的網路可程式設計\n\n> **_「該平台的一個關鍵優勢是與 Cloudflare 的可程式設計全球 CDN 的互通性以及利用智慧工作負載放置的部署模型。」– The Forrester Wave™：邊緣開發平台，2023 年第四季度_**\n\nWorkers 在[Cloudflare 的全球網路](https://www.cloudflare.com/network/)上執行，提供 [API](https://developers.cloudflare.com/workers/runtime-apis/cache/) 來直接讀取和寫入本機快取，並直接在 Worker 接收的[請求物件](https://developers.cloudflare.com/workers/runtime-apis/request/#incomingrequestcfproperties)上公開來自 Cloudflare CDN 的上下文。\n\n與 Cloudflare 網路的緊密整合使開發人員能夠構建、保護和連線全球分散式應用程式，而無需部署到特定區域。[Smart Placement](https://developers.cloudflare.com/workers/configuration/smart-placement/) 可最佳化 Workers，以在產生最快整體效能的位置執行，可能是距離資料最近的位置，或者是距離使用者最近的位置。[Hyperdrive](https://developers.cloudflare.com/hyperdrive/learning/how-hyperdrive-works/) 自動集中資料庫連線，允許在世界各地執行的 Workers 在查詢 PostgreSQL 資料庫時重用它們，從而避免擴展挑戰，導致難以在無伺服器架構中使用傳統資料庫。[Cron Triggers](https://developers.cloudflare.com/workers/configuration/cron-triggers/) 允許長達 15 分鐘的 CPU 時間，允許運算密集型背景工作。\n\nCloudflare 超越了邊緣運算——它是無處不在的運算。我們利用自己的網路使您的應用程式發揮最佳效能，該網路由真實世界的資料塑造並根據存取模式和程式設計範例進行定制。\n\n### 無需成為分散式系統專家即可部署分散式系統\n\n> **「推薦客戶一致強調了上線的簡便性，沒有任何背景的開發人員可以在幾分鐘內在全球範圍內交付工作負載，並在一周內生產出高品質的應用程式。」 – The Forrester Wave™：邊緣開發平台，2023 年第四季度**\n\nWorkers 使任何開發人員都能夠部署全球分散式應用程式，而無需成為分散式系統專家或設定雲端基礎結構的專家。\n\n\\- 當您部署Worker 時，Cloudflare 會在幕後將其分發到全球範圍內。但對您來說，它是可以在[本機執行和測試](https://developers.cloudflare.com/workers/observability/local-development-and-testing/)的單個應用程式，使用與您的 Workers 在生產中執行的相同[開放原始碼 JavaScript 執行階段](https://github.com/cloudflare/workerd)。\n\n\\- 當您部署 [Durable Object](https://developers.cloudflare.com/durable-objects/) 來協調即時狀態時，您就構建了一個分散式應用程式，但不必學習 RPC 通訊協定和擴展基礎結構，而是使用前端開發人員都知道且每天依賴的 Web 標準 API 在 JavaScript 中為整個應用程式進行了程式設計。\n\n\\- 使用 [Cloudflare Queues](https://developers.cloudflare.com/queues/) 對訊息進行排隊和批量處理，只需在現有 Worker 中新增幾行 JavaScript 即可。\n\n\\- 當您使用 [Cloudflare Pages](https://pages.cloudflare.com/) 建立 Web 應用程式時，只需連線到 GitHub 存儲庫，您就可以使用預覽 URL 設定完整的連續構建和部署管道。\n\n以前只編寫前端程式碼的開發人員現在可以構建後端，使他們的應用程式具有即時性和反應性。因為等待基礎結構專家提供資源而僵持不前的團隊可以立即進行原型設計，而無需等到下周。編寫和部署 Worker 令人熟悉且易於理解，這使得工程團隊能夠更快地行動，並且開銷更少。\n\n為什麼團隊能夠如此迅速地開始？\n\n1.  Workers 使用前端開發人員和構建 Web 應用程式的任何人每天都在使用的 [Web 標準 API](https://developers.cloudflare.com/workers/runtime-apis/)。Cloudflare 是 Web 互通性執行階段社群群組 ([WinterCG](https://wintercg.org/)) 的創始成員，致力於[執行階段之間的互通性](https://blog.cloudflare.com/socket-api-works-javascript-runtimes-wintercg-polyfill-connect/)。\n2.  開發人員每天使用的工具都是我們平台的原生工具。我們為所有 API 發佈 [TypeScript](https://www.npmjs.com/package/@cloudflare/workers-types) 類型，並支援在透過 Wrangler CLI 或透過     Cloudflare 儀表板（該儀表板本身由流行的     VSCode 編輯器[提供支援](https://blog.cloudflare.com/improved-quick-edit/)）中的代碼編輯器創作和部署時編譯 TypeScript。\n3.  支援開發人員喜歡用來構建的開放原始碼[框架](https://developers.cloudflare.com/pages/framework-guides/)。Workers 執行階段[原生提供](https://developers.cloudflare.com/workers/runtime-apis/nodejs/)了越來越多的來自 Node.js 的 API，允許現有的開放原始碼庫在 Workers 上運作。越來越多開發人員依賴的新開放原始碼專案從設計之初就能在所有 WinterCG 執行階段中運作。每天都有更多的 JavaScript 生態系統在 Workers 上運作。\n\n### 利用 GPU、LLM 等技術進軍人工智慧領域\n\n> **_「其宏偉願景拒絕將未來的足跡限制在邊緣，他們目的明確地在路線圖上構建出各項功能，這表明它將越來越有能力在工作負載方面與公有雲端超級擴展器一較高下。」– The Forrester Wave™：邊緣開發平台，2023 年第四季度_**\n\n我們正在為大規模生產應用程式構建一個完整的運算平台。由於每個公司和每個開發人員現在都在構建或試驗 AI，Cloudflare 已[將 GPU 整合成為我們開發人員平台的一部分](https://blog.cloudflare.com/workers-ai)。我們讓 AI 的入門變得和交付全球工作負載一樣簡單。11 月中旬，我們實現了在全球 [100 多個城市](https://blog.cloudflare.com/workers-ai-update-stable-diffusion-code-llama-workers-ai-in-100-cities/)執行 Workers AI Inference 的目標，到 2024 年底，Workers AI 將在 Cloudflare 開展業務的幾乎每個城市執行。\n\n[Workers AI](https://developers.cloudflare.com/workers-ai/) 允許開發人員使用最新的開放原始碼 AI 模型構建應用程式，而無需佈建任何基礎結構或為昂貴的未使用容量付費。我們正在對此進行擴展，以支援[直接從 Hugging Face 到 Workers AI 部署模型](https://blog.cloudflare.com/partnering-with-hugging-face-deploying-ai-easier-affordable/)，以獲得更廣泛的 AI 模型集。與在特定資料中心佈建具有 GPU 的虛擬機器不同，我們構建此功能是為了將整個網路視為一個巨大的運算資源，在正確的時間在正確的位置執行模型來滿足開發人員的需求。\n\n除了模型推理之外，我們還加倍支援 Web 標準 API，並在 Workers 平台內提供 [WebGPU](https://blog.cloudflare.com/webgpu-in-workers/)API。雖然我們因為被認可為領先邊緣平台而感到自豪，但我們不僅如此，我們還是一個用於開發完整堆疊應用程式的平台，甚至是那些需要在一年前還很少使用或需要的運算能力的應用程式。\n\n我們很高興向您展示後續將推出的內容，包括跨Cloudflare 產品[管理機密的新方法](https://blog.cloudflare.com/secrets-store/)、改進的可觀察性以及用於發佈變更的更好工具。每天，我們都會看到在我們的平台上構建了更先進階應用程式，我們致力於將其與工具相匹配，以服務於對任務最關鍵的工作負載——我們自己也使用這些工具來在我們的平台中構建我們自己的產品。\n\n按一下[此處](https://www.cloudflare.com/lp/forrester-wave-edge-development-q4-2023/)下載報告。"
    },
    {
      "url": "https://blog.cloudflare.com/zh-cn/forrester-wave-edge-development-2023-zh-cn/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/zh-cn/forrester-wave-edge-development-2023-zh-cn/",
        "loadedTime": "2023-12-05T02:33:07.061Z",
        "referrerUrl": "https://blog.cloudflare.com/forrester-wave-edge-development-2023/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/zh-cn/forrester-wave-edge-development-2023-zh-cn/",
        "title": "Cloudflare 被 Forrester Edge Development Platforms Wave 评为 2023 年第四季度领导者",
        "description": "Forrester 将 Cloudflare 评为 2023 年第四季度 “Forrester Wave™：边缘开发平台”的领导者，在当前产品类别中得分最高。",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Forrester 将 Cloudflare 评为 2023 年第四季度 “Forrester Wave™：边缘开发平台”的领导者，在当前产品类别中得分最高。\n根据首席分析师 Devin Dickerson 的报告，“Cloudflare 的边缘开发平台为企业提供了创建全栈分布式应用程序所需的构建块，使开发人员无需成为 CAP 定理的专家即可利用全球分布式计算、存储和可编程安全网络。”\n超过 100 万名开发人员正在使用开发人员平台产品构建应用程序，其中包括 Workers、Pages、R2、KV、Queues、Durable Objects、D1、Stream、Images等。开发人员可以使用 Cloudflare 的全套计算、存储和开发人员服务，轻松部署高度分布式全栈应用程序。\nWorkers 使 Cloudflare 的网络可编程\n“该平台的一个关键优势是与 Cloudflare 的可编程全球 CDN 的互操作性以及利用智能工作负载放置的部署模型。”– The Forrester Wave™：边缘开发平台，2023 年第四季度\nWorkers 在 Cloudflare 的全球网络上运行，提供 API 来直接读取和写入本地缓存，并直接在 Worker 接收的请求对象上公开来自 Cloudflare CDN 的上下文。\n与 Cloudflare 网络的紧密集成使开发人员能够构建、保护和连接全球分布式应用程序，而无需部署到特定区域。Smart Placement 可优化 Workers，以在产生最快整体性能的位置运行，可能是距离数据最近的位置，或者是距离用户最近的位置。Hyperdrive 自动集中数据库连接，允许在世界各地运行的 Workers 在查询 PostgreSQL 数据库时重用它们，从而避免扩展挑战，导致难以在无服务器架构中使用传统数据库。Cron Triggers 允许长达 15 分钟的 CPU 时间，允许计算密集型后台工作。\nCloudflare 超越了边缘计算——它是无处不在的计算。我们利用自己的网络使您的应用程序发挥最佳性能，该网络由真实世界的数据塑造并根据访问模式和编程范例进行定制。\n无需成为分布式系统专家即可部署分布式系统\n“推荐客户一致强调了上线的简便性，没有任何背景的开发人员可以在几分钟内在全球范围内交付工作负载，并在一周内生产出高质量的应用程序。” – The Forrester Wave™：边缘开发平台，2023 年第四季度\nWorkers 使任何开发人员都能够部署全球分布式应用程序，而无需成为分布式系统专家或配置云基础设施的专家。\n- 当您部署Worker 时，Cloudflare 会在幕后将其分发到全球范围内。但\n对您来说，它是可以在本地运行和测试的单个应用程序，使用与您的 Workers 在生产中运行的相同开源 JavaScript 运行时。\n- 当您部署 Durable Object 来协调实时状态时，您就构建了一个分布式应用程序，但不必学习 RPC 协议和扩展基础设施，而是使用前端开发人员都知道且每天依赖的 Web 标准 API 在 JavaScript 中编写了整个应用程序。\n- 使用 Cloudflare Queues 对消息进行排队和批量处理，只需在现有 Worker 中添加几行 JavaScript 即可。\n- 当您使用 Cloudflare Pages 创建 Web 应用程序时，只需连接到 GitHub 存储库，您就可以使用预览 URL 设置完整的连续构建和部署管道。\n以前只编写前端代码的开发人员现在可以构建后端，使他们的应用程序具有实时性和反应性。因为等待基础设施专家提供资源而僵持不前的团队可以立即进行原型设计，而无需等到下周。编写和部署 Worker 令人熟悉且易于理解，这使得工程团队能够更快地行动，并且开销更少。\n为什么团队能够如此迅速地开始？\nWorkers 使用前端开发人员和构建 Web 应用程序的任何人每天都在使用的 Web 标准 API。Cloudflare 是 Web 互操作运行时社区小组 (WinterCG) 的创始成员，致力于运行时之间的互操作性。\n开发人员每天使用的工具都是我们平台的原生工具。我们为所有 API 发布 TypeScript 类型，并支持在通过 Wrangler CLI 或通过 Cloudflare 仪表板（该仪表板本身由流行的 VSCode 编辑器提供支持）中的代码编辑器创作和部署时编译 TypeScript。\n支持开发人员喜欢用来构建的开源框架。Workers 运行时原生提供了越来越多的来自 Node.js 的 API，允许现有的开源库在 Workers 上运行。越来越多开发人员依赖的新开源项目从设计之初就能在所有 WinterCG 运行时中运行。每天都有更多的 JavaScript 生态系统在 Workers 上运行。\n利用 GPU、LLM 等技术进军人工智能领域\n“其宏伟愿景拒绝将未来的足迹限制在边缘，他们目的明确地在路线图上构建出各项功能，这表明它将越来越有能力在工作负载方面与公共云超级扩展器一较高下。”– The Forrester Wave™：边缘开发平台，2023 年第四季度\n我们正在为大规模生产应用程序构建一个完整的计算平台。由于每个公司和每个开发人员现在都在构建或试验 AI，Cloudflare 已将 GPU 集成为我们开发人员平台的一部分。我们让 AI 的入门变得和交付全球工作负载一样简单。11 月中旬，我们实现了在全球 100 多个城市运行 Workers AI Inference 的目标，到 2024 年底，Workers AI 将在 Cloudflare 开展业务的几乎每个城市运行。\nWorkers AI 允许开发人员使用最新的开源 AI 模型构建应用程序，而无需配置任何基础设施或为昂贵的未使用容量付费。我们正在对此进行扩展，以支持直接从 Hugging Face 到 Workers AI 部署模型，以获得更广泛的 AI 模型集。与在特定数据中心配置具有 GPU 的虚拟机不同，我们构建此功能是为了将整个网络视为一个巨大的计算资源，在正确的时间在正确的位置运行模型来满足开发人员的需求。\n除了模型推理之外，我们还加倍支持 Web 标准 API，并在 Workers 平台内提供 WebGPUAPI。虽然我们因为被认可为领先边缘平台而感到自豪，但我们不仅如此，我们还是一个用于开发全栈应用程序的平台，甚至是那些需要在一年前还很少使用或需要的计算能力的应用程序。\n我们很高兴向您展示后续将推出的内容，包括跨Cloudflare 产品管理机密的新方法、改进的可观察性以及用于发布更改的更好工具。每天，我们都会看到在我们的平台上构建了更先进的应用程序，我们致力于将其与工具相匹配，以服务于对任务最关键的工作负载——我们自己也使用这些工具来在我们的平台中构建我们自己的产品。\n点击此处下载报告。",
      "markdown": "Forrester 将 Cloudflare 评为 2023 年第四季度 “Forrester Wave™：边缘开发平台”的领导者，在当前产品类别中得分最高。\n\n根据首席分析师 Devin Dickerson 的[报告](https://www.cloudflare.com/lp/forrester-wave-edge-development-q4-2023/)，“Cloudflare 的边缘开发平台为企业提供了创建全栈分布式应用程序所需的构建块，使开发人员无需成为 CAP 定理的专家即可利用全球分布式计算、存储和可编程安全网络。”\n\n超过 100 万名开发人员正在使用开发人员平台产品构建应用程序，其中包括 [Workers](https://workers.cloudflare.com/)、[Pages](https://pages.cloudflare.com/)、[R2](https://developers.cloudflare.com/r2/)、[KV](https://developers.cloudflare.com/kv/)、[Queues](https://developers.cloudflare.com/queues/)、[Durable Objects](https://developers.cloudflare.com/durable-objects/)、[D1](https://developers.cloudflare.com/d1/)、[Stream](https://developers.cloudflare.com/stream/)、[Images](https://developers.cloudflare.com/images/)等。开发人员可以使用 Cloudflare 的全套计算、存储和开发人员服务，轻松部署高度分布式全栈应用程序。\n\n### Workers 使 Cloudflare 的网络可编程\n\n> **_“该平台的一个关键优势是与 Cloudflare 的可编程全球 CDN 的互操作性以及利用智能工作负载放置的部署模型。”– The Forrester Wave™：边缘开发平台，2023 年第四季度_**\n\nWorkers 在 [Cloudflare 的全球网络](https://www.cloudflare.com/network/)上运行，提供 [API](https://developers.cloudflare.com/workers/runtime-apis/cache/) 来直接读取和写入本地缓存，并直接在 Worker 接收的[请求对象](https://developers.cloudflare.com/workers/runtime-apis/request/#incomingrequestcfproperties)上公开来自 Cloudflare CDN 的上下文。\n\n与 Cloudflare 网络的紧密集成使开发人员能够构建、保护和连接全球分布式应用程序，而无需部署到特定区域。[Smart Placement](https://developers.cloudflare.com/workers/configuration/smart-placement/) 可优化 Workers，以在产生最快整体性能的位置运行，可能是距离数据最近的位置，或者是距离用户最近的位置。[Hyperdrive](https://developers.cloudflare.com/hyperdrive/learning/how-hyperdrive-works/) 自动集中数据库连接，允许在世界各地运行的 Workers 在查询 PostgreSQL 数据库时重用它们，从而避免扩展挑战，导致难以在无服务器架构中使用传统数据库。[Cron Triggers](https://developers.cloudflare.com/workers/configuration/cron-triggers/) 允许长达 15 分钟的 CPU 时间，允许计算密集型后台工作。\n\nCloudflare 超越了边缘计算——它是无处不在的计算。我们利用自己的网络使您的应用程序发挥最佳性能，该网络由真实世界的数据塑造并根据访问模式和编程范例进行定制。\n\n### 无需成为分布式系统专家即可部署分布式系统\n\n> **_“推荐客户一致强调了上线的简便性，没有任何背景的开发人员可以在几分钟内在全球范围内交付工作负载，并在一周内生产出高质量的应用程序。” – The Forrester Wave™：边缘开发平台，2023 年第四季度_**\n\nWorkers 使任何开发人员都能够部署全球分布式应用程序，而无需成为分布式系统专家或配置云基础设施的专家。\n\n\\- 当您部署Worker 时，Cloudflare 会在幕后将其分发到全球范围内。但  \n对您来说，它是可以在[本地运行和测试](https://developers.cloudflare.com/workers/observability/local-development-and-testing/)的单个应用程序，使用与您的 Workers 在生产中运行的相同[开源 JavaScript 运行时](https://github.com/cloudflare/workerd)。\n\n\\- 当您部署 [Durable Object](https://developers.cloudflare.com/durable-objects/) 来协调实时状态时，您就构建了一个分布式应用程序，但不必学习 RPC 协议和扩展基础设施，而是使用前端开发人员都知道且每天依赖的 Web 标准 API 在 JavaScript 中编写了整个应用程序。\n\n\\- 使用 [Cloudflare Queues](https://developers.cloudflare.com/queues/) 对消息进行排队和批量处理，只需在现有 Worker 中添加几行 JavaScript 即可。\n\n\\- 当您使用 [Cloudflare Pages](https://pages.cloudflare.com/) 创建 Web 应用程序时，只需连接到 GitHub 存储库，您就可以使用预览 URL 设置完整的连续构建和部署管道。\n\n以前只编写前端代码的开发人员现在可以构建后端，使他们的应用程序具有实时性和反应性。因为等待基础设施专家提供资源而僵持不前的团队可以立即进行原型设计，而无需等到下周。编写和部署 Worker 令人熟悉且易于理解，这使得工程团队能够更快地行动，并且开销更少。\n\n为什么团队能够如此迅速地开始？\n\n1.  Workers 使用前端开发人员和构建 Web 应用程序的任何人每天都在使用的 [Web 标准 API](https://developers.cloudflare.com/workers/runtime-apis/)。Cloudflare 是 Web 互操作运行时社区小组 ([WinterCG](https://wintercg.org/)) 的创始成员，致力于[运行时之间的互操作性](https://blog.cloudflare.com/socket-api-works-javascript-runtimes-wintercg-polyfill-connect/)。\n2.  开发人员每天使用的工具都是我们平台的原生工具。我们为所有 API 发布 [TypeScript](https://www.npmjs.com/package/@cloudflare/workers-types) 类型，并支持在通过 Wrangler CLI 或通过     Cloudflare 仪表板（该仪表板本身由流行的     VSCode 编辑器[提供支持](https://blog.cloudflare.com/improved-quick-edit/)）中的代码编辑器创作和部署时编译 TypeScript。\n3.  支持开发人员喜欢用来构建的开源[框架](https://developers.cloudflare.com/pages/framework-guides/)。Workers 运行时[原生提供](https://developers.cloudflare.com/workers/runtime-apis/nodejs/)了越来越多的来自 Node.js 的 API，允许现有的开源库在 Workers 上运行。越来越多开发人员依赖的新开源项目从设计之初就能在所有 WinterCG 运行时中运行。每天都有更多的 JavaScript 生态系统在 Workers 上运行。\n\n### 利用 GPU、LLM 等技术进军人工智能领域\n\n> **_“其宏伟愿景拒绝将未来的足迹限制在边缘，他们目的明确地在路线图上构建出各项功能，这表明它将越来越有能力在工作负载方面与公共云超级扩展器一较高下。”– The Forrester Wave™：边缘开发平台，2023 年第四季度_**\n\n我们正在为大规模生产应用程序构建一个完整的计算平台。由于每个公司和每个开发人员现在都在构建或试验 AI，Cloudflare 已[将 GPU 集成为我们开发人员平台的一部分](https://blog.cloudflare.com/workers-ai)。我们让 AI 的入门变得和交付全球工作负载一样简单。11 月中旬，我们实现了在全球 [100 多个城市](https://blog.cloudflare.com/workers-ai-update-stable-diffusion-code-llama-workers-ai-in-100-cities/)运行 Workers AI Inference 的目标，到 2024 年底，Workers AI 将在 Cloudflare 开展业务的几乎每个城市运行。\n\n[Workers AI](https://developers.cloudflare.com/workers-ai/) 允许开发人员使用最新的开源 AI 模型构建应用程序，而无需配置任何基础设施或为昂贵的未使用容量付费。我们正在对此进行扩展，以支持[直接从 Hugging Face 到 Workers AI 部署模型](https://blog.cloudflare.com/partnering-with-hugging-face-deploying-ai-easier-affordable/)，以获得更广泛的 AI 模型集。与在特定数据中心配置具有 GPU 的虚拟机不同，我们构建此功能是为了将整个网络视为一个巨大的计算资源，在正确的时间在正确的位置运行模型来满足开发人员的需求。\n\n除了模型推理之外，我们还加倍支持 Web 标准 API，并在 Workers 平台内提供 [WebGPU](https://blog.cloudflare.com/webgpu-in-workers/)API。虽然我们因为被认可为领先边缘平台而感到自豪，但我们不仅如此，我们还是一个用于开发全栈应用程序的平台，甚至是那些需要在一年前还很少使用或需要的计算能力的应用程序。\n\n我们很高兴向您展示后续将推出的内容，包括跨Cloudflare 产品[管理机密的新方法](https://blog.cloudflare.com/secrets-store/)、改进的可观察性以及用于发布更改的更好工具。每天，我们都会看到在我们的平台上构建了更先进的应用程序，我们致力于将其与工具相匹配，以服务于对任务最关键的工作负载——我们自己也使用这些工具来在我们的平台中构建我们自己的产品。\n\n点击[此处](https://www.cloudflare.com/lp/forrester-wave-edge-development-q4-2023/)下载报告。"
    },
    {
      "url": "https://blog.cloudflare.com/ko-kr/forrester-wave-edge-development-2023-ko-kr/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/ko-kr/forrester-wave-edge-development-2023-ko-kr/",
        "loadedTime": "2023-12-05T02:33:20.322Z",
        "referrerUrl": "https://blog.cloudflare.com/forrester-wave-edge-development-2023/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/ko-kr/forrester-wave-edge-development-2023-ko-kr/",
        "title": "Cloudflare, 2023년 4분기 Forrester Wave 에지 개발 플랫폼 부문 리더로 선정",
        "description": "Forrester에서 Cloudflare를 The Forrester Wave™: 2023년 4분기 에지 개발 플랫폼 부문에서 리더로 선정했습니다. Cloudflare는 해당 부문에서 최고 점수를 받았습니다.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/28/2023\n10 min read\nForrester에서 Cloudflare를 The Forrester Wave™: 2023년 4분기 에지 개발 플랫폼 부문에서 리더로 선정했습니다. Cloudflare는 해당 부문에서 최고 점수를 받았습니다.\n수석 분석가인 Devin Dickerson의 보고서에 따르면 \"Cloudflare의 에지 개발 플랫폼은 기업에서 전체 스택 분산 애플리케이션을 만드는 데 필요한 빌딩 블록을 제공하며, 개발자가 CAP 정리에 대한 전문가가 아니어도 전 세계적으로 분산된 컴퓨팅, 스토리지, 프로그래밍 가능한 보안 네트워크를 활용할 수 있도록 합니다.\"\n백만 명 이상의 개발자가 Workers, Pages, R2, KV, Queues, Durable Objects, D1, Stream, Images 등의 개발자 플랫폼 제품을 사용하여 애플리케이션을 개발하고 있습니다. 이들 개발자는Cloudflare의 컴퓨팅, 스토리지, 개발자 서비스 전체 제품군을 사용하여 고도로 분산된 전체 스택 애플리케이션을 쉽게 배포할 수 있습니다.\nWorkers로 Cloudflare의 네트워크를 프로그래밍 가능하게 만듭니다\n\" 이 플랫폼의 핵심 강점은 지능형 워크로드 배치를 활용하는 배포 모델과 결합된 Cloudflare의 프로그래밍 가능한 글로벌 CDN과 상호 운용이 가능하다는 점입니다.\" - The Forrester Wave™: 에지 개발 플랫폼, 2023년 4분기\nWorkers는 Cloudflare의 전역 네트워크에 걸쳐 실행되고, 로컬 캐시에서 직접 읽고 쓸 수 있는 API를 제공하며, Worker에서 수신하는 요청 개체에 Cloudflare의 CDN 컨텍스트를 직접 노출합니다.\n개발자는 이와 같은 Cloudflare 네트워크와의 긴밀한 통합을 통해 특정 지역에 배포하지 않고도 전 세계에 분산된 애플리케이션을 빌드, 보호, 연결할 수 있습니다. Smart Placement는 데이터와 가장 가까운 위치 또는 사용자와 가장 가까운 위치 등 전체적으로 가장 빠른 성능을 제공하는 위치에서 실행되도록Workers를 최적화합니다. Hyperdrive는 데이터베이스 연결을 자동으로 풀링하여 전 세계에서 실행되는 Workers에서 PostgreSQL 데이터베이스를 쿼리할 때 재사용할 수 있도록 지원하므로 서버리스 아키텍처에서 기존 데이터베이스를 사용하기 어렵게 만드는 확장 문제를 피할 수 있습니다. Cron Trigger는 최대 15분까지 CPU 시간을 허용하므로 컴퓨팅 집약적인 백그라운드 작업을 수행할 수 있습니다.\nCloudflare에서는 에지 컴퓨팅을 넘어서서 모든 곳에서 컴퓨팅 기능을 제공합니다. Cloudflare에서는 실제 데이터에 기반하고 액세스 패턴과 프로그래밍 패러다임에 맞게 조정된 애플리케이션이 최고의 성능을 발휘하도록 우리 네트워크를 활용합니다.\n분산 시스템 전문가가 아니어도 분산 시스템을 배포할 수 있음\n\" 추천하는 고객은 온보딩이 쉽다는 것을 지속해서 언급합니다. 사전 배경 지식이 없는 개발자라도 몇 분 만에 전 세계에 워크로드를 제공하고 일주일 이내에 프로덕션 품질의 앱을 배포할 수 있으니까요.\" - The Forrester Wave™: 에지 개발 플랫폼, 2023년 4분기 \nWorkers는 분산 시스템 전문가나 클라우드 인프라 구성 전문가가 아니더라도 모든 개발자가 전 세계에 분산된 앱을 배포할 수 있도록 지원합니다.\n- 사용자가 Worker를 배포하면 Cloudflare에서는 백그라운드에서 이를 전 세계에 배포합니다. 하지만 사용자에게 있어 Worker란 프로덕션 환경에서 Workers가 실행되는 것과 동일한 오픈 소스 JavaScript 런타임을 사용하여 로컬에서 실행하고 테스트할 수 있는 단일 애플리케이션에 불과합니다.\n- 실시간 상태를 조정하기 위해 Durable Object 를 배포하면 실시간 상태를 조정하기 위해 분산형 앱이 구축되지만, RPC 프로토콜을 학습하고 인프라를 확장하는 대신, 프런트 엔드 개발자가 알고 매일 의존하는 웹 표준 API를 사용하여 모든 것을 JavaScript로 프로그래밍하게 됩니다.\n- Cloudflare Queues를 사용하여 메시지 배치를 대기열에 추가하고 처리하려면 기존 Worker에 JavaScript 몇 줄만 추가하면 됩니다.\n- Cloudflare Pages를 사용하여 웹 애플리케이션을 만들 경우, GitHub 리포지토리에 연결만 해도 미리 보기URL이 포함된 완전한 연속 빌드 및 배포 파이프라인을 설정하게 됩니다.\n이전에는 프런트 엔드 코드만 작성하던 개발자가 백엔드를 구축하여 앱을 실시간 및 반응형으로 만들 수 있습니다. 인프라 전문가가 리소스를 프로비저닝할 때까지 기다려야 했던 팀에서는 다음 주가 아닌 오늘 바로 프로토타이핑을 시작할 수 있습니다. Worker 을 작성하고 배포하는 것은 친숙하고 접근하기 쉬우므로 엔지니어링 팀이 오버헤드를 줄이면서 더 빠르게 작업할 수 있습니다.\n팀에서 이처럼 빠르게 시작할 수 있는 이유는 무엇일까요?\nWorkers는 프런트 엔드 개발자와 웹 애플리케이션을 구축하는 모든 사람이 이미 매일 사용하고 있는 웹 표준 API를 사용합니다. Cloudflare에서는 웹 상호 운용성 런타임 커뮤니티 그룹(WinterCG)의 창립 멤버이며, 런타임 전반의 상호 운용성을 위해 전념하고 있습니다.\n개발자가 이미 매일 사용하고 있는 도구는 Cloudflare 플랫폼에 기본으로 제공됩니다. Cloudflare에서는 모든 API에 대해TypeScript 유형을 게시하고, Wrangler CLI를 통해 또는 Cloudflare 대시보드의 코드 편집기를 통해 작성 및 배포할 때 TypeScript 컴파일링을 지원하며, 이 편집기 자체는 널리 사용되는 VSCode 편집기로 구동됩니다.\n개발자가 빌드 도구로 선호하는 오픈 소스 프레임워크가 지원됩니다. 점점 더 많은 Node.js의 API 세트가 Workers 런타임에서 기본 제공되므로 기존의 오픈 소스 라이브러리가 Workers에서 작동할 수 있습니다. 또한 개발자가 의존하는 새로운 오픈 소스 프로젝트가 처음부터 모든 WinterCG 런타임에서 작동하도록 설계되는 경우가 점점 더 많아지고 있습니다. 매일 더 많은 JavaScript 생태계가 Workers에서 작동합니다.\nGPU, LLM 등을 통한 AI로의 확장\n\"우수한 비전을 지닌 Cloudflare에서는 향후 풋프린트를 에지로 제한하지 않으며, 로드맵에 따라 기능을 구축하는 목적 중심 접근 방식을 통해 워크로드를 위한 퍼블릭 클라우드 하이퍼스케일러에 대응할 수 있도록 입지를 계속 공고히 해나갈 것입니다. \" – The Forrester Wave™: 에지 개발 플랫폼, 2023년 4분기\nCloudflare에서는 대규모 프로덕션 애플리케이션을 위한 완전한 컴퓨팅 플랫폼을 구축하고 있습니다. 그리고 모든 기업과 모든 개발자가 AI를 구축하거나 실험하고 있는 지금, Cloudflare에서는 GPU를 개발자 플랫폼의 일부로 통합했습니다. 따라서 글로벌 워크로드를 제공하는 것만큼이나 AI를 쉽게 시작할 수 있게 되었습니다. 11월 중순에 전 세계100여 개의 도시에서 Workers AI 추론을 실행한다는 목표를 달성했으며, 2024년 말에는 Cloudflare에서 진출한 거의 모든 도시에서 Workers AI가 실행될 것입니다.\nWorkers AI를 사용하면 개발자가 인프라를 프로비저닝하거나 사용하지 않는 용량에 대한 비용을 지불하지 않고도 최신 오픈 소스 AI 모델을 사용하여 앱을 빌드할 수 있습니다. Cloudflare에서는 이 기능을 확장해서 Hugging Face에서 Workers AI로 직접 모델을 배포할 수 있도록 지원하여 더욱 다양한AI 모델을 지원합니다. 또한 특정 데이터 센터에서 GPU로 가상 머신을 프로비저닝하는 것과 달리, 당사의 전체 네트워크를 하나의 거대한 컴퓨팅 리소스로 취급하여 적시에 적재적소에 모델을 실행하여 개발자의 요구 사항을 충족할 수 있도록 구축하고 있습니다.\n모델 추론 외에도 웹 표준 API를 지원하고 Workers 플랫폼 내에서 WebGPU API를 사용할 수 있도록 하는 등 지원을 두 배로 늘리고 있습니다. 우리는 우리 플랫폼이 최첨단이라고 인정받는 것에 자부심을 느끼지만, 거기에서 그치지 않고, 불과 1년 전만 해도 거의 사용되지 않거나 필요하지 않았던 컴퓨팅 성능을 필요로 하는 앱까지 전체 스택 앱을 개발할 수 있는 플랫폼이기도 합니다.\nCloudflare 제품 전반에서 기밀을 관리하는 새로운 방법, 향상된 통합 가시성, 변경 사항 릴리스를 위한 더 나은 도구 등에 대한 다음과 같은 소식을 알려드리게 되어 기쁩니다. 우리는 매일 Cloudflare 플랫폼에서 더 많은 첨단 애플리케이션이 구축되는 것을 보고 있으며, 이들 애플리케이션을 가장 핵심적인 워크로드(자체 플랫폼에서 제품을 구축하는 데 사용하는 것과 동일한 워크로드)를 지원하는 도구와 매칭하기 위해 노력하고 있습니다.\n여기에서 보고서를 다운로드하세요.\nCloudflare는 전체 기업 네트워크 를 보호하고, 고객이 인터넷 규모의 애플리케이션을 효율적으로 구축하도록 도우며, 웹사이트 또는 인터넷 애플리케이션 을 가속하고, DDoS 공격 을 막으며, 해커의 침입을 방지 하고, Zero Trust로의 여정을 도와드릴 수 있습니다. \n아무 장치로 1.1.1.1 에 방문해서 인터넷을 더 빠르고 안전하게 만드는 Cloudflare의 무료 앱을 시작해 보세요. \n더 나은 인터넷을 만들겠다는 Cloudflare의 사명을 더욱 자세히 알아보려면, 이곳 을 확인해 보세요. 새로운 방향성을 제시하는 직업을 찾고 있다면, 채용 중인 직무 목록을 확인해 보세요. \nForrester (KO) 한국어",
      "markdown": "11/28/2023\n\n*   [![Dawn Parzych](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/04/Untitled---1-of-1.jpeg)](https://blog.cloudflare.com/author/dawn/)\n*   [![Brendan Irvine-Broque](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/09/IMG_2312.JPG)](https://blog.cloudflare.com/author/brendan-irvine-broque/)\n\n10 min read\n\nForrester에서 Cloudflare를 The Forrester Wave™: 2023년 4분기 에지 개발 플랫폼 부문에서 리더로 선정했습니다. Cloudflare는 해당 부문에서 최고 점수를 받았습니다.\n\n수석 분석가인 Devin Dickerson의 [보고서](https://www.cloudflare.com/lp/forrester-wave-edge-development-q4-2023/)에 따르면 \"Cloudflare의 에지 개발 플랫폼은 기업에서 전체 스택 분산 애플리케이션을 만드는 데 필요한 빌딩 블록을 제공하며, 개발자가 CAP 정리에 대한 전문가가 아니어도 전 세계적으로 분산된 컴퓨팅, 스토리지, 프로그래밍 가능한 보안 네트워크를 활용할 수 있도록 합니다.\"\n\n백만 명 이상의 개발자가 [Workers](https://workers.cloudflare.com/), [Pages](https://pages.cloudflare.com/), [R2](https://developers.cloudflare.com/r2/), [KV](https://developers.cloudflare.com/kv/), [Queues](https://developers.cloudflare.com/queues/), [Durable Objects](https://developers.cloudflare.com/durable-objects/), [D1](https://developers.cloudflare.com/d1/), [Stream](https://developers.cloudflare.com/stream/), [Images](https://developers.cloudflare.com/images/) 등의 개발자 플랫폼 제품을 사용하여 애플리케이션을 개발하고 있습니다. 이들 개발자는Cloudflare의 컴퓨팅, 스토리지, 개발자 서비스 전체 제품군을 사용하여 고도로 분산된 전체 스택 애플리케이션을 쉽게 배포할 수 있습니다.\n\n### Workers로 Cloudflare의 네트워크를 프로그래밍 가능하게 만듭니다\n\n> **_\" 이 플랫폼의 핵심 강점은 지능형 워크로드 배치를 활용하는 배포 모델과 결합된 Cloudflare의 프로그래밍 가능한 글로벌 CDN과 상호 운용이 가능하다는 점입니다.\"  - The Forrester Wave™: 에지 개발 플랫폼, 2023년 4분기_**\n\nWorkers는 [Cloudflare의 전역 네트워크](https://www.cloudflare.com/network/)에 걸쳐 실행되고, 로컬 캐시에서 직접 읽고 쓸 수 있는 [API](https://developers.cloudflare.com/workers/runtime-apis/cache/)를 제공하며, Worker에서 수신하는 [요청 개체](https://developers.cloudflare.com/workers/runtime-apis/request/#incomingrequestcfproperties)에 Cloudflare의 CDN 컨텍스트를 직접 노출합니다.\n\n개발자는 이와 같은 Cloudflare 네트워크와의 긴밀한 통합을 통해 특정 지역에 배포하지 않고도 전 세계에 분산된 애플리케이션을 빌드, 보호, 연결할 수 있습니다. [Smart Placement](https://developers.cloudflare.com/workers/configuration/smart-placement/)는 데이터와 가장 가까운 위치 또는 사용자와 가장 가까운 위치 등 전체적으로 가장 빠른 성능을 제공하는 위치에서 실행되도록Workers를 최적화합니다. [Hyperdrive](https://developers.cloudflare.com/hyperdrive/learning/how-hyperdrive-works/)는 데이터베이스 연결을 자동으로 풀링하여 전 세계에서 실행되는 Workers에서 PostgreSQL 데이터베이스를 쿼리할 때 재사용할 수 있도록 지원하므로 서버리스 아키텍처에서 기존 데이터베이스를 사용하기 어렵게 만드는 확장 문제를 피할 수 있습니다. [Cron Trigger](https://developers.cloudflare.com/workers/configuration/cron-triggers/)는 최대 15분까지 CPU 시간을 허용하므로 컴퓨팅 집약적인 백그라운드 작업을 수행할 수 있습니다.\n\nCloudflare에서는 에지 컴퓨팅을 넘어서서 모든 곳에서 컴퓨팅 기능을 제공합니다. Cloudflare에서는 실제 데이터에 기반하고 액세스 패턴과 프로그래밍 패러다임에 맞게 조정된 애플리케이션이 최고의 성능을 발휘하도록 우리 네트워크를 활용합니다.\n\n### 분산 시스템 전문가가 아니어도 분산 시스템을 배포할 수 있음\n\n> **_\" 추천하는 고객은 온보딩이 쉽다는 것을 지속해서 언급합니다. 사전 배경 지식이 없는 개발자라도 몇 분 만에 전 세계에 워크로드를 제공하고 일주일 이내에 프로덕션 품질의 앱을 배포할 수 있으니까요.\" - The Forrester Wave™: 에지 개발 플랫폼, 2023년 4분기_**\n\nWorkers는 분산 시스템 전문가나 클라우드 인프라 구성 전문가가 아니더라도 모든 개발자가 전 세계에 분산된 앱을 배포할 수 있도록 지원합니다.\n\n\\- 사용자가 Worker를 배포하면 Cloudflare에서는 백그라운드에서 이를 전 세계에 배포합니다. 하지만 사용자에게 있어 Worker란 프로덕션 환경에서 Workers가 실행되는 것과 동일한 [오픈 소스 JavaScript 런타임](https://github.com/cloudflare/workerd)을 사용하여 [로컬에서 실행하고 테스트](https://developers.cloudflare.com/workers/observability/local-development-and-testing/)할 수 있는 단일 애플리케이션에 불과합니다.\n\n\\- 실시간 상태를 조정하기 위해 [Durable Object](https://developers.cloudflare.com/durable-objects/) 를 배포하면 실시간 상태를 조정하기 위해 분산형 앱이 구축되지만, RPC 프로토콜을 학습하고 인프라를 확장하는 대신, 프런트 엔드 개발자가 알고 매일 의존하는 웹 표준 API를 사용하여 모든 것을 JavaScript로 프로그래밍하게 됩니다.\n\n\\- [Cloudflare Queues](https://developers.cloudflare.com/queues/)를 사용하여 메시지 배치를 대기열에 추가하고 처리하려면 기존 Worker에 JavaScript 몇 줄만 추가하면 됩니다.\n\n\\- [Cloudflare Pages](https://pages.cloudflare.com/)를 사용하여 웹 애플리케이션을 만들 경우, GitHub 리포지토리에 연결만 해도 미리 보기URL이 포함된 완전한 연속 빌드 및 배포 파이프라인을 설정하게 됩니다.\n\n이전에는 프런트 엔드 코드만 작성하던 개발자가 백엔드를 구축하여 앱을 실시간 및 반응형으로 만들 수 있습니다. 인프라 전문가가 리소스를 프로비저닝할 때까지 기다려야 했던 팀에서는 다음 주가 아닌 오늘 바로 프로토타이핑을 시작할 수 있습니다. Worker 을 작성하고 배포하는 것은 친숙하고 접근하기 쉬우므로 엔지니어링 팀이 오버헤드를 줄이면서 더 빠르게 작업할 수 있습니다.\n\n팀에서 이처럼 빠르게 시작할 수 있는 이유는 무엇일까요?\n\n1.  Workers는 프런트 엔드 개발자와 웹 애플리케이션을 구축하는 모든 사람이 이미 매일 사용하고 있는 [웹 표준 API](https://developers.cloudflare.com/workers/runtime-apis/)를 사용합니다. Cloudflare에서는 웹 상호 운용성 런타임 커뮤니티 그룹([WinterCG](https://wintercg.org/))의 창립 멤버이며, [런타임 전반의 상호 운용성](https://blog.cloudflare.com/socket-api-works-javascript-runtimes-wintercg-polyfill-connect/)을 위해 전념하고 있습니다.\n2.  개발자가 이미 매일 사용하고 있는 도구는 Cloudflare 플랫폼에 기본으로 제공됩니다. Cloudflare에서는 모든 API에 대해[TypeScript](https://www.npmjs.com/package/@cloudflare/workers-types) 유형을 게시하고, Wrangler CLI를 통해 또는     Cloudflare 대시보드의 코드 편집기를 통해 작성 및 배포할 때 TypeScript 컴파일링을 지원하며, 이 편집기 자체는 널리 사용되는 VSCode 편집기로 [구동](https://blog.cloudflare.com/improved-quick-edit/)됩니다.\n3.  개발자가 빌드 도구로 선호하는 오픈 소스 [프레임워크](https://developers.cloudflare.com/pages/framework-guides/)가 지원됩니다. 점점 더 많은 Node.js의 API 세트가 Workers 런타임에서 [기본 제공](https://developers.cloudflare.com/workers/runtime-apis/nodejs/)되므로 기존의 오픈 소스 라이브러리가 Workers에서 작동할 수 있습니다. 또한 개발자가 의존하는 새로운 오픈 소스 프로젝트가 처음부터 모든 WinterCG 런타임에서 작동하도록 설계되는 경우가 점점 더 많아지고 있습니다. 매일 더 많은 JavaScript 생태계가 Workers에서 작동합니다.\n\n### GPU, LLM 등을 통한 AI로의 확장\n\n> **_\"우수한 비전을 지닌 Cloudflare에서는 향후 풋프린트를 에지로 제한하지 않으며, 로드맵에 따라 기능을 구축하는 목적 중심 접근 방식을 통해 워크로드를 위한 퍼블릭 클라우드 하이퍼스케일러에 대응할 수 있도록 입지를 계속 공고히 해나갈 것입니다. \" – The Forrester Wave™: 에지 개발 플랫폼, 2023년 4분기_**\n\nCloudflare에서는 대규모 프로덕션 애플리케이션을 위한 완전한 컴퓨팅 플랫폼을 구축하고 있습니다. 그리고 모든 기업과 모든 개발자가 AI를 구축하거나 실험하고 있는 지금, Cloudflare에서는 [GPU를 개발자 플랫폼의 일부로 통합](https://blog.cloudflare.com/workers-ai)했습니다. 따라서 글로벌 워크로드를 제공하는 것만큼이나 AI를 쉽게 시작할 수 있게 되었습니다. 11월 중순에 전 세계[100여 개의 도시](https://blog.cloudflare.com/workers-ai-update-stable-diffusion-code-llama-workers-ai-in-100-cities/)에서 Workers AI 추론을 실행한다는 목표를 달성했으며, 2024년 말에는 Cloudflare에서 진출한 거의 모든 도시에서 Workers AI가 실행될 것입니다.\n\n[Workers AI](https://developers.cloudflare.com/workers-ai/)를 사용하면 개발자가 인프라를 프로비저닝하거나 사용하지 않는 용량에 대한 비용을 지불하지 않고도 최신 오픈 소스 AI 모델을 사용하여 앱을 빌드할 수 있습니다. Cloudflare에서는 이 기능을 확장해서 [Hugging Face에서 Workers AI로 직접 모델을 배포](https://blog.cloudflare.com/partnering-with-hugging-face-deploying-ai-easier-affordable/)할 수 있도록 지원하여 더욱 다양한AI 모델을 지원합니다. 또한 특정 데이터 센터에서 GPU로 가상 머신을 프로비저닝하는 것과 달리, 당사의 전체 네트워크를 하나의 거대한 컴퓨팅 리소스로 취급하여 적시에 적재적소에 모델을 실행하여 개발자의 요구 사항을 충족할 수 있도록 구축하고 있습니다.\n\n모델 추론 외에도 웹 표준 API를 지원하고 Workers 플랫폼 내에서 [WebGPU](https://blog.cloudflare.com/webgpu-in-workers/) API를 사용할 수 있도록 하는 등 지원을 두 배로 늘리고 있습니다. 우리는 우리 플랫폼이 최첨단이라고 인정받는 것에 자부심을 느끼지만, 거기에서 그치지 않고, 불과 1년 전만 해도 거의 사용되지 않거나 필요하지 않았던 컴퓨팅 성능을 필요로 하는 앱까지 전체 스택 앱을 개발할 수 있는 플랫폼이기도 합니다.\n\nCloudflare 제품 전반에서 [기밀을 관리하는 새로운 방법](https://blog.cloudflare.com/secrets-store/), 향상된 통합 가시성, 변경 사항 릴리스를 위한 더 나은 도구 등에 대한 다음과 같은 소식을 알려드리게 되어 기쁩니다. 우리는 매일 Cloudflare 플랫폼에서 더 많은 첨단 애플리케이션이 구축되는 것을 보고 있으며, 이들 애플리케이션을 가장 핵심적인 워크로드(자체 플랫폼에서 제품을 구축하는 데 사용하는 것과 동일한 워크로드)를 지원하는 도구와 매칭하기 위해 노력하고 있습니다.\n\n[여기](https://www.cloudflare.com/lp/forrester-wave-edge-development-q4-2023/)에서 보고서를 다운로드하세요.\n\nCloudflare는 [전체 기업 네트워크](https://www.cloudflare.com/ko-kr/network-services/) 를 보호하고, 고객이 [인터넷 규모의 애플리케이션을 효율적으로](https://workers.cloudflare.com/) 구축하도록 도우며, [웹사이트 또는 인터넷 애플리케이션](https://www.cloudflare.com/ko-kr/performance/accelerate-internet-applications/) 을 가속하고, [DDoS 공격](https://www.cloudflare.com/ko-kr/ddos/) 을 막으며, [해커의 침입을 방지](https://www.cloudflare.com/ko-kr/application-security/) 하고, [Zero Trust로의 여정을](https://www.cloudflare.com/ko-kr/products/zero-trust/) 도와드릴 수 있습니다.\n\n아무 장치로 [1.1.1.1](https://1.1.1.1/) 에 방문해서 인터넷을 더 빠르고 안전하게 만드는 Cloudflare의 무료 앱을 시작해 보세요.\n\n더 나은 인터넷을 만들겠다는 Cloudflare의 사명을 더욱 자세히 알아보려면, [이곳](https://www.cloudflare.com/ko-kr/learning/what-is-cloudflare/) 을 확인해 보세요. 새로운 방향성을 제시하는 직업을 찾고 있다면, [채용 중인 직무](https://cloudflare.com/ko-kr/careers) 목록을 확인해 보세요.\n\n[Forrester (KO)](https://blog.cloudflare.com/tag/forrester-ko/) [한국어](https://blog.cloudflare.com/tag/korean-ko/)"
    },
    {
      "url": "https://blog.cloudflare.com/ja-jp/forrester-wave-edge-development-2023-ja-jp/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/ja-jp/forrester-wave-edge-development-2023-ja-jp/",
        "loadedTime": "2023-12-05T02:33:20.229Z",
        "referrerUrl": "https://blog.cloudflare.com/forrester-wave-edge-development-2023/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/ja-jp/forrester-wave-edge-development-2023-ja-jp/",
        "title": "CloudflareがForrester Edge Development Platforms Wave、2023年第4四半期でリーダーに選出されました",
        "description": "Forresterは、「The Forrester Wave™：エッジ開発プラットフォーム、2023年第4四半期」において、Cloudflareをリーダーとして認識し、現在の提供カテゴリで最高スコアを獲得しました。",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Forresterは、「The Forrester Wave™：エッジ開発プラットフォーム、2023年第4四半期」において、Cloudflareをリーダーとして認識し、現在の提供カテゴリで最高スコアを獲得しました。\n主席アナリストのDevin Dickerson氏の報告によれば、Cloudflareのエッジ開発プラットフォームは、企業がフルスタックの分散アプリケーションを構築するために必要な構成要素を提供しています。このプラットフォームは、開発者がCAP定理の専門家でなくても、コンピューティング、ストレージ、プログラム可能なセキュリティを含むグローバルな分散ネットワークを活用できるようになっています。\n100万人以上の開発者が、Developer Platform製品を使用して以下のアプリケーションを構築しています。Workers、Pages、 R2、KV、Queues、Durable Objects、D1、Stream、Imagesなど。開発者は、Cloudflareのコンピューティング、ストレージ、開発者向けサービス一式をフルに使用して、高度に分散されたフルスタックアプリケーションを簡単にデプロイできます。\nWorkersは、Cloudflareのネットワークをプログラム可能にします\n「このプラットフォームの主な強みは、Cloudflareのプログラム可能なグローバルインテリジェントなワークロードの配置を活用した導入モデルと組み合わせたCDNとの相互運用性です。」–Forrester Wave™：エッジ開発プラットフォーム、2023年第4四半期\nCloudflareのグローバルネットワークでWorkersが実行され、ローカルキャッシュから直接読み書きするAPIを提供し、Workerが受け取るリクエストオブジェクト上にCloudflareのCDNから直接コンテキストを公開します。\nCloudflareのネットワークとの密接な統合により、開発者は特定の地域にデプロイすることなく、グローバルに分散したアプリケーションを構築、保護、接続することができます。Smart Placementは、Workersを最適な位置に配置して、データに最も近い場所であれ、ユーザーに最も近い場所であれ、全体として最速のパフォーマンスを実現します。Hyperdriveはデータベース接続を自動的にプールし、世界中で稼働しているWorkerがPostgreSQLデータベースにクエリを行う際に、接続を再利用できるようにします。これにより、従来のデータベースをサーバーレスアーキテクチャで利用する際のスケーリングの課題を回避できます。Cronトリガーは、最大で15分のCPU時間を許容し、計算集約的なバックグラウンド作業を実行できるようにします。\nCloudflareはエッジコンピューティングを超えています。Cloudflareは、あらゆる場所でのコンピューティングです。実世界のデータに基づき、アクセスパターンやプログラミングパラダイムに合わせて、お客様のアプリケーションが最高のパフォーマンスを発揮するように、当社はネットワークを活用しています。\n分散システムの専門家でなくても、分散システムをデプロイできます\n「リファレンスカスタマーは、一貫してオンボーディングの容易さを強調しています。バックグラウンドのない開発者でさえ、数分で世界中にワークロードを提供し、1週間以内にプロダクション品質のアプリケーションを構築できます。」–Forrester Wave™：エッジ開発プラットフォーム、2023年第4四半期\nWorkersでは、分散システムの専門家やクラウドインフラストラクチャの設定の専門家になる必要がなく、あらゆる開発者がグローバルに分散したアプリケーションをデプロイできるようにします。\n- Workerをデプロイすると、Cloudflareはその裏でそれを世界中に分散させます。しかし、お客様にとっては、ローカルで実行およびテストできる単一のアプリケーションです。プロダクション環境では、同じオープンソースのJavaScriptランタイムを使用してWorkersを実行できます。\n- Durable Objectをデプロイしてリアルタイムの状態を調整すると、分散アプリケーショが構築されますが、RPCプロトコルやスケールインフラストラクチャを学ぶ必要はありません。代わりに、JavaScriptで、フロントエンド開発者が日々頼りにしているWeb標準のAPIを使用して、すべてのものをプログラムすることができます。\n- Cloudflare Queuesでメッセージのバッチをキューに入れ処理するには、既存のWorkerにJavaScriptを数行追加するだけです。\n- Cloudflare Pagesで、Webアプリケーションを作成すると、GitHubリポジトリに接続するだけで、プレビューURLを使用した完全な継続的ビルドとデプロイのパイプラインがセットアップされます。\n以前はフロントエンドのコードしか書かなかった開発者が、バックエンドを構築し、アプリをリアルタイムかつリアクティブにすることができます。インフラストラクチャの専門家がリソースのセットアップを手詰まりで進めないでいたチームは、来週ではなく今日からプロトタイプを作成できます。Workerの書き込みとデプロイが身近で利用しやすいため、これにより、エンジニアリングチームはオーバーヘッドを減らしながら、より迅速に進めることができます。\nなぜチームはこれほど早くスタートできるのでしょうか？\nWorkersは、Web標準のAPIを使用して、フロントエンド開発者やWebアプリケーションを構築する誰もがすでに日常的に使用しています。CloudflareはWeb-interoperable Runtimes Community Group（WinterCG）の創設メンバーであり、ランタイム全体の相互運用性に取り組んでいます。\n開発者がすでに毎日使用しているツールは、当社のプラットフォームにネイティブです。すべてのAPIに対してTypeScriptのタイプを公開しており、Wrangler CLIやCloudflareダッシュボードのコードエディタ—（これ自体が人気のあるVSCodeエディタを搭載しています）を介してオーサリングやデプロイを行う際に、TypeScriptのコンパイルをサポートしています。\n開発者が好んで構築するオープンソースフレームワークがサポートされています。Node.jsから一連のAPIの増加により、Workersランタイムでネイティブに利用できるようになり、既存のオープンソースライブラリがWorkers上で動作するようになりました。そして、ますます、開発者が依存する新しいオープンソースプロジェクトは、WinterCGのすべてのランタイムにわたり動作するように、初日から設計されています。日々、JavaScriptエコシステムの多くがWorkers上で動作しています。\nGPU、LLMなどでAIに進出\nその優れたビジョンにより、同社は将来の実績をエッジに制約することを拒み、ロードマップ上の機能を構築する際の狙いを持ったアプローチからは、ワークロードにおいてパブリッククラウドのハイパースケーラーに対抗するますます有利なポジショニングを築いていくことが示唆されています。–Forrester Wave™：エッジ開発プラットフォーム、2023年第4四半期\n当社は、プロダクションアプリケーション用の完全なコンピューティングプラットフォームを大規模に構築しています。そして、企業、開発者すべてがAIを構築または体験している今、Cloudflareは、GPUを取り入れ、それを当社の開発者プラットフォームの一部に統合しました。当社は、グローバルなワークロードを提供するのと同様に、AIを簡単に始められるようにしました。11月中旬には、世界中の100以上の都市でWorkers AI Inferenceを稼働させるという目標を達成し、2024年末までには、Cloudflareの支店があるほぼすべての都市でWorkers AIを稼働させる予定です。\nWorkers AIにより、開発者がインフラストラクチャをプロビジョニングしたり、高価な未使用容量にお金を払うことなく、最新のオープンソースAIモデルを使用してアプリケーションを構築することができるようになります。当社はこれを拡張し、さらに広範な種類のAIモデルに対応するため、Hugging FaceからWorkers AIへ直接モデルをデプロイするサポートを提供します。また、特定のデータセンターでGPUを搭載したVMをプロビジョニングするのとは異なり、当社は、ネットワーク全体を1つの巨大なコンピューティングリソースとして扱い、開発者のニーズに応じてモデルを適切な場所で適切なタイミングで実行できるように構築しています。\nモデル推論に加えて、Web標準のAPIを強力にサポートし、Workersプラットフォーム内からWebGPUAPIを利用可能にしています。当社は最先端のプラットフォームとして認識されていることを誇りに思っていますが、それだけではありません。—当社はフルスタックのアプリケーションを開発するためのプラットフォームであり、わずか1年前には、ほとんどの人が利用しなかった、あるいは必要としなかった計算能力を必要とするアプリケーションも開発しています。\nCloudflare製品全体でシークレットを管理する新しい方法、可観測性の向上、変更をリリースするためのより優れたツールなど、次の展開をご覧いただけるのを楽しみにしています。日々、当社のプラットフォーム上で構築されるより高度なアプリケーションを目にしており、その高度なアプリケーションを、最もミッションクリティカルなワークロード（独自のプラットフォーム上で製品を構築するために使用しているのと同じワークロード）に対応するツールと適合させることに注力しています。\nこちらからレポートをダウンロードしてください。",
      "markdown": "Forresterは、「The Forrester Wave™：エッジ開発プラットフォーム、2023年第4四半期」において、Cloudflareをリーダーとして認識し、現在の提供カテゴリで最高スコアを獲得しました。\n\n主席アナリストのDevin Dickerson氏の[報告](https://www.cloudflare.com/lp/forrester-wave-edge-development-q4-2023/)によれば、Cloudflareのエッジ開発プラットフォームは、企業がフルスタックの分散アプリケーションを構築するために必要な構成要素を提供しています。このプラットフォームは、開発者がCAP定理の専門家でなくても、コンピューティング、ストレージ、プログラム可能なセキュリティを含むグローバルな分散ネットワークを活用できるようになっています。\n\n100万人以上の開発者が、Developer Platform製品を使用して以下のアプリケーションを構築しています。[Workers](https://workers.cloudflare.com/)、[Pages](https://pages.cloudflare.com/)、 [R2](https://developers.cloudflare.com/r2/)、[KV](https://developers.cloudflare.com/kv/)、[Queues](https://developers.cloudflare.com/queues/)、[Durable Objects](https://developers.cloudflare.com/durable-objects/)、[D1](https://developers.cloudflare.com/d1/)、[Stream](https://developers.cloudflare.com/stream/)、[Images](https://developers.cloudflare.com/images/)など。開発者は、Cloudflareのコンピューティング、ストレージ、開発者向けサービス一式をフルに使用して、高度に分散されたフルスタックアプリケーションを簡単にデプロイできます。\n\n### Workersは、Cloudflareのネットワークをプログラム可能にします\n\n> **_「このプラットフォームの主な強みは、Cloudflareのプログラム可能なグローバルインテリジェントなワークロードの配置を活用した導入モデルと組み合わせたCDNとの相互運用性です。」–Forrester Wave™：エッジ開発プラットフォーム、2023年第4四半期_**\n\n[Cloudflareのグローバルネットワークで](https://www.cloudflare.com/network/)Workersが実行され、ローカルキャッシュから直接読み書きする[API](https://developers.cloudflare.com/workers/runtime-apis/cache/)を提供し、Workerが受け取る[リクエストオブジェクト](https://developers.cloudflare.com/workers/runtime-apis/request/#incomingrequestcfproperties)上にCloudflareのCDNから直接コンテキストを公開します。\n\nCloudflareのネットワークとの密接な統合により、開発者は特定の地域にデプロイすることなく、グローバルに分散したアプリケーションを構築、保護、接続することができます。[Smart Placement](https://developers.cloudflare.com/workers/configuration/smart-placement/)は、Workersを最適な位置に配置して、データに最も近い場所であれ、ユーザーに最も近い場所であれ、全体として最速のパフォーマンスを実現します。[Hyperdrive](https://developers.cloudflare.com/hyperdrive/learning/how-hyperdrive-works/)はデータベース接続を自動的にプールし、世界中で稼働しているWorkerがPostgreSQLデータベースにクエリを行う際に、接続を再利用できるようにします。これにより、従来のデータベースをサーバーレスアーキテクチャで利用する際のスケーリングの課題を回避できます。[Cronトリガー](https://developers.cloudflare.com/workers/configuration/cron-triggers/)は、最大で15分のCPU時間を許容し、計算集約的なバックグラウンド作業を実行できるようにします。\n\nCloudflareはエッジコンピューティングを超えています。Cloudflareは、あらゆる場所でのコンピューティングです。実世界のデータに基づき、アクセスパターンやプログラミングパラダイムに合わせて、お客様のアプリケーションが最高のパフォーマンスを発揮するように、当社はネットワークを活用しています。\n\n### 分散システムの専門家でなくても、分散システムをデプロイできます\n\n> **_「リファレンスカスタマーは、一貫してオンボーディングの容易さを強調しています。バックグラウンドのない開発者でさえ、数分で世界中にワークロードを提供し、1週間以内にプロダクション品質のアプリケーションを構築できます。」–Forrester Wave™：エッジ開発プラットフォーム、2023年第4四半期_**\n\nWorkersでは、分散システムの専門家やクラウドインフラストラクチャの設定の専門家になる必要がなく、あらゆる開発者がグローバルに分散したアプリケーションをデプロイできるようにします。\n\n\\- Workerをデプロイすると、Cloudflareはその裏でそれを世界中に分散させます。しかし、お客様にとっては、[ローカルで実行およびテスト](https://developers.cloudflare.com/workers/observability/local-development-and-testing/)できる単一のアプリケーションです。プロダクション環境では、同じ[オープンソースのJavaScriptランタイム](https://github.com/cloudflare/workerd)を使用してWorkersを実行できます。\n\n\\- [Durable Object](https://developers.cloudflare.com/durable-objects/)をデプロイしてリアルタイムの状態を調整すると、分散アプリケーショが構築されますが、RPCプロトコルやスケールインフラストラクチャを学ぶ必要はありません。代わりに、JavaScriptで、フロントエンド開発者が日々頼りにしているWeb標準のAPIを使用して、すべてのものをプログラムすることができます。\n\n\\- [Cloudflare Queues](https://developers.cloudflare.com/queues/)でメッセージのバッチをキューに入れ処理するには、既存のWorkerにJavaScriptを数行追加するだけです。\n\n\\- [Cloudflare Pages](https://pages.cloudflare.com/)で、Webアプリケーションを作成すると、GitHubリポジトリに接続するだけで、プレビューURLを使用した完全な継続的ビルドとデプロイのパイプラインがセットアップされます。\n\n以前はフロントエンドのコードしか書かなかった開発者が、バックエンドを構築し、アプリをリアルタイムかつリアクティブにすることができます。インフラストラクチャの専門家がリソースのセットアップを手詰まりで進めないでいたチームは、来週ではなく今日からプロトタイプを作成できます。Workerの書き込みとデプロイが身近で利用しやすいため、これにより、エンジニアリングチームはオーバーヘッドを減らしながら、より迅速に進めることができます。\n\nなぜチームはこれほど早くスタートできるのでしょうか？\n\n1.  Workersは、[Web標準のAPI](https://developers.cloudflare.com/workers/runtime-apis/)を使用して、フロントエンド開発者やWebアプリケーションを構築する誰もがすでに日常的に使用しています。CloudflareはWeb-interoperable Runtimes Community Group（[WinterCG](https://wintercg.org/)）の創設メンバーであり、[ランタイム全体の相互運用性](https://blog.cloudflare.com/socket-api-works-javascript-runtimes-wintercg-polyfill-connect/)に取り組んでいます。\n2.  開発者がすでに毎日使用しているツールは、当社のプラットフォームにネイティブです。すべてのAPIに対して[TypeScript](https://www.npmjs.com/package/@cloudflare/workers-types)のタイプを公開しており、Wrangler CLIやCloudflareダッシュボードのコードエディタ—（これ自体が人気のあるVSCodeエディタを[搭載して](https://blog.cloudflare.com/improved-quick-edit/)います）を介してオーサリングやデプロイを行う際に、TypeScriptのコンパイルをサポートしています。\n3.  開発者が好んで構築するオープンソース[フレームワーク](https://developers.cloudflare.com/pages/framework-guides/)がサポートされています。Node.jsから一連のAPIの増加により、Workersランタイムで[ネイティブに利用できる](https://developers.cloudflare.com/workers/runtime-apis/nodejs/)ようになり、既存のオープンソースライブラリがWorkers上で動作するようになりました。そして、ますます、開発者が依存する新しいオープンソースプロジェクトは、WinterCGのすべてのランタイムにわたり動作するように、初日から設計されています。日々、JavaScriptエコシステムの多くがWorkers上で動作しています。\n\n### GPU、LLMなどでAIに進出\n\n> **_その優れたビジョンにより、同社は将来の実績をエッジに制約することを拒み、ロードマップ上の機能を構築する際の狙いを持ったアプローチからは、ワークロードにおいてパブリッククラウドのハイパースケーラーに対抗するますます有利なポジショニングを築いていくことが示唆されています。–Forrester Wave™：エッジ開発プラットフォーム、2023年第4四半期_**\n\n当社は、プロダクションアプリケーション用の完全なコンピューティングプラットフォームを大規模に構築しています。そして、企業、開発者すべてがAIを構築または体験している今、Cloudflareは、[GPUを取り入れ、それを当社の開発者プラットフォームの一部に統合しました](https://blog.cloudflare.com/workers-ai)。当社は、グローバルなワークロードを提供するのと同様に、AIを簡単に始められるようにしました。11月中旬には、世界中の[100以上の都市で](https://blog.cloudflare.com/workers-ai-update-stable-diffusion-code-llama-workers-ai-in-100-cities/)Workers AI Inferenceを稼働させるという目標を達成し、2024年末までには、Cloudflareの支店があるほぼすべての都市でWorkers AIを稼働させる予定です。\n\n[Workers AI](https://developers.cloudflare.com/workers-ai/)により、開発者がインフラストラクチャをプロビジョニングしたり、高価な未使用容量にお金を払うことなく、最新のオープンソースAIモデルを使用してアプリケーションを構築することができるようになります。当社はこれを拡張し、さらに広範な種類のAIモデルに対応するため、[Hugging FaceからWorkers AIへ直接モデルをデプロイする](https://blog.cloudflare.com/partnering-with-hugging-face-deploying-ai-easier-affordable/)サポートを提供します。また、特定のデータセンターでGPUを搭載したVMをプロビジョニングするのとは異なり、当社は、ネットワーク全体を1つの巨大なコンピューティングリソースとして扱い、開発者のニーズに応じてモデルを適切な場所で適切なタイミングで実行できるように構築しています。\n\nモデル推論に加えて、Web標準のAPIを強力にサポートし、Workersプラットフォーム内から[WebGPU](https://blog.cloudflare.com/webgpu-in-workers/)APIを利用可能にしています。当社は最先端のプラットフォームとして認識されていることを誇りに思っていますが、それだけではありません。—当社はフルスタックのアプリケーションを開発するためのプラットフォームであり、わずか1年前には、ほとんどの人が利用しなかった、あるいは必要としなかった計算能力を必要とするアプリケーションも開発しています。\n\nCloudflare製品全体で[シークレットを管理する新しい方法](https://blog.cloudflare.com/secrets-store/)、可観測性の向上、変更をリリースするためのより優れたツールなど、次の展開をご覧いただけるのを楽しみにしています。日々、当社のプラットフォーム上で構築されるより高度なアプリケーションを目にしており、その高度なアプリケーションを、最もミッションクリティカルなワークロード（独自のプラットフォーム上で製品を構築するために使用しているのと同じワークロード）に対応するツールと適合させることに注力しています。\n\n[こちらから](https://www.cloudflare.com/lp/forrester-wave-edge-development-q4-2023/)レポートをダウンロードしてください。"
    },
    {
      "url": "https://blog.cloudflare.com/de-de/SLUG-de-de/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/de-de/slug-de-de/",
        "loadedTime": "2023-12-05T02:33:27.223Z",
        "referrerUrl": "https://blog.cloudflare.com/forrester-wave-edge-development-2023/",
        "depth": 2,
        "httpStatusCode": 404
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/de-de/slug-de-de/",
        "title": "The Cloudflare Blog",
        "description": null,
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Page not found\nSorry, we can't find the page you are looking for.\nYou may have used an outdated link, or you may have typed the address incorrectly.\nError Code: 404",
      "markdown": "## Page not found\n\nSorry, we can't find the page you are looking for.\n\nYou may have used an outdated link, or you may have typed the address incorrectly.\n\nError Code: 404"
    },
    {
      "url": "https://blog.cloudflare.com/es-es/forrester-wave-edge-development-2023-es-es/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/es-es/forrester-wave-edge-development-2023-es-es/",
        "loadedTime": "2023-12-05T02:33:31.445Z",
        "referrerUrl": "https://blog.cloudflare.com/forrester-wave-edge-development-2023/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/es-es/forrester-wave-edge-development-2023-es-es/",
        "title": "Cloudflare, reconocida empresa líder en el informe Forrester Edge Development Platforms Wave, 4º trimestre de 2023",
        "description": "Forrester ha reconocido a Cloudflare como empresa líder en el informe The Forrester Wave™: Edge Development Platforms, 4º trimestre de 2023.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/27/2023\n5 min read\nForrester ha reconocido a Cloudflare como empresa líder en el informe The Forrester Wave™: Edge Development Platforms, 4º trimestre de 2023, con la puntuación más alta en la categoría de solución actual. \nSegún el informe de Devin Dickerson, el analista principal, \"la plataforma de desarrollo en el perímetro de Cloudflare proporciona los elementos esenciales que las empresas necesitan para crear aplicaciones distribuidas integrales, y permite a los desarrolladores beneficiarse de una red distribuida globalmente de computación, almacenamiento y seguridad programable sin que sean expertos en el teorema CAP\".\nMás de un millón de desarrolladores están creando aplicaciones utilizando los productos de la plataforma para desarrolladores. Estos productos incluyen, entre otros, Workers, Pages, R2, KV, Queues, Durable Objects, D1, Stream, Images y más. Los desarrolladores pueden implementar fácilmente aplicaciones integrales altamente distribuidas utilizando todo el conjunto de servicios de computación, almacenamiento y para desarrolladores de Cloudflare.\nCon Workers, la red de Cloudflare es programable\n\"Uno de los puntos fuertes de la plataforma es la interoperabilidad con la CDN global programable de Cloudflare en combinación con un modelo de implementación que utiliza la colocación inteligente de la carga de trabajo\". – The Forrester Wave™: Edge Development Platforms, 4º trimestre de 2023\nWorkers se ejecuta en la red global de Cloudflare, proporciona las API para leer y escribir directamente en la caché local, y expone contexto de la CDN de Cloudflare directamente en el objeto de solicitud que recibe un Worker.\nEsta estrecha integración con la red de Cloudflare permite a los desarrolladores crear, proteger y conectar aplicaciones distribuidas globalmente, sin implementaciones en regiones específicas. Smart Placement optimiza Workers para su ejecución en la ubicación que ofrezca el rendimiento global más rápido, ya sea la ubicación más próxima a los datos o la más cercana al usuario. Hyperdrive agrupa automáticamente las conexiones de base de datos, lo que permite la ejecución de Workers en todo el mundo a fin de poder reutilizarlos en las consultas a las bases de datos PostgreSQL. Esto evita las dificultades de escalado que obstaculizan la utilización de las bases de datos tradicionales con una arquitectura sin servidor. Cron Triggers permite hasta 15 minutos de tiempo de CPU, lo que posibilita la realización de tareas de fondo que requieran muchos recursos informáticos.\nCloudflare va más allá del proceso perimetral: los procesos se pueden realizar en todas partes. Utilizamos nuestra red para mejorar el rendimiento de tu aplicación, basada en datos del mundo real y personalizada con patrones de acceso y paradigmas de programación.\nImplementa sistemas distribuidos, sin necesidad de ser un experto en sistemas distribuidos\n\"Nuestros clientes de referencia destacan sistemáticamente la sencillez de los procesos de incorporación, que permite que los desarrolladores sin experiencia previa puedan entregar cargas de trabajo en todo el mundo en cuestión de minutos, y aplicaciones con calidad de producción en una semana\". – The Forrester Wave™: Edge Development Platforms, 4º trimestre de 2023 \nCon Workers, cualquier desarrollador puede implementar aplicaciones distribuidas globalmente, sin necesidad de convertirse en un experto en sistemas distribuidos o en la configuración de una infraestructura de nube.\n- Cuando implementas un Worker, Cloudflare realiza el trabajo interno necesario para distribuirlo en todo el mundo. Sin embargo, para ti, se trata de una sola aplicación que puedes ejecutar y probar localmente, utilizando el mismo entorno de ejecución JavaScript de código abierto en el que se ejecutan tus Workers en producción.\n- Cuando implementas un Durable Object para coordinar el estado en tiempo real, has desarrollado una aplicación distribuida, pero en lugar de tener que aprender protocolos RPC y escalar la infraestructura, lo has programado todo en JavaScript utilizando las API web estándar que los desarrolladores del front-end conocen y en las que confían a diario.\n- La colocación en cola y el proceso de lotes de mensajes con Cloudflare Queues simplemente requiere añadir algunas líneas más de código JavaScript a un Worker existente.\n- Al crear una aplicación web con Cloudflare Pages, hemos configurado un canal de desarrollo e implementación integral continuo con URL de vista previa, simplemente mediante la conexión a un repositorio GitHub.\nLos desarrolladores que anteriormente escribían código de front-end pueden desarrollar el back-end, y crear una aplicación en tiempo real y reactiva. Los equipos atascados a la espera de que los expertos en infraestructura les proporcionen los recursos necesarios pueden empezar a crear un prototipo hoy, no dentro de una semana. Escribir e implementar un Worker es una tarea conocida y accesible, lo que permite a los equipos de ingeniería avanzar más rápido, con menos carga.\n¿Por qué los equipos pueden empezar a utilizarlo tan rápido?\nWorkers utiliza las API web estándar que los desarrolladores de front-end y cualquier persona que desarrolle aplicaciones web ya utiliza a diario. Cloudflare fue miembro fundador del grupo comunitario Web-interoperable Runtimes (WinterCG) y está comprometida con la interoperabilidad en los entornos de ejecución.\nLas herramientas que los desarrolladores utilizan a diario son nativas de nuestra plataforma. Publicamos tipos TypeScript para todas las API, y admitimos la compilación de TypeScript al crear e implementar mediante la interfaz de línea de comandos de Wrangler o mediante el editor de código en el panel de control de Cloudflare (él mismo basado en el popular editor VSCode.\nAdmitimos las infraestructuras de código abierto con las que los desarrolladores prefieren crear. Un conjunto cada vez mayor de API de Node.js están disponibles de forma nativa en el entorno de ejecución de Workers, lo que permite que las bibliotecas de código abierto existentes funcionen en Workers. Asimismo, cada vez más los nuevos proyectos de código abierto de los que dependen los desarrolladores están diseñados desde el primer día para que funcionen en todos los entornos de ejecución de WinterCG. Cada día más miembros del ecosistema JavaScript funcionan en Workers.\nAmpliación a la IA con las GPU, los LLM y más\n\"La visión superior de la empresa se niega a limitar el espacio futuro al perímetro, y su enfoque sistemático del desarrollo de funciones que figuran en el plan de desarrollo sugiere que la empresa estará cada vez mejor posicionada para absorber las cargas de trabajo de los proveedores de hiperescalabilidad en la nube pública\". – The Forrester Wave™: Edge Development Platforms, 4º trimestre de 2023\nHemos desarrollado una completa plataforma de procesos para aplicaciones en producción a escala. Como ahora todos los desarrolladores y empresas están desarrollando o experimentando con la IA, Cloudflare ha hecho que las GPU sean una parte integral de nuestra plataforma para desarrolladores. Hemos hecho que sea tan fácil empezar a utilizar la IA como entregar una carga de trabajo global. A mediados de noviembre, logramos nuestro objetivo de que Workers AI Inference se ejecute en más de 100 ciudades de todo el mundo y, a finales de 2024, Workers AI se ejecutará en prácticamente todas las ciudades donde Cloudflare está presente.\nWorkers AI permite a los desarrolladores crear aplicaciones utilizando los últimos modelos de IA de código abierto, sin que tengan que suministrar ninguna infraestructura o pagar altos costes por capacidad no utilizada. Estamos ampliando la solución para que admita la implementación de modelos directamente desde Hugging Face en Workers AI, para un conjunto cada vez mayor de modelos de IA. A diferencia del suministro de una máquina virtual con una GPU en un centro de datos específico, la estamos desarrollando de tal forma que podemos tratar toda nuestra red como un gigantesco recurso de proceso, que ejecuta los modelos en el lugar adecuado en el momento oportuno para dar servicio a las necesidades de los desarrolladores.\nMás allá de la inferencia de modelos, estamos apostando por ofrecer compatibilidad con las API web estándar y por hacer que la API WebGPU esté disponible desde la plataforma Workers. Nos complace ser reconocidos como una plataforma perimetral líder. Pero no somos solo eso. Somos una plataforma para el desarrollo de aplicaciones integrales, incluso aquellas que requieren una potencia de proceso que apenas hace un año muy pocos utilizaban o necesitaban.\nNos complace mostrarte las próximas novedades, que incluyen una nueva forma de gestionar los secretos en los productos de Cloudflare, mayor observabilidad y mejores herramientas para la publicación de cambios. Vemos que cada día se desarrollan aplicaciones más avanzadas en nuestra plataforma, y estamos decididos a estar a su altura con herramientas que sirvan las cargas de trabajo más críticas (las mismas herramientas que utilizamos nosotros para desarrollar nuestros productos en nuestra propia plataforma).\nDescarga el informe aquí.\nProtegemos redes corporativas completas, ayudamos a los clientes a desarrollar aplicaciones web de forma eficiente, aceleramos cualquier sitio o aplicación web, prevenimos contra los ataques DDoS , mantenemos a raya a los hackers, y podemos ayudarte en tu recorrido hacia la seguridad Zero Trust. \nVisita 1.1.1.1 desde cualquier dispositivo para empezar a utilizar nuestra aplicación gratuita y beneficiarte de una navegación más rápida y segura. \nPara saber más sobre nuestra misión de ayudar a mejorar Internet, empieza aquí. Si estás buscando un nuevo rumbo profesional, consulta nuestras ofertas de empleo. \nForrester (ES) Español",
      "markdown": "11/27/2023\n\n*   [![Brendan Irvine-Broque](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/09/IMG_2312.JPG)](https://blog.cloudflare.com/author/brendan-irvine-broque/)\n*   [![Dawn Parzych](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/04/Untitled---1-of-1.jpeg)](https://blog.cloudflare.com/author/dawn/)\n\n5 min read\n\nForrester ha reconocido a Cloudflare como empresa líder en el informe The Forrester Wave™: Edge Development Platforms, 4º trimestre de 2023, con la puntuación más alta en la categoría de solución actual.\n\nSegún [el informe](https://www.cloudflare.com/lp/forrester-wave-edge-development-q4-2023/) de Devin Dickerson, el analista principal, \"la plataforma de desarrollo en el perímetro de Cloudflare proporciona los elementos esenciales que las empresas necesitan para crear aplicaciones distribuidas integrales, y permite a los desarrolladores beneficiarse de una red distribuida globalmente de computación, almacenamiento y seguridad programable sin que sean expertos en el teorema CAP\".\n\nMás de un millón de desarrolladores están creando aplicaciones utilizando los productos de la plataforma para desarrolladores. Estos productos incluyen, entre otros, [Workers](https://workers.cloudflare.com/), [Pages](https://pages.cloudflare.com/), [R2](https://developers.cloudflare.com/r2/), [KV](https://developers.cloudflare.com/kv/), [Queues](https://developers.cloudflare.com/queues/), [Durable Objects](https://developers.cloudflare.com/durable-objects/), [D1](https://developers.cloudflare.com/d1/), [Stream](https://developers.cloudflare.com/stream/), [Images](https://developers.cloudflare.com/images/) y más. Los desarrolladores pueden implementar fácilmente aplicaciones integrales altamente distribuidas utilizando todo el conjunto de servicios de computación, almacenamiento y para desarrolladores de Cloudflare.\n\n## Con Workers, la red de Cloudflare es programable\n\n> **_\"Uno de los puntos fuertes de la plataforma es la interoperabilidad con la CDN global programable de Cloudflare en combinación con un modelo de implementación que utiliza la colocación inteligente de la carga de trabajo\".  – The Forrester Wave™: Edge Development Platforms, 4º trimestre de 2023_**\n\nWorkers se ejecuta en la [red global de Cloudflare](https://www.cloudflare.com/network/), proporciona las [API](https://developers.cloudflare.com/workers/runtime-apis/cache/) para leer y escribir directamente en la caché local, y expone contexto de la CDN de Cloudflare directamente en el [objeto de solicitud](https://developers.cloudflare.com/workers/runtime-apis/request/#incomingrequestcfproperties) que recibe un Worker.\n\nEsta estrecha integración con la red de Cloudflare permite a los desarrolladores crear, proteger y conectar aplicaciones distribuidas globalmente, sin implementaciones en regiones específicas. [Smart Placement](https://developers.cloudflare.com/workers/configuration/smart-placement/) optimiza Workers para su ejecución en la ubicación que ofrezca el rendimiento global más rápido, ya sea la ubicación más próxima a los datos o la más cercana al usuario. [Hyperdrive](https://developers.cloudflare.com/hyperdrive/learning/how-hyperdrive-works/) agrupa automáticamente las conexiones de base de datos, lo que permite la ejecución de Workers en todo el mundo a fin de poder reutilizarlos en las consultas a las bases de datos PostgreSQL. Esto evita las dificultades de escalado que obstaculizan la utilización de las bases de datos tradicionales con una arquitectura sin servidor. [Cron Triggers](https://developers.cloudflare.com/workers/configuration/cron-triggers/) permite hasta 15 minutos de tiempo de CPU, lo que posibilita la realización de tareas de fondo que requieran muchos recursos informáticos.\n\nCloudflare va más allá del proceso perimetral: los procesos se pueden realizar en todas partes. Utilizamos nuestra red para mejorar el rendimiento de tu aplicación, basada en datos del mundo real y personalizada con patrones de acceso y paradigmas de programación.\n\n## Implementa sistemas distribuidos, sin necesidad de ser un experto en sistemas distribuidos\n\n> **_\"Nuestros clientes de referencia destacan sistemáticamente la sencillez de los procesos de incorporación, que permite que los desarrolladores sin experiencia previa puedan entregar cargas de trabajo en todo el mundo en cuestión de minutos, y aplicaciones con calidad de producción en una semana\". – The Forrester Wave™: Edge Development Platforms, 4º trimestre de 2023_**\n> \n> Con Workers, cualquier desarrollador puede implementar aplicaciones distribuidas globalmente, sin necesidad de convertirse en un experto en sistemas distribuidos o en la configuración de una infraestructura de nube.\n\n\\- Cuando implementas un Worker, Cloudflare realiza el trabajo interno necesario para distribuirlo en todo el mundo. Sin embargo, para ti, se trata de una sola aplicación que puedes [ejecutar y probar localmente](https://developers.cloudflare.com/workers/observability/local-development-and-testing/), utilizando el mismo [entorno de ejecución JavaScript de código abierto](https://github.com/cloudflare/workerd) en el que se ejecutan tus Workers en producción.\n\n\\- Cuando implementas un [Durable Object](https://developers.cloudflare.com/durable-objects/) para coordinar el estado en tiempo real, has desarrollado una aplicación distribuida, pero en lugar de tener que aprender protocolos RPC y escalar la infraestructura, lo has programado todo en JavaScript utilizando las API web estándar que los desarrolladores del front-end conocen y en las que confían a diario.\n\n\\- La colocación en cola y el proceso de lotes de mensajes con [Cloudflare Queues](https://developers.cloudflare.com/queues/) simplemente requiere añadir algunas líneas más de código JavaScript a un Worker existente.\n\n\\- Al crear una aplicación web con [Cloudflare Pages](https://pages.cloudflare.com/), hemos configurado un canal de desarrollo e implementación integral continuo con URL de vista previa, simplemente mediante la conexión a un repositorio GitHub.\n\nLos desarrolladores que anteriormente escribían código de front-end pueden desarrollar el back-end, y crear una aplicación en tiempo real y reactiva. Los equipos atascados a la espera de que los expertos en infraestructura les proporcionen los recursos necesarios pueden empezar a crear un prototipo hoy, no dentro de una semana. Escribir e implementar un Worker es una tarea conocida y accesible, lo que permite a los equipos de ingeniería avanzar más rápido, con menos carga.\n\n¿Por qué los equipos pueden empezar a utilizarlo tan rápido?\n\n1.  Workers utiliza las [API web estándar](https://developers.cloudflare.com/workers/runtime-apis/) que los desarrolladores de front-end y cualquier persona que desarrolle aplicaciones web ya utiliza a diario. Cloudflare fue miembro fundador del grupo comunitario Web-interoperable Runtimes ([WinterCG](https://wintercg.org/)) y está comprometida con la [interoperabilidad en los entornos de ejecución](https://blog.cloudflare.com/socket-api-works-javascript-runtimes-wintercg-polyfill-connect/).\n2.  Las herramientas que los desarrolladores utilizan a diario son nativas de nuestra plataforma. Publicamos tipos [TypeScript](https://www.npmjs.com/package/@cloudflare/workers-types) para todas las API, y admitimos la compilación de TypeScript al crear e implementar mediante la interfaz de línea de comandos de Wrangler o mediante el editor de código en el panel de control de Cloudflare (él mismo [basado en](https://blog.cloudflare.com/improved-quick-edit/) el popular editor VSCode.\n3.  Admitimos las [infraestructuras](https://developers.cloudflare.com/pages/framework-guides/) de código abierto con las que los desarrolladores prefieren crear. Un conjunto cada vez mayor de API de Node.js están [disponibles de forma nativa](https://developers.cloudflare.com/workers/runtime-apis/nodejs/) en el entorno de ejecución de Workers, lo que permite que las bibliotecas de código abierto existentes funcionen en Workers. Asimismo, cada vez más los nuevos proyectos de código abierto de los que dependen los desarrolladores están diseñados desde el primer día para que funcionen en todos los entornos de ejecución de WinterCG. Cada día más miembros del ecosistema JavaScript funcionan en Workers.\n\n### Ampliación a la IA con las GPU, los LLM y más\n\n> **_\"La visión superior de la empresa se niega a limitar el espacio futuro al perímetro, y su enfoque sistemático del desarrollo de funciones que figuran en el plan de desarrollo sugiere que la empresa estará cada vez mejor posicionada para absorber las cargas de trabajo de los proveedores de hiperescalabilidad en la nube pública\". – The Forrester Wave™: Edge Development Platforms, 4º trimestre de 2023_**\n\nHemos desarrollado una completa plataforma de procesos para aplicaciones en producción a escala. Como ahora todos los desarrolladores y empresas están desarrollando o experimentando con la IA, Cloudflare ha hecho que las [GPU sean una parte integral de nuestra plataforma para desarrolladores](https://blog.cloudflare.com/es-es/workers-ai-es-es/). Hemos hecho que sea tan fácil empezar a utilizar la IA como entregar una carga de trabajo global. A mediados de noviembre, logramos nuestro objetivo de que Workers AI Inference se ejecute en [más de 100 ciudades](https://blog.cloudflare.com/workers-ai-update-stable-diffusion-code-llama-workers-ai-in-100-cities/) de todo el mundo y, a finales de 2024, Workers AI se ejecutará en prácticamente todas las ciudades donde Cloudflare está presente.\n\n[Workers AI](https://developers.cloudflare.com/workers-ai/) permite a los desarrolladores crear aplicaciones utilizando los últimos modelos de IA de código abierto, sin que tengan que suministrar ninguna infraestructura o pagar altos costes por capacidad no utilizada. Estamos ampliando la solución para que admita la [implementación de modelos directamente desde Hugging Face en Workers AI](https://blog.cloudflare.com/partnering-with-hugging-face-deploying-ai-easier-affordable/), para un conjunto cada vez mayor de modelos de IA. A diferencia del suministro de una máquina virtual con una GPU en un centro de datos específico, la estamos desarrollando de tal forma que podemos tratar toda nuestra red como un gigantesco recurso de proceso, que ejecuta los modelos en el lugar adecuado en el momento oportuno para dar servicio a las necesidades de los desarrolladores.\n\nMás allá de la inferencia de modelos, estamos apostando por ofrecer compatibilidad con las API web estándar y por hacer que la API [WebGPU](https://blog.cloudflare.com/webgpu-in-workers/) esté disponible desde la plataforma Workers. Nos complace ser reconocidos como una plataforma perimetral líder. Pero no somos solo eso. Somos una plataforma para el desarrollo de aplicaciones integrales, incluso aquellas que requieren una potencia de proceso que apenas hace un año muy pocos utilizaban o necesitaban.\n\nNos complace mostrarte las próximas novedades, que incluyen [una nueva forma de gestionar los secretos](https://blog.cloudflare.com/secrets-store/) en los productos de Cloudflare, mayor observabilidad y mejores herramientas para la publicación de cambios. Vemos que cada día se desarrollan aplicaciones más avanzadas en nuestra plataforma, y estamos decididos a estar a su altura con herramientas que sirvan las cargas de trabajo más críticas (las mismas herramientas que utilizamos nosotros para desarrollar nuestros productos en nuestra propia plataforma).\n\nDescarga el informe [aquí](https://www.cloudflare.com/lp/forrester-wave-edge-development-q4-2023/).\n\nProtegemos [redes corporativas completas](https://www.cloudflare.com/es-es/network-services/), ayudamos a los clientes a desarrollar [aplicaciones web de forma eficiente](https://workers.cloudflare.com/), aceleramos cualquier [sitio o aplicación web, prevenimos contra los ataques DDoS](https://www.cloudflare.com/es-es/ddos/) , mantenemos a raya [a los hackers](https://www.cloudflare.com/es-es/application-security/), y podemos ayudarte en [tu recorrido hacia la seguridad Zero Trust](https://www.cloudflare.com/es-es/products/zero-trust/).\n\nVisita [1.1.1.1](https://1.1.1.1/) desde cualquier dispositivo para empezar a utilizar nuestra aplicación gratuita y beneficiarte de una navegación más rápida y segura.\n\nPara saber más sobre nuestra misión de ayudar a mejorar Internet, empieza [aquí](https://www.cloudflare.com/es-es/learning/what-is-cloudflare/). Si estás buscando un nuevo rumbo profesional, consulta [nuestras ofertas de empleo](https://cloudflare.com/es-es/careers).\n\n[Forrester (ES)](https://blog.cloudflare.com/tag/forrester-es/) [Español](https://blog.cloudflare.com/tag/spanish-es/)"
    },
    {
      "url": "https://blog.cloudflare.com/fr-fr/forrester-wave-edge-development-2023-fr-fr/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/fr-fr/forrester-wave-edge-development-2023-fr-fr/",
        "loadedTime": "2023-12-05T02:33:31.836Z",
        "referrerUrl": "https://blog.cloudflare.com/forrester-wave-edge-development-2023/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/fr-fr/forrester-wave-edge-development-2023-fr-fr/",
        "title": "Cloudflare a été désignée Leader dans le rapport The Forrester New Wave™: Edge Development Platforms du quatrième trimestre 2023",
        "description": "Forrester a reconnu Cloudflare comme un Leader dans le rapport The Forrester Wave™: Edge Development Platforms du quatrième trimestre 2023.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Cloudflare a été désignée Leader dans le rapport The Forrester New Wave™: Edge Development Platforms du quatrième trimestre 2023\n11/27/2023\n6 min read\nForrester a reconnu Cloudflare comme un Leader dans le rapport The Forrester Wave™: Edge Development Platforms du quatrième trimestre 2023, consacré aux plateformes de développement Edge. Cloudflare a remporté le meilleur score dans la catégorie des offres actuelles. \nSelon le rapport rédigé par Devin Dickerson, Principal Analyst, « la plateforme de développement Edge de Cloudflare fournit les composantes fondamentales indispensables aux entreprises pour créer des applications distribuées complètes, et permet aux développeurs de bénéficier d'un réseau mondialement distribué de traitement, de stockage et de sécurité programmable, sans être experts du théorème CAP. »\nPlus d'un million de développeurs créent des applications avec les produits de la plateforme pour développeurs de Cloudflare, parmi lesquels Workers, Pages, R2, KV, Queues, Durable Objects, D1, Stream, Images et d'autres également. Les développeurs peuvent facilement déployer des applications full-stack hautement distribuées grâce à la suite complète de services de traitement, de stockage et de développement de Cloudflare.\nWorkers rend le réseau de Cloudflare programmable\n« L'un des principaux atouts de la plateforme est l'interopérabilité avec le réseau CDN mondial programmable de Cloudflare, associée à un modèle de déploiement reposant sur le placement intelligent des charges de travail. » – The Forrester Wave™: Edge Development Platforms, quatrième trimestre 2023\nWorkers s'exécute sur l'ensemble du réseau mondial de Cloudflare, fournit des API permettant d'exécuter des opérations de lecture et d'écriture directement dans le cache local, et expose le contexte provenant du réseau CDN de Cloudflare sur l'objet d'une requête reçu par une instance Workers.\nCette intégration étroite avec le réseau de Cloudflare permet aux développeurs de créer, protéger et connecter des applications distribuées partout dans le monde, sans les déployer dans des régions spécifiques. Smart Placement optimise les instances Workers afin qu'elles s'exécutent à l'endroit offrant les meilleures performances mondiales, qu'il s'agisse de l'endroit le plus proche des données ou le plus proche de l'utilisateur. Hyperdrive met automatiquement en commun les connexions aux bases de données, permettant aux instances Workers en cours d'exécution dans le monde entier de les réutiliser lors de l'interrogation de bases de données PostgreSQL, évitant ainsi les défis liés à l'évolutivité qui rendent difficile l'utilisation de bases de données traditionnelles avec une architecture serverless. Les Cron Triggers autorisent jusqu'à 15 minutes de temps processeur, permettant l'exécution en tâche de fond d'opérations exigeant une puissance de calcul élevée.\nCloudflare va au-delà de l'informatique de périphérie avec « l'informatique omniprésente » Nous utilisons notre réseau pour rendre vos applications aussi performantes que possible, en les optimisant à partir de données réelles et en les adaptant aux modèles d'accès et aux paradigmes de programmation.\nDéployez des systèmes distribués, même sans être un expert des systèmes distribués\n« Les clients de référence soulignent systématiquement la facilité d'intégration, qui permet aux développeurs sans expérience préalable de mettre en œuvre des charges de travail dans le monde entier en quelques minutes seulement et des applications de qualité en une semaine. » – The Forrester Wave™: Edge Development Platforms, 4e trimestre 2023 \n\nWorkers permet à n'importe quel développeur de déployer des applications distribuées à l'échelle mondiale, sans nécessiter qu'il devienne un expert des systèmes distribués ou de la configuration d'infrastructures de cloud.\n- Lorsque vous déployez une instance Workers, Cloudflare la distribue, en arrière-plan, dans le monde entier. Pour vous, cependant, il s'agit d'une application unique, que vous pouvez exécuter et tester localement avec le même runtime open source JavaScript que celui sur lequel s'exécutent vos instances Workers en production.\n- Lorsque vous déployez une instance Durable Objects pour coordonner un état en temps réel, vous avez développé une application distribuée ; toutefois, au lieu de devoir apprendre à maîtriser les protocoles RPC et gérer l'évolutivité de l'infrastructure, vous avez programmé l'ensemble de la solution en code JavaScript, avec des API web standard que les développeurs frontaux connaissent et utilisent quotidiennement.\n- La mise en file d'attente et le traitement par lot de messages avec Cloudflare Queues ne nécessitent que l'ajout de quelques lignes de code JavaScript à une instance Workers existante.\n- Lorsque vous créez une application web avec Cloudflare Pages, vous avez configuré un pipeline complet de développement et de déploiement continus avec des URL de prévisualisation, simplement en vous connectant à un référentiel GitHub.\nLes développeurs qui ne créaient auparavant que du code frontal sont désormais en mesure de développer le back-end, et ainsi, de créer une application réactive, en temps réel. Les équipes contraintes d'attendre le provisionnement de ressources par les experts en infrastructure peuvent maintenant commencer à créer des prototypes aujourd'hui, plutôt que la semaine prochaine. L'écriture et le déploiement d'une instance Workers sont des opérations familières et accessibles, qui permettent aux équipes d'ingénieurs de progresser plus rapidement, avec moins de frais généraux.\nPourquoi les équipes sont-elles en mesure de commencer à travailler si rapidement ?\nLes instances Workers utilisent des API web standard qu'emploient déjà quotidiennement les développeurs frontaux et toutes les personnes créant des applications web. Cloudflare était un membre fondateur du groupe Web-interoperable Runtimes Community Group (WinterCG), et s'engage à promouvoir l'interopérabilité des différents runtimes.\nLes outils qu'utilisent déjà les développeurs tous les jours sont nativement intégrés à notre plateforme. Nous publions des types TypeScript pour toutes les API et prenons en charge la compilation de TypeScript lors de la création et du déploiement via l'interface de ligne de commande Wrangler ou l'éditeur de code du tableau de bord de Cloudflare, qui repose lui-même sur le célèbre éditeur VSCode.\nLes cadres open source plébiscités par les développeurs sont pris en charge. Un ensemble grandissant d'API de Node.js est nativement disponible dans le runtime Workers, permettant ainsi l'utilisation des bibliothèques open source existantes dans Workers. Et de plus en plus, les nouveaux projets open source dont dépendent les développeurs sont fondamentalement conçus pour s'exécuter sur l'ensemble des runtimes WinterCG. Chaque jour, une plus grande partie de l'écosystème JavaScript devient compatible avec Workers.\nDévelopper autour de l'IA avec les processeurs graphiques, les grands modèles de langage et bien davantage\n« La vision supérieure de l'entreprise refuse de limiter la future empreinte à la périphérie du réseau et son approche systématique du développement de fonctionnalités figurant sur la feuille de route suggère que l'entreprise sera de mieux en mieux placée pour s'attaquer aux charges de travail des acteurs de l'hyperscalarité sur cloud public. » – The Forrester Wave™: Edge Development Platforms, quatrième trimestre 2023\nNous développons une plateforme de traitement complète pour les applications de production à grande échelle. Et puisque toutes les entreprises et tous les développeurs créent des solutions basées sur l'IA ou expérimentent cette dernière, Cloudflare a intégré les processeurs graphiques (GPU) à sa plateforme pour développeurs. Nous avons veillé à ce qu'il soit aussi facile de faire vos premiers pas avec l'IA que de déployer une charge de travail mondiale. À la mi-novembre, nous avons atteint notre objectif, qui était l'exécution de l'inférence de Workers AI dans plus de 100 villes du monde entier ; et d'ici la fin de l'année 2024, Workers AI s'exécutera dans pratiquement toutes les villes où Cloudflare est présente.\nWorkers AIpermet aux développeurs de créer des applications utilisant les tout derniers modèles d'IA open source, sans devoir provisionner une infrastructure, ni s'acquitter du coût élevé de la capacité inutilisée. Nous étendons cette possibilité à la prise en charge du déploiement de modèles directement depuis Hugging Face dans Workers AI, pour un ensemble encore plus vaste de modèles d'IA. Et contrairement au provisionnement d'une machine virtuelle avec un processeur graphique dans un datacenter spécifique, nous développons ce système afin de pouvoir utiliser l'ensemble de notre réseau comme une immense ressource de calcul, en exécutant les modèles à l'endroit et au moment opportuns afin de répondre aux besoins des développeurs.\nAu-delà de l'inférence de modèles, nous redoublons d'efforts afin d'assurer la prise en charge des API web standard et de rendre l'API WebGPU disponible depuis la plateforme Workers. Si nous sommes fiers d'être reconnus comme une plateforme innovante, nous sommes bien plus que cela : nous sommes une plateforme dédiée au développement d'applications full-stack, même celles qui nécessitent une puissance de calcul qui, il y a encore un an de cela, n'était utilisée, voire même demandée que par un nombre très restreint d'utilisateurs.\nNous sommes ravis de vous présenter la suite des événements, notamment une nouvelle façon de gérer les secrets sur l'ensemble des produits Cloudflare, une observabilité améliorée et de meilleurs outils de publication des changements. Chaque jour, nous assistons au développement d'applications plus avancées sur notre plateforme, et nous nous engageons à suivre ce modèle en proposant des outils dédiés à la diffusion des charges de travail les plus vitales – les mêmes outils que nous utilisons nous-mêmes pour développer nos produits sur notre plateforme.\nTéléchargez le rapport ici.\nNous protégeons des réseaux d'entreprise entiers, aidons nos clients à développer efficacement des applications à l'échelle d'Internet, accélérons n'importe quel site web ou application Internet, repoussons les attaques DDoS, maintenons les pirates à distance et pouvons vous aider dans votre parcours vers le Zero Trust. \nRendez-vous sur 1.1.1.1 depuis n'importe quel appareil pour commencer à utiliser notre application gratuite, qui rend votre navigation Internet plus rapide et plus sûre. \nPour en savoir plus sur notre mission visant à bâtir un meilleur Internet, cliquez ici. Si vous cherchez de nouvelles perspectives professionnelles, consultez nos postes vacants. \nForrester (FR) Français",
      "markdown": "## Cloudflare a été désignée Leader dans le rapport The Forrester New Wave™: Edge Development Platforms du quatrième trimestre 2023\n\n11/27/2023\n\n*   [![Dawn Parzych](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/04/Untitled---1-of-1.jpeg)](https://blog.cloudflare.com/author/dawn/)\n*   [![Brendan Irvine-Broque](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/09/IMG_2312.JPG)](https://blog.cloudflare.com/author/brendan-irvine-broque/)\n\n6 min read\n\nForrester a reconnu Cloudflare comme un Leader dans le rapport The Forrester Wave™: Edge Development Platforms du quatrième trimestre 2023, consacré aux plateformes de développement Edge. Cloudflare a remporté le meilleur score dans la catégorie des offres actuelles.\n\nSelon [le rapport](https://www.cloudflare.com/lp/forrester-wave-edge-development-q4-2023/) rédigé par Devin Dickerson, Principal Analyst, « la plateforme de développement Edge de Cloudflare fournit les composantes fondamentales indispensables aux entreprises pour créer des applications distribuées complètes, et permet aux développeurs de bénéficier d'un réseau mondialement distribué de traitement, de stockage et de sécurité programmable, sans être experts du théorème CAP. »\n\nPlus d'un million de développeurs créent des applications avec les produits de la plateforme pour développeurs de Cloudflare, parmi lesquels [Workers](https://workers.cloudflare.com/), [Pages](https://pages.cloudflare.com/), [R2](https://developers.cloudflare.com/r2/), [KV](https://developers.cloudflare.com/kv/), [Queues](https://developers.cloudflare.com/queues/), [Durable Objects](https://developers.cloudflare.com/durable-objects/), [D1](https://developers.cloudflare.com/d1/), [Stream](https://developers.cloudflare.com/stream/), [Images](https://developers.cloudflare.com/images/) et d'autres également. Les développeurs peuvent facilement déployer des applications full-stack hautement distribuées grâce à la suite complète de services de traitement, de stockage et de développement de Cloudflare.\n\n### Workers rend le réseau de Cloudflare programmable\n\n> **_« L'un des principaux atouts de la plateforme est l'interopérabilité avec le réseau CDN mondial programmable de Cloudflare, associée à un modèle de déploiement reposant sur le placement intelligent des charges de travail. »  – The Forrester Wave™: Edge Development Platforms, quatrième trimestre 2023_**\n\nWorkers s'exécute sur l'ensemble du [réseau mondial de Cloudflare](https://www.cloudflare.com/network/), fournit des [API](https://developers.cloudflare.com/workers/runtime-apis/cache/) permettant d'exécuter des opérations de lecture et d'écriture directement dans le cache local, et expose le contexte provenant du réseau CDN de Cloudflare sur l'[objet d'une requête](https://developers.cloudflare.com/workers/runtime-apis/request/#incomingrequestcfproperties) reçu par une instance Workers.\n\nCette intégration étroite avec le réseau de Cloudflare permet aux développeurs de créer, protéger et connecter des applications distribuées partout dans le monde, sans les déployer dans des régions spécifiques. [Smart Placement](https://developers.cloudflare.com/workers/configuration/smart-placement/) optimise les instances Workers afin qu'elles s'exécutent à l'endroit offrant les meilleures performances mondiales, qu'il s'agisse de l'endroit le plus proche des données ou le plus proche de l'utilisateur. [Hyperdrive](https://developers.cloudflare.com/hyperdrive/learning/how-hyperdrive-works/) met automatiquement en commun les connexions aux bases de données, permettant aux instances Workers en cours d'exécution dans le monde entier de les réutiliser lors de l'interrogation de bases de données PostgreSQL, évitant ainsi les défis liés à l'évolutivité qui rendent difficile l'utilisation de bases de données traditionnelles avec une architecture serverless. Les [Cron Triggers](https://developers.cloudflare.com/workers/configuration/cron-triggers/) autorisent jusqu'à 15 minutes de temps processeur, permettant l'exécution en tâche de fond d'opérations exigeant une puissance de calcul élevée.\n\nCloudflare va au-delà de l'informatique de périphérie avec « l'informatique omniprésente » Nous utilisons notre réseau pour rendre vos applications aussi performantes que possible, en les optimisant à partir de données réelles et en les adaptant aux modèles d'accès et aux paradigmes de programmation.\n\n## Déployez des systèmes distribués, même sans être un expert des systèmes distribués\n\n> **_« Les clients de référence soulignent systématiquement la facilité d'intégration, qui permet aux développeurs sans expérience préalable de mettre en œuvre des charges de travail dans le monde entier en quelques minutes seulement et des applications de qualité en une semaine. » – The Forrester Wave™: Edge Development Platforms, 4e trimestre 2023_**\n\n  \nWorkers permet à n'importe quel développeur de déployer des applications distribuées à l'échelle mondiale, sans nécessiter qu'il devienne un expert des systèmes distribués ou de la configuration d'infrastructures de cloud.\n\n\\- Lorsque vous déployez une instance Workers, Cloudflare la distribue, en arrière-plan, dans le monde entier. Pour vous, cependant, il s'agit d'une application unique, que vous pouvez [exécuter et tester localement](https://developers.cloudflare.com/workers/observability/local-development-and-testing/) avec le même [runtime open source JavaScript](https://github.com/cloudflare/workerd) que celui sur lequel s'exécutent vos instances Workers en production.\n\n\\- Lorsque vous déployez une instance [Durable Objects](https://developers.cloudflare.com/durable-objects/) pour coordonner un état en temps réel, vous avez développé une application distribuée ; toutefois, au lieu de devoir apprendre à maîtriser les protocoles RPC et gérer l'évolutivité de l'infrastructure, vous avez programmé l'ensemble de la solution en code JavaScript, avec des API web standard que les développeurs frontaux connaissent et utilisent quotidiennement.\n\n\\- La mise en file d'attente et le traitement par lot de messages avec [Cloudflare Queues](https://developers.cloudflare.com/queues/) ne nécessitent que l'ajout de quelques lignes de code JavaScript à une instance Workers existante.\n\n\\- Lorsque vous créez une application web avec [Cloudflare Pages](https://pages.cloudflare.com/), vous avez configuré un pipeline complet de développement et de déploiement continus avec des URL de prévisualisation, simplement en vous connectant à un référentiel GitHub.\n\nLes développeurs qui ne créaient auparavant que du code frontal sont désormais en mesure de développer le back-end, et ainsi, de créer une application réactive, en temps réel. Les équipes contraintes d'attendre le provisionnement de ressources par les experts en infrastructure peuvent maintenant commencer à créer des prototypes aujourd'hui, plutôt que la semaine prochaine. L'écriture et le déploiement d'une instance Workers sont des opérations familières et accessibles, qui permettent aux équipes d'ingénieurs de progresser plus rapidement, avec moins de frais généraux.\n\nPourquoi les équipes sont-elles en mesure de commencer à travailler si rapidement ?\n\n1.  Les instances Workers utilisent des [API web standard](https://developers.cloudflare.com/workers/runtime-apis/) qu'emploient déjà quotidiennement les développeurs frontaux et toutes les personnes créant des applications web. Cloudflare était un membre fondateur du groupe Web-interoperable Runtimes Community Group ([WinterCG](https://wintercg.org/)), et s'engage à promouvoir l'[interopérabilité des différents runtimes](https://blog.cloudflare.com/socket-api-works-javascript-runtimes-wintercg-polyfill-connect/).\n2.  Les outils qu'utilisent déjà les développeurs tous les jours sont nativement intégrés à notre plateforme. Nous publions des types [TypeScript](https://www.npmjs.com/package/@cloudflare/workers-types) pour toutes les API et prenons en charge la compilation de TypeScript lors de la création et du déploiement via l'interface de ligne de commande Wrangler ou l'éditeur de code du tableau de bord de Cloudflare, qui [repose lui-même sur](https://blog.cloudflare.com/improved-quick-edit/) le célèbre éditeur VSCode.\n3.  Les [cadres](https://developers.cloudflare.com/pages/framework-guides/) open source plébiscités par les développeurs sont pris en charge. Un ensemble grandissant d'API de Node.js est [nativement disponible](https://developers.cloudflare.com/workers/runtime-apis/nodejs/) dans le runtime Workers, permettant ainsi l'utilisation des bibliothèques open source existantes dans Workers. Et de plus en plus, les nouveaux projets open source dont dépendent les développeurs sont fondamentalement conçus pour s'exécuter sur l'ensemble des runtimes WinterCG. Chaque jour, une plus grande partie de l'écosystème JavaScript devient compatible avec Workers.\n\n### Développer autour de l'IA avec les processeurs graphiques, les grands modèles de langage et bien davantage\n\n> **_« La vision supérieure de l'entreprise refuse de limiter la future empreinte à la périphérie du réseau et son approche systématique du développement de fonctionnalités figurant sur la feuille de route suggère que l'entreprise sera de mieux en mieux placée pour s'attaquer aux charges de travail des acteurs de l'hyperscalarité sur cloud public. » – The Forrester Wave™: Edge Development Platforms, quatrième trimestre 2023_**\n\nNous développons une plateforme de traitement complète pour les applications de production à grande échelle. Et puisque toutes les entreprises et tous les développeurs créent des solutions basées sur l'IA ou expérimentent cette dernière, Cloudflare [a intégré les processeurs graphiques (GPU) à sa plateforme pour développeurs](https://blog.cloudflare.com/workers-ai). Nous avons veillé à ce qu'il soit aussi facile de faire vos premiers pas avec l'IA que de déployer une charge de travail mondiale. À la mi-novembre, nous avons atteint notre objectif, qui était l'exécution de l'inférence de Workers AI dans [plus de 100 villes](https://blog.cloudflare.com/workers-ai-update-stable-diffusion-code-llama-workers-ai-in-100-cities/) du monde entier ; et d'ici la fin de l'année 2024, Workers AI s'exécutera dans pratiquement toutes les villes où Cloudflare est présente.\n\n[Workers AI](https://developers.cloudflare.com/workers-ai/)permet aux développeurs de créer des applications utilisant les tout derniers modèles d'IA open source, sans devoir provisionner une infrastructure, ni s'acquitter du coût élevé de la capacité inutilisée. Nous étendons cette possibilité à la prise en charge du [déploiement de modèles directement depuis Hugging Face dans Workers AI](https://blog.cloudflare.com/partnering-with-hugging-face-deploying-ai-easier-affordable/), pour un ensemble encore plus vaste de modèles d'IA. Et contrairement au provisionnement d'une machine virtuelle avec un processeur graphique dans un datacenter spécifique, nous développons ce système afin de pouvoir utiliser l'ensemble de notre réseau comme une immense ressource de calcul, en exécutant les modèles à l'endroit et au moment opportuns afin de répondre aux besoins des développeurs.\n\nAu-delà de l'inférence de modèles, nous redoublons d'efforts afin d'assurer la prise en charge des API web standard et de rendre l'API [WebGPU](https://blog.cloudflare.com/webgpu-in-workers/) disponible depuis la plateforme Workers. Si nous sommes fiers d'être reconnus comme une plateforme innovante, nous sommes bien plus que cela : nous sommes une plateforme dédiée au développement d'applications full-stack, même celles qui nécessitent une puissance de calcul qui, il y a encore un an de cela, n'était utilisée, voire même demandée que par un nombre très restreint d'utilisateurs.\n\nNous sommes ravis de vous présenter la suite des événements, notamment [une nouvelle façon de gérer les secrets](https://blog.cloudflare.com/secrets-store/) sur l'ensemble des produits Cloudflare, une observabilité améliorée et de meilleurs outils de publication des changements. Chaque jour, nous assistons au développement d'applications plus avancées sur notre plateforme, et nous nous engageons à suivre ce modèle en proposant des outils dédiés à la diffusion des charges de travail les plus vitales – les mêmes outils que nous utilisons nous-mêmes pour développer nos produits sur notre plateforme.\n\nTéléchargez le rapport [ici](https://www.cloudflare.com/lp/forrester-wave-edge-development-q4-2023/).\n\nNous protégeons [des réseaux d'entreprise entiers](https://www.cloudflare.com/fr-fr/network-services/), aidons nos clients à développer [efficacement des applications à l'échelle d'Internet](https://workers.cloudflare.com/), accélérons n'importe quel [site web ou application Internet,](https://www.cloudflare.com/fr-fr/performance/accelerate-internet-applications/) repoussons [les attaques DDoS](https://www.cloudflare.com/fr-fr/ddos/), maintenons [les pirates à distance](https://www.cloudflare.com/fr-fr/application-security/) et pouvons vous aider dans [votre parcours vers le Zero Trust](https://www.cloudflare.com/fr-fr/products/zero-trust/).\n\nRendez-vous sur [1.1.1.1](https://1.1.1.1/) depuis n'importe quel appareil pour commencer à utiliser notre application gratuite, qui rend votre navigation Internet plus rapide et plus sûre.\n\nPour en savoir plus sur notre mission visant à bâtir un meilleur Internet, cliquez [ici](https://www.cloudflare.com/fr-fr/learning/what-is-cloudflare/). Si vous cherchez de nouvelles perspectives professionnelles, consultez [nos postes vacants](https://cloudflare.com/fr-fr/careers).\n\n[Forrester (FR)](https://blog.cloudflare.com/tag/forrester-fr/) [Français](https://blog.cloudflare.com/tag/french-fr/)"
    },
    {
      "url": "https://blog.cloudflare.com/socket-api-works-javascript-runtimes-wintercg-polyfill-connect/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/socket-api-works-javascript-runtimes-wintercg-polyfill-connect/",
        "loadedTime": "2023-12-05T02:33:39.625Z",
        "referrerUrl": "https://blog.cloudflare.com/forrester-wave-edge-development-2023/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/socket-api-works-javascript-runtimes-wintercg-polyfill-connect/",
        "title": "A Socket API that works across JavaScript runtimes — announcing a WinterCG spec and Node.js implementation of connect()",
        "description": "Engineers from Cloudflare and Vercel have published a specification of the connect() sockets API for review by the community, along with a Node.js compatible implementation of connect() that developers can start using today",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "09/28/2023\n4 min read\nEarlier this year, we announced a new API for creating outbound TCP sockets — connect(). From day one, we’ve been working with the Web-interoperable Runtimes Community Group (WinterCG) community to chart a course toward making this API a standard, available across all runtimes and platforms — including Node.js.\nToday, we’re sharing that we’ve reached a new milestone in the path to making this API available across runtimes — engineers from Cloudflare and Vercel have published a draft specification of the connect() sockets API for review by the community, along with a Node.js compatible implementation of the connect() API that developers can start using today.\nThis implementation helps both application developers and maintainers of libraries and frameworks:\nMaintainers of existing libraries that use the node:net and node:tls APIs can use it to more easily add support for runtimes where node:net and node:tls are not available.\nJavaScript frameworks can use it to make connect() available in local development, making it easier for application developers to target runtimes that provide connect().\nWhy create a new standard? Why connect()?\nAs we described when we first announced connect(), to-date there has not been a standard API across JavaScript runtimes for creating and working with TCP or UDP sockets. This makes it harder for maintainers of open-source libraries to ensure compatibility across runtimes, and ultimately creates friction for application developers who have to navigate which libraries work on which platforms.\nWhile Node.js provides the node:net and node:tls APIs, these APIs were designed over 10 years ago in the very early days of the Node.js project and remain callback-based. As a result, they can be hard to work with, and expose configuration in ways that don’t fit serverless platforms or web browsers.\nThe connect() API fills this gap by incorporating the best parts of existing socket APIs and prior proposed standards, based on feedback from the JavaScript community — including contributors to Node.js. Libraries like pg (node-postgres on Github) are already using the connect() API.\nThe connect() specification\nAt time of writing, the draft specification of the Sockets API defines the following API:\ndictionary SocketAddress { DOMString hostname; unsigned short port; }; typedef (DOMString or SocketAddress) AnySocketAddress; enum SecureTransportKind { \"off\", \"on\", \"starttls\" }; [Exposed=*] dictionary SocketOptions { SecureTransportKind secureTransport = \"off\"; boolean allowHalfOpen = false; }; [Exposed=*] interface Connect { Socket connect(AnySocketAddress address, optional SocketOptions opts); }; interface Socket { readonly attribute ReadableStream readable; readonly attribute WritableStream writable; readonly attribute Promise<undefined> closed; Promise<undefined> close(); Socket startTls(); }; \nThe proposed API is Promise-based and reuses existing standards whenever possible. For example, ReadableStream and WritableStream are used for the read and write ends of the socket. This makes it easy to pipe data from a TCP socket to any other library or existing code that accepts a ReadableStream as input, or to write to a TCP socket via a WritableStream.\nThe entrypoint of the API is the connect() function, which takes a string containing both the hostname and port separated by a colon, or an object with discrete hostname and port fields. It returns a Socket object which represents a socket connection. An instance of this object exposes attributes and methods for working with the connection.\nA connection can be established in plain-text or TLS mode, as well as a special “starttls” mode which allows the socket to be easily upgraded to TLS after some period of plain-text data transfer, by calling the startTls() method on the Socket object. No need to create a new socket or switch to using a separate set of APIs once the socket is upgraded to use TLS.\nFor example, to upgrade a socket using the startTLS pattern, you might do something like this:\nimport { connect } from \"@arrowood.dev/socket\" const options = { secureTransport: \"starttls\" }; const socket = connect(\"address:port\", options); const secureSocket = socket.startTls(); // The socket is immediately writable // Relies on web standard WritableStream const writer = secureSocket.writable.getWriter(); const encoder = new TextEncoder(); const encoded = encoder.encode(\"hello\"); await writer.write(encoded); \nEquivalent code using the node:net and node:tls APIs:\nimport net from 'node:net' import tls from 'node:tls' const socket = new net.Socket(HOST, PORT); socket.once('connect', () => { const options = { socket }; const secureSocket = tls.connect(options, () => { // The socket can only be written to once the // connection is established. // Polymorphic API, uses Node.js streams secureSocket.write('hello'); } }) \nUse the Node.js implementation of connect() in your library\nTo make it easier for open-source library maintainers to adopt the connect() API, we’ve published an implementation of connect() in Node.js that allows you to publish your library such that it works across JavaScript runtimes, without having to maintain any runtime-specific code.\nTo get started, install it as a dependency:\nnpm install --save @arrowood.dev/socket\nAnd import it in your library or application:\nimport { connect } from \"@arrowood.dev/socket\"\nWhat’s next for connect()?\nThe wintercg/proposal-sockets-api is published as a draft, and the next step is to solicit and incorporate feedback. We’d love your feedback, particularly if you maintain an open-source library or make direct use of the node:net or node:tls APIs.\nOnce feedback has been incorporated, engineers from Cloudflare, Vercel and beyond will be continuing to work towards contributing an implementation of the API directly to Node.js as a built-in API.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nBirthday Week Product News Cloudflare Workers Developers TCP \nRelated Posts\nOctober 02, 2023 2:00PM\nBirthday Week recap: everything we announced — plus an AI-powered opportunity for startups\nNeed a recap or refresher on all the big Birthday Week news this week? This recap has you covered...\nBy \nSeptember 28, 2018 8:40PM\nBirthday Week Wrap-Up: Every day is launch day at Cloudflare\nOur customers are accustomed to us launching new services, features, and functionality at a feverish pace, but recently, we’ve been especially active. This week we celebrated our 8th Birthday Week by announcing new offerings that benefit our customers and the global Internet community....\nBy \nJanuary 07, 2022 3:57PM\nCloudflare Innovation Weeks 2021\nAs we start planning our 2022 Innovation Weeks, we are reflecting back on the highlights from each of these weeks...\nBy \nSeptember 27, 2019 8:00PM\nBirthday Week 2019 Wrap-up\nThis week we celebrated Cloudflare’s 9th birthday by launching a variety of new offerings that support our mission: to help build a better Internet. Below is a summary recap of how we celebrated Birthday Week 2019....\nBy",
      "markdown": "09/28/2023\n\n*   [![Dominik Picheta](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/09/_tmp_mini_magick20230403-40-vmc77s.jpg)](https://blog.cloudflare.com/author/dominik/)\n*   [![James M Snell](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/11/mecoffee.jpg)](https://blog.cloudflare.com/author/jasnell/)\n*   [![Ethan Arrowood (Guest author)](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/09/Ethan-Arrowood.jpeg)](https://blog.cloudflare.com/author/ethan-arrowood/)\n\n4 min read\n\n![A Socket API that works across JavaScript runtimes — announcing a WinterCG spec and Node.js implementation of  connect()](https://blog.cloudflare.com/content/images/2023/09/Connect---__-WinterCG.png)\n\nEarlier this year, we [announced a new API for creating outbound TCP sockets](https://blog.cloudflare.com/workers-tcp-socket-api-connect-databases/) — [connect()](https://developers.cloudflare.com/workers/runtime-apis/tcp-sockets?cf_target_id=6F3FD2F2360D5526EEE56A7398DB7D9D). From day one, we’ve been working with the [Web-interoperable Runtimes Community Group (WinterCG) community](https://wintercg.org/) to chart a course toward making this API a standard, available across all runtimes and platforms — including Node.js.\n\nToday, we’re sharing that we’ve reached a new milestone in the path to making this API available across runtimes — engineers from Cloudflare and Vercel have published [a draft specification of the connect() sockets API](https://sockets-api.proposal.wintercg.org/) for review by the community, along with a Node.js compatible [implementation of the connect() API](https://github.com/Ethan-Arrowood/socket) that developers can start using today.\n\nThis implementation helps both application developers and maintainers of libraries and frameworks:\n\n1.  Maintainers of existing libraries that use the [node:net](https://nodejs.org/api/net.html) and [node:tls](https://nodejs.org/api/tls.html) APIs can use it to more easily add support for runtimes where node:net and node:tls are not available.\n2.  JavaScript frameworks can use it to make connect() available in local development, making it easier for application developers to target runtimes that provide connect().\n\n### Why create a new standard? Why connect()?\n\nAs we [described when we first announced connect()](https://blog.cloudflare.com/workers-tcp-socket-api-connect-databases/), to-date there has not been a standard API across JavaScript runtimes for creating and working with TCP or UDP sockets. This makes it harder for maintainers of open-source libraries to ensure compatibility across runtimes, and ultimately creates friction for application developers who have to navigate which libraries work on which platforms.\n\nWhile Node.js provides the [node:net](https://nodejs.org/api/net.html) and [node:tls](https://nodejs.org/api/tls.html) APIs, these APIs were designed over 10 years ago in the very early days of the Node.js project and remain callback-based. As a result, they can be hard to work with, and expose configuration in ways that don’t fit serverless platforms or web browsers.\n\nThe connect() API fills this gap by incorporating the best parts of existing socket APIs and [prior proposed standards](https://github.com/WICG/direct-sockets/blob/main/docs/explainer.md), based on feedback from the JavaScript community — including contributors to Node.js. Libraries like [pg](https://www.npmjs.com/package/pg) ([node-postgres](https://github.com/brianc/node-postgres) on Github) are already using the connect() API.\n\n### The connect() specification\n\nAt time of writing, the [draft specification of the Sockets API](https://sockets-api.proposal.wintercg.org/) defines the following API:\n\n```\ndictionary SocketAddress {\n  DOMString hostname;\n  unsigned short port;\n};\n\ntypedef (DOMString or SocketAddress) AnySocketAddress;\n\nenum SecureTransportKind { \"off\", \"on\", \"starttls\" };\n\n[Exposed=*]\ndictionary SocketOptions {\n  SecureTransportKind secureTransport = \"off\";\n  boolean allowHalfOpen = false;\n};\n\n[Exposed=*]\ninterface Connect {\n  Socket connect(AnySocketAddress address, optional SocketOptions opts);\n};\n\ninterface Socket {\n  readonly attribute ReadableStream readable;\n  readonly attribute WritableStream writable;\n\n  readonly attribute Promise<undefined> closed;\n  Promise<undefined> close();\n\n  Socket startTls();\n};\n```\n\nThe proposed API is Promise-based and reuses existing standards whenever possible. For example, [ReadableStream](https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream) and [WritableStream](https://developer.mozilla.org/en-US/docs/Web/API/WritableStream) are used for the read and write ends of the socket. This makes it easy to pipe data from a TCP socket to any other library or existing code that accepts a ReadableStream as input, or to write to a TCP socket via a WritableStream.\n\nThe entrypoint of the API is the connect() function, which takes a string containing both the hostname and port separated by a colon, or an object with discrete hostname and port fields. It returns a Socket object which represents a socket connection. An instance of this object exposes attributes and methods for working with the connection.\n\nA connection can be established in plain-text or TLS mode, as well as a special “starttls” mode which allows the socket to be easily upgraded to TLS after some period of plain-text data transfer, by calling the startTls() method on the Socket object. No need to create a new socket or switch to using a separate set of APIs once the socket is upgraded to use TLS.\n\nFor example, to upgrade a socket using the startTLS pattern, you might do something like this:\n\n```\nimport { connect } from \"@arrowood.dev/socket\"\n\nconst options = { secureTransport: \"starttls\" };\nconst socket = connect(\"address:port\", options);\nconst secureSocket = socket.startTls();\n// The socket is immediately writable\n// Relies on web standard WritableStream\nconst writer = secureSocket.writable.getWriter();\nconst encoder = new TextEncoder();\nconst encoded = encoder.encode(\"hello\");\nawait writer.write(encoded);\n```\n\nEquivalent code using the node:net and node:tls APIs:\n\n```\nimport net from 'node:net'\nimport tls from 'node:tls'\n\nconst socket = new net.Socket(HOST, PORT);\nsocket.once('connect', () => {\n  const options = { socket };\n  const secureSocket = tls.connect(options, () => {\n    // The socket can only be written to once the\n    // connection is established.\n    // Polymorphic API, uses Node.js streams\n    secureSocket.write('hello');\n  }\n})\n```\n\n### Use the Node.js implementation of connect() in your library\n\nTo make it easier for open-source library maintainers to adopt the connect() API, we’ve published an [implementation of connect() in Node.js](https://github.com/Ethan-Arrowood/socket) that allows you to publish your library such that it works across JavaScript runtimes, without having to maintain any runtime-specific code.  \n\nTo get started, install it as a dependency:\n\n```\nnpm install --save @arrowood.dev/socket\n```\n\nAnd import it in your library or application:\n\n```\nimport { connect } from \"@arrowood.dev/socket\"\n```\n\n### What’s next for connect()?\n\nThe [wintercg/proposal-sockets-api](https://github.com/wintercg/proposal-sockets-api/) is published as a draft, and the next step is to solicit and incorporate feedback. We’d love your feedback, particularly if you maintain an open-source library or make direct use of the node:net or node:tls APIs.\n\nOnce feedback has been incorporated, engineers from Cloudflare, Vercel and beyond will be continuing to work towards contributing an implementation of the API directly to Node.js as a built-in API.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [Product News](https://blog.cloudflare.com/tag/product-news/) [Cloudflare Workers](https://blog.cloudflare.com/tag/workers/) [Developers](https://blog.cloudflare.com/tag/developers/) [TCP](https://blog.cloudflare.com/tag/tcp/)\n\nRelated Posts\n\nOctober 02, 2023 2:00PM\n\n[\n\n## Birthday Week recap: everything we announced — plus an AI-powered opportunity for startups\n\n](https://blog.cloudflare.com/birthday-week-2023-wrap-up/)\n\nNeed a recap or refresher on all the big Birthday Week news this week? This recap has you covered...\n\nBy \n\nSeptember 28, 2018 8:40PM\n\n[\n\n## Birthday Week Wrap-Up: Every day is launch day at Cloudflare\n\n](https://blog.cloudflare.com/birthday-week-2018-wrap-up/)\n\nOur customers are accustomed to us launching new services, features, and functionality at a feverish pace, but recently, we’ve been especially active. This week we celebrated our 8th Birthday Week by announcing new offerings that benefit our customers and the global Internet community....\n\nBy \n\nJanuary 07, 2022 3:57PM\n\n[\n\n## Cloudflare Innovation Weeks 2021\n\n](https://blog.cloudflare.com/2021-innovations-weeks/)\n\nAs we start planning our 2022 Innovation Weeks, we are reflecting back on the highlights from each of these weeks...\n\nBy \n\nSeptember 27, 2019 8:00PM\n\n[\n\n## Birthday Week 2019 Wrap-up\n\n](https://blog.cloudflare.com/birthday-week-2019-wrap-up/)\n\nThis week we celebrated Cloudflare’s 9th birthday by launching a variety of new offerings that support our mission: to help build a better Internet. Below is a summary recap of how we celebrated Birthday Week 2019....\n\nBy"
    },
    {
      "url": "https://blog.cloudflare.com/partnering-with-hugging-face-deploying-ai-easier-affordable/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/partnering-with-hugging-face-deploying-ai-easier-affordable/",
        "loadedTime": "2023-12-05T02:33:48.043Z",
        "referrerUrl": "https://blog.cloudflare.com/forrester-wave-edge-development-2023/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/partnering-with-hugging-face-deploying-ai-easier-affordable/",
        "title": "Partnering with Hugging Face to make deploying AI easier and more affordable than ever 🤗",
        "description": "Today, we’re excited to announce that we are partnering with Hugging Face to make AI models more accessible and affordable than ever before to developers.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "09/27/2023\n3 min read\nThis post is also available in 简体中文, 日本語, 한국어, Deutsch, Français and Español.\nToday, we’re excited to announce that we are partnering with Hugging Face to make AI models more accessible and affordable than ever before to developers.\nThere are three things we look forward to making available to developers over the coming months:\nWe’re excited to bring serverless GPU models to Hugging Face — no more wrangling infrastructure or paying for unused capacity. Just pick your model, and go;\nBringing popular Hugging Face optimized models to Cloudflare’s model catalog;\nIntroduce Cloudflare integrations as a part of Hugging Face’s Inference solutions.\nHosting over 500,000 models and serving over one million model downloads a day, Hugging Face is the go-to place for developers to add AI to their applications.\nMeanwhile, over the past six years at Cloudflare, our goal has been to make it as easy as possible for developers to bring their ideas and applications to life on our developer platform.\nAs AI has become a critical part of every application, this partnership has felt like a natural match to put tools in the hands of developers to make deploying AI easy and affordable.\n“Hugging Face and Cloudflare both share a deep focus on making the latest AI innovations as accessible and affordable as possible for developers. We’re excited to offer serverless GPU services in partnership with Cloudflare to help developers scale their AI apps from zero to global, with no need to wrangle infrastructure or predict the future needs of your application — just pick your model and deploy.” \n— Clem Delangue, CEO of Hugging Face.\nWe’re excited to share what’s to come, so we wanted to give you a sneak peek into what’s ahead.\nHugging Face models at your fingertips\nAs a developer, when you have an idea, you want to be able to act on it as quickly as possible. Through our partnership, we’re excited to provide you with familiar models, regardless of where you’re getting started.\nIf you’re using Cloudflare’s developer platform to build applications, we’re excited to bring Hugging Face models into the flow as a native part of the experience. You will soon be able to deploy Hugging Face models, optimized for performance and speed, right from Cloudflare’s dashboard.\nAlternatively, if you’re used to perusing and finding your models on Hugging Face, you will soon be able to deploy them directly from the Hugging Face UI directly to Workers AI.\nBoth of our teams are committed to building the best developer experiences possible, so we look forward to continuing to file away any friction that gets in developers’ ways of building the next big AI idea.\nBringing serverless GPU inference to Hugging Face users\nHugging Face offers multiple inference solutions to serve predictions from the 500,000 models hosted on the platform without managing infrastructure, from the free and rate-limited Inference API, to dedicated infrastructure deployments with Inference Endpoints, and even in-browser edge inference with Transformers.js.\nWe look forward to working closely with the teams at Hugging Face to enable new experiences powered by Cloudflare: from new serverless GPU inference solutions, to new edge use cases - stay tuned!\nSee you soon!\nWe couldn’t wait to share the news with our developers about our partnership, and can’t wait to put these experiences in the hands of developers over the coming months.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nBirthday Week \nRelated Posts\nOctober 02, 2023 2:00PM\nBirthday Week recap: everything we announced — plus an AI-powered opportunity for startups\nNeed a recap or refresher on all the big Birthday Week news this week? This recap has you covered...\nBy \nSeptember 28, 2018 8:40PM\nBirthday Week Wrap-Up: Every day is launch day at Cloudflare\nOur customers are accustomed to us launching new services, features, and functionality at a feverish pace, but recently, we’ve been especially active. This week we celebrated our 8th Birthday Week by announcing new offerings that benefit our customers and the global Internet community....\nBy \nJanuary 07, 2022 3:57PM\nCloudflare Innovation Weeks 2021\nAs we start planning our 2022 Innovation Weeks, we are reflecting back on the highlights from each of these weeks...\nBy \nSeptember 27, 2019 8:00PM\nBirthday Week 2019 Wrap-up\nThis week we celebrated Cloudflare’s 9th birthday by launching a variety of new offerings that support our mission: to help build a better Internet. Below is a summary recap of how we celebrated Birthday Week 2019....\nBy",
      "markdown": "09/27/2023\n\n*   [![Rita Kozlov](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/05/Rita-Kozlov.jpeg)](https://blog.cloudflare.com/author/rita/)\n*   [![Philipp Schmid (Guest Author)](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/09/Philipp-Schmid-1.jpeg)](https://blog.cloudflare.com/author/philipp-schmid-guest-author/)\n*   [![Jeff Boudier (Guest Author)](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/09/Jeff-Boudier.jpeg)](https://blog.cloudflare.com/author/jeff-boudier-guest-author/)\n\n3 min read\n\nThis post is also available in [简体中文](https://blog.cloudflare.com/zh-cn/partnering-with-hugging-face-deploying-ai-easier-affordable-zh-cn/), [日本語](https://blog.cloudflare.com/ja-jp/partnering-with-hugging-face-deploying-ai-easier-affordable-ja-jp/), [한국어](https://blog.cloudflare.com/ko-kr/partnering-with-hugging-face-deploying-ai-easier-affordable-ko-kr/), [Deutsch](https://blog.cloudflare.com/de-de/partnering-with-hugging-face-deploying-ai-easier-affordable-de-de/), [Français](https://blog.cloudflare.com/fr-fr/partnering-with-hugging-face-deploying-ai-easier-affordable-fr-fr/) and [Español](https://blog.cloudflare.com/es-es/partnering-with-hugging-face-deploying-ai-easier-affordable-es-es/).\n\n![Partnering with Hugging Face to make deploying AI easier and more affordable than ever 🤗](https://blog.cloudflare.com/content/images/2023/09/image3-26.png)\n\nToday, we’re excited to announce that we are partnering with Hugging Face to make AI models more accessible and affordable than ever before to developers.\n\nThere are three things we look forward to making available to developers over the coming months:\n\n1.  We’re excited to bring serverless GPU models to Hugging Face — no more wrangling infrastructure or paying for unused capacity. Just pick your model, and go;\n2.  Bringing popular Hugging Face optimized models to Cloudflare’s model catalog;\n3.  Introduce Cloudflare integrations as a part of Hugging Face’s Inference solutions.\n\nHosting over 500,000 models and serving over one million model downloads a day, Hugging Face is the go-to place for developers to add AI to their applications.\n\nMeanwhile, over the past six years at Cloudflare, our goal has been to make it as easy as possible for developers to bring their ideas and applications to life on our developer platform.\n\nAs AI has become a critical part of every application, this partnership has felt like a natural match to put tools in the hands of developers to make deploying AI easy and affordable.\n\n> _“Hugging Face and Cloudflare both share a deep focus on making the latest AI innovations as accessible and affordable as possible for developers. We’re excited to offer serverless GPU services in partnership with Cloudflare to help developers scale their AI apps from zero to global, with no need to wrangle infrastructure or predict the future needs of your application — just pick your model and deploy.”  \n> — **Clem Delangue**, CEO of Hugging Face._\n\nWe’re excited to share what’s to come, so we wanted to give you a sneak peek into what’s ahead.\n\n### Hugging Face models at your fingertips\n\nAs a developer, when you have an idea, you want to be able to act on it as quickly as possible. Through our partnership, we’re excited to provide you with familiar models, regardless of where you’re getting started.\n\nIf you’re using Cloudflare’s developer platform to build applications, we’re excited to bring Hugging Face models into the flow as a native part of the experience. You will soon be able to deploy Hugging Face models, optimized for performance and speed, right from Cloudflare’s dashboard.\n\n![](https://blog.cloudflare.com/content/images/2023/09/image1-24.png)\n\nAlternatively, if you’re used to perusing and finding your models on Hugging Face, you will soon be able to deploy them directly from the Hugging Face UI directly to Workers AI.\n\n![](https://blog.cloudflare.com/content/images/2023/09/group_4.png)\n\nBoth of our teams are committed to building the best developer experiences possible, so we look forward to continuing to file away any friction that gets in developers’ ways of building the next big AI idea.\n\n### Bringing serverless GPU inference to Hugging Face users\n\nHugging Face offers multiple inference solutions to serve predictions from the 500,000 models hosted on the platform without managing infrastructure, from the free and rate-limited [Inference API](https://huggingface.co/docs/api-inference/index), to dedicated infrastructure deployments with [Inference Endpoints](https://huggingface.co/inference-endpoints), and even in-browser edge inference with [Transformers.js](https://huggingface.co/docs/transformers.js/index).\n\nWe look forward to working closely with the teams at Hugging Face to enable new experiences powered by Cloudflare: from new serverless GPU inference solutions, to new edge use cases - stay tuned!\n\n### See you soon!\n\nWe couldn’t wait to share the news with our developers about our partnership, and can’t wait to put these experiences in the hands of developers over the coming months.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/)\n\nRelated Posts\n\nOctober 02, 2023 2:00PM\n\n[\n\n## Birthday Week recap: everything we announced — plus an AI-powered opportunity for startups\n\n](https://blog.cloudflare.com/birthday-week-2023-wrap-up/)\n\nNeed a recap or refresher on all the big Birthday Week news this week? This recap has you covered...\n\nBy \n\nSeptember 28, 2018 8:40PM\n\n[\n\n## Birthday Week Wrap-Up: Every day is launch day at Cloudflare\n\n](https://blog.cloudflare.com/birthday-week-2018-wrap-up/)\n\nOur customers are accustomed to us launching new services, features, and functionality at a feverish pace, but recently, we’ve been especially active. This week we celebrated our 8th Birthday Week by announcing new offerings that benefit our customers and the global Internet community....\n\nBy \n\nJanuary 07, 2022 3:57PM\n\n[\n\n## Cloudflare Innovation Weeks 2021\n\n](https://blog.cloudflare.com/2021-innovations-weeks/)\n\nAs we start planning our 2022 Innovation Weeks, we are reflecting back on the highlights from each of these weeks...\n\nBy \n\nSeptember 27, 2019 8:00PM\n\n[\n\n## Birthday Week 2019 Wrap-up\n\n](https://blog.cloudflare.com/birthday-week-2019-wrap-up/)\n\nThis week we celebrated Cloudflare’s 9th birthday by launching a variety of new offerings that support our mission: to help build a better Internet. Below is a summary recap of how we celebrated Birthday Week 2019....\n\nBy"
    },
    {
      "url": "https://blog.cloudflare.com/improved-quick-edit/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/improved-quick-edit/",
        "loadedTime": "2023-12-05T02:33:51.749Z",
        "referrerUrl": "https://blog.cloudflare.com/forrester-wave-edge-development-2023/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/improved-quick-edit/",
        "title": "A whole new Quick Edit in Cloudflare Workers",
        "description": "We’re proud to announce an improved dashboard editor experience that allows users to edit multimodule Workers, use real edge previews, and debug their Workers more easily - all powered by Workers and VSCode for the Web.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "05/17/2023\n6 min read\nQuick Edit is a development experience for Cloudflare Workers, embedded right within the Cloudflare dashboard. It’s the fastest way to get up and running with a new worker, and lets you quickly preview and deploy changes to your code.\nWe’ve spent a lot of recent time working on upgrading the local development experience to be as useful as possible, but the Quick Edit experience for editing Workers has stagnated since the release of workers.dev. It’s time to give Quick Edit some love and bring it up to scratch with the expectations of today's developers.\nBefore diving into what’s changed—a quick overview of the current Quick Edit experience:\nWe used the robust Monaco editor, which took us pretty far—it’s even what VSCode uses under the hood! However, Monaco is fairly limited in what it can do. Developers are used to the full power of their local development environment, with advanced IntelliSense support and all the power of a full-fledged IDE. Compared to that, a single file text editor is a step-down in expressiveness and functionality.\nVSCode for Web\nToday, we’re rolling out a new Quick Edit experience for Workers, powered by VSCode for Web. This is a huge upgrade, allowing developers to work in a familiar environment. This isn’t just about familiarity though—using VSCode for Web to power Quick Edit unlocks significant new functionality that was previously only possible with a local development setup using Wrangler.\nSupport for multiple modules!\nCloudflare Workers released support for the Modules syntax in 2021, which is the recommended way to write Workers. It leans into modern JavaScript by leveraging the ES Module syntax, and lets you define Workers by exporting a default object containing event handlers.\nexport default { async fetch(request, env) { return new Response(\"Hello, World!\") } } \nThere are two sides of the coin when it comes to ES Modules though: exports and imports. Until now, if you wanted to organise your worker in multiple modules you had to use Wrangler and a local development setup. Now, you’ll be able to write multiple modules in the dashboard editor, and import them, just as you can locally. We haven’t enabled support for importing modules from npm yet, but that’s something we’re actively exploring—stay tuned!\nEdge Preview\nWhen editing a worker in the dashboard, Cloudflare spins up a preview of your worker, deployed from the code you’re currently working on. This helps speed up the feedback loop when developing a worker, and makes it easy to test changes without impacting production traffic (see also, wrangler dev).\nHowever, the in-dashboard preview hasn’t historically been a high-fidelity match for the deployed Workers runtime. There were various differences in behaviour between the dashboard preview environment and a deployed worker, and it was difficult to have full confidence that a worker that worked in the preview would work in the deployed environment.\nThat changes today! We’ve changed the dashboard preview environment to use the same system that powers wrangler dev. This means that your preview worker will be run on Cloudflare's global network, the same environment as your deployed workers.\nHelpful error messages\nIn the previous dashboard editor, the experience when your code throws an error wasn’t great. Unless you wrap your worker code in a try-catch handler, the preview will show a blank page when your worker throws an error. This can make it really tricky to debug your worker, and is pretty frustrating. With the release of the new Quick Editor, we now wrap your worker with error handling code that shows helpful error pages, complete with error stack traces and detailed descriptions.\nTypechecking\nTypeScript is incredibly popular, and developers are more and more used to writing their workers in TypeScript. While the dashboard editor still only allows JavaScript files (and you’re unable to write TypeScript directly) we wanted to support modern typed JavaScript development as much as we could. To that end, the new dashboard editor has full support for JSDoc TypeScript syntax, with the TypeScript environment for workers preloaded. This means that writing code with type errors will show a familiar squiggly red line, and Cloudflare APIs like HTMLRewriter will be autocompleted.\nHow we built it\nIt wouldn’t be a Cloudflare blog post without a deep dive into the nuts and bolts of what we’ve built!\nFirst, an overview—how does this work at a high level? We embed VSCode for Web in the Cloudflare dashboard as an iframe, and communicate with it over a MessageChannel. When the iframe is loaded, the Cloudflare dashboard sends over the contents of your worker to a VSCode for Web extension. This extension seeds an in-memory filesystem from which VSCode for Web reads. When you edit files in VSCode for Web, the updated files are sent back over the same MessageChannel to the Cloudflare dashboard, where they’re uploaded as a previewed worker to Cloudflare's global network.\nAs with any project of this size, the devil is in the details. Let’s focus on a specific area —how we communicate with VSCode for Web’s iframe from the Cloudflare dashboard.\nThe MessageChannel browser API enables relatively easy cross-frame communication—in this case, from an iframe embedder to the iframe itself. To use it, you construct an instance and access the port1 and port2 properties:\nconst channel = new MessageChannel() // The MessagePort you keep a hold of channel.port1 // The MessagePort you send to the iframe channel.port2 \nWe store a reference to the MessageChannel to use across component renders with useRef(), since React would otherwise create a new MessageChannel instance with every render.\nWith that out of the way, all that remains is to send channel.port2 to VSCode for Web’s iframe, via a call to postMessage().\n// A reference to the iframe embedding VSCode for Web const editor = document.getElementById(\"vscode\") // Wait for the iframe to load editor.addEventListener('load', () => { // Send over the MessagePort editor.contentWindow.postMessage('PORT', '*', [ channel.port2 ]); }); \nAn interesting detail here is how the MessagePort is sent over to the iframe. The third argument to postMessage() indicates a sequence of Transferable objects. This transfers ownership of port2 to the iframe, which means that any attempts to access it in the original context will throw an exception.\nAt this stage the dashboard has loaded an iframe containing VSCode for Web, initialised a MessageChannel, and sent over a MessagePort to the iframe. Let’s switch context—the iframe now needs to catch the MessagePort and start using it to communicate with the embedder (Cloudflare’s dashboard).\nwindow.onmessage = (e) => { if (e.data === \"PORT\") { // An instance of a MessagePort const port = e.ports[0] } }; \nRelatively straightforward! With not that much code, we’ve set up communication and can start sending more complex messages across. Here’s an example of how we send over the initial worker content from the dashboard to the VSCode for Web iframe:\n// In the Cloudflare dashboard // The modules that make up your worker const files = [ { path: 'index.js', contents: ` import { hello } from \"./world.js\" export default { fetch(request) { return new Response(hello) } }` }, { path: 'world.js', contents: `export const hello = \"Hello World\"` } ]; channel.port1.postMessage({ type: 'WorkerLoaded', // The worker name name: 'your-worker-name', // The worker's main module entrypoint: 'index.js', // The worker's modules files: files }); \nIf you’d like to learn more about our approach, you can explore the code we’ve open sourced as part of this project, including the VSCode extension we’ve written to load data from the Cloudflare dashboard, our patches to VSCode, and our VSCode theme.\nWe’re not done!\nThis is a huge overhaul of the dashboard editing experience for Workers, but we’re not resting on our laurels! We know there’s a long way to go before developing a worker in the browser will offer the same experience as developing a worker locally with Wrangler, and we’re working on ways to close that gap. In particular, we’re working on adding Typescript support to the editor, and supporting syncing to external Git providers like GitHub and GitLab.\nWe’d love to hear any feedback from you on the new editing experience—come say hi and ask us any questions you have on the Cloudflare Discord!\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nDeveloper Week Developers Cloudflare Workers",
      "markdown": "05/17/2023\n\n*   [![Samuel Macleod](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/04/IMG_5133.jpg)](https://blog.cloudflare.com/author/samuel/)\n*   [![Adam Murray](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/04/adam_headshot-1.jpg)](https://blog.cloudflare.com/author/adam-murray/)\n\n6 min read\n\n![A whole new Quick Edit in Cloudflare Workers](https://blog.cloudflare.com/content/images/2023/05/image1-42.png)\n\nQuick Edit is a development experience for Cloudflare Workers, embedded right within the Cloudflare dashboard. It’s the fastest way to get up and running with a new worker, and lets you quickly preview and deploy changes to your code.\n\nWe’ve spent a lot of recent time working on upgrading the _local_ development experience to be as [useful as possible](https://blog.cloudflare.com/miniflare-and-workerd/), but the Quick Edit experience for editing Workers has stagnated since the release of [workers.dev](https://blog.cloudflare.com/just-write-code-improving-developer-experience-for-cloudflare-workers/). It’s time to give Quick Edit some love and bring it up to scratch with the expectations of today's developers.\n\nBefore diving into what’s changed—a quick overview of the current Quick Edit experience:\n\n![](https://blog.cloudflare.com/content/images/2023/05/download-11.png)\n\nWe used the robust [Monaco editor](https://microsoft.github.io/monaco-editor/), which took us pretty far—it’s even what VSCode uses under the hood! However, Monaco is fairly limited in what it can do. Developers are used to the full power of their local development environment, with advanced IntelliSense support and all the power of a full-fledged IDE. Compared to that, a single file text editor is a step-down in expressiveness and functionality.\n\n## VSCode for Web\n\nToday, we’re rolling out a new Quick Edit experience for Workers, powered by [VSCode for Web](https://code.visualstudio.com/docs/editor/vscode-web). This is a huge upgrade, allowing developers to work in a familiar environment. This isn’t just about familiarity though—using VSCode for Web to power Quick Edit unlocks significant new functionality that was previously only possible with a local development setup using [Wrangler](https://blog.cloudflare.com/10-things-i-love-about-wrangler/).\n\n![](https://blog.cloudflare.com/content/images/2023/05/download--1--7.png)\n\n### Support for multiple modules!\n\nCloudflare Workers released support for the [Modules syntax](https://blog.cloudflare.com/workers-javascript-modules/) in 2021, which is the recommended way to write Workers. It leans into modern JavaScript by leveraging the ES Module syntax, and lets you define Workers by exporting a default object containing event handlers.\n\n```\nexport default {\n async fetch(request, env) {\n   return new Response(\"Hello, World!\")\n }\n}\n```\n\nThere are two sides of the coin when it comes to ES Modules though: exports _and imports_. Until now, if you wanted to organise your worker in multiple modules you had to use Wrangler and a local development setup. Now, you’ll be able to write multiple modules in the dashboard editor, and import them, just as you can locally. We haven’t enabled support for importing modules from npm yet, but that’s something we’re actively exploring—stay tuned!\n\n![](https://blog.cloudflare.com/content/images/2023/05/download--2--6.png)\n\n### Edge Preview\n\n![](https://blog.cloudflare.com/content/images/2023/05/download--3--4.png)\n\nWhen editing a worker in the dashboard, Cloudflare spins up a preview of your worker, deployed from the code you’re currently working on. This helps speed up the feedback loop when developing a worker, and makes it easy to test changes without impacting production traffic (see also, [wrangler dev](https://blog.cloudflare.com/announcing-wrangler-dev-the-edge-on-localhost/)).\n\nHowever, the in-dashboard preview hasn’t historically been a high-fidelity match for the deployed Workers runtime. There were various differences in behaviour between the dashboard preview environment and a deployed worker, and it was difficult to have full confidence that a worker that worked in the preview would work in the deployed environment.\n\nThat changes today! We’ve changed the dashboard preview environment to use the same system that powers [`wrangler dev`](https://blog.cloudflare.com/announcing-wrangler-dev-the-edge-on-localhost/). This means that your preview worker will be run on Cloudflare's global network, the same environment as your deployed workers.\n\n### Helpful error messages\n\nIn the previous dashboard editor, the experience when your code throws an error wasn’t great. Unless you wrap your worker code in a try-catch handler, the preview will show a blank page when your worker throws an error. This can make it really tricky to debug your worker, and is pretty frustrating. With the release of the new Quick Editor, we now wrap your worker with error handling code that shows helpful error pages, complete with error stack traces and detailed descriptions.\n\n![](https://blog.cloudflare.com/content/images/2023/05/download--4--4.png)\n\n### Typechecking\n\nTypeScript is incredibly popular, and developers are more and more used to writing their workers in TypeScript. While the dashboard editor still only allows JavaScript files (and you’re unable to write TypeScript directly) we wanted to support modern typed JavaScript development as much as we could. To that end, the new dashboard editor has full support for [JSDoc TypeScript syntax](https://www.typescriptlang.org/docs/handbook/type-checking-javascript-files.html), with [the TypeScript environment for workers preloaded](https://www.npmjs.com/package/@cloudflare/workers-types). This means that writing code with type errors will show a familiar squiggly red line, and Cloudflare APIs like HTMLRewriter will be autocompleted.\n\n![](https://blog.cloudflare.com/content/images/2023/05/download--5--4.png)\n\n## How we built it\n\nIt wouldn’t be a Cloudflare blog post without a deep dive into the nuts and bolts of what we’ve built!\n\nFirst, an overview—how does this work at a high level? We embed VSCode for Web in the Cloudflare dashboard as an `iframe`, and communicate with it over a [`MessageChannel`](https://developer.mozilla.org/en-US/docs/Web/API/MessageChannel). When the `iframe` is loaded, the Cloudflare dashboard sends over the contents of your worker to a VSCode for Web extension. This extension seeds an in-memory filesystem from which VSCode for Web reads. When you edit files in VSCode for Web, the updated files are sent back over the same `MessageChannel` to the Cloudflare dashboard, where they’re uploaded as a previewed worker to Cloudflare's global network.\n\nAs with any project of this size, the devil is in the details. Let’s focus on a specific area —how we communicate with VSCode for Web’s `iframe` from the Cloudflare dashboard.\n\nThe [`MessageChannel`](https://developer.mozilla.org/en-US/docs/Web/API/MessageChannel) browser API enables relatively easy cross-frame communication—in this case, from an iframe embedder to the iframe itself. To use it, you construct an instance and access the `port1` and `port2` properties:\n\n```\nconst channel = new MessageChannel()\n\n// The MessagePort you keep a hold of\nchannel.port1\n\n// The MessagePort you send to the iframe\nchannel.port2\n```\n\nWe store a reference to the `MessageChannel` to use across component renders with `useRef()`, since React would otherwise create a new `MessageChannel` instance with every render.\n\nWith that out of the way, all that remains is to send `channel.port2` to VSCode for Web’s iframe, via a call to `postMessage()`.\n\n```\n// A reference to the iframe embedding VSCode for Web\nconst editor = document.getElementById(\"vscode\")\n\n// Wait for the iframe to load \neditor.addEventListener('load', () => {\n\t// Send over the MessagePort\neditor.contentWindow.postMessage('PORT', '*', [\nchannel.port2\n]);\n});\n```\n\nAn interesting detail here is how the `MessagePort` is sent over to the iframe. The third argument to `postMessage()` indicates a sequence of [Transferable objects](https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Transferable_objects). This _transfers_ ownership of `port2` to the iframe, which means that any attempts to access it in the original context will throw an exception.\n\nAt this stage the dashboard has loaded an iframe containing VSCode for Web, initialised a `MessageChannel`, and sent over a `MessagePort` to the iframe. Let’s switch context—the iframe now needs to catch the `MessagePort` and start using it to communicate with the embedder (Cloudflare’s dashboard).\n\n```\nwindow.onmessage = (e) => {\nif (e.data === \"PORT\") {\n\t// An instance of a MessagePort\nconst port = e.ports[0]\n}\n};\n```\n\nRelatively straightforward! With not _that_ much code, we’ve set up communication and can start sending more complex messages across. Here’s an example of how we send over the initial worker content from the dashboard to the VSCode for Web iframe:\n\n```\n// In the Cloudflare dashboard\n\n// The modules that make up your worker\nconst files = [\n  {\n    path: 'index.js',\n    contents: `\n\t\timport { hello } from \"./world.js\"\nexport default {\n\t\t\tfetch(request) {\n\t\t\t\treturn new Response(hello)\n\t\t\t}\n\t\t}`\n  },\n  {\n    path: 'world.js',\n    contents: `export const hello = \"Hello World\"`\n  }\n];\n\nchannel.port1.postMessage({\n  type: 'WorkerLoaded',\n  // The worker name\n  name: 'your-worker-name',\n  // The worker's main module\n  entrypoint: 'index.js',\n  // The worker's modules\n  files: files\n});\n```\n\nIf you’d like to learn more about our approach, you can explore the code we’ve open sourced as part of this project, including the [VSCode extension](https://github.com/cloudflare/workers-sdk/tree/main/packages/quick-edit-extension) we’ve written to load data from the Cloudflare dashboard, our [patches to VSCode](https://github.com/cloudflare/workers-sdk/tree/main/packages/quick-edit), and our [VSCode theme](https://github.com/cloudflare/workers-sdk/tree/main/packages/solarflare-theme).\n\n## We’re not done!\n\nThis is a huge overhaul of the dashboard editing experience for Workers, but we’re not resting on our laurels! We know there’s a long way to go before developing a worker in the browser will offer the same experience as developing a worker locally with Wrangler, and we’re working on ways to close that gap. In particular, we’re working on adding Typescript support to the editor, and supporting syncing to external Git providers like GitHub and GitLab.\n\nWe’d love to hear any feedback from you on the new editing experience—come say hi and ask us any questions you have on the [Cloudflare Discord](https://discord.cloudflare.com/)!\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Developer Week](https://blog.cloudflare.com/tag/developer-week/) [Developers](https://blog.cloudflare.com/tag/developers/) [Cloudflare Workers](https://blog.cloudflare.com/tag/workers/)"
    },
    {
      "url": "https://blog.cloudflare.com/webgpu-in-workers/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/webgpu-in-workers/",
        "loadedTime": "2023-12-05T02:33:59.372Z",
        "referrerUrl": "https://blog.cloudflare.com/forrester-wave-edge-development-2023/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/webgpu-in-workers/",
        "title": "You can now use WebGPU in Cloudflare Workers",
        "description": "Today, we are introducing WebGPU support to Cloudflare Workers. This blog will explain why it's important, why we did it, how you can use it, and what comes next.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "09/27/2023\n12 min read\nThe browser as an app platform is real and stronger every day; long gone are the Browser Wars. Vendors and standard bodies have done amazingly well over the last years, working together and advancing web standards with new APIs that allow developers to build fast and powerful applications, finally comparable to those we got used to seeing in the native OS' environment.\nToday, browsers can render web pages and run code that interfaces with an extensive catalog of modern Web APIs. Things like networking, rendering accelerated graphics, or even accessing low-level hardware features like USB devices are all now possible within the browser sandbox.\nOne of the most exciting new browser APIs that browser vendors have been rolling out over the last months is WebGPU, a modern, low-level GPU programming interface designed for high-performance 2D and 3D graphics and general purpose GPU compute.\nToday, we are introducing WebGPU support to Cloudflare Workers. This blog will explain why it's important, why we did it, how you can use it, and what comes next.\nThe history of the GPU in the browser\nTo understand why WebGPU is a big deal, we must revisit history and see how browsers went from relying only on the CPU for everything in the early days to taking advantage of GPUs over the years.\nIn 2011, WebGL 1, a limited port of OpenGL ES 2.0, was introduced, providing an API for fast, accelerated 3D graphics in the browser for the first time. By then, this was somewhat of a revolution in enabling gaming and 3D visualizations in the browser. Some of the most popular 3D animation frameworks, like Three.js, launched in the same period. Who doesn't remember going to the (now defunct) Google Chrome Experiments page and spending hours in awe exploring the demos? Another option then was using the Flash Player, which was still dominant in the desktop environment, and their Stage 3D API.\nLater, in 2017, based on the learnings and shortcomings of its predecessor, WebGL 2 was a significant upgrade and brought more advanced GPU capabilities like shaders and more flexible textures and rendering.\nWebGL, however, has proved to be a steep and complex learning curve for developers who want to take control of things, do low-level 3D graphics using the GPU, and not use 3rd party abstraction libraries.\nFurthermore and more importantly, with the advent of machine learning and cryptography, we discovered that GPUs are great not only at drawing graphics but can be used for other applications that can take advantage of things like high-speed data or blazing-fast matrix multiplications, and one can use them to perform general computation. This became known as GPGPU, short for general-purpose computing on graphics processing units.\nWith this in mind, in the native desktop and mobile operating system worlds, developers started using more advanced frameworks like CUDA, Metal, DirectX 12, or Vulkan. WebGL stayed behind. To fill this void and bring the browser up to date, in 2017, companies like Google, Apple, Intel, Microsoft, Kronos, and Mozilla created the GPU for Web Community Working Group to collaboratively design the successor of WebGL and create the next modern 3D graphics and computation capabilities APIs for the Web.\nWhat is WebGPU\nWebGPU was developed with the following advantages in mind:\nLower Level Access - WebGPU provides lower-level, direct access to the GPU vs. the high-level abstractions in WebGL. This enables more control over GPU resources.\nMulti-Threading - WebGPU can leverage multi-threaded rendering and compute, allowing improved CPU/GPU parallelism compared to WebGL, which relies on a single thread.\nCompute Shaders - First-class support for general-purpose compute shaders for GPGPU tasks, not just graphics. WebGL compute is limited.\nSafety - WebGPU ensures memory and GPU access safety, avoiding common WebGL pitfalls.\nPortability - WGSL shader language targets cross-API portability across GPU vendors vs. GLSL in WebGL.\nReduced Driver Overhead - The lower level Vulkan/Metal/D3D12 basis improves overhead vs. OpenGL drivers in WebGL.\nPipeline State Objects - Predefined pipeline configs avoid per-draw driver overhead in WebGL.\nMemory Management - Finer-grained buffer and resource management vs. WebGL.\nThe “too long didn't read” version is that WebGPU provides lower-level control over the GPU hardware with reduced overhead. It's safer, has multi-threading, is focused on compute, not just graphics, and has portability advantages compared to WebGL.\nIf these aren't reasons enough to get excited, developers are also looking at WebGPU as an option for native platforms, not just the Web. For instance, you can use this C API that mimics the JavaScript specification. If you think about this and the power of WebAssembly, you can effectively have a truly platform-agnostic GPU hardware layer that you can use to develop platforms for any operating system or browser.\nMore than just graphics\nAs explained above, besides being a graphics API, WebGPU makes it possible to perform tasks such as:\nMachine Learning - Implement ML applications like neural networks and computer vision algorithms using WebGPU compute shaders and matrices.\nScientific Computing - Perform complex scientific computation like physics simulations and mathematical modeling using the GPU.\nHigh Performance Computing - Unlock breakthrough performance for parallel workloads by connecting WebGPU to languages like Rust, C/C++ via WebAssembly.\nWGSL, the shader language for WebGPU, is what enables the general-purpose compute feature. Shaders, or more precisely, compute shaders, have no user-defined inputs or outputs and are used for computing arbitrary information. Here are some examples of simple WebGPU compute shaders if you want to learn more.\nWebGPU in Workers\nWe've been watching WebGPU since the API was published. Its general-purpose compute features perfectly fit our Workers' ecosystem and capabilities and align well with our vision of providing our customers multiple compute and hardware options and bringing GPU workloads to our global network, close to clients.\nCloudflare also has a track record of pioneering support for emerging web standards on our network and services, accelerating their adoption for our customers. Examples of these are Web Crypto API, HTTP/2, HTTP/3, TLS 1.3, or Early hints, but there are more.\nBringing WebGPU to Workers was both natural and timely. Today, we are announcing that we have released a version of workerd, the open-sourced JavaScript / Wasm runtime that powers Cloudflare Workers, with WebGPU support, that you can start playing and developing applications with, locally.\nStarting today anyone can run this on their personal computer and experiment with WebGPU-enabled workers. Implementing local development first allows us to put this API in the hands of our customers and developers earlier and get feedback that will guide the development of this feature for production use.\nBut before we dig into code examples, let's explain how we built it.\nHow we built WebGPU on top of Workers\nTo implement the WebGPU API, we took advantage of Dawn, an open-source library backed by Google, the same used in Chromium and Chrome, that provides applications with an implementation of the WebGPU standard. It also provides the webgpu.h headers file, the de facto reference for all the other implementations of the standard.\nDawn can interoperate with Linux, MacOS, and Windows GPUs by interfacing with each platform's native GPU frameworks. For example, when an application makes a WebGPU draw call, Dawn will convert that draw command into the equivalent Vulkan, Metal, or Direct3D 12 API call, depending on the platform.\nFrom an application standpoint, Dawn handles the interactions with the underlying native graphics APIs that communicate directly with the GPU drivers. Dawn essentially acts as a middle layer that translates the WebGPU API calls into calls for the platform's native graphics API.\nCloudflare workerd is the underlying open-source runtime engine that executes Workers code. It shares most of its code with the same runtime that powers Cloudflare Workers' production environment but with some changes designed to make it more portable to other environments. We then have release cycles that aim to synchronize both codebases; more on that later. Workerd is also used with wrangler, our command-line tool for building and interacting with Cloudflare Workers, to support local development.\nThe WebGPU code that interfaces with the Dawn library can be found here, and can easily be enabled with a flag, checked here.\njsg::Ref<api::gpu::GPU> Navigator::getGPU(CompatibilityFlags::Reader flags) { // is this a durable object? KJ_IF_MAYBE (actor, IoContext::current().getActor()) { JSG_REQUIRE(actor->getPersistent() != nullptr, TypeError, \"webgpu api is only available in Durable Objects (no storage)\"); } else { JSG_FAIL_REQUIRE(TypeError, \"webgpu api is only available in Durable Objects\"); }; JSG_REQUIRE(flags.getWebgpu(), TypeError, \"webgpu needs the webgpu compatibility flag set\"); return jsg::alloc<api::gpu::GPU>(); } \nThe WebGPU API can only be accessed using Durable Objects, which are essentially global singleton instances of Cloudflare Workers. There are two important reasons for this:\nWebGPU code typically wants to store the state between requests, for example, loading an AI model into the GPU memory once and using it multiple times for inference.\nNot all Cloudflare servers have GPUs yet, so although the worker that receives the request is typically the closest one available, the Durable Object that uses WebGPU will be instantiated where there are GPU resources available, which may not be on the same machine.\nUsing Durable Objects instead of regular Workers allow us to address both of these issues.\nThe WebGPU Hello World in Workers\nWrangler uses Miniflare 3, a fully-local simulator for Workers, which in turn is powered by workerd. This means you can start experimenting and doing WebGPU code locally on your machine right now before we prepare things in our production environment.\nLet’s get coding then.\nSince Workers doesn't render graphics yet, we started with implementing the general-purpose GPU (GPGPU) APIs in the WebGPU specification. In other words, we fully support the part of the API that the compute shaders and the compute pipeline require, but we are not yet focused on fragment or vertex shaders used in rendering pipelines.\nHere’s a typical “hello world” in WebGPU. This Durable Object script will output the name of the GPU device that workerd found in your machine to your console.\nconst adapter = await navigator.gpu.requestAdapter(); const adapterInfo = await adapter.requestAdapterInfo([\"device\"]); console.log(adapterInfo.device); \nA more interesting example, though, is a simple compute shader. In this case, we will fill a results buffer with an incrementing value taken from the iteration number via global_invocation_id.\nFor this, we need two buffers, one to store the results of the computations as they happen (storageBuffer) and another to copy the results at the end (mappedBuffer).\nWe then dispatch four workgroups, meaning that the increments can happen in parallel. This parallelism and programmability are two key reasons why compute shaders and GPUs provide an advantage for things like machine learning inference workloads. Other advantages are:\nBandwidth - GPUs have a very high memory bandwidth, up to 10-20x more than CPUs. This allows fast reading and writing of all the model parameters and data needed for inference.\nFloating-point performance - GPUs are optimized for high floating point operation throughput, which are used extensively in neural networks. They can deliver much higher TFLOPs than CPUs.\nLet’s look at the code:\n// Create device and command encoder const adapter = await navigator.gpu.requestAdapter(); const device = await adapter.requestDevice(); const encoder = device.createCommandEncoder(); // Storage buffer const storageBuffer = device.createBuffer({ size: 4 * Float32Array.BYTES_PER_ELEMENT, // 4 float32 values usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC, }); // Mapped buffer const mappedBuffer = device.createBuffer({ size: 4 * Float32Array.BYTES_PER_ELEMENT, usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST, }); // Create shader that writes incrementing numbers to storage buffer const computeShaderCode = ` @group(0) @binding(0) var<storage, read_write> result : array<f32>; @compute @workgroup_size(1) fn main(@builtin(global_invocation_id) gid : vec3<u32>) { result[gid.x] = f32(gid.x); } `; // Create compute pipeline const computePipeline = device.createComputePipeline({ layout: \"auto\", compute: { module: device.createShaderModule({ code: computeShaderCode }), entryPoint: \"main\", }, }); // Bind group const bindGroup = device.createBindGroup({ layout: computePipeline.getBindGroupLayout(0), entries: [{ binding: 0, resource: { buffer: storageBuffer } }], }); // Dispatch compute work const computePass = encoder.beginComputePass(); computePass.setPipeline(computePipeline); computePass.setBindGroup(0, bindGroup); computePass.dispatchWorkgroups(4); computePass.end(); // Copy from storage to mapped buffer encoder.copyBufferToBuffer( storageBuffer, 0, mappedBuffer, 0, 4 * Float32Array.BYTES_PER_ELEMENT //mappedBuffer.size ); // Submit and read back result const gpuBuffer = encoder.finish(); device.queue.submit([gpuBuffer]); await mappedBuffer.mapAsync(GPUMapMode.READ); console.log(new Float32Array(mappedBuffer.getMappedRange())); // [0, 1, 2, 3] \nNow that we covered the basics of WebGPU and compute shaders, let's move to something more demanding. What if we could perform machine learning inference using Workers and GPUs?\nONNX WebGPU demo\nThe ONNX runtime is a popular open-source cross-platform, high performance machine learning inferencing accelerator. Wonnx is a GPU-accelerated version of the same engine, written in Rust, that can be compiled to WebAssembly and take advantage of WebGPU in the browser. We are going to run it in Workers using a combination of workers-rs, our Rust bindings for Cloudflare Workers, and the workerd WebGPU APIs.\nFor this demo, we are using SqueezeNet. This small image classification model can run under lower resources but still achieves similar levels of accuracy on the ImageNet image classification validation dataset as larger models like AlexNet.\nIn essence, our worker will receive any uploaded image and attempt to classify it according to the 1000 ImageNet classes. Once ONNX runs the machine learning model using the GPU, it will return the list of classes with the highest probability scores. Let’s go step by step.\nFirst we load the model from R2 into the GPU memory the first time the Durable Object is called:\n#[durable_object] pub struct Classifier { env: Env, session: Option<wonnx::Session>, } impl Classifier { async fn ensure_session(&mut self) -> Result<()> { match self.session { Some(_) => worker::console_log!(\"DO already has a session\"), None => { // No session, so this should be the first request. In this case // we will fetch the model from R2, build a wonnx session, and // store it for subsequent requests. let model_bytes = fetch_model(&self.env).await?; let session = wonnx::Session::from_bytes(&model_bytes) .await .map_err(|err| err.to_string())?; worker::console_log!(\"session created in DO\"); self.session = Some(session); } }; Ok(()) } } \nThis is only required once, when the Durable Object is instantiated. For subsequent requests, we retrieve the model input tensor, call the existing session for the inference, and return to the calling worker the result tensor converted to JSON:\nlet request_data: ArrayBase<OwnedRepr<f32>, Dim<[usize; 4]>> = serde_json::from_str(&req.text().await?)?; let mut input_data = HashMap::new(); input_data.insert(\"data\".to_string(), request_data.as_slice().unwrap().into()); let result = self .session .as_ref() .unwrap() // we know the session exists .run(&input_data) .await .map_err(|err| err.to_string())?; ... let probabilities: Vec<f32> = result .into_iter() .next() .ok_or(\"did not obtain a result tensor from session\")? .1 .try_into() .map_err(|err: TensorConversionError| err.to_string())?; let do_response = serde_json::to_string(&probabilities)?; Response::ok(do_response) \nOn the Worker script itself, we load the uploaded image and pre-process it into a model input tensor:\nlet image_file: worker::File = match req.form_data().await?.get(\"file\") { Some(FormEntry::File(buf)) => buf, Some(_) => return Response::error(\"`file` part of POST form must be a file\", 400), None => return Response::error(\"missing `file`\", 400), }; let image_content = image_file.bytes().await?; let image = load_image(&image_content)?; \nFinally, we call the GPU Durable Object, which runs the model and returns the most likely classes of our image:\nlet probabilities = execute_gpu_do(image, stub).await?; let mut probabilities = probabilities.iter().enumerate().collect::<Vec<_>>(); probabilities.sort_unstable_by(|a, b| b.1.partial_cmp(a.1).unwrap()); Response::ok(LABELS[probabilities[0].0]) \nWe packaged this demo in a public repository, so you can also run it. Make sure that you have a Rust compiler, Node.js, Git and curl installed, then clone the repository:\ngit clone https://github.com/cloudflare/workers-wonnx.git cd workers-wonnx \nUpload the model to the local R2 simulator:\nnpx wrangler@latest r2 object put model-bucket-dev/opt-squeeze.onnx --local --file models/opt-squeeze.onnx \nAnd then run the Worker locally:\nnpx wrangler@latest dev \nWith the Worker running and waiting for requests you can then open another terminal window and upload one of the image examples in the same repository using curl:\n> curl -F \"file=@images/pelican.jpeg\" http://localhost:8787 n02051845 pelican \nIf everything goes according to plan the result of the curl command will be the most likely class of the image.\nNext steps and final words\nOver the upcoming weeks, we will merge the workerd WebGPU code in the Cloudflare Workers production environment and make it available globally, on top of our growing GPU nodes fleet. We didn't do it earlier because that environment is subject to strict security and isolation requirements. For example, we can't break the security model of our process sandbox and have V8 talking to the GPU hardware directly, that would be a problem; we must create a configuration where another process is closer to the GPU and use IPC (inter-process communication) to talk to it. Other things like managing resource allocation and billing are being sorted out.\nFor now, we wanted to get the good news out that we will support WebGPU in Cloudflare Workers and ensure that you can start playing and coding with it today and learn from it. WebGPU and general-purpose computing on GPUs is still in its early days. We presented a machine-learning demo, but we can imagine other applications taking advantage of this new feature, and we hope you can show us some of them.\nAs usual, you can talk to us on our Developers Discord or the Community forum; the team will be listening. We are eager to hear from you and learn about what you're building.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nBirthday Week Cloudflare Workers Standards",
      "markdown": "09/27/2023\n\n*   [![André Cruz](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/02/andre2.jpg)](https://blog.cloudflare.com/author/andre-cruz/)\n*   [![Celso Martinho](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/Celso-Martinho.png)](https://blog.cloudflare.com/author/celso/)\n\n12 min read\n\n![](https://blog.cloudflare.com/content/images/2023/09/image1-27.png)\n\nThe browser as an app platform is real and stronger every day; long gone are the Browser Wars. Vendors and standard bodies have done amazingly well over the last years, working together and advancing web standards with new [APIs](https://www.cloudflare.com/learning/security/api/what-is-an-api/) that allow developers to build fast and powerful applications, finally comparable to those we got used to seeing in the native OS' environment.\n\nToday, browsers can render web pages and run code that interfaces with an [extensive catalog of modern Web APIs](https://developer.mozilla.org/en-US/docs/Web/API). Things like networking, rendering accelerated graphics, or even accessing low-level hardware features like USB devices are all now possible within the browser sandbox.\n\nOne of the most exciting new browser APIs that browser vendors have been rolling out over the last months is WebGPU, a modern, low-level GPU programming interface designed for high-performance 2D and 3D graphics and general purpose GPU compute.\n\nToday, we are introducing [WebGPU](https://developer.chrome.com/blog/webgpu-release/) support to Cloudflare Workers. This blog will explain why it's important, why we did it, how you can use it, and what comes next.\n\n### The history of the GPU in the browser\n\nTo understand why WebGPU is a big deal, we must revisit history and see how browsers went from relying only on the CPU for everything in the early days to taking advantage of GPUs over the years.\n\nIn 2011, [WebGL 1](https://en.wikipedia.org/wiki/WebGL), a limited port of [OpenGL ES 2.0](https://www.khronos.org/opengles/), was introduced, providing an API for fast, accelerated 3D graphics in the browser for the first time. By then, this was somewhat of a revolution in enabling gaming and 3D visualizations in the browser. Some of the most popular 3D animation frameworks, like [Three.js](https://threejs.org/), launched in the same period. Who doesn't remember going to the (now defunct) [Google Chrome Experiments](https://en.wikipedia.org/wiki/Google_Chrome_Experiments) page and spending hours in awe exploring the demos? Another option then was using the Flash Player, which was still dominant in the desktop environment, and their [Stage 3D](https://en.wikipedia.org/wiki/Stage3D) API.\n\nLater, in 2017, based on the learnings and shortcomings of its predecessor, WebGL 2 was a significant upgrade and brought more advanced GPU capabilities like shaders and more flexible textures and rendering.\n\nWebGL, however, has proved to be a steep and complex learning curve for developers who want to take control of things, do low-level 3D graphics using the GPU, and not use 3rd party abstraction libraries.\n\nFurthermore and more importantly, with the advent of machine learning and cryptography, we discovered that GPUs are great not only at drawing graphics but can be used for other applications that can take advantage of things like high-speed data or blazing-fast matrix multiplications, and one can use them to perform general computation. This became known as [GPGPU](https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units), short for general-purpose computing on graphics processing units.\n\nWith this in mind, in the native desktop and mobile operating system worlds, developers started using more advanced frameworks like [CUDA](https://en.wikipedia.org/wiki/CUDA), [Metal](https://developer.apple.com/metal/), [DirectX 12](https://en.wikipedia.org/wiki/DirectX#DirectX_12), or [Vulkan](https://www.vulkan.org/learn#key-resources). WebGL stayed behind. To fill this void and bring the browser up to date, in 2017, companies like Google, Apple, Intel, Microsoft, Kronos, and Mozilla created the [GPU for Web Community Working Group](https://www.w3.org/community/gpu/) to collaboratively design the successor of WebGL and create the next modern 3D graphics and computation capabilities APIs for the Web.\n\n### What is WebGPU\n\nWebGPU was developed with the following advantages in mind:\n\n*   **Lower Level Access** - WebGPU provides lower-level, direct access to the GPU vs. the high-level abstractions in WebGL. This enables more control over GPU resources.\n*   **Multi-Threading** - WebGPU can leverage multi-threaded rendering and compute, allowing improved CPU/GPU parallelism compared to WebGL, which relies on a single thread.\n*   **Compute Shaders** - First-class support for general-purpose compute shaders for GPGPU tasks, not just graphics. WebGL compute is limited.\n*   **Safety** - WebGPU ensures memory and GPU access safety, avoiding common WebGL pitfalls.\n*   **Portability** - WGSL shader language targets cross-API portability across GPU vendors vs. GLSL in WebGL.\n*   **Reduced Driver Overhead** - The lower level Vulkan/Metal/D3D12 basis improves overhead vs. OpenGL drivers in WebGL.\n*   **Pipeline State Objects** - Predefined pipeline configs avoid per-draw driver overhead in WebGL.\n*   **Memory Management** - Finer-grained buffer and resource management vs. WebGL.\n\nThe “too long didn't read” version is that WebGPU provides lower-level control over the GPU hardware with reduced overhead. It's safer, has multi-threading, is focused on compute, not just graphics, and has portability advantages compared to WebGL.\n\nIf these aren't reasons enough to get excited, developers are also looking at WebGPU as an option for native platforms, not just the Web. For instance, you can use this [C API](https://github.com/webgpu-native/webgpu-headers/blob/main/webgpu.h) that mimics the JavaScript specification. If you think about this and the power of WebAssembly, you can effectively have a truly platform-agnostic GPU hardware layer that you can use to [develop](https://developer.chrome.com/blog/webgpu-cross-platform/) platforms for any operating system or browser.\n\n### More than just graphics\n\nAs explained above, besides being a graphics API, WebGPU makes it possible to perform tasks such as:\n\n*   **Machine Learning** - Implement ML applications like neural networks and computer vision algorithms using WebGPU compute shaders and matrices.\n*   **Scientific Computing** - Perform complex scientific computation like physics simulations and mathematical modeling using the GPU.\n*   **High Performance Computing** - Unlock breakthrough performance for parallel workloads by connecting WebGPU to languages like Rust, C/C++ via [WebAssembly](https://webassembly.org/).\n\n[WGSL](https://gpuweb.github.io/gpuweb/wgsl/), the shader language for WebGPU, is what enables the general-purpose compute feature. Shaders, or more precisely, [compute shaders](https://www.khronos.org/opengl/wiki/Compute_Shader), have no user-defined inputs or outputs and are used for computing arbitrary information. Here are [some examples](https://webgpufundamentals.org/webgpu/lessons/webgpu-compute-shaders.html) of simple WebGPU compute shaders if you want to learn more.\n\n### WebGPU in Workers\n\nWe've been watching WebGPU since the API was published. Its general-purpose compute features perfectly fit our Workers' ecosystem and capabilities and align well with our vision of providing our customers multiple compute and hardware options and bringing GPU workloads to our global network, close to clients.\n\nCloudflare also has a track record of pioneering support for emerging web standards on our network and services, accelerating their adoption for our customers. Examples of these are [Web Crypto API](https://developers.cloudflare.com/workers/runtime-apis/web-crypto/), [HTTP/2](https://blog.cloudflare.com/introducing-http2/), [HTTP/3](https://blog.cloudflare.com/http3-the-past-present-and-future/), [TLS 1.3](https://blog.cloudflare.com/introducing-tls-1-3/), or [Early hints](https://blog.cloudflare.com/early-hints/), but [there are more](https://developers.cloudflare.com/workers/runtime-apis/).\n\nBringing WebGPU to Workers was both natural and timely. Today, we are announcing that we have released a version of [workerd](https://github.com/cloudflare/workerd), the open-sourced JavaScript / Wasm runtime that powers Cloudflare Workers, with [WebGPU support](https://github.com/cloudflare/workerd/tree/main/src/workerd/api/gpu), that you can start playing and developing applications with, locally.\n\nStarting today anyone can run this on their personal computer and experiment with WebGPU-enabled workers. Implementing local development first allows us to put this API in the hands of our customers and developers earlier and get feedback that will guide the development of this feature for production use.\n\nBut before we dig into code examples, let's explain how we built it.\n\n### How we built WebGPU on top of Workers\n\n![](https://blog.cloudflare.com/content/images/2023/09/image2-22.png)\n\nTo implement the WebGPU API, we took advantage of [Dawn](https://dawn.googlesource.com/dawn/), an open-source library backed by Google, the same used in Chromium and Chrome, that provides applications with an implementation of the WebGPU standard. It also provides the [webgpu.h](https://github.com/webgpu-native/webgpu-headers/blob/main/webgpu.h) headers file, the de facto reference for all the other implementations of the standard.\n\nDawn can interoperate with Linux, MacOS, and Windows GPUs by interfacing with each platform's native GPU frameworks. For example, when an application makes a WebGPU draw call, Dawn will convert that draw command into the equivalent Vulkan, Metal, or Direct3D 12 API call, depending on the platform.\n\nFrom an application standpoint, Dawn handles the interactions with the underlying native graphics APIs that communicate directly with the GPU drivers. Dawn essentially acts as a middle layer that translates the WebGPU API calls into calls for the platform's native graphics API.\n\nCloudflare [workerd](https://blog.cloudflare.com/workerd-open-source-workers-runtime/) is the underlying open-source runtime engine that executes Workers code. It shares most of its code with the same runtime that powers Cloudflare Workers' production environment but with some changes designed to make it more portable to other environments. We then have release cycles that aim to synchronize both codebases; more on that later. Workerd is also used with [wrangler](https://github.com/cloudflare/workers-sdk), our command-line tool for building and interacting with Cloudflare Workers, to support local development.\n\nThe WebGPU code that interfaces with the Dawn library can be found [here](https://github.com/cloudflare/workerd/tree/main/src/workerd/api/gpu), and can easily be enabled with a flag, checked [here](https://github.com/cloudflare/workerd/blob/main/src/workerd/api/global-scope.c%2B%2B#L728).\n\n```\njsg::Ref<api::gpu::GPU> Navigator::getGPU(CompatibilityFlags::Reader flags) {\n  // is this a durable object?\n  KJ_IF_MAYBE (actor, IoContext::current().getActor()) {\n    JSG_REQUIRE(actor->getPersistent() != nullptr, TypeError,\n                \"webgpu api is only available in Durable Objects (no storage)\");\n  } else {\n    JSG_FAIL_REQUIRE(TypeError, \"webgpu api is only available in Durable Objects\");\n  };\n\n  JSG_REQUIRE(flags.getWebgpu(), TypeError, \"webgpu needs the webgpu compatibility flag set\");\n\n  return jsg::alloc<api::gpu::GPU>();\n}\n```\n\nThe WebGPU API can only be accessed using [Durable Objects](https://developers.cloudflare.com/durable-objects/), which are essentially global singleton instances of Cloudflare Workers. There are two important reasons for this:\n\n*   WebGPU code typically wants to store the state between requests, for example, loading an [AI model](https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/) into the GPU memory once and using it multiple times for inference.\n*   Not all Cloudflare servers have GPUs yet, so although the worker that receives the request is typically the closest one available, the Durable Object that uses WebGPU will be instantiated where there are GPU resources available, which may not be on the same machine.\n\nUsing Durable Objects instead of regular Workers allow us to address both of these issues.\n\n### The WebGPU Hello World in Workers\n\nWrangler uses Miniflare 3, a [fully-local simulator for Workers](https://blog.cloudflare.com/wrangler3/), which in turn is powered by workerd. This means you can start experimenting and doing WebGPU code locally on your machine right now before we prepare things in our production environment.\n\nLet’s get coding then.\n\nSince Workers doesn't render graphics yet, we started with implementing the general-purpose GPU (GPGPU) APIs in the [WebGPU specification](https://www.w3.org/TR/webgpu/). In other words, we fully support the part of the API that the [compute shaders and the compute pipeline](https://www.w3.org/TR/webgpu/#gpucomputepipeline) require, but we are not yet focused on fragment or vertex shaders used in rendering pipelines.\n\nHere’s a typical “hello world” in WebGPU. This Durable Object script will output the name of the GPU device that workerd found in your machine to your console.\n\n```\nconst adapter = await navigator.gpu.requestAdapter();\nconst adapterInfo = await adapter.requestAdapterInfo([\"device\"]);\nconsole.log(adapterInfo.device);\n```\n\nA more interesting example, though, is a simple compute shader. In this case, we will fill a results buffer with an incrementing value taken from the iteration number via `global_invocation_id`.\n\nFor this, we need two buffers, one to store the results of the computations as they happen (`storageBuffer`) and another to copy the results at the end (`mappedBuffer`).\n\nWe then dispatch four workgroups, meaning that the increments can happen in parallel. This parallelism and programmability are two key reasons why compute shaders and GPUs provide an advantage for things like machine learning inference workloads. Other advantages are:\n\n*   **Bandwidth** - GPUs have a very high memory bandwidth, up to 10-20x more than CPUs. This allows fast reading and writing of all the model parameters and data needed for inference.\n*   **Floating-point performance** - GPUs are optimized for high floating point operation throughput, which are used extensively in neural networks. They can deliver much higher [TFLOPs than CPUs](https://www.tomshardware.com/reviews/gpu-hierarchy,4388.html).\n\nLet’s look at the code:\n\n```\n// Create device and command encoder\nconst adapter = await navigator.gpu.requestAdapter();\nconst device = await adapter.requestDevice();\nconst encoder = device.createCommandEncoder();\n\n// Storage buffer\nconst storageBuffer = device.createBuffer({\n  size: 4 * Float32Array.BYTES_PER_ELEMENT, // 4 float32 values\n  usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC,\n});\n\n// Mapped buffer\nconst mappedBuffer = device.createBuffer({\n  size: 4 * Float32Array.BYTES_PER_ELEMENT,\n  usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST,\n});\n\n// Create shader that writes incrementing numbers to storage buffer\nconst computeShaderCode = `\n    @group(0) @binding(0)\n    var<storage, read_write> result : array<f32>;\n\n    @compute @workgroup_size(1)\n    fn main(@builtin(global_invocation_id) gid : vec3<u32>) {\n      result[gid.x] = f32(gid.x);\n    }\n`;\n\n// Create compute pipeline\nconst computePipeline = device.createComputePipeline({\n  layout: \"auto\",\n  compute: {\n    module: device.createShaderModule({ code: computeShaderCode }),\n    entryPoint: \"main\",\n  },\n});\n\n// Bind group\nconst bindGroup = device.createBindGroup({\n  layout: computePipeline.getBindGroupLayout(0),\n  entries: [{ binding: 0, resource: { buffer: storageBuffer } }],\n});\n\n// Dispatch compute work\nconst computePass = encoder.beginComputePass();\ncomputePass.setPipeline(computePipeline);\ncomputePass.setBindGroup(0, bindGroup);\ncomputePass.dispatchWorkgroups(4);\ncomputePass.end();\n\n// Copy from storage to mapped buffer\nencoder.copyBufferToBuffer(\n  storageBuffer,\n  0,\n  mappedBuffer,\n  0,\n  4 * Float32Array.BYTES_PER_ELEMENT //mappedBuffer.size\n);\n\n// Submit and read back result\nconst gpuBuffer = encoder.finish();\ndevice.queue.submit([gpuBuffer]);\n\nawait mappedBuffer.mapAsync(GPUMapMode.READ);\nconsole.log(new Float32Array(mappedBuffer.getMappedRange()));\n// [0, 1, 2, 3]\n```\n\nNow that we covered the basics of WebGPU and compute shaders, let's move to something more demanding. What if we could perform machine learning inference using Workers and GPUs?\n\n### ONNX WebGPU demo\n\nThe [ONNX runtime](https://github.com/microsoft/onnxruntime) is a popular open-source cross-platform, high performance machine learning inferencing accelerator. [Wonnx](https://github.com/webonnx/wonnx) is a GPU-accelerated version of the same engine, written in Rust, that can be compiled to WebAssembly and take advantage of WebGPU in the browser. We are going to run it in Workers using a combination of [workers-rs](https://github.com/cloudflare/workers-rs), our Rust bindings for Cloudflare Workers, and the workerd WebGPU APIs.\n\nFor this demo, we are using [SqueezeNet](https://www.kdnuggets.com/2016/09/deep-learning-reading-group-squeezenet.html). This small image classification model can run under lower resources but still achieves similar levels of accuracy on the [ImageNet](https://en.wikipedia.org/wiki/ImageNet) image classification validation dataset as larger models like [AlexNet](https://en.wikipedia.org/wiki/AlexNet).\n\nIn essence, our worker will receive any uploaded image and attempt to classify it according to the 1000 ImageNet classes. Once ONNX runs the machine learning model using the GPU, it will return the list of classes with the highest probability scores. Let’s go step by step.\n\nFirst we load the model from R2 into the GPU memory the first time the Durable Object is called:\n\n```\n#[durable_object]\npub struct Classifier {\n    env: Env,\n    session: Option<wonnx::Session>,\n}\n\nimpl Classifier {\n    async fn ensure_session(&mut self) -> Result<()> {\n        match self.session {\n            Some(_) => worker::console_log!(\"DO already has a session\"),\n            None => {\n                // No session, so this should be the first request. In this case\n                // we will fetch the model from R2, build a wonnx session, and\n                // store it for subsequent requests.\n                let model_bytes = fetch_model(&self.env).await?;\n                let session = wonnx::Session::from_bytes(&model_bytes)\n                    .await\n                    .map_err(|err| err.to_string())?;\n                worker::console_log!(\"session created in DO\");\n                self.session = Some(session);\n            }\n        };\n        Ok(())\n    }\n}\n```\n\nThis is only required once, when the Durable Object is instantiated. For subsequent requests, we retrieve the model input tensor, call the existing session for the inference, and return to the calling worker the result tensor converted to JSON:\n\n```\n        let request_data: ArrayBase<OwnedRepr<f32>, Dim<[usize; 4]>> =\n            serde_json::from_str(&req.text().await?)?;\n        let mut input_data = HashMap::new();\n        input_data.insert(\"data\".to_string(), request_data.as_slice().unwrap().into());\n\n        let result = self\n            .session\n            .as_ref()\n            .unwrap() // we know the session exists\n            .run(&input_data)\n            .await\n            .map_err(|err| err.to_string())?;\n...\n        let probabilities: Vec<f32> = result\n            .into_iter()\n            .next()\n            .ok_or(\"did not obtain a result tensor from session\")?\n            .1\n            .try_into()\n            .map_err(|err: TensorConversionError| err.to_string())?;\n\n        let do_response = serde_json::to_string(&probabilities)?;\n        Response::ok(do_response)\n```\n\nOn the Worker script itself, we load the uploaded image and pre-process it into a model input tensor:\n\n```\n    let image_file: worker::File = match req.form_data().await?.get(\"file\") {\n        Some(FormEntry::File(buf)) => buf,\n        Some(_) => return Response::error(\"`file` part of POST form must be a file\", 400),\n        None => return Response::error(\"missing `file`\", 400),\n    };\n    let image_content = image_file.bytes().await?;\n    let image = load_image(&image_content)?;\n```\n\nFinally, we call the GPU Durable Object, which runs the model and returns the most likely classes of our image:\n\n```\n    let probabilities = execute_gpu_do(image, stub).await?;\n    let mut probabilities = probabilities.iter().enumerate().collect::<Vec<_>>();\n    probabilities.sort_unstable_by(|a, b| b.1.partial_cmp(a.1).unwrap());\n    Response::ok(LABELS[probabilities[0].0])\n```\n\nWe packaged this demo in a public repository, so you can also run it. Make sure that you have a [Rust](https://www.rust-lang.org/) compiler, [Node.js](https://nodejs.org/en), [Git](https://git-scm.com/) and [curl](https://curl.se/) installed, then clone the repository:\n\n```\ngit clone https://github.com/cloudflare/workers-wonnx.git\ncd workers-wonnx\n```\n\nUpload the model to the local R2 simulator:\n\n```\nnpx wrangler@latest r2 object put model-bucket-dev/opt-squeeze.onnx --local --file models/opt-squeeze.onnx\n```\n\nAnd then run the Worker locally:\n\n```\nnpx wrangler@latest dev\n```\n\nWith the Worker running and waiting for requests you can then open another terminal window and upload one of the image examples in the same repository using curl:\n\n```\n> curl -F \"file=@images/pelican.jpeg\" http://localhost:8787\nn02051845 pelican\n```\n\nIf everything goes according to plan the result of the curl command will be the most likely class of the image.\n\n### Next steps and final words\n\nOver the upcoming weeks, we will merge the workerd WebGPU code in the Cloudflare Workers production environment and make it available globally, on top of our growing GPU nodes fleet. We didn't do it earlier because that environment is subject to strict security and isolation requirements. For example, we can't break the [security model](https://developers.cloudflare.com/workers/learning/security-model/) of our process sandbox and have V8 talking to the GPU hardware directly, that would be a problem; we must create a configuration where another process is closer to the GPU and use IPC (inter-process communication) to talk to it. Other things like managing resource allocation and billing are being sorted out.\n\nFor now, we wanted to get the good news out that we will support WebGPU in Cloudflare Workers and ensure that you can start playing and coding with it today and learn from it. WebGPU and general-purpose computing on GPUs is still in its early days. We presented a machine-learning demo, but we can imagine other applications taking advantage of this new feature, and we hope you can show us some of them.\n\nAs usual, you can talk to us on our [Developers Discord](https://discord.cloudflare.com/) or the [Community forum](https://community.cloudflare.com/c/developers/39); the team will be listening. We are eager to hear from you and learn about what you're building.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [Cloudflare Workers](https://blog.cloudflare.com/tag/workers/) [Standards](https://blog.cloudflare.com/tag/standards/)"
    },
    {
      "url": "https://blog.cloudflare.com/secrets-store/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/secrets-store/",
        "loadedTime": "2023-12-05T02:34:09.343Z",
        "referrerUrl": "https://blog.cloudflare.com/forrester-wave-edge-development-2023/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/secrets-store/",
        "title": "Announcing Cloudflare Secrets Store",
        "description": "Introducing Secrets Store by Cloudflare - the ultimate solution for managing your application secrets securely. Safeguard sensitive information, track access, and maintain ease of use.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Announcing Cloudflare Secrets Store\n05/18/2023\n5 min read\nThis post is also available in 简体中文, 日本語, Deutsch, Français and Español.\nWe’re excited to announce Secrets Store - Cloudflare’s new secrets management offering!\nA secrets store does exactly what the name implies - it stores secrets. Secrets are variables that are used by developers that contain sensitive information - information that only authorized users and systems should have access to.\nIf you’re building an application, there are various types of secrets that you need to manage. Every system should be designed to have identity & authentication data that verifies some form of identity in order to grant access to a system or application. One example of this is API tokens for making read and write requests to a database. Failure to store these tokens securely could lead to unauthorized access of information - intentional or accidental.\nThe stakes with secrets management are high. Every gap in the storage of these values has potential to lead to a data leak or compromise. A security administrator’s worst nightmare.\nDevelopers are primarily focused on creating applications, they want to build quickly, they want their system to be performant, and they want it to scale. For them, secrets management is about ease of use, performance, and reliability. On the other hand, security administrators are tasked with ensuring that these secrets remain secure. It’s their responsibility to safeguard sensitive information, ensure that security best practices are met, and to manage any fallout of an incident such as a data leak or breach. It’s their job to verify that developers at their company are building in a secure and foolproof manner.\nIn order for developers to build at high velocity and for security administrators to feel at ease, companies need to adopt a highly reliable and secure secrets manager. This should be a system that ensures that sensitive information is stored with the highest security measures, while maintaining ease of use that will allow engineering teams to efficiently build.\nWhy Cloudflare is building a secrets store\nCloudflare’s mission is to help build a better Internet - that means a more secure Internet. We recognize our customers’ need for a secure, centralized repository for storing sensitive data. Within the Cloudflare ecosystem, are various places where customers need to store and access API and authorization tokens, shared secrets, and sensitive information. It’s our job to make it easy for customers to manage these values securely.\nThe need for secrets management goes beyond Cloudflare. Customers have sensitive data that they manage everywhere - at their cloud provider, on their own infrastructure, across machines. Our plan is to make our Secrets Store a one-stop shop for all of our customer’s secrets.\nThe evolution of secrets at Cloudflare\nIn 2020, we launched environment variables and secrets for Cloudflare Workers, allowing customers to create and encrypt variables across their Worker scripts. By doing this, developers can obfuscate the value of a variable so that it’s no longer available in plaintext and can only be accessed by the Worker.\nAdoption and use of these secrets is quickly growing. We now have more than three million Workers scripts that reference variables and secrets managed through Cloudflare. One piece of feedback that we continue to hear from customers is that these secrets are scoped too narrowly. \nToday, customers can only use a variable or secret within the Worker that it’s associated with. Instead, customers have secrets that they share across Workers. They don’t want to re-create those secrets and focus their time on keeping them in sync. They want account level secrets that are managed in one place but are referenced across multiple Workers scripts and functions.\nOutside of Workers, there are many use cases for secrets across Cloudflare services.\nInside our Web Application Firewall (WAF), customers can make rules that look for authorization headers in order to grant or deny access to requests. Today, when customers create these rules, they put the authorization header value in plaintext, so that anyone with WAF access in the Cloudflare account can see its value. What we’ve heard from our customers is that even internally, engineers should not have access to this type of information. Instead, what our customers want is one place to manage the value of this header or token, so that only authorized users can see, create, and rotate this value. Then when creating a WAF rule, engineers can just reference the associated secret e.g.“account.mysecretauth”. By doing this, we help our customers secure their system by reducing the access scope and enhance management of this value by keeping it updated in one place.\nWith new Cloudflare products and features quickly developing, we’re hearing more and more use cases for a centralized secrets manager. One that can be used to store Access Service tokens or shared secrets for Webhooks.\nWith the new account level Secrets Store, we’re excited to give customers the tools they need to manage secrets across Cloudflare services.\nSecuring the Secret Store\nTo have a secrets store, there are a number of measures that need to be in place, and we’re committing to providing these for our customers.\nFirst, we’re going to give the tools that our customers need to restrict access to secrets. We will have scope permissions that will allow admins to choose which users can view, create, edit, or remove secrets. We also plan to add the same level of granularity to our services - giving customers the ability to say “only allow this Worker to access this secret and only allow this set of Firewall rules to access that secret”.\nNext, we’re going to give our customers extensive audits that will allow them to track the access and use of their secrets. Audit logs are crucial for security administrators. They can be used to alert team members that a secret was used by an unauthorized service or that a compromised secret is being accessed when it shouldn’t be. We will give customers audit logs for every secret-related event, so that customers can see exactly who is making changes to secrets and which services are accessing and when.\nIn addition to the built-in security of the Secrets Store, we’re going to give customers the tools to rotate their encryption keys on-demand or at a cadence that fits the right security posture for them.\nSign up for the beta\nWe’re excited to get the Secrets Store in our customer’s hands. If you’re interested in using this, please fill out this form, and we’ll reach out to you when it’s ready to use.\nWatch on Cloudflare TV\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nDeveloper Week Developers Cloudflare Workers Security",
      "markdown": "## Announcing Cloudflare Secrets Store\n\n05/18/2023\n\n*   [![Dina Kozlov](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2019/06/headshot.jpg)](https://blog.cloudflare.com/author/dina/)\n\n5 min read\n\nThis post is also available in [简体中文](https://blog.cloudflare.com/zh-cn/secrets-store-zh-cn/), [日本語](https://blog.cloudflare.com/ja-jp/secrets-store-ja-jp/), [Deutsch](https://blog.cloudflare.com/de-de/secrets-store-de-de/), [Français](https://blog.cloudflare.com/fr-fr/secrets-store-fr-fr/) and [Español](https://blog.cloudflare.com/es-es/secrets-store-es-es/).\n\n![Announcing Cloudflare Secrets Store](https://blog.cloudflare.com/content/images/2023/05/image4-15.png)\n\nWe’re excited to announce Secrets Store - Cloudflare’s new secrets management offering!\n\nA secrets store does exactly what the name implies - it stores secrets. Secrets are variables that are used by developers that contain sensitive information - information that only authorized users and systems should have access to.\n\nIf you’re building an application, there are various types of secrets that you need to manage. Every system should be designed to have identity & authentication data that verifies some form of identity in order to grant access to a system or application. One example of this is API tokens for making read and write requests to a database. Failure to store these tokens securely could lead to unauthorized access of information - intentional or accidental.\n\nThe stakes with [secrets management](https://www.cloudflare.com/learning/security/glossary/secrets-management/) are high. Every gap in the storage of these values has potential to lead to a [data leak](https://www.cloudflare.com/learning/access-management/what-is-dlp/) or compromise. A security administrator’s worst nightmare.\n\nDevelopers are primarily focused on creating applications, they want to build quickly, they want their system to be performant, and they want it to scale. For them, secrets management is about ease of use, performance, and reliability. On the other hand, security administrators are tasked with ensuring that these secrets remain secure. It’s their responsibility to safeguard sensitive information, ensure that security best practices are met, and to manage any fallout of an incident such as a data leak or breach. It’s their job to verify that developers at their company are building in a secure and foolproof manner.\n\nIn order for developers to build at high velocity and for security administrators to feel at ease, companies need to adopt a highly reliable and secure secrets manager. This should be a system that ensures that sensitive information is stored with the highest security measures, while maintaining ease of use that will allow engineering teams to efficiently build.\n\n### Why Cloudflare is building a secrets store\n\nCloudflare’s mission is to help build a better Internet - that means a more secure Internet. We recognize our customers’ need for a secure, centralized repository for storing sensitive data. Within the Cloudflare ecosystem, are various places where customers need to store and access API and authorization tokens, shared secrets, and sensitive information. It’s our job to make it easy for customers to manage these values securely.\n\nThe need for secrets management goes beyond Cloudflare. Customers have sensitive data that they manage everywhere - at their cloud provider, on their own infrastructure, across machines. Our plan is to make our Secrets Store a one-stop shop for all of our customer’s secrets.\n\n### The evolution of secrets at Cloudflare\n\nIn 2020, we launched [environment variables and secrets](https://blog.cloudflare.com/workers-secrets-environment/) for Cloudflare Workers, allowing customers to create and encrypt variables across their Worker scripts. By doing this, developers can obfuscate the value of a variable so that it’s no longer available in plaintext and can only be accessed by the Worker.\n\n![](https://blog.cloudflare.com/content/images/2023/05/image2-26.png)\n\nAdoption and use of these secrets is quickly growing. We now have more than three million Workers scripts that reference variables and secrets managed through Cloudflare. One piece of feedback that we continue to hear from customers is that these secrets are scoped too narrowly.\n\nToday, customers can only use a variable or secret within the Worker that it’s associated with. Instead, customers have secrets that they share across Workers. They don’t want to re-create those secrets and focus their time on keeping them in sync. They want account level secrets that are managed in one place but are referenced across multiple Workers scripts and functions.\n\nOutside of Workers, there are many use cases for secrets across Cloudflare services.\n\nInside our [Web Application Firewall (WAF)](https://www.cloudflare.com/waf/), customers can make rules that look for authorization headers in order to grant or deny access to requests. Today, when customers create these rules, they put the authorization header value in plaintext, so that anyone with WAF access in the Cloudflare account can see its value. What we’ve heard from our customers is that even internally, engineers should not have access to this type of information. Instead, what our customers want is one place to manage the value of this header or token, so that only authorized users can see, create, and rotate this value. Then when creating a WAF rule, engineers can just reference the associated secret e.g.“account.mysecretauth”. By doing this, we help our customers secure their system by reducing the access scope and enhance management of this value by keeping it updated in one place.\n\n![](https://blog.cloudflare.com/content/images/2023/05/image1-45.png)\n\nWith new Cloudflare products and features quickly developing, we’re hearing more and more use cases for a centralized secrets manager. One that can be used to store [Access Service tokens](https://developers.cloudflare.com/cloudflare-one/identity/service-tokens/) or shared secrets for [Webhooks](https://developers.cloudflare.com/fundamentals/notifications/create-notifications/configure-webhooks/).\n\nWith the new account level Secrets Store, we’re excited to give customers the tools they need to manage secrets across Cloudflare services.\n\n### Securing the Secret Store\n\nTo have a secrets store, there are a number of measures that need to be in place, and we’re committing to providing these for our customers.\n\nFirst, we’re going to give the tools that our customers need to restrict access to secrets. We will have scope permissions that will allow admins to choose which users can view, create, edit, or remove secrets. We also plan to add the same level of granularity to our services - giving customers the ability to say “only allow this Worker to access this secret and only allow this set of Firewall rules to access that secret”.\n\n![](https://blog.cloudflare.com/content/images/2023/05/image3-15.png)\n\nNext, we’re going to give our customers extensive audits that will allow them to track the access and use of their secrets. Audit logs are crucial for security administrators. They can be used to alert team members that a secret was used by an unauthorized service or that a compromised secret is being accessed when it shouldn’t be. We will give customers audit logs for every secret-related event, so that customers can see exactly who is making changes to secrets and which services are accessing and when.\n\nIn addition to the built-in security of the Secrets Store, we’re going to give customers the tools to rotate their encryption keys on-demand or at a cadence that fits the right security posture for them.\n\n## Sign up for the beta\n\nWe’re excited to get the Secrets Store in our customer’s hands. If you’re interested in using this, please fill out this [form](https://www.cloudflare.com/lp/secrets-store), and we’ll reach out to you when it’s ready to use.\n\n### Watch on Cloudflare TV\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Developer Week](https://blog.cloudflare.com/tag/developer-week/) [Developers](https://blog.cloudflare.com/tag/developers/) [Cloudflare Workers](https://blog.cloudflare.com/tag/workers/) [Security](https://blog.cloudflare.com/tag/security/)"
    },
    {
      "url": "https://blog.cloudflare.com/cloudflare-is-named-a-leader-in-the-forrester-wave-for-ddos-mitigation-solutions/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/cloudflare-is-named-a-leader-in-the-forrester-wave-for-ddos-mitigation-solutions/",
        "loadedTime": "2023-12-05T02:34:23.230Z",
        "referrerUrl": "https://blog.cloudflare.com/forrester-wave-edge-development-2023/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/cloudflare-is-named-a-leader-in-the-forrester-wave-for-ddos-mitigation-solutions/",
        "title": "Cloudflare recognized as a 'Leader' in The Forrester Wave for DDoS Mitigation Solutions",
        "description": "Cloudflare is named a ‘Leader’ in Forrester Wave™ for DDoS Mitigation Solutions, Q1 2021",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Cloudflare recognized as a 'Leader' in The Forrester Wave for DDoS Mitigation Solutions\n03/02/2021\n8 min read\nWe’re thrilled to announce that Cloudflare has been named a leader in The Forrester WaveTM: DDoS Mitigation Solutions, Q1 2021. You can download a complimentary copy of the report here.\nAccording to the report, written by, Forrester Senior Analyst for Security and Risk, David Holmes, “Cloudflare protects against DDoS from the edge, and fast… customer references view Cloudflare’s edge network as a compelling way to protect and deliver applications.”\nUnmetered and unlimited DDoS protection for all\nCloudflare was founded with the mission to help build a better Internet — one where the impact of DDoS attacks is a thing of the past. Over the last 10 years, we have been unwavering in our efforts to protect our customers’ Internet properties from DDoS attacks of any size or kind. In 2017, we announced unmetered DDoS protection for free — as part of every Cloudflare service and plan including the Free plan — to make sure every organization can stay protected and available.\nThanks to our home-grown automated DDoS protection systems, we’re able to provide unmetered and unlimited DDoS protection for free. Our automated systems constantly analyze traffic samples asynchronously as to avoid impact to performance. They scan for DDoS attacks across layers 3-7 of the OSI model. They look for patterns in IP packets, HTTP requests and HTTP responses. When an attack is identified, a real-time signature is generated in the form of an ephemeral mitigation rule. The rule is propagated to the most optimal location in our edge for the most cost-efficient mitigation: either in the Linux kernel’s eXpress Data Path (XDP), Linux userspace iptables or in the HTTP reverse-proxy. A cost-efficient mitigation strategy means that we can mitigate the most volumetric, distributed attacks without impacting performance.\nRead more about how Cloudflare’s DDoS protection systems work here.\nDDoS attacks increasing\nWe’d like to say DDoS attacks are a thing of the past. But unfortunately, they are not.\nOn the contrary, we continue to see the frequency, sophistication, and geographical distribution of DDoS attacks rise every quarter - in quantity or size. See our reports from last year (Q1 ‘20, Q2 ‘20, Q3 ‘20, and Q4 ‘20) and view overall Internet traffic trends here on Cloudflare Radar.\nOver the past year, Cloudflare has seen and automatically mitigated some of the largest and arguably the most creative cyber attacks. As attackers are getting bolder and smarter in their ways, organizations are looking for ways to battle these kinds of attacks with no disruption to the services they provide.\nDDoS attacks in 2020\nOrganizations are being extorted under threat of DDoS\nIn January this year, we shared the story of how we helped a Fortune Global 500 company stay online and protected whilst they were targeted by a ransom DDoS attack. They weren’t the only one. In fact, in the fourth quarter of 2020, 17% of surveyed Cloudflare customers reported receiving a ransom or a threat of DDoS attack. In Q1 2021, this increased to 26% — roughly 1 out of every 4 respondents reported a ransom threat and a subsequent DDoS attack on their network infrastructure.\nWhether organizations are targeted with ransom attacks or amateur ‘cyber vandalism’, it's important for organizations to utilize an always-on, automated DDoS protection service that doesn’t require manual human intervention in the hour of need. We take great pride in being able to provide this level of protection to our customers.\nContinuous improvement\nAs attacks have continued to evolve, and the number of customers using our services has increased, Cloudflare has continually invested in our technology to stay several steps ahead of attackers. We’ve made significant investments in bolstering our mitigation capacity, honing our detection algorithms, and providing better analytics capabilities to our customers. Our aim is to make impact from DDoS attacks a thing of the past, for all customers, just like spam in the 90s.\nIn 2019, we rolled out our autonomous DDoS detection and mitigation system, dosd. This component of our mitigation stack is fully software-defined, leverages Linux’s eXpress Data Path (XDP), and allows us to quickly and automatically deploy eBPF rules that run on each packet received for inspection — mitigating the most sophisticated attacks within less than 3 seconds on average at the edge and other common attacks instantly. It works by detecting patterns in the attack traffic and then quickly deploying rules autonomously to drop the offenders at wire speed. Additionally, because dosd operates independently within each data center, with no reliance on a centralized data center, it greatly increases the resilience of our network.\nWhile dosd is great at mitigating attacks by detecting patterns in the traffic, what about patternless attacks? That’s where flowtrackd comes in, our novel TCP state classification engine, built in 2020, to defend against disruptive L3/L4 attacks targeting our Magic Transit customers. It’s able to detect and mitigate the most randomized, sophisticated attacks. Additionally, at L7, we also learn our customer’s traffic baselines and identify when their origin is in distress. When an origin server shows signs of deterioration, our systems begin soft mitigation in order to reduce the impact on the server and allow it to recuperate.\nBuilding advanced DDoS protection systems is not only about the detection, but also about cost efficient mitigation. We aim to mitigate attacks without impacting performance that can be caused due to excessive computational consumption. This requirement is why we introduced IP Jails to the world: IP Jails is a gatebot capability that mitigates the most volumetric and distributed attacks without impacting performance. Gatebot activates IP Jails when attacks become significantly volumetric, and then instead of blocking at L7, IP Jails temporarily drops the connection of the offending IP address that generated the request which matched the attack signature that gatebot created. IP Jails leverages the Linux iptables mechanism to drop packets at wirespeed. Dropping L7 attacks at L4, is significantly more cost-efficient, and benefits both our customers and our Site Reliability Engineering team.\nLastly, to provide our customers better visibility and insight into the increasingly sophisticated attacks we’re seeing and mitigating, we released the Firewall Analytics dashboard in 2019. This dashboard provides insights into both HTTP application security and DDoS activity at L7, allowing customers to configure rules directly from within analytics dashboards thus tightening the feedback loop for responding to events. Later in 2020, we released an equivalent dashboard for L3/4 activity for our enterprise Magic Transit and Spectrum customers, in the form of the Network Analytics dashboard. Network Analytics provides insight into packet-level traffic and DDoS attack activity, along with periodical Insights and Trends. To complement the dashboards and provide our users the right information as they need it, we rolled out real-time DDoS alerts and also periodical DDoS reports -- right into your inboxes. Or if you prefer, directly into your SIEM dashboards.\nCloudflare received the top score in the strategy category\nThis year, due to our advanced DDoS protection capabilities, Cloudflare received the top score in the strategy category and among the top three in the current offering category. Additionally, we were given the highest possible scores in 15 criteria in the report, including:\nThreat detection\nBurst attacks\nResponse automation\nSpeed of implementation\nProduct vision\nPerformance\nSecurity operation center (SOC) service\nWe believe that our standing stems from the sustained investments we’ve made over the last few years in our global Anycast network — which serves as the foundation of all services we provide to our customers.\nOur network is architected for scale — every service runs on every server in every Cloudflare data center that spans over 200 cities globally. And as opposed to some of the other vendors in the report, every Cloudflare service is delivered from every one of our edge data centers.\nIntegrated security and performance\nA leading application performance monitoring company that uses Cloudflare’s services for serverless compute and content delivery recently told us that they wanted to consolidate their performance and security services under one provider. They got rid of their incumbent L3 services provider and onboarded Cloudflare for their application and network services (with Magic Transit) for easier management and better support.\nWe see this more and more. The benefits of using a single cloud provider for bundled security and performance services are plentiful:\nEasier management — users can manage all of Cloudflare’s services such as DDoS protection, WAF, CDN, bot management and serverless compute from a single dashboard and a single API endpoint.\nDeep service integration - all of our services are deeply integrated which allows our users to truly leverage the power of Cloudflare. As an example, Bot Management rules are implemented with our Application Firewall.\nEasier troubleshooting — instead of having to reach out to multiple providers, our customers have a single point of contact when troubleshooting. Additionally, we provide immediate human response in our under attack hotline.\nLower latency — because every one of our services are delivered from all of our data centers, there are no performance penalties. As an example, there are no additional routing hops between the DDoS service to Bot Management service to CDN service.\nHowever, not all cloud services are built the same, i.e. most vendors today do not have a comprehensive and robust solution to offer. Cloudflare’s unique architecture enables it to offer an integrated solution that comprises an all-star cast featuring the following to name a few:\nCDN: Cloudflare CDN recognized as a Gartner Peer Insights \"Customer's Choice\" in 2020 for Global CDN1\nDDoS: Received the highest number of high scores in the 2020 Gartner report for Solution Comparison for DDoS Cloud Scrubbing Centers2\nWAF: Cloudflare is a CHALLENGER in the 2020 Gartner Magic Quadrant for Web Application Firewall (receiving the highest placement in the ‘Ability to Execute’)3\nZero Trust: Cloudflare is a LEADER in the Omdia Market Radar: Zero-Trust Access Report, 20204\nBot Management: Leader in the 2020 SPARK Matrix of Bot Management Market5\nIntegrated solution: Innovation leader in the Global Holistic Web Protection Market for 2020 by Frost & Sullivan6\nWe are pleased to be named a LEADER in The Forrester Wave™: for DDoS Mitigation Solutions, Q1 2021 report, and will continue to work tirelessly to remain, as the report puts it, a “compelling way to protect and deliver applications” for our customers.\nFor more information about Cloudflare’s DDoS protection, reach out to us here or hands-on evaluation of Cloudflare, sign up here.\n.........\n1https://www.gartner.com/reviews/market/global-cdn/vendor/cloudflare/product/cloudflare-cdn\n2https://www.gartner.com/en/documents/3983636/solution-comparison-for-ddos-cloud-scrubbing-centers\n3Gartner, “Magic Quadrant for Web Application Firewalls'', Analyst(s): Jeremy D'Hoinne, Adam Hils, John Watts, Rajpreet Kaur, October 19, 2020. https://www.gartner.com/doc/reprints?id=1-249JQ6L1&ct=200929&st=sb\n4https://www.cloudflare.com/lp/omdia-zero-trust\n5https://www.cloudflare.com/lp/qks-bot-management-leader/\n6https://www.cloudflare.com/lp/frost-radar-holistic-web/\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nDDoS Network Forrester",
      "markdown": "## Cloudflare recognized as a 'Leader' in The Forrester Wave for DDoS Mitigation Solutions\n\n03/02/2021\n\n*   [![Vivek Ganti](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2020/08/Vivek-Ganti.jpeg)](https://blog.cloudflare.com/author/vivek/)\n*   [![Omer Yoachimik](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/06/Screenshot-2023-06-02-at-9.38.13-AM.png)](https://blog.cloudflare.com/author/omer/)\n\n8 min read\n\n![](https://blog.cloudflare.com/content/images/2021/03/image5.png)\n\nWe’re thrilled to announce that Cloudflare has been named a leader in The Forrester WaveTM: DDoS Mitigation Solutions, Q1 2021. You can download a complimentary copy of the report [here](https://www.cloudflare.com/forrester-wave-ddos-mitigation-2021/).\n\nAccording to the report, written by, Forrester Senior Analyst for Security and Risk, David Holmes, “Cloudflare protects against DDoS from the edge, and fast… customer references view Cloudflare’s edge network as a compelling way to protect and deliver applications.”\n\n### Unmetered and unlimited DDoS protection for all\n\nCloudflare was founded with the mission to help build a better Internet — one where the impact of [DDoS attacks](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/) is a thing of the past. Over the last 10 years, we have been unwavering in our efforts to protect our customers’ Internet properties from DDoS attacks of any size or kind. In 2017, we announced [unmetered DDoS protection](https://blog.cloudflare.com/unmetered-mitigation/) for free — as part of every Cloudflare service and plan including the Free plan — to make sure every organization can stay protected and available.\n\nThanks to our home-grown automated DDoS protection systems, we’re able to provide unmetered and unlimited DDoS protection for free. Our automated systems constantly analyze traffic samples asynchronously as to avoid impact to performance. They scan for DDoS attacks across layers 3-7 of the [OSI model](https://www.cloudflare.com/learning/ddos/glossary/open-systems-interconnection-model-osi/). They look for patterns in IP packets, HTTP requests and HTTP responses. When an attack is identified, a real-time signature is generated in the form of an ephemeral mitigation rule. The rule is propagated to the most optimal location in our edge for the most cost-efficient mitigation: either in the Linux kernel’s [eXpress Data Path (XDP)](https://blog.cloudflare.com/l4drop-xdp-ebpf-based-ddos-mitigations/), Linux userspace [iptables](https://en.wikipedia.org/wiki/Iptables) or in the [HTTP reverse-proxy](https://www.cloudflare.com/learning/cdn/glossary/reverse-proxy/). A cost-efficient mitigation strategy means that we can mitigate the most volumetric, distributed attacks without impacting performance.\n\n![](https://blog.cloudflare.com/content/images/2021/03/image1-3.png)\n\nRead more about how Cloudflare’s DDoS protection systems work [here](https://blog.cloudflare.com/moobot-vs-gatebot-cloudflare-automatically-blocks-botnet-ddos-attack-topping-at-654-gbps/#ddos-detect-mitigate).\n\n### DDoS attacks increasing\n\nWe’d like to say DDoS attacks are a thing of the past. But unfortunately, they are not.\n\nOn the contrary, we continue to see the frequency, sophistication, and geographical distribution of DDoS attacks rise every quarter - in quantity or size. See our reports from last year ([Q1 ‘20](https://blog.cloudflare.com/network-layer-ddos-attack-trends-for-q1-2020), [Q2 ‘20](https://blog.cloudflare.com/network-layer-ddos-attack-trends-for-q2-2020), [Q3 ‘20](https://blog.cloudflare.com/network-layer-ddos-attack-trends-for-q3-2020), and [Q4 ‘20](https://blog.cloudflare.com/network-layer-ddos-attack-trends-for-q4-2020)) and view overall Internet traffic trends here on [Cloudflare Radar](https://radar.cloudflare.com/).\n\nOver the past year, Cloudflare has seen and automatically mitigated some of the [largest](https://blog.cloudflare.com/mitigating-a-754-million-pps-ddos-attack-automatically/) and arguably the [most creative](https://blog.cloudflare.com/beat-an-acoustics-inspired-ddos-attack/) cyber attacks. As attackers are getting bolder and smarter in their ways, organizations are looking for ways to battle these kinds of attacks with no disruption to the services they provide.\n\n![](https://blog.cloudflare.com/content/images/2021/03/image3-1.png)\n\nDDoS attacks in 2020\n\n### Organizations are being extorted under threat of DDoS\n\nIn January this year, we shared the story of [how we helped a Fortune Global 500 company](https://blog.cloudflare.com/ransom-ddos-attacks-target-a-fortune-global-500-company/) stay online and protected whilst they were targeted by a [ransom DDoS attack](https://www.cloudflare.com/learning/ddos/ransom-ddos-attack/). They weren’t the only one. In fact, in the fourth quarter of 2020, 17% of surveyed Cloudflare customers reported receiving a ransom or a threat of DDoS attack. In Q1 2021, this increased to 26% — roughly 1 out of every 4 respondents reported a ransom threat and a subsequent DDoS attack on their network infrastructure.\n\n![](https://blog.cloudflare.com/content/images/2021/03/image4-1.png \"Chart\")\n\nWhether organizations are targeted with ransom attacks or amateur ‘cyber vandalism’, it's important for organizations to utilize an _always-on, automated_ DDoS protection service that doesn’t require manual human intervention in the hour of need. We take great pride in being able to provide this level of protection to our customers.\n\n### Continuous improvement\n\nAs attacks have continued to evolve, and the number of customers using our services has increased, Cloudflare has continually invested in our technology to stay several steps ahead of attackers. We’ve made significant investments in bolstering our mitigation capacity, honing our detection algorithms, and providing better analytics capabilities to our customers. Our aim is to make impact from DDoS attacks a thing of the past, for all customers, just like spam in the 90s.\n\nIn 2019, we [rolled out our autonomous DDoS detection and mitigation system](https://blog.cloudflare.com/rolling-with-the-punches-shifting-attack-tactics-dropping-packets-faster-cheaper-at-the-edge/), dosd. This component of our mitigation stack is fully software-defined, leverages Linux’s eXpress Data Path (XDP), and allows us to quickly and automatically deploy eBPF rules that run on each packet received for inspection — mitigating the most sophisticated attacks within less than 3 seconds on average at the edge and other common attacks instantly. It works by detecting patterns in the attack traffic and then quickly deploying rules autonomously to drop the offenders at wire speed. Additionally, because dosd operates independently within each data center, with no reliance on a centralized data center, it greatly increases the resilience of our network.\n\nWhile dosd is great at mitigating attacks by detecting patterns in the traffic, what about patternless attacks? That’s where [flowtrackd](https://blog.cloudflare.com/announcing-flowtrackd/) comes in, our novel TCP state classification engine, built in 2020, to defend against disruptive L3/L4 attacks targeting our [Magic Transit customers](https://blog.cloudflare.com/magic-transit/). It’s able to detect and mitigate [the most randomized, sophisticated attacks](https://blog.cloudflare.com/beat-an-acoustics-inspired-ddos-attack/). Additionally, at L7, we also learn our customer’s traffic baselines and [identify when their origin is in distress](https://blog.cloudflare.com/rolling-with-the-punches-shifting-attack-tactics-dropping-packets-faster-cheaper-at-the-edge/#low). When an origin server shows signs of deterioration, our systems begin _soft_ mitigation in order to reduce the impact on the server and allow it to recuperate.\n\nBuilding advanced DDoS protection systems is not only about the detection, but also about cost efficient mitigation. We aim to mitigate attacks without impacting performance that can be caused due to excessive computational consumption. This requirement is why we [introduced IP Jails](https://blog.cloudflare.com/rolling-with-the-punches-shifting-attack-tactics-dropping-packets-faster-cheaper-at-the-edge/#jails) to the world: IP Jails is a [gatebot](https://blog.cloudflare.com/meet-gatebot-a-bot-that-allows-us-to-sleep/) capability that mitigates the most volumetric and distributed attacks without impacting performance. Gatebot activates IP Jails when attacks become significantly volumetric, and then instead of blocking at L7, IP Jails temporarily drops the connection of the offending IP address that generated the request which matched the attack signature that gatebot created. IP Jails leverages the Linux iptables mechanism to drop packets at wirespeed. Dropping L7 attacks at L4, is significantly more cost-efficient, and benefits both our customers and our Site Reliability Engineering team.\n\nLastly, to provide our customers better visibility and insight into the increasingly sophisticated attacks we’re seeing and mitigating, [we released](https://blog.cloudflare.com/new-firewall-tab-and-analytics/) the [Firewall Analytics](https://support.cloudflare.com/hc/en-us/articles/360024520152-Understanding-Cloudflare-Firewall-Analytics) dashboard in 2019. This dashboard provides insights into both HTTP application security and DDoS activity at L7, allowing customers to configure [rules](https://support.cloudflare.com/hc/en-us/articles/360016473712-Cloudflare-Firewall-Rules) directly from within analytics dashboards thus tightening the feedback loop for responding to events. Later in 2020, [we released](https://blog.cloudflare.com/announcing-network-analytics/) an equivalent dashboard for L3/4 activity for our enterprise Magic Transit and Spectrum customers, in the form of the [Network Analytics](https://support.cloudflare.com/hc/en-us/articles/360038696631-Understanding-Cloudflare-Network-Analytics) dashboard. Network Analytics provides insight into packet-level traffic and DDoS attack activity, along with periodical [Insights and Trends](https://blog.cloudflare.com/announcing-spectrum-ddos-analytics-and-ddos-insights-trends/). To complement the dashboards and provide our users the right information as they need it, [we rolled out real-time DDoS alerts](https://blog.cloudflare.com/announcing-ddos-alerts/) and also periodical [DDoS reports](https://support.cloudflare.com/hc/en-us/articles/360053233231-Understanding-Cloudflare-DDoS-reports) -- right into your inboxes. Or if you prefer, directly into your [SIEM dashboards](https://developers.cloudflare.com/logs/).\n\n### Cloudflare received the top score in the strategy category\n\nThis year, due to our advanced DDoS protection capabilities, Cloudflare received the top score in the strategy category and among the top three in the current offering category. Additionally, we were given the highest possible scores in 15 criteria in the report, including:\n\n*   Threat detection\n*   Burst attacks\n*   Response automation\n*   Speed of implementation\n*   Product vision\n*   Performance\n*   Security operation center (SOC) service\n\nWe believe that our standing stems from the sustained investments we’ve made over the last few years in our [global Anycast network](https://www.cloudflare.com/network) — which serves as the foundation of all services we provide to our customers.\n\nOur network is architected for scale — every service runs on every server in every Cloudflare data center that spans over 200 cities globally. And as opposed to some of the other vendors in the report, every Cloudflare service is delivered from every one of our edge data centers.\n\n### Integrated security and performance\n\nA leading application performance monitoring company that uses Cloudflare’s services for serverless compute and content delivery recently told us that they wanted to consolidate their performance and security services under one provider. They got rid of their incumbent L3 services provider and onboarded Cloudflare for their application and network services (with [Magic Transit](https://www.cloudflare.com/magic-transit/)) for easier management and better support.\n\nWe see this more and more. The benefits of using a single cloud provider for bundled security and performance services are plentiful:\n\n*   **Easier management** — users can manage all of Cloudflare’s services such as [DDoS protection](https://www.cloudflare.com/ddos/), [WAF](https://www.cloudflare.com/waf/), [CDN](https://www.cloudflare.com/cdn/), [bot management](https://www.cloudflare.com/products/bot-management/) and [serverless compute](https://workers.cloudflare.com/) from a single dashboard and a single API endpoint.\n*   **Deep service integration** - all of our services are deeply integrated which allows our users to truly leverage the power of Cloudflare. As an example, Bot Management rules are implemented with our Application Firewall.\n*   **Easier troubleshooting** — instead of having to reach out to multiple providers, our customers have a single point of contact when troubleshooting. Additionally, we provide immediate human response in our [under attack hotline](https://www.cloudflare.com/ddos/under-attack/).\n*   **Lower latency** — because every one of our services are delivered from all of our data centers, there are no performance penalties. As an example, there are no additional routing hops between the DDoS service to Bot Management service to [CDN service](https://www.cloudflare.com/learning/cdn/what-is-a-cdn/).\n\nHowever, not all cloud services are built the same, i.e. most vendors today do not have a comprehensive and robust solution to offer. Cloudflare’s unique architecture enables it to offer an integrated solution that comprises an all-star cast featuring the following to name a few:\n\n*   **CDN**: Cloudflare CDN recognized as a Gartner Peer Insights \"Customer's Choice\" in 2020 for Global CDN1\n*   **DDoS**: Received the highest number of high scores in the 2020 Gartner report for Solution Comparison for DDoS Cloud Scrubbing Centers2\n*   **WAF**: Cloudflare is a CHALLENGER in the 2020 Gartner Magic Quadrant for Web Application Firewall (receiving the highest placement in the ‘Ability to Execute’)3\n*   **Zero Trust**: Cloudflare is a LEADER in the Omdia Market Radar: Zero-Trust Access Report, 20204\n*   **Bot Management**: Leader in the 2020 SPARK Matrix of Bot Management Market5\n*   **Integrated solution**: Innovation leader in the Global Holistic Web Protection Market for 2020 by Frost & Sullivan6\n\nWe are pleased to be named a LEADER in The Forrester Wave™: for DDoS Mitigation Solutions, Q1 2021 report, and will continue to work tirelessly to remain, as the report puts it, a “compelling way to protect and deliver applications” for our customers.\n\nFor more information about Cloudflare’s DDoS protection, [reach out to us here](https://www.cloudflare.com/enterprise) or hands-on evaluation of Cloudflare, sign up [here](https://dash.cloudflare.com/sign-up).\n\n.........  \n1[https://www.gartner.com/reviews/market/global-cdn/vendor/cloudflare/product/cloudflare-cdn](https://www.gartner.com/reviews/market/global-cdn/vendor/cloudflare/product/cloudflare-cdn)  \n2[https://www.gartner.com/en/documents/3983636/solution-comparison-for-ddos-cloud-scrubbing-centers](https://www.gartner.com/en/documents/3983636/solution-comparison-for-ddos-cloud-scrubbing-centers)  \n3Gartner, “Magic Quadrant for Web Application Firewalls'', Analyst(s): Jeremy D'Hoinne, Adam Hils, John Watts, Rajpreet Kaur, October 19, 2020. [https://www.gartner.com/doc/reprints?id=1-249JQ6L1&ct=200929&st=sb](https://www.gartner.com/doc/reprints?id=1-249JQ6L1&ct=200929&st=sb)  \n4[https://www.cloudflare.com/lp/omdia-zero-trust](https://www.cloudflare.com/lp/omdia-zero-trust)  \n5[https://www.cloudflare.com/lp/qks-bot-management-leader/](https://www.cloudflare.com/lp/qks-bot-management-leader/)  \n6[https://www.cloudflare.com/lp/frost-radar-holistic-web/](https://www.cloudflare.com/lp/frost-radar-holistic-web/)\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[DDoS](https://blog.cloudflare.com/tag/ddos/) [Network](https://blog.cloudflare.com/tag/network/) [Forrester](https://blog.cloudflare.com/tag/forrester/)"
    },
    {
      "url": "https://blog.cloudflare.com/forrester-wave-edge-development-2021/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/forrester-wave-edge-development-2021/",
        "loadedTime": "2023-12-05T02:34:27.606Z",
        "referrerUrl": "https://blog.cloudflare.com/forrester-wave-edge-development-2023/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/forrester-wave-edge-development-2021/",
        "title": "Cloudflare recognized as a 'Leader' in The Forrester New Wave for Edge Development Platforms",
        "description": "Forrester’s New Wave for Edge Development Platforms has just been announced. We’re thrilled that they have named Cloudflare a leader.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Cloudflare recognized as a 'Leader' in The Forrester New Wave for Edge Development Platforms\n10/27/2021\n3 min read\nThis post is also available in 简体中文, 繁體中文, 한국어 and 日本語.\nForrester’s New Wave for Edge Development Platforms has just been announced. We’re thrilled that they have named Cloudflare a leader (you can download a complimentary copy of the report here).\nSince the very beginning, Cloudflare has sought to help developers building on the web, and since the introduction of Workers in 2017, Cloudflare has enabled developers to deploy their applications to the edge itself.\nAccording to the report by Forrester Vice President, Principal Analyst, Jeffrey Hammond, Cloudflare “offers strong compute, data services and web development capabilities. Alongside Workers, Workers KV adds edge data storage. Pages, Stream and Images provide higher level platform services for modern web workloads. Cloudflare has an intuitive developer experience, fast, global deployment of updated code, and minimal cold start times.”\nReimagining development for the modern web\nBuilding on the web has come a long way. The idea that one might have to buy a physical machine in order to build a website seems incomprehensible now. The cloud has played a major role in making it easier for developers to get started. However, since the advent of the cloud, things have stalled — and innovation has become more incremental. That means that while developers don’t have to think about buying a server, they’re still tasked with thinking about where in the world it is, how to add concurrency to handle increasing traffic, and how to make them secure.\nWe wanted to abstract that all away. Our aim: to reimagine what things might look like if developers could truly just think about the application they wanted to build. Leaving the scaling, speed, and even compliance, to us.\nOf course, reimagining things is always scary. There’s no guarantee that taking a new approach is going to work — it usually requires a leap of faith.\nIt’s been gratifying to see developers flock to our platform — and the applications they’ve been able to build, free of scalability and latency constraints, have been phenomenal.\nIt’s also gratifying to be named a Leader in Edge Development Platforms by Forrester — one of the preeminent analyst firms in the industry. We feel it really does provide industry recognition to the approach we bet on four years ago.\nCloudflare is the most differentiated among all the vendors evaluated\nWe received a differentiated rating in the following criteria:\nDeveloper experience\nProgramming model\nPlatform execution model\n“Day 2+” experience\nIntegrations\nRoadmap\nVision\nMarket approach\nWhile being able to build our platform atop Cloudflare’s network gave us an advantage in eliminating latency from the start, we knew that wasn’t enough to compel developers to think in a new way. Since the release of Workers, we have relentlessly focused on making the experience of building a new application as easy as possible at every step of the way: from onboarding, through day 2, and beyond.\nThis approach extends beyond tooling, and to how we think about additional services developers need in order to complete their applications. For example, in thinking about providing data solutions on the edge, we again wanted to make the distributed nature of the system just work, rather than making developers think about it, which is what led us to develop Durable Objects. With Durable Objects, Cloudflare can make intelligent decisions about where to store the data based on access patterns (or compliance — whichever is most important to the developer), rather than forcing the developer to think about regions.\nAs we expand our offering, it’s important to us that it continues to be intuitive and easy for developers to solve problems.\nWe’re just getting started\nBut, we’re not stopping here. As our cofounder Michelle likes to say, we’re just getting started. We recognize this is just the beginning of the journey to bring the full stack to the edge. We have some exciting announcements coming in the next couple of weeks — stay tuned!\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nForrester",
      "markdown": "## Cloudflare recognized as a 'Leader' in The Forrester New Wave for Edge Development Platforms\n\n10/27/2021\n\n*   [![Rita Kozlov](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/05/Rita-Kozlov.jpeg)](https://blog.cloudflare.com/author/rita/)\n*   [![James Allworth](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2020/10/jamesallworth-20171002-024-bw-centered-square.jpg)](https://blog.cloudflare.com/author/james-allworth/)\n\n3 min read\n\n_This post is also available in [简体中文](https://blog.cloudflare.com/zh-cn/forrester-wave-edge-development-2021-zh-cn/), [繁體中文](https://blog.cloudflare.com/zh-tw/forrester-wave-edge-development-2021-zh-tw/), [한국어](https://blog.cloudflare.com/ko-kr/forrester-wave-edge-development-2021-ko-kr/) and [日本語](https://blog.cloudflare.com/ja-jp/forrester-wave-edge-development-2021-ja-jp)._\n\n![Cloudflare recognized as a 'Leader' in The Forrester New Wave for Edge Development Platforms](https://blog.cloudflare.com/content/images/2021/10/BDES-2443_Forrester_New_Wave_report_LinkedIn-2-1.png)\n\nForrester’s New Wave for Edge Development Platforms has just been announced. We’re thrilled that they have named Cloudflare a leader (you can download a complimentary copy of the report [here](https://www.cloudflare.com/forrester-wave-edge-development-2021)).\n\nSince the very beginning, Cloudflare has sought to help developers building on the web, and since the introduction of Workers in 2017, Cloudflare has enabled developers to deploy their applications to the edge itself.\n\n> According to the report by Forrester Vice President, Principal Analyst, Jeffrey Hammond, Cloudflare “**offers strong compute, data services and web development capabilities.** Alongside Workers, Workers KV adds edge data storage. Pages, Stream and Images provide higher level platform services for modern web workloads. Cloudflare has an intuitive developer experience, fast, global deployment of updated code, and minimal cold start times.”\n\n![](https://blog.cloudflare.com/content/images/2021/10/unnamed-10.png)\n\n### Reimagining development for the modern web\n\nBuilding on the web has come a long way. The idea that one might have to buy a physical machine in order to build a website seems incomprehensible now. The cloud has played a major role in making it easier for developers to get started. However, since the advent of the cloud, things have stalled — and innovation has become more incremental. That means that while developers don’t have to think about _buying_ a server, they’re still tasked with thinking about where in the world it is, how to add concurrency to handle increasing traffic, and how to make them secure.\n\nWe wanted to abstract that all away. Our aim: to reimagine what things might look like if developers could truly just think about the application they wanted to build. Leaving the scaling, speed, and even compliance, to us.\n\nOf course, reimagining things is always scary. There’s no guarantee that taking a new approach is going to work — it usually requires a leap of faith.\n\nIt’s been gratifying to see developers flock to our platform — and the applications they’ve been able to build, free of scalability and latency constraints, have been phenomenal.\n\nIt’s also gratifying to be named a Leader in Edge Development Platforms by Forrester — one of the preeminent analyst firms in the industry. We feel it really does provide industry recognition to the approach we bet on four years ago.\n\n### Cloudflare is the most differentiated among all the vendors evaluated\n\nWe received a differentiated rating in the following criteria:\n\n*   Developer experience\n*   Programming model\n*   Platform execution model\n*   “Day 2+” experience\n*   Integrations\n*   Roadmap\n*   Vision\n*   Market approach\n\nWhile being able to build our platform atop Cloudflare’s network gave us an advantage in eliminating latency from the start, we knew that wasn’t enough to compel developers to think in a new way. Since the release of Workers, we have relentlessly focused on making the experience of building a new application as easy as possible at every step of the way: from onboarding, through day 2, and beyond.\n\nThis approach extends beyond tooling, and to how we think about additional services developers need in order to complete their applications. For example, in thinking about providing data solutions on the edge, we again wanted to make the distributed nature of the system just work, rather than making developers think about it, which is what led us to develop Durable Objects. With Durable Objects, Cloudflare can make intelligent decisions about where to store the data based on access patterns (or compliance — whichever is most important to the developer), rather than forcing the developer to think about regions.\n\nAs we expand our offering, it’s important to us that it continues to be intuitive and easy for developers to solve problems.\n\n### We’re just getting started\n\nBut, we’re not stopping here. As our cofounder Michelle likes to say, we’re just getting started. We recognize this is just the beginning of the journey to bring the full stack to the edge. We have some exciting announcements coming in the next couple of weeks — stay tuned!\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Forrester](https://blog.cloudflare.com/tag/forrester/)"
    },
    {
      "url": "https://blog.cloudflare.com/birthday-week-2018-wrap-up/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/birthday-week-2018-wrap-up/",
        "loadedTime": "2023-12-05T02:34:41.660Z",
        "referrerUrl": "https://blog.cloudflare.com/debugging-cloudflare-workers/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/birthday-week-2018-wrap-up/",
        "title": "Birthday Week Wrap-Up: Every day is launch day at Cloudflare",
        "description": "Our customers are accustomed to us launching new services, features, and functionality at a feverish pace, but recently, we’ve been especially active. This week we celebrated our 8th Birthday Week by announcing new offerings that benefit our customers and the global Internet community.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "09/28/2018\n4 min read\nOur customers are accustomed to us launching new services, features, and functionality at a feverish pace, but recently, we’ve been especially active. This week we celebrated our 8th Birthday Week by announcing new offerings that benefit our customers and the global Internet community. Our mission is to help build a better Internet, and we’re convinced that launching new capabilities that benefit not only our customers, but also the broader Internet overall, is the best way to fulfill our mission.\nHelping build a better Internet, one launch at a time\nAs an organization, we could choose to celebrate Cloudflare’s birthday in lots of different ways (a press release, a company party, or fun gifts for all our employees). But at Cloudflare, we have a unique birthday tradition: we roll up our sleeves and give our customers and the Internet community a new capability (i.e. a gift) every day of our birthday week.\nSome of this past week’s launches have been entirely new offerings, like providing key-value storage across Cloudflare’s global cloud network with Cloudflare Workers KV. Other birthday week launches help improve the overall Internet ecosystem: the Bandwidth Alliance reduces data transfer charges from major cloud hosts and Cloudflare Registrar reduces the hidden fees typical of many domain registration providers. Other new offerings are focused on improving the Internet’s security and performance and are completely free to use. For example, Encrypted SNI helps fix one of the security holes in the Internet, and our support of the QUIC protocol promises to help make mobile browsing faster. \nWe believe the only real way to help build a better Internet is to keep innovating, keep building, and keep launching -- every single day. In fact, our prelude to this year’s Birthday Week was Crypto Week, a full week dedicated to announcing new technologies that use cryptography to make the Internet better. No promises, but it is entirely imaginable that in coming years Cloudflare won’t just be celebrating a Birthday Week, but we’ll be launching new capabilities every day of a Birthday Month!\nBelow is a wrap-up of the capabilities launched this past week.\nBirthday Week Announcements\nDay 1: ENCRYPTED SNI\nCloudflare is fixing one of the core Internet bugs by keeping hostnames private using Encrypted Server Name Indication (SNI). All domains on Cloudflare using our authoritative name servers get Encrypted SNI enabled by default. Explore the protection of Encrypted SNI.\nDay 2: SUPPORT FOR QUIC (Beta)\nCloudflare is looking forward to the standardization of the new QUIC protocol being developed by the IETF. Applications are being accepted for early access to our test implementation that allows developers to validate their QUIC deployments before supported web browsers become available. Learn more about QUIC.\nDay 3: BANDWIDTH ALLIANCE\nThe Bandwidth Alliance is a group of forward-thinking cloud and networking companies that are committed to discounting or waiving data transfer (also known as bandwidth) fees for shared customers. Learn how the Bandwidth Alliance reduces costs.\nDay 4: CLOUDFLARE REGISTRAR (Early Access)\nCloudflare Registrar lets you securely register and manage your domain name with transparent, no-markup pricing that eliminates surprise renewal fees and hidden add-on charges. Be one of the first to transfer your domain to Cloudflare. Register for early access to the Cloudflare Registrar.\nDay 5: WORKERS KV (Beta)\nCloudflare Workers KV provides access to a secure low latency key-value store at all 153 Cloudflare data centers. Developers can use Cloudflare Workers and Workers KV to augment existing applications or to build entirely new applications on top of Cloudflare's global cloud network. Workers KV scales seamlessly to support applications serving dozens or millions of users. Explore how Workers KV allows for serverless key-value storage.\nYou’ve been hearing a lot from us; now hear from those who inspire us!\nAnother way we’ll soon be serving the Internet community is by hosting the fourth annual Cloudflare Internet Summit next week at our San Francisco office on Thursday, October 4th. We don’t spend any time talking about Cloudflare at the Internet Summit. Instead we facilitate discussions with the people who inspire and challenge us. The Internet Summit focuses on the future of the Internet and will feature a series of fireside chats, intimate panel discussions, and lively conversations from some of the brightest thought leaders, executives, entrepreneurs, researchers, and operators. Tickets are almost sold out, so register for the Cloudflare Internet Summit now or plan to tune in to our live stream!\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nBirthday Week Product News Registrar Cloudflare Workers Cloudflare Workers KV",
      "markdown": "09/28/2018\n\n*   [![Jake Anderson](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2018/09/IMG_1383.jpg)](https://blog.cloudflare.com/author/jake-anderson/)\n\n4 min read\n\nOur customers are accustomed to us launching new services, features, and functionality at a feverish pace, but recently, we’ve been especially active. This week we celebrated our [8th Birthday Week](https://blog.cloudflare.com/cloudflare-turns-8/) by announcing new offerings that benefit our customers and the global Internet community. Our mission is to help build a better Internet, and we’re convinced that launching new capabilities that benefit not only our customers, but also the broader Internet overall, is the best way to fulfill our mission.\n\n![](https://blog.cloudflare.com/content/images/2018/09/Birthday-Week.gif)\n\n### Helping build a better Internet, one launch at a time\n\nAs an organization, we could choose to celebrate Cloudflare’s birthday in lots of different ways (a press release, a company party, or fun gifts for all our employees). But at Cloudflare, we have a unique birthday tradition: we roll up our sleeves and give our customers and the Internet community a new capability (i.e. a gift) every day of our birthday week.\n\nSome of this past week’s launches have been entirely new offerings, like providing key-value storage across Cloudflare’s global cloud network with [Cloudflare Workers KV](https://blog.cloudflare.com/introducing-workers-kv/).  Other birthday week launches help improve the overall Internet ecosystem: the [Bandwidth Alliance](https://blog.cloudflare.com/bandwidth-alliance/) reduces data transfer charges from major cloud hosts and [Cloudflare Registrar](https://blog.cloudflare.com/cloudflare-registrar/) reduces the hidden fees typical of many domain registration providers. Other new offerings are focused on improving the Internet’s security and performance and are completely free to use. For example, [Encrypted SNI](https://blog.cloudflare.com/esni/) helps fix one of the security holes in the Internet, and our support of the [QUIC protocol](https://blog.cloudflare.com/the-quicening/) promises to help make mobile browsing faster.  \n\nWe believe the only real way to help build a better Internet is to keep innovating, keep building, and keep launching -- every single day.  In fact, our prelude to this year’s Birthday Week was [Crypto Week](https://blog.cloudflare.com/crypto-week-2018/), a full week dedicated to announcing new technologies that use cryptography to make the Internet better.  No promises, but it is entirely imaginable that in coming years Cloudflare won’t just be celebrating a Birthday Week, but we’ll be launching new capabilities every day of a Birthday Month!\n\nBelow is a wrap-up of the capabilities launched this past week.\n\n### Birthday Week Announcements\n\n![](https://blog.cloudflare.com/content/images/2018/09/Cloudflare-ESNI.png)\n\n#### Day 1: [ENCRYPTED SNI](https://blog.cloudflare.com/esni/)\n\nCloudflare is fixing one of the core Internet bugs by keeping hostnames private using Encrypted Server Name Indication (SNI). All domains on Cloudflare using our authoritative name servers get Encrypted SNI enabled by default. [Explore the protection of Encrypted SNI](https://blog.cloudflare.com/esni/).\n\n![](https://blog.cloudflare.com/content/images/2018/09/Cloudflare-Support-for-QUIC.png)\n\n#### Day 2: [SUPPORT FOR QUIC (Beta)](https://blog.cloudflare.com/the-quicening/)\n\nCloudflare is looking forward to the standardization of the new QUIC protocol being developed by the IETF. Applications are being accepted for early access to our test implementation that allows developers to validate their QUIC deployments before supported web browsers become available. [Learn more about QUIC](https://blog.cloudflare.com/the-quicening/).\n\n![](https://blog.cloudflare.com/content/images/2018/09/Cloudflare-Bandwidth-Alliance.png)\n\n#### Day 3: [BANDWIDTH ALLIANCE](https://blog.cloudflare.com/bandwidth-alliance/)\n\nThe Bandwidth Alliance is a group of forward-thinking cloud and networking companies that are committed to discounting or waiving data transfer (also known as bandwidth) fees for shared customers.  [Learn how the Bandwidth Alliance reduces costs](https://blog.cloudflare.com/bandwidth-alliance/).\n\n![](https://blog.cloudflare.com/content/images/2018/09/Cloudflare-Registrar-2.png)\n\n#### Day 4: [CLOUDFLARE REGISTRAR (Early Access)](https://blog.cloudflare.com/cloudflare-registrar/)\n\nCloudflare Registrar lets you securely register and manage your domain name with transparent, no-markup pricing that eliminates surprise renewal fees and hidden add-on charges. Be one of the first to transfer your domain to Cloudflare. [Register for early access to the Cloudflare Registrar](https://www.cloudflare.com/products/registrar/).\n\n![](https://blog.cloudflare.com/content/images/2018/09/Cloudflare-Workers-KV.png)\n\n#### Day 5: [WORKERS KV (Beta)](https://blog.cloudflare.com/introducing-workers-kv/)\n\nCloudflare Workers KV provides access to a secure low latency key-value store at all 153 Cloudflare data centers. Developers can use Cloudflare Workers and Workers KV to augment existing applications or to build entirely new applications on top of Cloudflare's global cloud network. Workers KV scales seamlessly to support applications serving dozens or millions of users. [Explore how Workers KV allows for serverless key-value storage](https://www.cloudflare.com/products/workers-kv/).\n\n![](https://blog.cloudflare.com/content/images/2018/09/Cloudflare-Birthday-Cupcake-1.png)\n\n### You’ve been hearing a lot from us; now hear from those who inspire us!\n\nAnother way we’ll soon be serving the Internet community is by hosting the fourth annual [Cloudflare Internet Summit](https://www.cloudflare.com/internetsummit/) next week at our San Francisco office on Thursday, October 4th. We don’t spend any time talking about Cloudflare at the Internet Summit. Instead we facilitate discussions with the people who inspire and challenge us. The Internet Summit focuses on the future of the Internet and will feature a series of fireside chats, intimate panel discussions, and lively conversations from some of the brightest thought leaders, executives, entrepreneurs, researchers, and operators. Tickets are almost sold out, so [register for the Cloudflare Internet Summit now](https://www.cloudflare.com/internetsummit/) or plan to tune in to our live stream!\n\n![](https://blog.cloudflare.com/content/images/2018/09/Cloudflare-Internet-Summit.png)\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [Product News](https://blog.cloudflare.com/tag/product-news/) [Registrar](https://blog.cloudflare.com/tag/registrar/) [Cloudflare Workers](https://blog.cloudflare.com/tag/workers/) [Cloudflare Workers KV](https://blog.cloudflare.com/tag/cloudflare-workers-kv/)"
    },
    {
      "url": "https://blog.cloudflare.com/workerd-open-source-workers-runtime/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/workerd-open-source-workers-runtime/",
        "loadedTime": "2023-12-05T02:34:39.572Z",
        "referrerUrl": "https://blog.cloudflare.com/debugging-cloudflare-workers/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/workerd-open-source-workers-runtime/",
        "title": "Introducing workerd: the Open Source Workers runtime",
        "description": "workerd is the JavaScript/Wasm runtime code that powers Cloudflare Workers, now open source under the Apache 2.0 license.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "09/27/2022\n10 min read\nToday I'm proud to introduce the first beta release of workerd, the JavaScript/Wasm runtime based on the same code that powers Cloudflare Workers. workerd is Open Source under the Apache License version 2.0.\nworkerd shares most of its code with the runtime that powers Cloudflare Workers, but with some changes designed to make it more portable to other environments. The name \"workerd\" (pronounced \"worker dee\") comes from the Unix tradition of naming servers with a \"-d\" suffix standing for \"daemon\". The name is not capitalized because it is a program name, which are traditionally lower-case in Unix-like environments.\nWhat it's for\nSelf-hosting Workers\nworkerd can be used to self-host applications that you'd otherwise run on Cloudflare Workers. It is intended to be a production-ready web server for this purpose. workerd has been designed to be unopinionated about hosting environments, so that it should fit nicely into whatever server/VM/container hosting and orchestration system you prefer. It's just a web server.\nWorkers has always been based on standardized APIs, so that code is not locked into Cloudflare, and we work closely with other runtimes to promote compatibility. workerd provides another option to ensure that applications built on Workers can run anywhere, by leveraging the same underlying code to get exact, \"bug-for-bug\" compatibility.\nLocal development and testing\nworkerd is also designed to facilitate realistic local testing of Workers. Up until now, this has been achieved using Miniflare, which simulated the Workers API within a Node.js environment. Miniflare has worked well, but in a number of cases its behavior did not exactly match Workers running on Cloudflare. With the release of workerd, Miniflare and the Wrangler CLI tool will now be able to provide a more accurate simulation by leveraging the same runtime code we use in production.\nProgrammable proxies\nworkerd can act as an application host, a proxy, or both. It supports both forward and reverse proxy modes. In all cases, JavaScript code can be used to intercept and process requests and responses before forwarding them on. Traditional web servers and proxies have used bespoke configuration languages with quirks that are hard to master. Programming proxies in JavaScript instead provides more power while making the configuration easier to write and understand.\nWhat it is\nworkerd is not just another way to run JavaScript and Wasm. Our runtime is uniquely designed in a number of ways.\nServer-first\nMany non-browser JavaScript and Wasm runtimes are designed to be general-purpose: you can use them to build command-line apps, local GUI apps, servers, or anything in between. workerd is not. It specifically focuses on servers, in particular (for now, at least) HTTP servers.\nThis means in particular that workerd-based applications are event-driven at the top level. Applications do not open listen sockets and accept connections from them; instead, the runtime pushes events to the application. It may seem like a minor difference, but this basic change in perspective directly enables many of the features below.\nWeb standard APIs\nWherever possible, Workers (and workerd in particular) offers the same standard APIs found in web browsers, such as Fetch, URL, WebCrypto, and others. This means that code built on workerd is more likely to be portable to browsers as well as to other standards-based runtimes. When Workers launched five years ago, it was unusual for a non-browser to offer web APIs, but we are pleased to see that the broader JavaScript ecosystem is now converging on them.\nNanoservices\nworkerd is a nanoservice runtime. What does that mean?\nMicroservices have become popular over the last decade as a way to split monolithic servers into smaller components that could be maintained and deployed independently. For example, a company that offers several web applications with a common user authentication flow might have a separate team that maintains the authentication logic. In a monolithic model, the authentication logic might have been offered to the application teams as a library. However, this could be frustrating for the maintainers of that logic, as making any change might require waiting for every application team to deploy an update to their respective server. By splitting the authentication logic into a separate server that all the others talk to, the authentication team is able to deploy changes on their own schedule.\nHowever, microservices have a cost. What was previously a fast library call instead now requires communicating over a network. In addition to added overhead, this communication requires configuration and administration to ensure security and reliability. These costs become greater as the codebase is split into more and more services. Eventually, the costs outweigh the benefits.\nNanoservices are a new model that achieve the benefits of independent deployment with overhead closer to that of library calls. With workerd, many Workers can be configured to run in the same process. Each Worker runs in a separate \"isolate\", which gives the appearance of running independently of the others: each isolate loads separate code and has its own global scope. However, when one Worker explicitly sends a request to another Worker, the destination Worker actually runs in the same thread with zero latency. So, it performs more like a function call.\nWith nanoservices, teams can now break their code into many more independently-deployed pieces without worrying about the overhead.\n(Some in the industry prefer to call nanoservices \"functions\", implying that each individual function making up an application could be its own service. I feel, however, that this puts too much emphasis on syntax rather than logical functionality. That said, it is the same concept.)\nTo really make nanoservices work well, we had to minimize the baseline overhead of each service. This required designing workerd very differently from most other runtimes, so that common resources could be shared between services as much as possible. First, as mentioned, we run many nanoservices within a single process, to share basic process overhead and minimize context switching costs. A second big architectural difference between workerd and other runtimes is how it handles built-in APIs. Many runtimes implement significant portions of their built-in APIs in JavaScript, which must then be loaded separately into each isolate. workerd does not; all the APIs are implemented in native code, so that all isolates may share the same copy of that code. These design choices would be difficult to retrofit into another runtime, and indeed these needs are exactly why we chose to build a custom runtime for Workers from the start.\nHomogeneous deployment\nIn a typical microservices model, you might deploy different microservices to containers running across a cluster of machines, connected over a local network. You might manually choose how many containers to dedicate to each service, or you might configure some form of auto-scaling based on resource usage.\nworkerd offers an alternative model: Every machine runs every service.\nworkerd's nanoservices are much lighter-weight than typical containers. As a result, it's entirely reasonable to run a very large number of them – hundreds, maybe thousands – on a single server. This in turn means that you can simply deploy every service to every machine in your fleet.\nHomogeneous deployment means that you don't have to worry about scaling individual services. Instead, you can simply load balance requests across the entire cluster, and scale the cluster as needed. Overall, this can greatly reduce the amount of administration work needed.\nCloudflare itself has used the homogeneous model on our network since the beginning. Every one of Cloudflare's edge servers runs our entire software stack, so any server can answer any kind of request on its own. We've found it works incredibly well. This is why services on Cloudflare – including ones that use Workers – are able to go from no traffic at all to millions of requests per second instantly without trouble.\nCapability bindings: cleaner configuration and SSRF safety\nworkerd takes a different approach to most runtimes – indeed, to most software development platforms – in how an application accesses external resources.\nMost development platforms start from assuming that the application can talk to the whole world. It is up to the application to figure out exactly what it wants to talk to, and name it in some global namespace, such as using a URL. So, an application server that wants to talk to the authentication microservice might use code like this:\n// Traditional approach without capability bindings. fetch(\"https://auth-service.internal-network.example.com/api\", { method: \"POST\", body: JSON.stringify(authRequest), headers: { \"Authorization\": env.AUTH_SERVICE_TOKEN } }); \nIn workerd, we do things differently. An application starts out with no ability to talk to the rest of the world, and must be configured with specific capability bindings that provide it access to specific external resources. So, an application which needs to be able to talk to the authentication service would be configured with a binding called authService, and the code would look something like this:\n// Capability-based approach. Hostname doesn't matter; all // requests to AUTH_SERVICE.fetch() go to the auth service. env.AUTH_SERVICE.fetch(\"https://auth/api\", { method: \"POST\", body: JSON.stringify(authRequest), }); \nThis may at first appear to be a trivial difference. In both cases, we have to use configuration to control access to external services. In the traditional approach, we'd provide access tokens (and probably the service's hostname) as environment variables. In the new approach, the environment goes a bit further to provide a full-fledged object. Is this just syntax sugar?\nIt turns out, this slight change has huge advantages:\nFirst, we can now restrict the global fetch() function to accept only publicly-routable URLs. This makes applications totally immune to SSRF attacks! You cannot trick an application into accessing an internal service unintentionally if the code to access internal services is explicitly different. (In fact, the global fetch() is itself backed by a binding, which can be configured. workerd defaults to connecting it to the public internet, but you can also override it to permit private addresses if you want, or to route to a specific proxy service, or to be blocked entirely.)\nWith that done, we now have an interesting property: All internal services which an application uses must be configurable. This means:\nYou can easily see a complete list of the internal services an application talks to, without reading all the code.\nYou can always replace these services with mocks for testing purposes.\nYou can always configure an application to authenticate itself differently (e.g. client certificates) or use a different back end, without changing code.\nThe receiving end of a binding benefits, too. Take the authentication service example, above. The auth service may be another Worker running in workerd as a nanoservice. In this case, the auth service does not need to be bound to any actual network address. Instead, it may be made available strictly to other Workers through their bindings. In this case, the authentication service doesn't necessarily need to verify that a request received came from an allowed client – because only allowed clients are able to send requests to it in the first place.\nOverall, capability bindings allow simpler code that is secure by default, more composable, easier to test, and easier to understand and maintain.\nAlways backwards compatible\nCloudflare Workers has a hard rule against ever breaking a live Worker running in production. This same dedication to backwards compatibility extends to workerd.\nworkerd shares Workers' compatibility date system to manage breaking changes. Every Worker must be configured with a \"compatibility date\". The runtime then ensures that the API behaves exactly as it did on that date. At your leisure, you may check the documentation to see if new breaking changes are introduced at a future date, and update your code for them. Most such changes are minor and most code won't require any changes. However, you are never obliged to update. Old dates will continue to be supported by newer versions of workerd. It is always safe to update workerd itself without updating your code.\nWhat it's not\nTo avoid misleading or disappointing anyone, I need to take a moment to call out what workerd is not.\nworkerd is not a Secure Sandbox\nIt's important to note that workerd is not, on its own, a secure way to run possibly-malicious code. If you wish to run code you don't trust using workerd, you must enclose it in an additional sandboxing layer, such as a virtual machine configured for sandboxing.\nworkerd itself is designed such that a Worker should not be able to access any external resources to which it hasn't been granted a capability. However, a complete sandbox solution not only must be designed to restrict access, but also must account for the possibility of bugs – both in software and in hardware. workerd on its own is not sufficient to protect against hardware bugs like Spectre, nor can it adequately defend against the possibility of vulnerabilities in V8 or in workerd's own code.\nThe Cloudflare Workers service uses the same code found in workerd, but adds many additional layers of security on top to harden against such bugs. I described some of these in a past blog post. However, these measures are closely tied to our particular environment. For example, we rely on build automation to push V8 patches to production immediately upon becoming available; we separate customers according to risk profile; we rely on non-portable kernel features and assumptions about the host system to enforce security and resource limits. All of this is very specific to our environment, and cannot be packaged up in a reusable way.\nworkerd is not an independent project\nworkerd is the core of Cloudflare Workers, a fast-moving project developed by a dedicated team at Cloudflare. We are not throwing code over the wall to forget about, nor are we expecting volunteers to do our jobs for us. workerd's GitHub repository will be the canonical source used by Cloudflare Workers and our team will be doing much of their work directly in this repository. Just like V8 is developed primarily by the Chrome team for use in Chrome, workerd will be developed primarily by the Cloudflare Workers team for use in Cloudflare Workers.\nThis means we cannot promise that external contributions will sit on a level playing field with internal ones. Code reviews take time, and work that is needed for Cloudflare Workers will take priority. We also cannot promise we will accept every feature contribution. Even if the code is already written, reviews and maintenance have a cost. Within Cloudflare, we have a product management team who carefully evaluates what features we should and shouldn't offer, and plenty of ideas generated internally ultimately don't make the cut.\nIf you want to contribute a big new feature to workerd, your best bet is to talk to us before you write code, by raising an issue on GitHub early to get input. That way, you can find out if we're likely to accept a PR before you write it. We also might be able to give hints on how best to implement.\nIt's also important to note that while workerd's internal interfaces may sometimes appear clean and reusable, we cannot make any guarantee that those interfaces won't completely change on a whim. If you are trying to build on top of workerd internals, you will need to be prepared either to accept a fair amount of churn, or pin to a specific version.\nworkerd is not an off-the-shelf edge compute platform\nAs hinted above, the full Cloudflare Workers service involves a lot of technology beyond workerd itself, including additional security, deployment mechanisms, orchestration, and so much more. workerd itself is a portion of our runtime codebase, which is itself a small (albeit critical) piece of the overall Cloudflare Workers service.\nWe are pleased, though, that this means it is possible for us to release this code under a permissive Open Source license.\nTry the Beta\nAs of this blog post, workerd is in beta. If you want to try it out, \nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nBirthday Week Cloudflare Workers Developers Serverless Product News",
      "markdown": "09/27/2022\n\n*   [![Kenton Varda](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2020/08/avatar-github.jpg)](https://blog.cloudflare.com/author/kenton-varda/)\n\n10 min read\n\n![Introducing workerd: the Open Source Workers runtime](https://blog.cloudflare.com/content/images/2022/09/image1-49.png)\n\nToday I'm proud to introduce the first beta release of workerd, the JavaScript/Wasm runtime based on the same code that powers Cloudflare Workers. workerd is Open Source under the Apache License version 2.0.\n\nworkerd shares most of its code with the runtime that powers Cloudflare Workers, but with some changes designed to make it more portable to other environments. The name \"workerd\" (pronounced \"worker dee\") comes from the Unix tradition of naming servers with a \"-d\" suffix standing for \"daemon\". The name is not capitalized because it is a program name, which are traditionally lower-case in Unix-like environments.\n\n## What it's for\n\n### Self-hosting Workers\n\nworkerd can be used to self-host applications that you'd otherwise run on Cloudflare Workers. It is intended to be a production-ready web server for this purpose. workerd has been designed to be unopinionated about hosting environments, so that it should fit nicely into whatever server/VM/container hosting and orchestration system you prefer. It's just a web server.\n\nWorkers has always been based on standardized APIs, so that code is not locked into Cloudflare, and [we work closely with other runtimes to promote compatibility](https://blog.cloudflare.com/introducing-the-wintercg/). workerd provides another option to ensure that applications built on Workers can run anywhere, by leveraging the same underlying code to get exact, \"bug-for-bug\" compatibility.\n\n### Local development and testing\n\nworkerd is also designed to facilitate realistic local testing of Workers. Up until now, this has been achieved using [Miniflare](https://miniflare.dev/), which simulated the Workers API within a Node.js environment. Miniflare has worked well, but in a number of cases its behavior did not exactly match Workers running on Cloudflare. With the release of workerd, Miniflare and the Wrangler CLI tool will now be able to provide a more accurate simulation by leveraging the same runtime code we use in production.\n\n### Programmable proxies\n\nworkerd can act as an application host, a proxy, or both. It supports both forward and reverse proxy modes. In all cases, JavaScript code can be used to intercept and process requests and responses before forwarding them on. Traditional web servers and proxies have used bespoke configuration languages with quirks that are hard to master. Programming proxies in JavaScript instead provides more power while making the configuration easier to write and understand.\n\n## What it is\n\nworkerd is not just another way to run JavaScript and Wasm. Our runtime is uniquely designed in a number of ways.\n\n### Server-first\n\nMany non-browser JavaScript and Wasm runtimes are designed to be general-purpose: you can use them to build command-line apps, local GUI apps, servers, or anything in between. workerd is not. It specifically focuses on servers, in particular (for now, at least) HTTP servers.\n\nThis means in particular that workerd-based applications are event-driven at the top level. Applications do not open listen sockets and accept connections from them; instead, the runtime pushes events to the application. It may seem like a minor difference, but this basic change in perspective directly enables many of the features below.\n\n### Web standard APIs\n\nWherever possible, Workers (and workerd in particular) offers the same standard APIs found in web browsers, such as Fetch, URL, WebCrypto, and others. This means that code built on workerd is more likely to be portable to browsers as well as to other standards-based runtimes. When Workers launched five years ago, it was unusual for a non-browser to offer web APIs, but we are pleased to see that the broader JavaScript ecosystem is now converging on them.\n\n### Nanoservices\n\nworkerd is a nanoservice runtime. What does that mean?\n\nMicroservices have become popular over the last decade as a way to split monolithic servers into smaller components that could be maintained and deployed independently. For example, a company that offers several web applications with a common user authentication flow might have a separate team that maintains the authentication logic. In a monolithic model, the authentication logic might have been offered to the application teams as a library. However, this could be frustrating for the maintainers of that logic, as making any change might require waiting for every application team to deploy an update to their respective server. By splitting the authentication logic into a separate server that all the others talk to, the authentication team is able to deploy changes on their own schedule.\n\nHowever, microservices have a cost. What was previously a fast library call instead now requires communicating over a network. In addition to added overhead, this communication requires configuration and administration to ensure security and reliability. These costs become greater as the codebase is split into more and more services. Eventually, the costs outweigh the benefits.\n\nNanoservices are a new model that achieve the benefits of independent deployment with overhead closer to that of library calls. With workerd, many Workers can be configured to run in the same process. Each Worker runs in a separate \"isolate\", which gives the appearance of running independently of the others: each isolate loads separate code and has its own global scope. However, when one Worker explicitly sends a request to another Worker, the destination Worker actually runs in the same thread with zero latency. So, it performs more like a function call.\n\nWith nanoservices, teams can now break their code into many more independently-deployed pieces without worrying about the overhead.\n\n(Some in the industry prefer to call nanoservices \"functions\", implying that each individual function making up an application could be its own service. I feel, however, that this puts too much emphasis on syntax rather than logical functionality. That said, it is the same concept.)\n\nTo really make nanoservices work well, we had to minimize the baseline overhead of each service. This required designing workerd very differently from most other runtimes, so that common resources could be shared between services as much as possible. First, as mentioned, we run many nanoservices within a single process, to share basic process overhead and minimize context switching costs. A second big architectural difference between workerd and other runtimes is how it handles built-in APIs. Many runtimes implement significant portions of their built-in APIs in JavaScript, which must then be loaded separately into each isolate. workerd does not; all the APIs are implemented in native code, so that all isolates may share the same copy of that code. These design choices would be difficult to retrofit into another runtime, and indeed these needs are exactly why we chose to build a custom runtime for Workers from the start.\n\n### Homogeneous deployment\n\nIn a typical microservices model, you might deploy different microservices to containers running across a cluster of machines, connected over a local network. You might manually choose how many containers to dedicate to each service, or you might configure some form of auto-scaling based on resource usage.\n\nworkerd offers an alternative model: **Every machine runs every service.**\n\nworkerd's nanoservices are much lighter-weight than typical containers. As a result, it's entirely reasonable to run a very large number of them – hundreds, maybe thousands – on a single server. This in turn means that you can simply deploy every service to every machine in your fleet.\n\nHomogeneous deployment means that you don't have to worry about scaling individual services. Instead, you can simply load balance requests across the entire cluster, and scale the cluster as needed. Overall, this can greatly reduce the amount of administration work needed.\n\nCloudflare itself has used the homogeneous model on our network since the beginning. Every one of Cloudflare's edge servers runs our entire software stack, so any server can answer any kind of request on its own. We've found it works incredibly well. This is why services on Cloudflare – including ones that use Workers – are able to go from no traffic at all to millions of requests per second instantly without trouble.\n\n### Capability bindings: cleaner configuration and SSRF safety\n\nworkerd takes a different approach to most runtimes – indeed, to most software development platforms – in how an application accesses external resources.\n\nMost development platforms start from assuming that the application can talk to the whole world. It is up to the application to figure out exactly what it wants to talk to, and name it in some global namespace, such as using a URL. So, an application server that wants to talk to the authentication microservice might use code like this:\n\n```\n// Traditional approach without capability bindings.\nfetch(\"https://auth-service.internal-network.example.com/api\", {\n  method: \"POST\",\n  body: JSON.stringify(authRequest),\n  headers: { \"Authorization\": env.AUTH_SERVICE_TOKEN }\n});\n```\n\nIn workerd, we do things differently. An application starts out with no ability to talk to the rest of the world, and must be configured with specific _capability bindings_ that provide it access to _specific_ external resources. So, an application which needs to be able to talk to the authentication service would be configured with a binding called authService, and the code would look something like this:\n\n```\n// Capability-based approach. Hostname doesn't matter; all\n// requests to AUTH_SERVICE.fetch() go to the auth service.\nenv.AUTH_SERVICE.fetch(\"https://auth/api\", {\n method: \"POST\",\n body: JSON.stringify(authRequest),\n});\n```\n\nThis may at first appear to be a trivial difference. In both cases, we have to use configuration to [control access](https://www.cloudflare.com/learning/access-management/what-is-access-control/) to external services. In the traditional approach, we'd provide access tokens (and probably the service's hostname) as environment variables. In the new approach, the environment goes a bit further to provide a full-fledged object. Is this just syntax sugar?\n\nIt turns out, this slight change has huge advantages:\n\nFirst, we can now restrict the global fetch() function to accept only publicly-routable URLs. **This makes applications totally immune to SSRF attacks!** You cannot trick an application into accessing an internal service unintentionally if the code to access internal services is explicitly different. (In fact, the global fetch() is itself backed by a binding, which can be configured. workerd defaults to connecting it to the public internet, but you can also override it to permit private addresses if you want, or to route to a specific proxy service, or to be blocked entirely.)\n\nWith that done, we now have an interesting property: All internal services which an application uses _must_ be configurable. This means:\n\n*   You can easily see a complete list of the internal services an application talks to, without reading all the code.\n*   You can always replace these services with mocks for testing purposes.\n*   You can always configure an application to authenticate itself differently (e.g. client certificates) or use a different back end, without changing code.\n\nThe receiving end of a binding benefits, too. Take the authentication service example, above. The auth service may be another Worker running in workerd as a nanoservice. In this case, the auth service does not need to be bound to any actual network address. Instead, it may be made available strictly to other Workers through their bindings. In this case, the authentication service doesn't necessarily need to verify that a request received came from an allowed client – because only allowed clients are able to send requests to it in the first place.\n\nOverall, capability bindings allow simpler code that is secure by default, more composable, easier to test, and easier to understand and maintain.\n\n### Always backwards compatible\n\nCloudflare Workers has a hard rule against ever breaking a live Worker running in production. This same dedication to backwards compatibility extends to workerd.\n\nworkerd shares [Workers' compatibility date system](https://blog.cloudflare.com/backwards-compatibility-in-cloudflare-workers/) to manage breaking changes. Every Worker must be configured with a \"compatibility date\". The runtime then ensures that the API behaves exactly as it did on that date. At your leisure, you may [check the documentation](https://developers.cloudflare.com/workers/platform/compatibility-dates) to see if new breaking changes are introduced at a future date, and update your code for them. Most such changes are minor and most code won't require any changes. However, you are never obliged to update. Old dates will continue to be supported by newer versions of workerd. It is always safe to update workerd itself without updating your code.\n\n## What it's not\n\nTo avoid misleading or disappointing anyone, I need to take a moment to call out what workerd is not.\n\n### workerd is not a Secure Sandbox\n\nIt's important to note that workerd is not, on its own, a secure way to run possibly-malicious code. If you wish to run code you don't trust using workerd, you must enclose it in an additional sandboxing layer, such as a virtual machine configured for sandboxing.\n\nworkerd itself is designed such that a Worker should not be able to access any external resources to which it hasn't been granted a capability. However, a complete sandbox solution not only must be designed to restrict access, but also must account for the possibility of bugs – both in software and in hardware. workerd on its own is not sufficient to protect against hardware bugs like Spectre, nor can it adequately defend against the possibility of vulnerabilities in V8 or in workerd's own code.\n\nThe Cloudflare Workers service uses the same code found in workerd, but adds many additional layers of security on top to harden against such bugs. [I described some of these in a past blog post](https://blog.cloudflare.com/mitigating-spectre-and-other-security-threats-the-cloudflare-workers-security-model/). However, these measures are closely tied to our particular environment. For example, we rely on build automation to push V8 patches to production immediately upon becoming available; we separate customers according to risk profile; we rely on non-portable kernel features and assumptions about the host system to enforce security and resource limits. All of this is very specific to our environment, and cannot be packaged up in a reusable way.\n\n### workerd is not an independent project\n\nworkerd is the core of Cloudflare Workers, a fast-moving project developed by a dedicated team at Cloudflare. We are not throwing code over the wall to forget about, nor are we expecting volunteers to do our jobs for us. workerd's GitHub repository will be the canonical source used by Cloudflare Workers and our team will be doing much of their work directly in this repository. Just like V8 is developed primarily by the Chrome team for use in Chrome, workerd will be developed primarily by the Cloudflare Workers team for use in Cloudflare Workers.\n\nThis means we cannot promise that external contributions will sit on a level playing field with internal ones. Code reviews take time, and work that is needed for Cloudflare Workers will take priority. We also cannot promise we will accept every feature contribution. Even if the code is already written, reviews and maintenance have a cost. Within Cloudflare, we have a product management team who carefully evaluates what features we should and shouldn't offer, and plenty of ideas generated internally ultimately don't make the cut.\n\nIf you want to contribute a big new feature to workerd, your best bet is to talk to us before you write code, by raising an issue on GitHub early to get input. That way, you can find out if we're likely to accept a PR before you write it. We also might be able to give hints on how best to implement.\n\nIt's also important to note that while workerd's internal interfaces may sometimes appear clean and reusable, we cannot make any guarantee that those interfaces won't completely change on a whim. If you are trying to build on top of workerd internals, you will need to be prepared either to accept a fair amount of churn, or pin to a specific version.\n\n### workerd is not an off-the-shelf edge compute platform\n\nAs hinted above, the full Cloudflare Workers service involves a lot of technology beyond workerd itself, including additional security, deployment mechanisms, orchestration, and so much more. workerd itself is a portion of our runtime codebase, which is itself a small (albeit critical) piece of the overall Cloudflare Workers service.\n\nWe are pleased, though, that this means it is possible for us to release this code under a permissive Open Source license.\n\n## Try the Beta\n\nAs of this blog post, workerd is in beta. If you want to try it out,\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [Cloudflare Workers](https://blog.cloudflare.com/tag/workers/) [Developers](https://blog.cloudflare.com/tag/developers/) [Serverless](https://blog.cloudflare.com/tag/serverless/) [Product News](https://blog.cloudflare.com/tag/product-news/)"
    },
    {
      "url": "https://blog.cloudflare.com/icymi-developer-week-2022-announcements/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/icymi-developer-week-2022-announcements/",
        "loadedTime": "2023-12-05T02:34:54.763Z",
        "referrerUrl": "https://blog.cloudflare.com/debugging-cloudflare-workers/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/icymi-developer-week-2022-announcements/",
        "title": "ICYMI: Developer Week 2022 announcements",
        "description": "This week we made over 30 announcements, in case you missed any here’s a quick round-up.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/18/2022\n4 min read\nThis post is also available in 简体中文, 日本語, Deutsch, Français and Español.\nDeveloper Week 2022 has come to a close. Over the last week we’ve shared with you 31 posts on what you can build on Cloudflare and our vision and roadmap on where we’re headed. We shared product announcements, customer and partner stories, and provided technical deep dives. In case you missed any of the posts here’s a handy recap.\nProduct and feature announcements\nAnnouncement Summary \nWelcome to the Supercloud (and Developer Week 2022)\tOur vision of the cloud -- a model of cloud computing that promises to make developers highly productive at scaling from one to Internet-scale in the most flexible, efficient, and economical way.\t\nBuild applications of any size on Cloudflare with the Queues open beta\tBuild performant and resilient distributed applications with Queues. Available to all developers with a paid Workers plan. \t\nMigrate from S3 easily with the R2 Super Slurper\tA tool to easily and efficiently move objects from your existing storage provider to R2. \t\nGet started with Cloudflare Workers with ready-made templates\tSee what’s possible with Workers and get building faster with these starter templates. \t\nReduce origin load, save on cloud egress fees, and maximize cache hits with Cache Reserve\tCache Reserve is graduating to open beta – users can now test and integrate it into their content delivery strategy without any additional waiting. \t\nStore and process your Cloudflare Logs... with Cloudflare\tQuery Cloudflare logs stored on R2. \t\nUPDATE Supercloud SET status = 'open alpha' WHERE product = 'D1'\tD1, our first global relational database, is in open alpha. Start building and share your feedback with us. \t\nAutomate an isolated browser instance with just a few lines of code\tThe Browser Rendering API is an out of the box solution to run browser automation tasks with Puppeteer in Workers. \t\nBringing authentication and identification to Workers through Mutual TLS\tSend outbound requests with Workers through a mutually authenticated channel. \t\nSpice up your sites on Cloudflare Pages with Pages Functions General Availability\tEasily add dynamic content to your Pages projects with Functions. \t\nAnnouncing the first Workers Launchpad cohort and growth of the program to $2 billion\tWe were blown away by the interest in the Workers Launchpad Funding Program and are proud to introduce the first cohort. \t\nThe most programmable Supercloud with Cloudflare Snippets\tModify traffic routed through the Cloudflare CDN without having to write a Worker. \t\nKeep track of Workers’ code and configuration changes with Deployments\tTrack your changes to a Worker configuration, binding, and code. \t\nSend Cloudflare Workers logs to a destination of your choice with Workers Trace Events Logpush\tGain visibility into your Workers when logs are sent to your analytics platform or object storage. Available to all users on a Workers paid plan. \t\nImproved Workers TypeScript support\tBased on feedback from users we’ve improved our types and are open-sourcing the automatic generation scripts. \t\nTechnical deep dives\nAnnouncement Summary \nThe road to a more standards-compliant Workers API\tAn update on the work the WinterCG is doing on the creation of common API standards in JavaScript runtimes and how Workers is implementing them. \t\nIndexing millions of HTTP requests using Durable Objects \tIndexing and querying millions of logs stored in R2 using Workers, Durable Objects, and the Streams API. \t\nIteration isn't just for code: here are our latest API docs\tWe’ve revamped our API reference documentation to standardize our API content and improve the overall developer experience when using the Cloudflare APIs. \t\nMaking static sites dynamic with D1\tA template to build a D1-based comments API. \t\nThe Cloudflare API now uses OpenAPI schemas\tOpenAPI schemas are now available for the Cloudflare API. \t\nServer-side render full stack applications with Pages Functions\tRun server-side rendering in a Function using a variety of frameworks including Qwik, Astro, and SolidStart.\t\nIncremental adoption of micro-frontends with Cloudflare Workers\tHow to replace selected elements of a legacy client-side rendered application with server-side rendered fragments using Workers. \t\nHow we built it: the technology behind Cloudflare Radar 2.0\tDetails on how we rebuilt Radar using Pages, Remix, Workers, and R2. \t\nHow Cloudflare uses Terraform to manage Cloudflare\tHow we made it easier for our developers to make changes with the Cloudflare Terraform provider. \t\nNetwork performance Update: Developer Week 2022\tSee how fast Cloudflare Workers are compared to other solutions.\t\nHow Cloudflare instruments services using Workers Analytics Engine\tInstrumentation with Analytics Engine provides data to find bugs and helps us prioritize new features. \t\nDoubling down on local development with Workers:Miniflare meets workerd\tImproving local development using Miniflare3, now powered by workerd.\t\nCustomer and partner stories\nAnnouncement Summary \nCloudflare Workers scale too well and broke our infrastructure, so we are rebuilding it on Workers\tHow DevCycle re-architected their feature management tool using Workers. \t\nEasy Postgres integration with Workers and Neon.tech\tNeon.tech solves the challenges of connecting to Postgres from Workers\t\nXata Workers: client-side database access without client-side secrets\tXata uses Workers for Platform to reduce security risks of running untrusted code.\t\nTwilio Segment Edge SDK powered by Cloudflare Workers\tThe Segment Edge SDK, built on Workers, helps applications collect and track events from the client, and get access to realtime user state to personalize experiences.\t\nNext\nAnd that’s it for Developer Week 2022. But you can keep the conversation going by joining our Discord Community.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nDeveloper Week Developers Serverless Product News Supercloud \nRelated Posts\nJanuary 07, 2022 3:57PM\nCloudflare Innovation Weeks 2021\nAs we start planning our 2022 Innovation Weeks, we are reflecting back on the highlights from each of these weeks...\nBy \nApril 20, 2021 2:00PM\nStart building your own private network on Cloudflare today\nStarting today, your team can build a private network on Cloudflare’s network....\nBy \nApril 15, 2021 2:00PM\nA Zero Trust terminal in your web browser\nStarting today, your team can use that same platform to seamlessly connect to non-HTTP resources from inside of a browser with the same level of Zero Trust control available in web applications....\nBy \nMay 16, 2023 2:05PM\nAnnouncing database integrations: a few clicks to connect to Neon, PlanetScale and Supabase on Workers\nToday we’re announcing Database Integrations – making it seamless to connect to your database of choice on Workers. To start, we’ve added some of the most popular databases that support HTTP connections: Neon, PlanetScale and Supabase with more to come!...\nBy",
      "markdown": "11/18/2022\n\n*   [![Dawn Parzych](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/04/Untitled---1-of-1.jpeg)](https://blog.cloudflare.com/author/dawn/)\n\n4 min read\n\n_This post is also available in [简体中文](https://blog.cloudflare.com/zh-cn/icymi-developer-week-2022-announcements-zh-cn/), [日本語](https://blog.cloudflare.com/ja-jp/icymi-developer-week-2022-announcements-ja-jp/), [Deutsch](https://blog.cloudflare.com/de-de/icymi-developer-week-2022-announcements-de-de/), [Français](https://blog.cloudflare.com/fr-fr/icymi-developer-week-2022-announcements-fr-fr/) and [Español](https://blog.cloudflare.com/es-es/icymi-developer-week-2022-announcements-es-es/)._\n\n![](https://blog.cloudflare.com/content/images/2022/11/2022-Developer-Week-Hero-Dark_b-1.png)\n\nDeveloper Week 2022 has come to a close. Over the last week we’ve shared with you 31 posts on what you can build on Cloudflare and our vision and roadmap on where we’re headed. We shared product announcements, customer and partner stories, and provided technical deep dives. In case you missed any of the posts here’s a handy recap.\n\n## Product and feature announcements\n\n| Announcement | Summary |\n| --- | --- |\n| [Welcome to the Supercloud (and Developer Week 2022)](https://blog.cloudflare.com/welcome-to-the-supercloud-and-developer-week-2022/) | Our vision of the cloud -- a model of cloud computing that promises to make developers highly productive at scaling from one to Internet-scale in the most flexible, efficient, and economical way. |\n| [Build applications of any size on Cloudflare with the Queues open beta](https://blog.cloudflare.com/cloudflare-queues-open-beta) | Build performant and resilient distributed applications with Queues. Available to all developers with a paid Workers plan. |\n| [Migrate from S3 easily with the R2 Super Slurper](https://blog.cloudflare.com/cloudflare-r2-super-slurper/) | A tool to easily and efficiently move objects from your existing storage provider to R2. |\n| [Get started with Cloudflare Workers with ready-made templates](https://blog.cloudflare.com/cloudflare-workers-templates/) | See what’s possible with Workers and get building faster with these starter templates. |\n| [Reduce origin load, save on cloud egress fees, and maximize cache hits with Cache Reserve](https://blog.cloudflare.com/cache-reserve-open-beta/) | Cache Reserve is graduating to open beta – users can now test and integrate it into their content delivery strategy without any additional waiting. |\n| [Store and process your Cloudflare Logs... with Cloudflare](https://blog.cloudflare.com/announcing-logs-engine/) | Query Cloudflare logs stored on R2. |\n| [UPDATE Supercloud SET status = 'open alpha' WHERE product = 'D1'](https://blog.cloudflare.com/d1-open-alpha/) | D1, our first global relational database, is in open alpha. Start building and share your feedback with us. |\n| [Automate an isolated browser instance with just a few lines of code](https://blog.cloudflare.com/introducing-workers-browser-rendering-api/) | The Browser Rendering API is an out of the box solution to run browser automation tasks with Puppeteer in Workers. |\n| [Bringing authentication and identification to Workers through Mutual TLS](https://blog.cloudflare.com/mutual-tls-for-workers/) | Send outbound requests with Workers through a mutually authenticated channel. |\n| [Spice up your sites on Cloudflare Pages with Pages Functions General Availability](https://blog.cloudflare.com/pages-function-goes-GA/) | Easily add dynamic content to your Pages projects with Functions. |\n| [Announcing the first Workers Launchpad cohort and growth of the program to $2 billion](https://blog.cloudflare.com/launchpad-fall-22/) | We were blown away by the interest in the Workers Launchpad Funding Program and are proud to introduce the first cohort. |\n| [The most programmable Supercloud with Cloudflare Snippets](https://blog.cloudflare.com/snippets-announcement) | Modify traffic routed through the Cloudflare CDN without having to write a Worker. |\n| [Keep track of Workers’ code and configuration changes with Deployments](https://blog.cloudflare.com/deployments-for-workers) | Track your changes to a Worker configuration, binding, and code. |\n| [Send Cloudflare Workers logs to a destination of your choice with Workers Trace Events Logpush](https://blog.cloudflare.com/workers-logpush-GA) | Gain visibility into your Workers when logs are sent to your analytics platform or [object storage](https://www.cloudflare.com/learning/cloud/what-is-object-storage/). Available to all users on a Workers paid plan. |\n| [Improved Workers TypeScript support](https://blog.cloudflare.com/improving-workers-types) | Based on feedback from users we’ve improved our types and are open-sourcing the automatic generation scripts. |\n\n### Technical deep dives\n\n| Announcement | Summary |\n| --- | --- |\n| [The road to a more standards-compliant Workers API](https://blog.cloudflare.com/standards-compliant-workers-api/) | An update on the work the [WinterCG](https://github.com/wintercg) is doing on the creation of common API standards in JavaScript runtimes and how Workers is implementing them. |\n| [Indexing millions of HTTP requests using Durable Objects](https://blog.cloudflare.com/r2-rayid-retrieval) | Indexing and querying millions of logs stored in R2 using Workers, Durable Objects, and the Streams API. |\n| [Iteration isn't just for code: here are our latest API docs](https://blog.cloudflare.com/building-a-better-developer-experience-through-api-documentation) | We’ve revamped our API reference documentation to standardize our API content and improve the overall developer experience when using the Cloudflare APIs. |\n| [Making static sites dynamic with D1](https://blog.cloudflare.com/making-static-sites-dynamic-with-cloudflare-d1) | A template to build a D1-based comments API. |\n| [The Cloudflare API now uses OpenAPI schemas](https://blog.cloudflare.com/open-api-transition) | OpenAPI schemas are now available for the Cloudflare API. |\n| [Server-side render full stack applications with Pages Functions](https://blog.cloudflare.com/pages-full-stack-frameworks) | Run server-side rendering in a Function using a variety of frameworks including Qwik, Astro, and SolidStart. |\n| [Incremental adoption of micro-frontends with Cloudflare Workers](https://blog.cloudflare.com/fragment-piercing) | How to replace selected elements of a legacy client-side rendered application with server-side rendered fragments using Workers. |\n| [How we built it: the technology behind Cloudflare Radar 2.0](https://blog.cloudflare.com/technology-behind-radar2/) | Details on how we rebuilt Radar using Pages, Remix, Workers, and R2. |\n| [How Cloudflare uses Terraform to manage Cloudflare](https://blog.cloudflare.com/terraforming-cloudflare-at-cloudflare) | How we made it easier for our developers to make changes with the Cloudflare Terraform provider. |\n| [Network performance Update: Developer Week 2022](https://blog.cloudflare.com/network-performance-update-developer-week/) | See how fast Cloudflare Workers are compared to other solutions. |\n| [How Cloudflare instruments services using Workers Analytics Engine](https://blog.cloudflare.com/using-analytics-engine-to-improve-analytics-engine) | Instrumentation with Analytics Engine provides data to find bugs and helps us prioritize new features. |\n| [Doubling down on local development with Workers:Miniflare meets workerd](https://blog.cloudflare.com/miniflare-and-workerd) | Improving local development using Miniflare3, now powered by workerd. |\n\n### Customer and partner stories\n\n| Announcement | Summary |\n| --- | --- |\n| [Cloudflare Workers scale too well and broke our infrastructure, so we are rebuilding it on Workers](https://blog.cloudflare.com/devcycle-customer-story) | How DevCycle re-architected their feature management tool using Workers. |\n| [Easy Postgres integration with Workers and Neon.tech](https://blog.cloudflare.com/neon-postgres-database-from-workers) | Neon.tech solves the challenges of connecting to Postgres from Workers |\n| [Xata Workers: client-side database access without client-side secrets](https://blog.cloudflare.com/xata-customer-story) | Xata uses Workers for Platform to reduce security risks of running untrusted code. |\n| [Twilio Segment Edge SDK powered by Cloudflare Workers](https://blog.cloudflare.com/twilio-segment-sdk-powered-by-cloudflare-workers) | The Segment Edge SDK, built on Workers, helps applications collect and track events from the client, and get access to realtime user state to personalize experiences. |\n\n### Next\n\nAnd that’s it for Developer Week 2022. But you can keep the conversation going by joining our [Discord Community](https://discord.gg/cloudflaredev).\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Developer Week](https://blog.cloudflare.com/tag/developer-week/) [Developers](https://blog.cloudflare.com/tag/developers/) [Serverless](https://blog.cloudflare.com/tag/serverless/) [Product News](https://blog.cloudflare.com/tag/product-news/) [Supercloud](https://blog.cloudflare.com/tag/supercloud/)\n\nRelated Posts\n\nJanuary 07, 2022 3:57PM\n\n[\n\n## Cloudflare Innovation Weeks 2021\n\n](https://blog.cloudflare.com/2021-innovations-weeks/)\n\nAs we start planning our 2022 Innovation Weeks, we are reflecting back on the highlights from each of these weeks...\n\nBy \n\nApril 20, 2021 2:00PM\n\n[\n\n## Start building your own private network on Cloudflare today\n\n](https://blog.cloudflare.com/build-your-own-private-network-on-cloudflare/)\n\nStarting today, your team can build a private network on Cloudflare’s network....\n\nBy \n\nApril 15, 2021 2:00PM\n\n[\n\n## A Zero Trust terminal in your web browser\n\n](https://blog.cloudflare.com/browser-ssh-terminal-with-auditing/)\n\nStarting today, your team can use that same platform to seamlessly connect to non-HTTP resources from inside of a browser with the same level of Zero Trust control available in web applications....\n\nBy \n\nMay 16, 2023 2:05PM\n\n[\n\n## Announcing database integrations: a few clicks to connect to Neon, PlanetScale and Supabase on Workers\n\n](https://blog.cloudflare.com/announcing-database-integrations/)\n\nToday we’re announcing Database Integrations – making it seamless to connect to your database of choice on Workers. To start, we’ve added some of the most popular databases that support HTTP connections: Neon, PlanetScale and Supabase with more to come!...\n\nBy"
    },
    {
      "url": "https://blog.cloudflare.com/birthday-week-2019-wrap-up/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/birthday-week-2019-wrap-up/",
        "loadedTime": "2023-12-05T02:34:55.060Z",
        "referrerUrl": "https://blog.cloudflare.com/debugging-cloudflare-workers/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/birthday-week-2019-wrap-up/",
        "title": "Birthday Week 2019 Wrap-up",
        "description": "This week we celebrated Cloudflare’s 9th birthday by launching a variety of new offerings that support our mission: to help build a better Internet.  Below is a summary recap of how we celebrated Birthday Week 2019.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Birthday Week 2019 Wrap-up\n09/27/2019\n2 min read\nThis week we celebrated Cloudflare’s 9th birthday by launching a variety of new offerings that support our mission: to help build a better Internet. Below is a summary recap of how we celebrated Birthday Week 2019.\nCleaning up bad bots\nEvery day Cloudflare protects over 20 million Internet properties from malicious bots, and this week you were invited to join in the fight! Now you can enable “bot fight mode” in the Firewall settings of the Cloudflare Dashboard and we’ll start deploying CPU intensive code to traffic originating from malicious bots. This wastes the bots’ CPU resources and makes it more difficult and costly for perpetrators to deploy malicious bots at scale. We’ll also share the IP addresses of malicious bot traffic with our Bandwidth Alliance partners, who can help kick malicious bots offline. Join us in the battle against bad bots – and, as you can read here – you can help the climate too!\nBrowser Insights\nSpeed matters, and if you manage a website or app, you want to make sure that you’re delivering a high performing website to all of your global end users. Now you can enable Browser Insights in the Speed section of the Cloudflare Dashboard to analyze website performance from the perspective of your users’ web browsers. \nWARP, the wait is over\nSeveral months ago we announced WARP, a free mobile app purpose-built to address the security and performance challenges of the mobile Internet, while also respecting user privacy. After months of testing and development, this week we (finally) rolled out WARP to approximately 2 million wait-list customers. We also enabled WARP+, a WARP experience that uses Argo routing technology to route your mobile traffic across faster, less-congested, routes through the Internet. WARP and WARP+ are now available in the iOS and Android App stores and we can’t wait for you to give it a try!\nHTTP/3 Support\nLast year we announced early support for QUIC, a UDP based protocol that aims to make everything on the Internet work faster, with built-in encryption. The IETF subsequently decided that QUIC should be the foundation of the next generation of the HTTP protocol, HTTP/3. This week, Cloudflare was the first to introduce support for HTTP/3 in partnership with Google Chrome and Mozilla.\nWorkers Sites\nFinally, to wrap up our birthday week announcements, we announced Workers Sites. The Workers serverless platform continues to grow and evolve, and every day we discover new and innovative ways to help developers build and optimize their applications. Workers Sites enables developers to easily deploy lightweight static sites across Cloudflare’s global cloud platform without having to build out the traditional backend server infrastructure to support these sites.\nWe look forward to Birthday Week every year, as a chance to showcase some of our exciting new offerings — but we all know building a better Internet is about more than one week. It’s an effort that takes place all year long, and requires the help of our partners, employees and especially you — our customers. Thank you for being a customer, providing valuable feedback and helping us stay focused on our mission to help build a better Internet.\nCan’t get enough of this week’s announcements, or want to learn more? Register for next week’s Birthday Week Recap webinar to get the inside scoop on every announcement.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nBirthday Week Product News Security Analytics 1.1.1.1 \nRelated Posts\nOctober 02, 2023 2:00PM\nBirthday Week recap: everything we announced — plus an AI-powered opportunity for startups\nNeed a recap or refresher on all the big Birthday Week news this week? This recap has you covered...\nBy \nSeptember 28, 2018 8:40PM\nBirthday Week Wrap-Up: Every day is launch day at Cloudflare\nOur customers are accustomed to us launching new services, features, and functionality at a feverish pace, but recently, we’ve been especially active. This week we celebrated our 8th Birthday Week by announcing new offerings that benefit our customers and the global Internet community....\nBy \nJanuary 07, 2022 3:57PM\nCloudflare Innovation Weeks 2021\nAs we start planning our 2022 Innovation Weeks, we are reflecting back on the highlights from each of these weeks...\nBy \nNovember 18, 2022 9:13PM\nICYMI: Developer Week 2022 announcements\nThis week we made over 30 announcements, in case you missed any here’s a quick round-up....\nBy",
      "markdown": "## Birthday Week 2019 Wrap-up\n\n09/27/2019\n\n*   [![Jake Anderson](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2018/09/IMG_1383.jpg)](https://blog.cloudflare.com/author/jake-anderson/)\n\n2 min read\n\n![](https://blog.cloudflare.com/content/images/2019/09/Birthday-week-2019-header-copy@3x-2.png)\n\nThis week we celebrated Cloudflare’s 9th birthday by launching a variety of new offerings that support our mission: to help build a better Internet.  Below is a summary recap of how we celebrated Birthday Week 2019.\n\n## [Cleaning up bad bots](https://blog.cloudflare.com/cleaning-up-bad-bots/)\n\nEvery day Cloudflare protects over 20 million Internet properties from malicious bots, and this week you were invited to join in the fight!  Now you can enable “bot fight mode” in the Firewall settings of the Cloudflare Dashboard and we’ll start deploying CPU intensive code to traffic originating from malicious bots.  This wastes the bots’ CPU resources and makes it more difficult and costly for perpetrators to deploy malicious bots at scale. We’ll also share the IP addresses of malicious bot traffic with our [Bandwidth Alliance partners](https://www.cloudflare.com/bandwidth-alliance/), who can help kick malicious bots offline. Join us in the battle against bad bots – and, as you can read [here](https://blog.cloudflare.com/cleaning-up-bad-bots/) – you can help the climate too!\n\n## [Browser Insights](https://blog.cloudflare.com/introducing-browser-insights/)\n\nSpeed matters, and if you manage a website or app, you want to make sure that you’re delivering a high performing website to all of your global end users. Now you can enable Browser Insights in the Speed section of the Cloudflare Dashboard to analyze website performance from the perspective of your users’ web browsers.  \n\n## [WARP, the wait is over](https://blog.cloudflare.com/announcing-warp-plus/)\n\nSeveral months ago [we announced WARP](https://blog.cloudflare.com/1111-warp-better-vpn/), a free mobile app purpose-built to address the security and performance challenges of the mobile Internet, while also respecting user privacy.  After months of testing and development, this week we (finally) rolled out WARP to approximately 2 million wait-list customers.  We also [enabled WARP+](https://blog.cloudflare.com/announcing-warp-plus/), a WARP experience that uses Argo routing technology to route your mobile traffic across faster, less-congested, routes through the Internet.  WARP and WARP+ are now available in the iOS and Android App stores and we can’t wait for you to give it a try!\n\n## [HTTP/3 Support](https://blog.cloudflare.com/http3-the-past-present-and-future/)\n\nLast year we announced early support for QUIC, a UDP based protocol that aims to make everything on the Internet work faster, with built-in encryption. The IETF subsequently decided that QUIC should be the foundation of the next generation of the HTTP protocol, HTTP/3. This week, Cloudflare was the first to introduce support for HTTP/3 in partnership with Google Chrome and Mozilla.\n\n## [Workers Sites](https://blog.cloudflare.com/workers-sites)\n\nFinally, to wrap up our birthday week announcements, we announced Workers Sites. The Workers serverless platform continues to grow and evolve, and every day we discover new and innovative ways to help developers build and optimize their applications. Workers Sites enables developers to easily deploy lightweight static sites across Cloudflare’s global cloud platform without having to build out the traditional backend server infrastructure to support these sites.\n\nWe look forward to Birthday Week every year, as a chance to showcase some of our exciting new offerings — but we all know building a better Internet is about more than one week.  It’s an effort that takes place all year long, and requires the help of our partners, employees and especially you — our customers. Thank you for being a customer, providing valuable feedback and helping us stay focused on our mission to help build a better Internet.\n\nCan’t get enough of this week’s announcements, or want to learn more? [Register](https://www.cloudflare.com/webinars/2019-birthday-week-recap/) for next week’s Birthday Week Recap webinar to get the inside scoop on every announcement.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [Product News](https://blog.cloudflare.com/tag/product-news/) [Security](https://blog.cloudflare.com/tag/security/) [Analytics](https://blog.cloudflare.com/tag/analytics/) [1.1.1.1](https://blog.cloudflare.com/tag/1-1-1-1/)\n\nRelated Posts\n\nOctober 02, 2023 2:00PM\n\n[\n\n## Birthday Week recap: everything we announced — plus an AI-powered opportunity for startups\n\n](https://blog.cloudflare.com/birthday-week-2023-wrap-up/)\n\nNeed a recap or refresher on all the big Birthday Week news this week? This recap has you covered...\n\nBy \n\nSeptember 28, 2018 8:40PM\n\n[\n\n## Birthday Week Wrap-Up: Every day is launch day at Cloudflare\n\n](https://blog.cloudflare.com/birthday-week-2018-wrap-up/)\n\nOur customers are accustomed to us launching new services, features, and functionality at a feverish pace, but recently, we’ve been especially active. This week we celebrated our 8th Birthday Week by announcing new offerings that benefit our customers and the global Internet community....\n\nBy \n\nJanuary 07, 2022 3:57PM\n\n[\n\n## Cloudflare Innovation Weeks 2021\n\n](https://blog.cloudflare.com/2021-innovations-weeks/)\n\nAs we start planning our 2022 Innovation Weeks, we are reflecting back on the highlights from each of these weeks...\n\nBy \n\nNovember 18, 2022 9:13PM\n\n[\n\n## ICYMI: Developer Week 2022 announcements\n\n](https://blog.cloudflare.com/icymi-developer-week-2022-announcements/)\n\nThis week we made over 30 announcements, in case you missed any here’s a quick round-up....\n\nBy"
    },
    {
      "url": "https://blog.cloudflare.com/real-urls-for-amp-cached-content-using-cloudflare-workers/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/real-urls-for-amp-cached-content-using-cloudflare-workers/",
        "loadedTime": "2023-12-05T02:34:54.960Z",
        "referrerUrl": "https://blog.cloudflare.com/debugging-cloudflare-workers/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/real-urls-for-amp-cached-content-using-cloudflare-workers/",
        "title": "Real URLs for AMP Cached Content Using Cloudflare Workers",
        "description": "We’re excited to announce our solution for arguably the biggest issue affecting Accelerated Mobile Pages (AMP): the inability to use real origin URLs when serving AMP-cached content.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/13/2018\n9 min read\nToday, we’re excited to announce our solution for arguably the biggest issue affecting Accelerated Mobile Pages (AMP): the inability to use real origin URLs when serving AMP-cached content. To allow AMP caches to serve content under its origin URL, we implemented HTTP signed exchanges, which extend authenticity and integrity to content cached and served on behalf of a publisher. This logic lives on Cloudflare Workers, meaning that adding HTTP signed exchanges to your content is just a simple Workers application away. Publishers on Cloudflare can now take advantage of AMP performance and have AMP caches serve content with their origin URLs. We're thrilled to use Workers as a core component of this solution.\nHTTP signed exchanges are a crucial component of the emerging Web Packaging standard, a set of protocols used to package websites for distribution through optimized delivery systems like Google AMP. This announcement comes just in time for Chrome Dev Summit 2018, where our colleague Rustam Lalkaka spoke about our efforts to advance the Web Packaging standard.\nWhat is Web Packaging and Why Does it Matter?\nYou may already see the need for Web Packaging on a daily basis. On your smartphone, perhaps you’ve searched for Christmas greens, visited 1-800-Flowers directly from Google, and have been surprised to see content served under the URL https://google.com/amp/1800flowers.com/blog/flower-facts/types-of-christmas-greens/amp. This is an instance of AMP in action, where Google serves cached content so your desired web page loads faster. \nVisiting 1-800 Flowers through AMP without HTTP signed exchange\nGoogle cannot serve cached content under publisher URLs for clear security reasons. To securely present content from a URL, a TLS certificate for its domain is required. Google cannot provide 1-800-Flowers’ certificate on the vendor’s behalf, because it does not have the corresponding private key. Additionally, Google cannot, and should not be able to, sign content using the private key that corresponds to 1-800-Flowers’ certificate.\nThe inability to use original content URLs with AMP posed some serious issues. First, the google.com/amp URL prefix can strip URLs of their meaning. To the frustration of publishers, their content is no longer directly attributed to them by a URL (let alone a certificate). The publisher can no longer prove the integrity and authenticity of content served on their behalf.\nSecond, for web browsers the lack of a publisher’s URL can call the integrity and authenticity of a cached webpage into question. Namely, there’s no clear way to prove that this response is a cached version of an actual page published by 1-800-Flowers. Additionally, cookies are managed by third-party providers like Google instead of the publisher.\nEnter Web Packaging, a collection of specifications for “packaging” website content with information like certificates and their validity. The HTTP signed exchanges specification allows third-party caches to cache and service HTTPS requests with proof of integrity and authenticity.\nHTTP Signed Exchanges: Extending Trust with Cryptography\nIn the pre-AMP days, people expected to find a webpage’s content at one definitive URL. The publisher, who owns the domain of the definitive URL, would present a visitor with a certificate that corresponds to this domain and contains a public key.\nThe publisher would use the corresponding private key to sign a cryptographic handshake, which is used to derive shared symmetric keys that are used to encrypt the content and protect its integrity. \nThe visitor would then receive content encrypted and signed by the shared key. \nThe visitor’s browser then uses the shared key to verify the response’s signature and, in turn, the authenticity and integrity of the content received.\nWith services like AMP, however, online content may correspond to more than one URL. This introduces a problem: while only one domain actually corresponds to the webpage’s publisher, multiple domains can be responsible for serving a webpage. If a publisher allows AMP services to cache and serve their webpages, they must be able to sign their content even when AMP caches serve it for them. Only then can AMP-cached content prove its legitimacy.\nHTTP signed exchanges directly address the problem of extending publisher signatures to services like AMP. This IETF draft specifies how publishers may sign an HTTP request/response pair (an exchange). With a signed exchange, the publisher can assure the integrity and authenticity of a response to a specific request even before the client makes the request. Given a signed exchange, the publisher authorizes intermediates (like Google’s AMP Cache) to forward the exchanges; the intermediate responds to a given request with the corresponding response in the signed HTTP request/response pair. A browser can then verify the exchange signature to assert the intermediate response’s integrity and authenticity. \nThis is like handing out an answer key to a quiz signed by the instructor. Having a signed answer sheet is just as good as getting the answer from the teacher in real time.\nThe Technical Details\nAn HTTP signed exchange is generated by the following steps.\nFirst, the publisher uses MICE (Merkle Integrity Content Encoding) to provide a concise proof of integrity for the response included in the exchange. To start, the response is split into blocks of some record size bits long. Take, for example, a message ABCD, which is divided into record-size blocks A, B, C, and D. The first step to constructing a proof of integrity is to take the last block, D, and compute the following:\nproof(D) = SHA-256(D || 0x0) \nThis produces proof(D). Then, all consequent proof values for blocks are computed as follows:\nproof(C) = SHA-256(C || proof(D) || 0x1) proof(B) = SHA-256(B || proof(C) || 0x1) proof(A) = SHA-256(A || proof(B) || 0x1) \nThe generation of these proofs build the following tree:\nproof(A) /\\ / \\ / \\ A proof(B) /\\ / \\ / \\ B proof(C) /\\ / \\ / \\ C proof(D) | | D \n\nAs such, proof(A) is a 256-bit digest that a person who receives the real response should be able to recompute for themselves. If a recipient can recompute a tree head value identical to proof(A), they can verify the integrity of the response they received. In fact, this digest plays a similar role to the tree head of a Merkle Tree, which is recomputed and compared to the presented tree head to verify the membership of a particular node. The MICE-generated digest is stored in the Digest header of the response.\nNext, the publisher serializes the headers and payloads of a request/response pair into CBOR (Concise Binary Object Representation). CBOR’s key-value storage is structurally similar to JSON, but creates smaller message sizes.\nFinally, the publisher signs the CBOR-encoded request/response pair using the private key associated with the publisher’s certificate. This becomes the value of the sig parameter in the HTTP signed exchange.\nThe final HTTP signed exchange appears like the following:\nsig=*MEUCIQDXlI2gN3RNBlgFiuRNFpZXcDIaUpX6HIEwcZEc0cZYLAIga9DsVOMM+g5YpwEBdGW3sS+bvnmAJJiSMwhuBdqp5UY=*; integrity=\"digest/mi-sha256\"; validity-url=\"https://example.com/resource.validity.1511128380\"; cert-url=\"https://example.com/oldcerts\"; cert-sha256=*W7uB969dFW3Mb5ZefPS9Tq5ZbH5iSmOILpjv2qEArmI=*; date=1511128380; expires=1511733180 \nServices like AMP can send signed exchanges by using a new HTTP response format that includes the signature above in addition to the original response. \nWhen this signature is included in an AMP-cached response, a browser can verify the legitimacy of this response. First, the browser confirms that the certificate provided in cert-url corresponds to the request’s domain and is still valid. It next uses the certificate’s public key, as well as the headers and body values of request/response pair, to check the authenticity of the signature, sig. The browser then checks the integrity of the response using the given integrity algorithm, digest/mi-sha256 (aka MICE), and the contents of the Digest header. Now the browser can confirm that a response provided by a third party has the integrity and authenticity of the content’s original publisher. \nAfter all this behind-the-scenes work, the browser can now present the original URL of the content instead of one prefixed by google.com/amp. Yippee to solving one of AMP’s most substantial pain points! \nGenerating HTTP Signed Exchanges with Workers\nFrom the overview above, the process of generating an HTTP signed exchange is clearly involved. What if there were a way to automate the generation of HTTP signed exchanges and have services like AMP automatically pick them up? With Cloudflare Workers… we found a way you could have your HTTP origin exchange cake and eat it too!\nWe have already implemented HTTP signed exchanges for one of our customers, 1-800-Flowers. Code deployed in a Cloudflare Worker is responsible for fetching and generating information necessary to create this HTTP signed exchange. \nThis Worker works with Google AMP’s automatic caching. When Google’s search crawler crawls a site, it will ask for a signed exchange from the same URL if it initially responds with Vary: AMP-Cache-Transform. Our HTTP signed exchange Worker checks if we can generate a signed exchange and if the current document is valid AMP. If it is, that Vary header is returned. After Google’s crawler sees this Vary header in the response, it will send another request with the following two headers:\nAMP-Cache-Transform: google Accept: application/signed-exchange;v=b2 \nWhen our implementation sees these header values, it will attempt to generate and return an HTTP response with Content-Type: application/signed-exchange;v=b2. \nNow that Google has cached this page with the signed exchange produced by our Worker, the requested page will appear with the publisher’s URL instead of Google’s AMP Cache URL. Success!\nIf you’d like to see HTTP signed exchanges in action on 1-800-Flowers, follow these steps:\nInstall/open Chrome Beta for Android. (It should be version 71+).\nGo to goo.gl/webpackagedemo.\nSearch for “Christmas greens.”\nClick on the 1-800-Flowers link -- it should be about 3 spots down with the AMP icon next to it. Along the way to getting there you should see a blue box that says \"Results with the AMP icon use web packaging technology.\" If you see a different message, double check that you are using the correct Chrome Beta.\nAn example of AMP in action for 1-800-Flowers:\nVisiting 1-800 Flowers through AMP with HTTP signed exchange\nThe Future: Deploying HTTP Signed Exchanges as a Worker App\nPhew. There’s clearly a lot of infrastructure for publishers to build for distributing AMP content. Thankfully Cloudflare has one of the largest networks in the world, and we now have the ability to execute JavaScript at the edge with Cloudflare Workers. We have developed a prototype Worker that generates these exchanges, on the fly, for any domain. If you’d like to start experimenting with signed exchanges, we’d love to talk!\nSoon, we will release this as a Cloudflare Worker application to our AMP customers. We’re excited to bring a better AMP experience to internet users and advance the Web Packaging standard. Stay tuned!\nThe Big Picture\nWeb Packaging is not simply a technology that helps fix the URL for AMP pages, it’s a fundamental shift in the way that publishing works online. For the entire history of the web up until this point, publishers have relied on transport layer security (TLS) to ensure that the content that they send to readers is authentic. TLS is great for protecting communication from attackers but it does not provide any public verifiability. This means that if a website serves a specific piece of content to a specific user, that user has no way of proving that to the outside world. This is problematic when it comes to efforts to archive the web. \nServices like the Internet Archive crawl websites and keep a copy of what the website returns, but who’s to say they haven’t modified it? And who’s to say that the site didn’t serve a different version of the site to the crawler than it did to a set of readers? Web Packaging fixes this issue by allowing sites to digitally sign the actual content, not just the cryptographic keys used to transport data. This subtle change enables a profoundly new ability that we never knew we needed: the ability to record and archive content on the Internet in a trustworthy way. This ability is something that is lacking in the field of online publishing. If Web Packaging takes off as a general technology, it could be the first step in creating a trusted digital record for future generations to look back on.\nExcited about the future of Web Packaging and AMP? Check out Cloudflare Ampersand to see how we're implementing this future.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nServerless Cloudflare Workers JavaScript Security AMP",
      "markdown": "11/13/2018\n\n*   [![Gabbi Fisher](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2018/11/IMG_9719.jpg)](https://blog.cloudflare.com/author/gabbi/)\n*   [![Avery Harnish](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2018/11/portrait-1627.jpg)](https://blog.cloudflare.com/author/avery/)\n\n9 min read\n\n![](https://blog.cloudflare.com/content/images/2018/11/amp-share-copy@4x.png)\n\nToday, we’re excited to announce our solution for arguably the biggest issue affecting Accelerated Mobile Pages (AMP): the inability to use real origin URLs when serving AMP-cached content. To allow AMP caches to serve content under its origin URL, we implemented HTTP signed exchanges, which extend authenticity and integrity to content cached and served on behalf of a publisher. This logic lives on [Cloudflare Workers](https://www.cloudflare.com/products/cloudflare-workers/), meaning that adding HTTP signed exchanges to your content is just a simple Workers application away. Publishers on Cloudflare can now take advantage of AMP performance and have AMP caches serve content with their origin URLs. We're thrilled to use Workers as a core component of this solution.\n\nHTTP signed exchanges are a crucial component of the emerging Web Packaging standard, a set of protocols used to package websites for distribution through optimized delivery systems like Google AMP. This announcement comes just in time for Chrome Dev Summit 2018, where our colleague Rustam Lalkaka spoke about our efforts to advance the Web Packaging standard.\n\n### What is Web Packaging and Why Does it Matter?\n\nYou may already see the need for Web Packaging on a daily basis. On your smartphone, perhaps you’ve searched for Christmas greens, visited 1-800-Flowers directly from Google, and have been surprised to see content served under the URL [https://google.com/amp/1800flowers.com/blog/flower-facts/types-of-christmas-greens/amp](https://google.com/amp/1800flowers.com/blog/flower-facts/types-of-christmas-greens/amp). This is an instance of AMP in action, where Google serves cached content so your desired web page loads faster.\n\n![](https://blog.cloudflare.com/content/images/2018/11/ezgif-noAMP.gif)\n\nVisiting 1-800 Flowers through AMP without HTTP signed exchange\n\nGoogle cannot serve cached content under publisher URLs for clear security reasons. To securely present content from a URL, a TLS certificate for its domain is required. Google cannot provide 1-800-Flowers’ certificate on the vendor’s behalf, because it does not have the corresponding private key. Additionally, Google cannot, and should not be able to, sign content using the private key that corresponds to 1-800-Flowers’ certificate.\n\nThe inability to use original content URLs with AMP posed some serious issues. First, the google.com/amp URL prefix can strip URLs of their meaning. To the frustration of publishers, their content is no longer directly attributed to them by a URL (let alone a certificate). The publisher can no longer prove the integrity and authenticity of content served on their behalf.\n\nSecond, for web browsers the lack of a publisher’s URL can call the integrity and authenticity of a cached webpage into question. Namely, there’s no clear way to prove that this response is a cached version of an actual page published by 1-800-Flowers. Additionally, cookies are managed by third-party providers like Google instead of the publisher.\n\nEnter Web Packaging, a [collection of specifications](https://github.com/WICG/webpackage) for “packaging” website content with information like certificates and their validity. The [HTTP signed exchanges specification](https://wicg.github.io/webpackage/draft-yasskin-http-origin-signed-responses.html) allows third-party caches to cache and service HTTPS requests with proof of integrity and authenticity.\n\n### HTTP Signed Exchanges: Extending Trust with Cryptography\n\nIn the pre-AMP days, people expected to find a webpage’s content at one definitive URL. The publisher, who owns the domain of the definitive URL, would present a visitor with a certificate that corresponds to this domain and contains a public key.\n\n![](https://blog.cloudflare.com/content/images/2018/11/step-one@4x.png)\n\nThe publisher would use the corresponding private key to sign a cryptographic handshake, which is used to derive shared symmetric keys that are used to encrypt the content and protect its integrity.\n\n![](https://blog.cloudflare.com/content/images/2018/11/step-2@4x.png)\n\nThe visitor would then receive content encrypted and signed by the shared key.\n\n![](https://blog.cloudflare.com/content/images/2018/11/step-3@4x.png)\n\nThe visitor’s browser then uses the shared key to verify the response’s signature and, in turn, the authenticity and integrity of the content received.\n\n![](https://blog.cloudflare.com/content/images/2018/11/step-4@4x.png)\n\nWith services like AMP, however, online content may correspond to more than one URL. This introduces a problem: while only one domain actually corresponds to the webpage’s publisher, multiple domains can be responsible for serving a webpage. If a publisher allows AMP services to cache and serve their webpages, they must be able to sign their content even when AMP caches serve it for them. Only then can AMP-cached content prove its legitimacy.\n\n![](https://blog.cloudflare.com/content/images/2018/11/step-4-copy-4@4x.png)\n\nHTTP signed exchanges directly address the problem of extending publisher signatures to services like AMP. This [IETF draft](https://wicg.github.io/webpackage/draft-yasskin-http-origin-signed-responses.html) specifies how publishers may sign an HTTP request/response pair (an exchange). With a signed exchange, the publisher can assure the integrity and authenticity of a response to a specific request even before the client makes the request. Given a signed exchange, the publisher authorizes intermediates (like Google’s AMP Cache) to forward the exchanges; the intermediate responds to a given request with the corresponding response in the signed HTTP request/response pair. A browser can then verify the exchange signature to assert the intermediate response’s integrity and authenticity.\n\nThis is like handing out an answer key to a quiz signed by the instructor. Having a signed answer sheet is just as good as getting the answer from the teacher in real time.\n\n### The Technical Details\n\nAn HTTP signed exchange is generated by the following steps.  \nFirst, the publisher uses [MICE](https://tools.ietf.org/id/draft-thomson-http-mice-03.txt) (Merkle Integrity Content Encoding) to provide a concise proof of integrity for the response included in the exchange. To start, the response is split into blocks of some record size bits long. Take, for example, a message ABCD, which is divided into record-size blocks A, B, C, and D. The first step to constructing a proof of integrity is to take the last block, D, and compute the following:\n\n```\nproof(D) = SHA-256(D || 0x0)\n```\n\nThis produces proof(D). Then, all consequent proof values for blocks are computed as follows:\n\n```\nproof(C) = SHA-256(C || proof(D) || 0x1)\nproof(B) = SHA-256(B || proof(C) || 0x1)\nproof(A) = SHA-256(A || proof(B) || 0x1)\n```\n\nThe generation of these proofs build the following tree:\n\n```\n      proof(A)\n         /\\\n        /  \\\n       /    \\\n      A    proof(B)\n            /\\\n           /  \\\n          /    \\\n         B    proof(C)\n                /\\\n               /  \\\n              /    \\\n             C    proof(D)\n                    |\n                    |\n                    D\n```\n\n  \nAs such, proof(A) is a 256-bit digest that a person who receives the real response should be able to recompute for themselves. If a recipient can recompute a tree head value identical to proof(A), they can verify the integrity of the response they received. In fact, this digest plays a similar role to the tree head of a [Merkle Tree](https://blog.cloudflare.com/introducing-certificate-transparency-and-nimbus/), which is recomputed and compared to the presented tree head to verify the membership of a particular node. The MICE-generated digest is stored in the Digest header of the response.\n\nNext, the publisher serializes the headers and payloads of a request/response pair into [CBOR](https://tools.ietf.org/html/rfc7049) (Concise Binary Object Representation). CBOR’s key-value storage is structurally similar to JSON, but creates smaller message sizes.\n\nFinally, the publisher signs the CBOR-encoded request/response pair using the private key associated with the publisher’s certificate. This becomes the value of the sig parameter in the HTTP signed exchange.\n\nThe final HTTP signed exchange appears like the following:\n\n```\nsig=*MEUCIQDXlI2gN3RNBlgFiuRNFpZXcDIaUpX6HIEwcZEc0cZYLAIga9DsVOMM+g5YpwEBdGW3sS+bvnmAJJiSMwhuBdqp5UY=*;  \nintegrity=\"digest/mi-sha256\";  \nvalidity-url=\"https://example.com/resource.validity.1511128380\";  \ncert-url=\"https://example.com/oldcerts\";  \ncert-sha256=*W7uB969dFW3Mb5ZefPS9Tq5ZbH5iSmOILpjv2qEArmI=*;  \ndate=1511128380; expires=1511733180\n```\n\nServices like AMP can send signed exchanges by using a new HTTP response format that includes the signature above in addition to the original response.\n\n![](https://blog.cloudflare.com/content/images/2018/11/step-4-copy-3@4x.png)\n\nWhen this signature is included in an AMP-cached response, a browser can verify the legitimacy of this response. First, the browser confirms that the certificate provided in cert-url corresponds to the request’s domain and is still valid. It next uses the certificate’s public key, as well as the headers and body values of request/response pair, to check the authenticity of the signature, sig. The browser then checks the integrity of the response using the given integrity algorithm, digest/mi-sha256 (aka MICE), and the contents of the Digest header. Now the browser can confirm that a response provided by a third party has the integrity and authenticity of the content’s original publisher.\n\nAfter all this behind-the-scenes work, the browser can now present the original URL of the content instead of one prefixed by google.com/amp. Yippee to solving one of AMP’s most substantial pain points!\n\n### Generating HTTP Signed Exchanges with Workers\n\nFrom the overview above, the process of generating an HTTP signed exchange is clearly involved. What if there were a way to automate the generation of HTTP signed exchanges and have services like AMP automatically pick them up? With Cloudflare Workers… we found a way you could have your HTTP origin exchange cake and eat it too!\n\nWe have already implemented HTTP signed exchanges for one of our customers, [1-800-Flowers](https://www.1800flowers.com/). Code deployed in a Cloudflare Worker is responsible for fetching and generating information necessary to create this HTTP signed exchange.\n\nThis Worker works with Google AMP’s automatic caching. When Google’s search crawler crawls a site, it will ask for a signed exchange from the same URL if it initially responds with Vary: AMP-Cache-Transform. Our HTTP signed exchange Worker checks if we can generate a signed exchange and if the current document is valid AMP. If it is, that Vary header is returned. After Google’s crawler sees this Vary header in the response, it will send another request with the following two headers:\n\n```\nAMP-Cache-Transform: google\nAccept: application/signed-exchange;v=b2\n```\n\nWhen our implementation sees these header values, it will attempt to generate and return an HTTP response with Content-Type: application/signed-exchange;v=b2.\n\nNow that Google has cached this page with the signed exchange produced by our Worker, the requested page will appear with the publisher’s URL instead of Google’s AMP Cache URL. Success!\n\nIf you’d like to see HTTP signed exchanges in action on 1-800-Flowers, follow these steps:\n\n1.  Install/open Chrome Beta for Android. (It should be version 71+).\n2.  Go to [goo.gl/webpackagedemo](https://goo.gl/webpackagedemo).\n3.  Search for “Christmas greens.”\n4.  Click on the 1-800-Flowers link -- it should be about 3 spots down with the AMP icon next to it. Along the way to getting there you should see a blue box that says \"Results with the AMP icon use web packaging technology.\" If you see a different message, double check that you are using the correct Chrome Beta.  \n    An example of AMP in action for 1-800-Flowers:\n\n![](https://blog.cloudflare.com/content/images/2018/11/ezgif-w-amp.gif)\n\nVisiting 1-800 Flowers through AMP with HTTP signed exchange\n\n### The Future: Deploying HTTP Signed Exchanges as a Worker App\n\nPhew. There’s clearly a lot of infrastructure for publishers to build for distributing AMP content. Thankfully Cloudflare has [one of the largest networks in the world](https://www.cloudflare.com/network/), and we now have the ability to execute JavaScript at the edge with [Cloudflare Workers](https://www.cloudflare.com/network/). We have developed a prototype Worker that generates these exchanges, on the fly, for any domain. If you’d like to start experimenting with signed exchanges, [we’d love to talk](https://www.cloudflare.com/website-optimization/ampersand/)!\n\nSoon, we will release this as a Cloudflare Worker application to our AMP customers. We’re excited to bring a better AMP experience to internet users and advance the Web Packaging standard. Stay tuned!\n\n### The Big Picture\n\nWeb Packaging is not simply a technology that helps fix the URL for AMP pages, it’s a fundamental shift in the way that publishing works online. For the entire history of the web up until this point, publishers have relied on transport layer security (TLS) to ensure that the content that they send to readers is authentic. TLS is great for protecting communication from attackers but it does not provide any public verifiability. This means that if a website serves a specific piece of content to a specific user, that user has no way of proving that to the outside world. This is problematic when it comes to efforts to archive the web.\n\nServices like the Internet Archive crawl websites and keep a copy of what the website returns, but who’s to say they haven’t modified it? And who’s to say that the site didn’t serve a different version of the site to the crawler than it did to a set of readers? Web Packaging fixes this issue by allowing sites to digitally sign the actual content, not just the cryptographic keys used to transport data. This subtle change enables a profoundly new ability that we never knew we needed: the ability to record and archive content on the Internet in a trustworthy way. This ability is something that is lacking in the field of online publishing. If Web Packaging takes off as a general technology, it could be the first step in creating a trusted digital record for future generations to look back on.\n\nExcited about the future of Web Packaging and AMP? Check out [Cloudflare Ampersand](https://www.cloudflare.com/website-optimization/ampersand/) to see how we're implementing this future.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Serverless](https://blog.cloudflare.com/tag/serverless/) [Cloudflare Workers](https://blog.cloudflare.com/tag/workers/) [JavaScript](https://blog.cloudflare.com/tag/javascript/) [Security](https://blog.cloudflare.com/tag/security/) [AMP](https://blog.cloudflare.com/tag/amp-tag/)"
    },
    {
      "url": "https://blog.cloudflare.com/stories-from-our-recent-global-data-center-upgrade/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/stories-from-our-recent-global-data-center-upgrade/",
        "loadedTime": "2023-12-05T02:35:07.968Z",
        "referrerUrl": "https://blog.cloudflare.com/steve-bray-why-i-joined-cloudflare/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/stories-from-our-recent-global-data-center-upgrade/",
        "title": "Stories from our recent global data center upgrade",
        "description": "Each day at CloudFlare is full of surprises.  As it turns out, it takes a lot of work to stop massive attacks and to help make the web faster. Over the past six months, our entire team has contributed in every way imaginable to more than double the capacity of our global network.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "01/30/2014\n5 min read\nEach day at CloudFlare is full of surprises.\nAs it turns out, it takes a lot of work to stop massive attacks and to help make the web faster. Over the past six months, our entire team has contributed in every way imaginable to more than double the capacity of our global network. Below is a behind-the-scenes look into how we keep our global network running.\nAlong the way we’ve encountered many surprises—some fun and some cringe worthy—that have taught us about our team, our data centers and overcoming challenges that occasionally seem beyond our control.\nCloudFlare team: always online\nSan Jose, US (SJC): Our counsel, Ken, is great at pumpkin carving, and even better at standing up to protect the privacy rights of our users (including against Kanye West’s army of lawyers). What you may not have known is that he is happiest in the data center (not to mention our cabling was much prettier when he finished!). \nKen, our counsel\nLos Angeles, US (LAX): Our engineers monitor our network around the clock. Occasionally this means juggling multiple tasks. During our most recent upgrade, Joshua (Systems Reliability Engineer & super dad) managed to snatch a moment to put his kids to bed while managing simultaneous upgrades in Los Angeles and Stockholm.\nChicago, US (ORD): Just before our Chicago upgrade we learned that our carrier had misplaced a shipment of memory. Fortunately, Nitin (Special Projects) averted disaster and got the courier to radio the driver (it took some convincing!), find and grab our DIMMs, and get the install done in time. When most people think of Chicago they think of the ‘95-96 Chicago Bulls (arguably the greatest basketball team to step foot on this earth). When we think of Chicago, our minds turn to the bullwhip effect. We precisely plan every data center launch and upgrade throughout the entire supply chain—from cables to servers to shipping schedules—to control against situations where precautions amplify errors.\nDallas, US (DFW): At CloudFlare, stamina is key. Trey (Solution Engineer) experienced this first hand. Despite working through the night on our Dallas upgrade, he still managed to catch a 6:30 AM flight the following morning to San Antonio, where he ran a workshop for our friends at Rackspace. \nAshburn, US (IAD): An important customer meeting the following morning didn’t stop Matthew (CEO) and Trey (Solution Engineer) from working through the night to upgrade our Ashburn facility. Trey even realized he could use his toenail clippers to save time cutting zipties and keep the the install moving. \nHome is where the datacenter is\nHong Kong, HK (HKG): After speaking at an Internet security conference in China, Joshua (Special Projects Lead) spent three consecutive nights upgrading our Seoul, Tokyo and Hong Kong data centers. In addition to the data center, he found airport lounges and taxis to be equally habitable.\nLondon, GB (LHR): CloudFlare’s first international office opened in London in 2013. Since then we’ve enlisted an amazing team of engineers to keep our network humming 24x7x365. On more than a few occasions, James (Systems Reliability Engineer), Marty (Support Engineer) and Simon (Support Engineer) have found a warm room full of servers in our London facility a comfort on a bitter winter night.\nStockholm, SE (ARN): Simon (Support Engineer) managed to navigate to the local Kjell to pick up a few needed power adapters, and then braved the 1° C cold outside of our Stockholm data center for a bit longer than he would have liked while waiting for an access card.\nMiami, US (MIA): We take security seriously, and so do our data center partners. Justin (Systems Reliability Engineer) was at least a little intimidated to find guards armed with machine guns protecting the entrance of our Miami facility.\nTokyo, JP (NRT): We love our data centers so much that we even name our conference rooms after them. As our San Francisco office expands (we’ve now knocked down two walls!), we’ve named the latest NRT.\nNew challenges\nAtlanta, US (ATL): Each of our racks around the world are fitted with high tech PDUs (power distribution units) that allow for remote monitoring and power cycling. This allows us to monitor our infrastructure in real-time, and react at a moment’s notice. When our colocation provider in Atlanta told us that our PDU wouldn’t fit, giving up wasn’t an option. Joshua (Special Projects Lead) proposed a rack extender to do the trick!\nParis, FR (CDG): Imagine being told that equipment you had just shipped 5,000 miles across the globe was about to be sent right back. Jérôme (Network Engineer), one of our resident French speakers, saved the day and made sure our equipment stayed right where it belonged: working hard in support of one of our busiest datacenters. Merci Jérôme!\nSeattle, US (SEA): We install console servers with out-of-band, cellular Internet access in each of our data centers to remotely manage our infrastructure in the case our primary Internet connectivity is lost. While this makes it easier to address connectivity issues, installing the equipment itself can occasionally be more difficult. With a SIM card stubbornly lodged into our console server in Seattle, Jerome (Partner Engineer) used what he had available—namely, dental floss and a pair of forceps—to get the job done in a way that even MacGyver would approve.\nSeoul, KR (ICN): Korea ranks near the top of most challenging locations to import equipment into (right up there with Warsaw). Fortunately, having facilitated hundreds of shipments in dozens of countries, Nitin (Special Projects) was able to break through a two month logjam in which Incheon airport became a temporary home for some of our equipment. Among his other talents, Nitin can now hum the FedEx and DHL songs in nearly any language of your choice.\nValparaíso, CL (SCL): Sometimes events are just out of one’s control. To launch our newest data center we had to wait through multiple customs strikes before equipment could arrive. Still, the show went on. Tom (Network Engineer) worked through Christmas to shave 170ms off of latency for our users in Latin America.\nWhat’s in store for 2014, you ask? Over the next 12 months we will significantly expand our data center footprint, adding facilities in regions we currently lack coverage: Latin America, the Middle East, Africa, and parts of Asia. China is our second largest market, Brazil is our third: in 2014 we’ll be significantly expanding our network to better serve these customers.\nIf this sounds like fun, and if you enjoy a few surprises every once in a while, please consider joining us. We’re actively recruiting for someone passionate and talented to assist with our expansion.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nData Center California Texas USA Europe \nRelated Posts\nMarch 14, 2018 7:00PM\nFive new Cloudflare data centers across the United States\nWhen Cloudflare launched, three of the original five cities in our network were located in the United States. Since then, we have grown the breadth of the global network considerably to span 66 countries, and even added expanded the US footprint to twenty five locations....\nBy \nMay 12, 2017 10:29PM\nDetroit and San Diego Data Centers expand Cloudflare network to 26 North American cities\nCloudflare is excited to announce deployments in Detroit and San Diego, which are our 114th and 115th data centers respectively. They join Colombo, Sri Lanka and Cape Town, South Africa....\nBy \nApril 25, 2011 10:18PM\nCloudFlare's LA Data Center Now Online!\nWe just turned on our latest data center in Los Angeles. In its first minute, the facility processed more than 30,000 requests. We expect that number to rise over the next 24 hours....\nBy \nSeptember 02, 2010 12:05AM\nAnd Then There Were Three: Cloudflare's New Data Center\nCloudflare has expanded its data centers to three adding a new location in San Jose this week. By adding data centers to our platform, Cloudflare can deliver even faster site performance....\nBy",
      "markdown": "01/30/2014\n\n*   [![Nitin Rao](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2019/08/WtxhLSEUSgqYRMdQUwWp_1204a20b3d1d96bba523a6a2a5fa3cd73bd4fe59240a27ad6eb8c064c6792446.jpg)](https://blog.cloudflare.com/author/nitin-rao/)\n\n5 min read\n\nEach day at CloudFlare is full of surprises.\n\nAs it turns out, it takes a lot of work to stop massive [attacks](https://blog.cloudflare.com/the-ddos-that-almost-broke-the-internet) and to help make the web [faster](https://blog.cloudflare.com/railgun-gives-our-ecommerce-sites-the-edge). Over the past six months, our entire team has contributed in every way imaginable to more than double the capacity of our global network. Below is a behind-the-scenes look into how we keep our [global network](https://www.cloudflare.com/network-map) running.\n\nAlong the way we’ve encountered many surprises—some fun and some cringe worthy—that have taught us about our team, our data centers and overcoming challenges that occasionally seem beyond our control.\n\n#### CloudFlare team: always online\n\n*   **San Jose, US (SJC):** Our counsel, Ken, is great at [pumpkin carving](https://blog.cloudflare.com/cloud-o-ween), and even better at standing up to protect the privacy rights of our users (including against [Kanye West’s](https://twitter.com/eastdakota/status/420736462804365312) army of lawyers). What you may not have known is that he is happiest in the data center (not to mention our cabling was much prettier when he finished!).\n    \n    ![](https://blog.cloudflare.com/content/images/ken_1.jpg)\n    \n    _Ken, our counsel_\n    \n*   **Los Angeles, US (LAX):** Our engineers monitor our network around the clock. Occasionally this means juggling multiple tasks. During our most recent upgrade, Joshua (Systems Reliability Engineer & super dad) managed to snatch a moment to put his kids to bed while managing simultaneous upgrades in Los Angeles and Stockholm.\n*   **Chicago, US (ORD):** Just before our Chicago upgrade we learned that our carrier had misplaced a shipment of memory. Fortunately, Nitin (Special Projects) averted disaster and got the courier to radio the driver (it took some convincing!), find and grab our [DIMMs](https://en.wikipedia.org/wiki/DIMM), and get the install done in time. When most people think of Chicago they think of the ‘95-96 Chicago Bulls (arguably the greatest basketball team to step foot on this earth). When we think of Chicago, our minds turn to the [bullwhip effect](https://en.wikipedia.org/wiki/Bullwhip_effect). We precisely plan every data center launch and upgrade throughout the entire supply chain—from cables to servers to shipping schedules—to control against situations where precautions amplify errors.\n*   **Dallas, US (DFW):** At CloudFlare, stamina is key. Trey (Solution Engineer) experienced this first hand. Despite working through the night on our Dallas upgrade, he still managed to catch a 6:30 AM flight the following morning to San Antonio, where he ran a workshop for our friends at Rackspace.\n*   **Ashburn, US (IAD):** An important customer meeting the following morning didn’t stop Matthew (CEO) and Trey (Solution Engineer) from working through the night to upgrade our Ashburn facility. Trey even realized he could use his toenail clippers to save time cutting zipties and keep the the install moving.\n\n**Home is where the datacenter is**  \n![](https://blog.cloudflare.com/content/images/the-terminal-tom-hanks.jpg)\n\n*   **Hong Kong, HK (HKG):** After speaking at an Internet security conference in China, Joshua (Special Projects Lead) spent three consecutive nights upgrading our Seoul, Tokyo and Hong Kong data centers. In addition to the data center, he found airport lounges and taxis to be equally habitable.\n*   **London, GB (LHR):** CloudFlare’s first international office opened in [London](https://blog.cloudflare.com/cloudflare-london-is-open-for-business) in 2013. Since then we’ve enlisted an amazing team of engineers to keep our network humming 24x7x365. On more than a few occasions, James (Systems Reliability Engineer), Marty (Support Engineer) and Simon (Support Engineer) have found a warm room full of servers in our London facility a comfort on a bitter winter night.\n*   **Stockholm, SE (ARN):** Simon (Support Engineer) managed to navigate to the local [Kjell](http://www.kjell.com/) to pick up a few needed power adapters, and then braved the 1° C cold outside of our Stockholm data center for a bit longer than he would have liked while waiting for an access card.\n*   **Miami, US (MIA):** We take security seriously, and so do our data center partners. Justin (Systems Reliability Engineer) was at least a little intimidated to find guards armed with machine guns protecting the entrance of our Miami facility.\n*   **Tokyo, JP (NRT):** We love our data centers so much that we even name our conference rooms after them. As our [San Francisco office](https://blog.cloudflare.com/cloudflare-opens-its-office-in-san-francisco) expands (we’ve now knocked down two walls!), we’ve named the latest NRT.\n\n**New challenges**\n\n![](https://blog.cloudflare.com/content/images/image.jpg)\n\n*   **Atlanta, US (ATL):** Each of our racks around the world are fitted with high tech PDUs (power distribution units) that allow for remote monitoring and power cycling. This allows us to monitor our infrastructure in real-time, and react at a moment’s notice. When our colocation provider in Atlanta told us that our PDU wouldn’t fit, giving up wasn’t an option. Joshua (Special Projects Lead) proposed a rack extender to do the trick!\n*   **Paris, FR (CDG):** Imagine being told that equipment you had just shipped 5,000 miles across the globe was about to be sent right back. Jérôme (Network Engineer), one of our resident French speakers, saved the day and made sure our equipment stayed right where it belonged: working hard in support of one of our busiest datacenters. Merci Jérôme!\n*   **Seattle, US (SEA):** We install console servers with out-of-band, cellular Internet access in each of our data centers to remotely manage our infrastructure in the case our primary Internet connectivity is lost. While this makes it easier to address connectivity issues, installing the equipment itself can occasionally be more difficult. With a SIM card stubbornly lodged into our console server in Seattle, Jerome (Partner Engineer) used what he had available—namely, dental floss and a pair of forceps—to get the job done in a way that even MacGyver would approve.\n*   **Seoul, KR (ICN):** Korea ranks near the top of most challenging locations to import equipment into (right up there with Warsaw). Fortunately, having facilitated hundreds of shipments in dozens of countries, Nitin (Special Projects) was able to break through a two month logjam in which Incheon airport became a temporary home for some of our equipment. Among his other talents, Nitin can now hum the FedEx and DHL songs in nearly any language of your choice.\n*   **Valparaíso, CL (SCL):** Sometimes events are just out of one’s control. To launch our newest [data center](https://blog.cloudflare.com/bienvenido-a-chile-cloudflares-24th-data-center-now-live) we had to wait through multiple [customs strikes](http://www.idstrac.com/Blog/strikes-in-chile-prompted-delays-in-global-shipping/) before equipment could arrive. Still, the show went on. Tom (Network Engineer) worked through Christmas to shave 170ms off of latency for our users in Latin America.\n\nWhat’s in store for 2014, you ask? Over the next 12 months we will significantly expand our data center footprint, adding facilities in regions we currently lack coverage: Latin America, the Middle East, Africa, and parts of Asia. China is our second largest market, Brazil is our third: in 2014 we’ll be significantly expanding our network to better serve these customers.\n\nIf this sounds like fun, and if you enjoy a few surprises every once in a while, please consider joining us. We’re actively recruiting for someone passionate and talented to assist with our expansion.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Data Center](https://blog.cloudflare.com/tag/data-center/) [California](https://blog.cloudflare.com/tag/california/) [Texas](https://blog.cloudflare.com/tag/texas/) [USA](https://blog.cloudflare.com/tag/usa/) [Europe](https://blog.cloudflare.com/tag/europe/)\n\nRelated Posts\n\nMarch 14, 2018 7:00PM\n\n[\n\n## Five new Cloudflare data centers across the United States\n\n](https://blog.cloudflare.com/usa-expansion/)\n\nWhen Cloudflare launched, three of the original five cities in our network were located in the United States. Since then, we have grown the breadth of the global network considerably to span 66 countries, and even added expanded the US footprint to twenty five locations....\n\nBy \n\nMay 12, 2017 10:29PM\n\n[\n\n## Detroit and San Diego Data Centers expand Cloudflare network to 26 North American cities\n\n](https://blog.cloudflare.com/detroit-san-diego/)\n\nCloudflare is excited to announce deployments in Detroit and San Diego, which are our 114th and 115th data centers respectively. They join Colombo, Sri Lanka and Cape Town, South Africa....\n\nBy \n\nApril 25, 2011 10:18PM\n\n[\n\n## CloudFlare's LA Data Center Now Online!\n\n](https://blog.cloudflare.com/cloudflares-la-datacenter-now-online/)\n\nWe just turned on our latest data center in Los Angeles. In its first minute, the facility processed more than 30,000 requests. We expect that number to rise over the next 24 hours....\n\nBy \n\nSeptember 02, 2010 12:05AM\n\n[\n\n## And Then There Were Three: Cloudflare's New Data Center\n\n](https://blog.cloudflare.com/and-then-there-were-threecloudflares-new-data/)\n\nCloudflare has expanded its data centers to three adding a new location in San Jose this week. By adding data centers to our platform, Cloudflare can deliver even faster site performance....\n\nBy"
    },
    {
      "url": "https://blog.cloudflare.com/cloudflare-needs-go-programmers-in-london-and-san-francisco/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/cloudflare-needs-go-programmers-in-london-and-san-francisco/",
        "loadedTime": "2023-12-05T02:35:13.441Z",
        "referrerUrl": "https://blog.cloudflare.com/steve-bray-why-i-joined-cloudflare/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/cloudflare-needs-go-programmers-in-london-and-san-francisco/",
        "title": "CloudFlare hiring Go programmers in London and San Francisco",
        "description": "Are you familiar with the Go programming language and looking for a job in San Francisco or London? Then think about applying to CloudFlare. We're looking for people with experience writing Go in both locations.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "08/19/2014\n1 min read\nAre you familiar with the Go programming language and looking for a job in San Francisco or London? Then think about applying to CloudFlare. We're looking for people with experience writing Go in both locations.\n\nCC BY-SA 2.0 by Yuko Honda (cropped, resized)\nCloudFlare uses Go extensively to build our service and we need to people to build and maintain those systems. We've written a complete DNS server in Go, our Railgun service is all Go and we're moving more and more systems to Go programs.\nWe've recently written about our open source Red October Go project for securing secrets, and open-sourced our CFSSL Go-based PKI package. Go is now making its way into our data pipeline and be used for processing huge amounts of data.\nWe even have a Go-specific section on our GitHub.\nIf you're interested in working in Go on a high-performance global network like CloudFlare, send us an email.\nNot into Go? We're hiring for all sorts of other positions and technologies.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nUnited Kingdom Life @ Cloudflare Go Programming DNS \nRelated Posts\nJanuary 30, 2014 2:00PM\nStories from our recent global data center upgrade\nEach day at CloudFlare is full of surprises. As it turns out, it takes a lot of work to stop massive attacks and to help make the web faster. Over the past six months, our entire team has contributed in every way imaginable to more than double the capacity of our global network....\nBy \nJuly 30, 2021 2:00PM\nBuilding a sustainable workforce, through communities\nAt Cloudflare, we place a lot of value on the importance of diversity, equity and inclusion. Diversity, equity, and inclusion lead to better outcomes through improved decision-making, more innovative teams, stronger financial returns and simply a better place to work for everyone....\nBy \nOctober 24, 2019 2:00PM\nWho DDoS'd Austin?\nIt was a scorching Monday on July 22 as temperatures soared above 37°C (99°F) in Austin, TX, the live music capital of the world. Only hours earlier, the last crowds dispersed from the historic East 6th Street entertainment district....\nBy \nMay 28, 2014 2:15PM\nWelcome to Miami: HostingCon 2014\nThis year’s HostingCon will be held in Miami Beach, and the CloudFlare team is busy prepping. This is our fourth year at the show and our team is excited to see partners, customers and friends....\nBy",
      "markdown": "08/19/2014\n\n*   [![John Graham-Cumming](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2017/03/url-2.jpg)](https://blog.cloudflare.com/author/john-graham-cumming/)\n\n1 min read\n\nAre you familiar with the [Go](https://golang.org/) programming language and looking for a job in San Francisco or London? Then think about [applying](https://www.cloudflare.com/join-our-team) to CloudFlare. We're looking for people with experience writing Go in both locations.\n\n![](https://blog.cloudflare.com/content/images/6779040884_3f7bfeaf89_z_1.jpg)  \nCC BY-SA 2.0 by [Yuko Honda](https://www.flickr.com/photos/yukop/) (cropped, resized)\n\nCloudFlare uses Go extensively to build our service and we need to people to build and maintain those systems. We've written a complete [DNS server](https://blog.cloudflare.com/what-weve-been-doing-with-go/) in Go, our [Railgun](https://blog.cloudflare.com/go-at-cloudflare) service is all Go and we're moving more and more systems to Go programs.\n\nWe've recently written about our open source [Red October](https://blog.cloudflare.com/red-october-cloudflares-open-source-implementation-of-the-two-man-rule/) Go project for securing secrets, and open-sourced our [CFSSL](https://blog.cloudflare.com/introducing-cfssl/) Go-based PKI package. Go is now making its way into our data pipeline and be used for processing huge amounts of data.\n\nWe even have a [Go-specific section](http://cloudflare.github.io/#cat-Go) on our GitHub.\n\nIf you're interested in working in Go on a high-performance global network like CloudFlare, send us an [email](mailto:careers@cloudflare.com).\n\nNot into Go? We're hiring for [all sorts](https://www.cloudflare.com/join-our-team) of other positions and technologies.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[United Kingdom](https://blog.cloudflare.com/tag/united-kingdom/) [Life @ Cloudflare](https://blog.cloudflare.com/tag/life-at-cloudflare/) [Go](https://blog.cloudflare.com/tag/go/) [Programming](https://blog.cloudflare.com/tag/programming/) [DNS](https://blog.cloudflare.com/tag/dns/)\n\nRelated Posts\n\nJanuary 30, 2014 2:00PM\n\n[\n\n## Stories from our recent global data center upgrade\n\n](https://blog.cloudflare.com/stories-from-our-recent-global-data-center-upgrade/)\n\nEach day at CloudFlare is full of surprises. As it turns out, it takes a lot of work to stop massive attacks and to help make the web faster. Over the past six months, our entire team has contributed in every way imaginable to more than double the capacity of our global network....\n\nBy \n\nJuly 30, 2021 2:00PM\n\n[\n\n## Building a sustainable workforce, through communities\n\n](https://blog.cloudflare.com/building-a-sustainable-workforce-through-communities/)\n\nAt Cloudflare, we place a lot of value on the importance of diversity, equity and inclusion. Diversity, equity, and inclusion lead to better outcomes through improved decision-making, more innovative teams, stronger financial returns and simply a better place to work for everyone....\n\nBy \n\nOctober 24, 2019 2:00PM\n\n[\n\n## Who DDoS'd Austin?\n\n](https://blog.cloudflare.com/who-ddosd-austin/)\n\nIt was a scorching Monday on July 22 as temperatures soared above 37°C (99°F) in Austin, TX, the live music capital of the world. Only hours earlier, the last crowds dispersed from the historic East 6th Street entertainment district....\n\nBy \n\nMay 28, 2014 2:15PM\n\n[\n\n## Welcome to Miami: HostingCon 2014\n\n](https://blog.cloudflare.com/welcome-to-miami-hostingcon-2014/)\n\nThis year’s HostingCon will be held in Miami Beach, and the CloudFlare team is busy prepping. This is our fourth year at the show and our team is excited to see partners, customers and friends....\n\nBy"
    },
    {
      "url": "https://blog.cloudflare.com/building-a-sustainable-workforce-through-communities/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/building-a-sustainable-workforce-through-communities/",
        "loadedTime": "2023-12-05T02:35:23.363Z",
        "referrerUrl": "https://blog.cloudflare.com/steve-bray-why-i-joined-cloudflare/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/building-a-sustainable-workforce-through-communities/",
        "title": "Building a sustainable workforce, through communities",
        "description": "At Cloudflare, we place a lot of value on the importance of diversity, equity and inclusion. Diversity, equity, and inclusion lead to better outcomes through improved decision-making, more innovative teams, stronger financial returns and simply a better place to work for everyone.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "07/30/2021\n9 min read\nAt Cloudflare, we have our eyes set on an ambitious goal: to help build a better Internet. Today the company runs one of the world’s largest networks that powers approximately 25 million Internet properties. This is made possible by our 1,900 team members around the world. We believe the key to achieving our potential is to build diverse teams and create an environment where everyone can do their best work.\nThat is why we place a lot of value on the importance of diversity, equity and inclusion. Diversity, equity, and inclusion lead to better outcomes through improved decision-making, more innovative teams, stronger financial returns and simply a better place to work for everyone.\nTo become more diverse, equitable, and inclusive, we believe it’s important to focus on communities within and around our company.\nBuilding internal communities at Cloudflare\nAt Cloudflare, like most workplaces, there are built-in communities: your direct team, your cross-functional partners and (because we take onboarding very seriously) your new hire class. These communities, especially the first two, are important to help you get your job done. But we want more than that for our team at Cloudflare. We believe that community builds connection and fosters a sense of belonging.\nBecause of that, we have supported the growth of over 16 Employee Resource Groups (ERG’s). We use the term ERG broadly at Cloudflare. We have many ERG’s focused on traditionally under-represented groups in tech: Afroflare (Black, African diaspora), Latinflare, and Womenflare; groups that have been historically marginalized: Proudflare (LGBTQIA+), Cloudflarents (parents and caregivers); as well as interest and affinity groups like Mindflare and Soberflare. To read more about all of our ERGs, visit our diversity, equity, and inclusion webpage or read about them on our blog. In addition to creating a community of support and belonging, our ERGs also work to enhance career development of their members and contribute to the development of a more inclusive culture at Cloudflare.\nBuilding the skills to build communities\nWe define an inclusive culture as one where everyone feels safe, welcome and respected with a sense of belonging. We do not leave this to chance. We make investments in training and programs to develop and deepen the skills needed to nurture and preserve inclusive communities at Cloudflare.\nOne of our earliest offerings was Ally Skills training. The aim of this workshop is to help build awareness of the types of behavior and language which can be harmful to inclusivity at Cloudflare, and teach simple, everyday ways to support people who are targets of systemic oppression. During the workshop, team members share strategies on how to act as allies and how to create a long-lasting, inclusive culture at Cloudflare. As the program was being rolled out, the management team did the workshop together and quickly realized these were not skills reserved for ‘allies’ but it was our expectation that this was how all of our team members treated each other. These were necessary skills to be successful at Cloudflare. As a result, we reworked some pieces of the workshop and renamed it: How We Work Together.\nWe have also partnered with Paradigm IQ and Included to create a three-part Unconscious Bias Education Program. These workshops are a mix of eLearning and facilitated workshops where we learn about how to help mitigate unconscious bias and make our company a more welcoming and inclusive place for everyone. tEQuitable is an additional comprehensive resource which helps us create a safe, inclusive, and equitable workplace. They provide an independent sounding board where our employees may confidentially raise a concern, access a just-in-time learning platform, and get advice from professional Ombuds. They also help us identify systemic workplace issues and provide us with actionable recommendations for how to improve our workplace culture. What we especially love about tEQuitable is that it’s all about empowering our employees with tools and resources to address issues that may be impacting them, or they may witness impacting others, so we all play an active role in maintaining and nurturing our culture.\nOne other program worth highlighting is our Week On: Learning and Inclusion. This program came as a response to the murder of George Floyd in the US at the end of May 2020. Our Afroflare global leaders suggested we use Juneteenth as a full-day of deep learning from external experts on topics ranging from the history of race and racism to the psychological impact of racism on people of color. In 2021, we expanded it from a one-day program to a week full of programming with topics ranging from antiracism keynotes, inclusive people management workshops and inclusive recruiting practices.\nHolding ourselves accountable to an inclusive culture\nIncreasing awareness and skill-building is valuable, but it is not enough. We also have to hold ourselves accountable by analyzing data, setting goals and measuring progress objectively. Each year we set company-wide goals around our diversity, and for the last few years we’ve added individual goals for managers — one focused on building a more diverse team, and one focused on building an inclusive team culture.\nWe also place a high value on behaviors at Cloudflare. This is imperative because we believe that culture is defined by the behaviors we reward. So in order to have a healthy and inclusive culture, we must reward the behaviors that promote and preserve that. We have defined these behaviors as our Cloudflare Capabilities.\nWe screen for these Capabilities during our interview process, and they are used in performance and promotion conversations. We hold ourselves accountable by using a very simple formula: Performance = results + behaviors. Equally weighted.\nOur Recruiting Efforts\nSpeaking of interviewing, hiring is an important part of our diversity story. We believe that diverse teams win, and we put in a lot of effort to build diverse teams across the company. We have many team members who took unconventional paths into tech, and we believe that makes us stronger as a company. In fact, many of our job descriptions read: We realize people do not fit into neat boxes. We are looking for curious and empathetic individuals who are committed to developing themselves and learning new skills, and we are ready to help you do that. We cannot complete our mission without building a diverse and inclusive team.\nIn addition to an inclusive and expansive mindset around hiring, we also have interviews dedicated specifically to fit against our Capabilities, as well as leveraging technology and tools to help identify great talent who help to increase the diversity of our teams.\nWe have also made investments in events and partnerships that help support our diversity recruiting efforts. In August 2016, Cloudflare was one of the first companies to partner with Path Forward when it first launched its program in California. [Fun fact: that’s how I learned about Cloudflare and became interested in working here]. In Singapore, we have a similar partnership with Mums@Work.\nWe also engage with organizations and participate in events that help us reach talent from underrepresented groups. We have sponsored and spoke on stage at events like Lesbians Who Tech and Grace Hopper, where our co-founder, President and COO, Michelle Zatlyn, delivered the keynote in 2020. We regularly attend events and conferences hosted by AfroTech, Women Who Code, Girls Who Code, TAPIA, NSN, and more.\nEngaging with external communities\nOur ethos is to support and connect with external communities as well. Prior to the pandemic, when our offices were fully open and social and professional events were a thing, we regularly hosted external organizations to host events in our communal spaces. One example of such an organization is Wu Yee Children’s Services, a San Francisco Chinatown-based nonprofit that connects parents and caregivers to affordable childcare options, offers payment assistance to low-income families, and other family and community services. We were honored to host their orientation session. Another organization we hosted was Women Who Code SF. We regularly hosted their “ algorithm and interview prep” workshops, which helped women coders gain the skills they need to land good jobs in the tech industry. Unlike many of our tech company peers, we did not offer free lunch five days a week. It was important to us that our team members got out of the office and supported local businesses and restaurants. It is important that we do not isolate ourselves, but rather are part of a larger community.\nWe also believe in giving back to our local communities. Prior to COVID, Cloudflare dedicated one week every year to volunteer efforts. Coordinated across many of our large office locations, we would dedicate each day for a full week volunteering at employee-nominated, local non-profit organizations. Our participation pivoted to virtual during COVID, but we are anxious to return to in-person giving when we can.\nWhile we are proud of these efforts, it is in using Cloudflare products and services for good that is truly special. Cloudflare’s mission to help build a better Internet means we are in a unique position to help vulnerable websites, applications and services be safer, faster and more reliable online.\nA few to highlight:\nProject Galileo\nOrganizations working in the arts, human rights, civil society, journalism, or democracy, may apply for Project Galileo to get Cloudflare’s cybersecurity protection, for free. Since 2014, we’ve been leveraging our services to support vulnerable public interest web properties including, but are not limited to: minority rights organizations, human rights organizations, independent media outlets, arts groups, and democracy and voter protection programs.\nOur support of one of these organizations has blossomed over the years. We are proud to announce our partnership with The Trevor Project. Founded in 1998 by the creators of the Academy Award®-winning short film TREVOR, The Trevor Project is the leading national organization providing crisis intervention and suicide prevention services to lesbian, gay, bisexual, transgender, queer & questioning (LGBTQ) young people under 25. We support the organization through monetary donations, a partnership with our LGBTQIA+ Employee Resource Group, Proudflare, and free Cloudflare services through our Project Galileo Program.\nSince 2017, we have donated about $8 million in cybersecurity tools under Project Galileo.\nAthenian Project\nCloudflare launched the Athenian Project in 2017 to provide our highest level of cybersecurity services for free to state and local governments in the United States that run elections. The project is designed to protect these websites tied to elections including information related to voting and polling places, voter registration and sites that publish election results. And voter data from cyberattack, and keep them online. During the 2020 U.S. election, we worked closely with civil society and government agencies to share threat information that we saw targeted against these participants and protected more than 292 websites in 30 states, including the Missouri Secretary of State, Solano County in California and The Colorado Department of State.\nIn recognition that election security is a global issue, we recently announced our partnerships with the International Foundation for Electoral Systems, National Democratic Institute and International Republican Institute to extend our cybersecurity protections to election management bodies around the world, as well as organizations that support free and fair elections. We look forward to continuing our work to protect resources in the voting process and help build trust in democratic institutions around the world.\nProject Fairshot\nAround the world, governments, hospitals, and pharmacies are struggling to distribute the COVID-19 vaccine. Technical limitations are causing vaccine registration sites to crash under the load of registrations. At Cloudflare, we want to help. Cloudflare's Waiting Room feature allows organizations with more demand for a resource — be it concert tickets, new edition sneakers, or vaccines — to allow individuals to queue and then allocate access. Waiting Rooms can be deployed in front of any existing registration website without requiring code changes. As we watched the world struggle to fairly and efficiently distribute the COVID-19 vaccine we wanted to lend our technologies and expertise to help. Under Project Fair Shot, Cloudflare is providing Waiting Room to any government agency, hospital, pharmacy, or other organization facilitating the distribution of the COVID-19 vaccine for free until anyone who wants to be vaccinated can be, until at least 31-December 2021.\nWe all need to work together to get past this incredibly difficult time worldwide and are humbled to have helped so many different organizations around the world such as the County of San Luis Obispo, Verto Health, and the Ministry of Health for the Republic of Latvia, and more!\nWhy we are publishing our diversity data\nAt Cloudflare, we believe in being principled, curious and transparent. Publishing our diversity report is aligned with these values.\nWe are Principled: One of the Cloudflare Capabilities is “Do the Right Thing” — that includes long-term thinking about how we build an innovative and sustainable workforce. We have a fundamental belief that fairness is the right thing. We believe that equity is the right thing.\nWe are Curious: Creating a more diverse and sustainable workforce is hard work. We want to draw lessons from the things we try, and we want to learn from what others are trying. Sustainable communities is not a zero-sum game, and we believe we can all benefit as an active part of the broader community.\nWe believe in Transparency: For many years, we have been transparent with our team about our diversity data and our goals, and we have measured our progress regularly. Now we are taking the step to share publicly because we believe in accountability and accept the responsibility to build a diverse and sustainable workforce.\nYou can check out our Diversity, Equity, and Inclusion webpage with our diversity report here.\nWhile there is always more work to be done, we are grateful for the empathetic and curious team that makes Cloudflare what it is today. Together, we are optimistic we can build a better — and more inclusive — Internet.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nImpact Week Diversity Life @ Cloudflare Careers People",
      "markdown": "07/30/2021\n\n*   [![Janet Van Huysse](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2017/03/62620ddc05c0d0b467f1f57e8924066f.jpeg)](https://blog.cloudflare.com/author/janet-van-huysse/)\n\n9 min read\n\n![](https://blog.cloudflare.com/content/images/2021/07/Diversity-Equity-and-Inclusion.png)\n\nAt Cloudflare, we have our eyes set on an ambitious goal: to help build a better Internet. Today the company runs one of the world’s largest networks that powers approximately 25 million Internet properties. This is made possible by our 1,900 team members around the world. We believe the key to achieving our potential is to build diverse teams and create an environment where everyone can do their best work.\n\nThat is why we place a lot of value on the importance of diversity, equity and inclusion. Diversity, equity, and inclusion lead to better outcomes through improved decision-making, more innovative teams, stronger financial returns and simply a better place to work for everyone.\n\n![](https://blog.cloudflare.com/content/images/2021/07/Better-outcomes.png)\n\nTo become more diverse, equitable, and inclusive, we believe it’s important to focus on communities within and around our company.\n\n### Building internal communities at Cloudflare\n\nAt Cloudflare, like most workplaces, there are built-in communities: your direct team, your cross-functional partners and (because we take onboarding very seriously) your new hire class. These communities, especially the first two, are important to help you get your job done. But we want more than that for our team at Cloudflare. We believe that community builds connection and fosters a sense of belonging.\n\nBecause of that, we have supported the growth of over 16 Employee Resource Groups (ERG’s). We use the term ERG broadly at Cloudflare. We have many ERG’s focused on traditionally under-represented groups in tech: Afroflare (Black, African diaspora), Latinflare, and Womenflare; groups that have been historically marginalized: Proudflare (LGBTQIA+), Cloudflarents (parents and caregivers); as well as interest and affinity groups like Mindflare and Soberflare. To read more about all of our ERGs, visit our diversity, equity, and inclusion webpage or read about them on [our blog](https://blog.cloudflare.com/tag/employee-resource-groups/). In addition to creating a community of support and belonging, our ERGs also work to enhance career development of their members and contribute to the development of a more inclusive culture at Cloudflare.\n\n### Building the skills to build communities\n\nWe define an inclusive culture as one where everyone feels safe, welcome and respected with a sense of belonging. We do not leave this to chance. We make investments in training and programs to develop and deepen the skills needed to nurture and preserve inclusive communities at Cloudflare.\n\nOne of our earliest offerings was Ally Skills training. The aim of this workshop is to help build awareness of the types of behavior and language which can be harmful to inclusivity at Cloudflare, and teach simple, everyday ways to support people who are targets of systemic oppression. During the workshop, team members share strategies on how to act as allies and how to create a long-lasting, inclusive culture at Cloudflare. As the program was being rolled out, the management team did the workshop together and quickly realized these were not skills reserved for ‘allies’ but it was our expectation that this was how all of our team members treated each other. These were necessary skills to be successful at Cloudflare. As a result, we reworked some pieces of the workshop and renamed it: How We Work Together.\n\nWe have also partnered with [Paradigm IQ](https://www.paradigmiq.com/) and [Included](https://www.included.com/) to create a three-part Unconscious Bias Education Program. These workshops are a mix of eLearning and facilitated workshops where we learn about how to help mitigate unconscious bias and make our company a more welcoming and inclusive place for everyone. [tEQuitable](https://t.sidekickopen87.com/s3t/c/5/f18dQhb0S7kF8cpn71W1H9pwZ59hl3kW7_k2841CXdp3VP1kZh56kwlyW2dykbL7KQR4h101?te=W3R5hFj4cm2zwW4mKLS-4fGChZW3T3Qt83ZV6nw4mLXp1&si=8000000004382115&pi=221f4d24-b9e0-42c8-f8dc-712c84bc1631) is an additional comprehensive resource which helps us create a safe, inclusive, and equitable workplace. They provide an independent sounding board where our employees may confidentially raise a concern, access a just-in-time learning platform, and get advice from professional Ombuds. They also help us identify systemic workplace issues and provide us with actionable recommendations for how to improve our workplace culture. What we especially love about tEQuitable is that it’s all about empowering our employees with tools and resources to address issues that may be impacting them, or they may witness impacting others, so we all play an active role in maintaining and nurturing our culture.\n\nOne other program worth highlighting is our Week On: Learning and Inclusion. This program came as a response to the murder of George Floyd in the US at the end of May 2020. Our Afroflare global leaders suggested we use [Juneteenth](https://en.wikipedia.org/wiki/Juneteenth) as a full-day of deep learning from external experts on topics ranging from the history of race and racism to the psychological impact of racism on people of color. In 2021, we expanded it from a one-day program to a week full of programming with topics ranging from antiracism keynotes, inclusive people management workshops and inclusive recruiting practices.\n\n### Holding ourselves accountable to an inclusive culture\n\nIncreasing awareness and skill-building is valuable, but it is not enough. We also have to hold ourselves accountable by analyzing data, setting goals and measuring progress objectively. Each year we set company-wide goals around our diversity, and for the last few years we’ve added individual goals for managers — one focused on building a more diverse team, and one focused on building an inclusive team culture.\n\nWe also place a high value on behaviors at Cloudflare. This is imperative because we believe that culture is defined by the behaviors we reward. So in order to have a healthy and inclusive culture, we must reward the behaviors that promote and preserve that. We have defined these behaviors as our Cloudflare Capabilities.\n\n![](https://blog.cloudflare.com/content/images/2021/07/image3-14.png)\n\nWe screen for these Capabilities during our interview process, and they are used in performance and promotion conversations. We hold ourselves accountable by using a very simple formula: Performance = results + behaviors. Equally weighted.\n\n### Our Recruiting Efforts\n\nSpeaking of interviewing, hiring is an important part of our diversity story. We believe that diverse teams win, and we put in a lot of effort to build diverse teams across the company. We have many team members who took unconventional paths into tech, and we believe that makes us stronger as a company. In fact, many of our job descriptions read: _We realize people do not fit into neat boxes. We are looking for curious and empathetic individuals who are committed to developing themselves and learning new skills, and we are ready to help you do that. We cannot complete our mission without building a diverse and inclusive team._\n\nIn addition to an inclusive and expansive mindset around hiring, we also have interviews dedicated specifically to fit against our Capabilities, as well as leveraging technology and tools to help identify great talent who help to increase the diversity of our teams.\n\nWe have also made investments in events and partnerships that help support our diversity recruiting efforts. In August 2016, Cloudflare was one of the first companies to partner with [Path Forward](https://www.pathforward.org/) when it first launched its program in California. \\[[Fun fact](https://blog.cloudflare.com/discovering-great-talent-with-path-forward/): that’s how I learned about Cloudflare and became interested in working here\\]. In Singapore, we have a similar partnership with [Mums@Work](https://www.mumsatwork.net/).\n\nWe also engage with organizations and participate in events that help us reach talent from underrepresented groups. We have sponsored and spoke on stage at events like Lesbians Who Tech and Grace Hopper, where our co-founder, President and COO, Michelle Zatlyn, delivered the keynote in 2020. We regularly attend events and conferences hosted by AfroTech, Women Who Code, Girls Who Code, TAPIA, NSN, and more.\n\n### Engaging with external communities\n\nOur ethos is to support and connect with external communities as well. Prior to the pandemic, when our offices were fully open and social and professional events were a thing, we regularly hosted external organizations to host events in our communal spaces. One example of such an organization is [Wu Yee Children’s Services](https://www.wuyee.org/), a San Francisco Chinatown-based nonprofit that connects parents and caregivers to affordable childcare options, offers payment assistance to low-income families, and other family and community services. We were honored to host their orientation session. Another organization we hosted was [Women Who Code SF](https://www.womenwhocode.com/sf). We regularly hosted their “ algorithm and interview prep” workshops, which helped women coders gain the skills they need to land good jobs in the tech industry. Unlike many of our tech company peers, we did not offer free lunch five days a week. It was important to us that our team members got out of the office and supported local businesses and restaurants. It is important that we do not isolate ourselves, but rather are part of a larger community.\n\nWe also believe in giving back to our local communities. Prior to COVID, Cloudflare dedicated one week every year to volunteer efforts. Coordinated across many of our large office locations, we would dedicate each day for a full week volunteering at employee-nominated, local non-profit organizations. Our participation pivoted to virtual during COVID, but we are anxious to return to in-person giving when we can.\n\nWhile we are proud of these efforts, it is in using Cloudflare products and services for good that is truly special. Cloudflare’s mission to help build a better Internet means we are in a unique position to help vulnerable websites, applications and services be safer, faster and more reliable online.\n\nA few to highlight:\n\n### [Project Galileo](https://www.cloudflare.com/galileo/)\n\nOrganizations working in the arts, human rights, civil society, journalism, or democracy, may apply for Project Galileo to get Cloudflare’s cybersecurity protection, for free. Since 2014, we’ve been leveraging our services to support vulnerable public interest web properties including, but are not limited to: minority rights organizations, human rights organizations, independent media outlets, arts groups, and democracy and voter protection programs.\n\nOur support of one of these organizations has blossomed over the years. We are proud to announce our partnership with [The Trevor Project](https://www.thetrevorproject.org/). Founded in 1998 by the creators of the Academy Award®-winning short film TREVOR, The Trevor Project is the leading national organization providing crisis intervention and suicide prevention services to lesbian, gay, bisexual, transgender, queer & questioning (LGBTQ) young people under 25. We support the organization through monetary donations, a partnership with our LGBTQIA+ Employee Resource Group, Proudflare, and free Cloudflare services through our Project Galileo Program.\n\nSince 2017, we have donated about $8 million in cybersecurity tools under Project Galileo.\n\n### [Athenian Project](https://www.cloudflare.com/athenian/)\n\nCloudflare launched the Athenian Project in 2017 to provide our highest level of [cybersecurity](https://www.cloudflare.com/learning/security/what-is-cyber-security/) services for free to state and local governments in the United States that run elections. The project is designed to protect these websites tied to elections including information related to voting and polling places, voter registration and sites that publish election results. And voter data from cyberattack, and keep them online. During the 2020 U.S. election, [we worked closely](https://blog.cloudflare.com/2020-us-election-cybersecurity-analysis/) with civil society and government agencies to share threat information that we saw targeted against these participants and protected more than 292 websites in 30 states, including the [Missouri Secretary of State](https://www.cloudflare.com/case-studies/missouri-secretary-of-state/), [Solano County in California](https://www.cloudflare.com/case-studies/solano-county/) and [The Colorado Department of State](https://www.cloudflare.com/election-security/).\n\nIn recognition that election security is a global issue, we recently announced our partnerships with the International Foundation for Electoral Systems, National Democratic Institute and International Republican Institute to extend our cybersecurity protections to election management bodies around the world, as well as organizations that support free and fair elections. We look forward to continuing our work to protect resources in the voting process and help build trust in democratic institutions around the world.\n\n### [Project Fairshot](https://www.cloudflare.com/fair-shot/)\n\nAround the world, governments, hospitals, and pharmacies are struggling to distribute the COVID-19 vaccine. Technical limitations are causing vaccine registration sites to crash under the load of registrations. At Cloudflare, we want to help. Cloudflare's Waiting Room feature allows organizations with more demand for a resource — be it concert tickets, new edition sneakers, or vaccines — to allow individuals to queue and then allocate access. Waiting Rooms can be deployed in front of any existing registration website without requiring code changes. As we watched the world struggle to fairly and efficiently distribute the COVID-19 vaccine we wanted to lend our technologies and expertise to help. Under Project Fair Shot, Cloudflare is providing Waiting Room to any government agency, hospital, pharmacy, or other organization facilitating the distribution of the COVID-19 vaccine for free until anyone who wants to be vaccinated can be, until at least 31-December 2021.\n\nWe all need to work together to get past this incredibly difficult time worldwide and are humbled to have helped so many different organizations around the world such as the [County of San Luis Obispo](https://www.cloudflare.com/case-studies/county-of-san-luis-obispo/), [Verto Health](https://www.cloudflare.com/case-studies/verto/), and the [Ministry of Health for the Republic of Latvia](https://www.cloudflare.com/case-studies/latvia-ministry-of-health/), and [more](https://www.cloudflare.com/case-studies/?product=Waiting+Room)!\n\n### Why we are publishing our diversity data\n\nAt Cloudflare, we believe in being principled, curious and transparent. Publishing our diversity report is aligned with these values.\n\nWe are Principled: One of the Cloudflare Capabilities is “Do the Right Thing” — that includes long-term thinking about how we build an innovative and sustainable workforce. We have a fundamental belief that fairness is the right thing. We believe that equity is the right thing.\n\nWe are Curious: Creating a more diverse and sustainable workforce is hard work. We want to draw lessons from the things we try, and we want to learn from what others are trying. Sustainable communities is not a zero-sum game, and we believe we can all benefit as an active part of the broader community.\n\nWe believe in Transparency: For many years, we have been transparent with our team about our diversity data and our goals, and we have measured our progress regularly. Now we are taking the step to share publicly because we believe in accountability and accept the responsibility to build a diverse and sustainable workforce.\n\nYou can check out our Diversity, Equity, and Inclusion webpage with our diversity report [here](https://www.cloudflare.com/diversity-equity-and-inclusion/).\n\nWhile there is always more work to be done, we are grateful for the empathetic and curious team that makes Cloudflare what it is today. Together, we are optimistic we can build a better — and more inclusive — Internet.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Impact Week](https://blog.cloudflare.com/tag/impact-week/) [Diversity](https://blog.cloudflare.com/tag/diversity/) [Life @ Cloudflare](https://blog.cloudflare.com/tag/life-at-cloudflare/) [Careers](https://blog.cloudflare.com/tag/careers/) [People](https://blog.cloudflare.com/tag/people/)"
    },
    {
      "url": "https://blog.cloudflare.com/who-ddosd-austin/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/who-ddosd-austin/",
        "loadedTime": "2023-12-05T02:35:30.430Z",
        "referrerUrl": "https://blog.cloudflare.com/steve-bray-why-i-joined-cloudflare/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/who-ddosd-austin/",
        "title": "Dogfooding Magic Transit by DDoSing our Austin office",
        "description": "Dogfooding Magic Transit served as a valuable experiment for us with lots of lessons learned both from the engineering and procedural aspects.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Who DDoS'd Austin?\n10/24/2019\n6 min read\nIt was a scorching Monday on July 22 as temperatures soared above 37°C (99°F) in Austin, TX, the live music capital of the world. Only hours earlier, the last crowds dispersed from the historic East 6th Street entertainment district. A few blocks away, Cloudflarians were starting to make their way to the office. Little did those early arrivers know that they would soon be unknowingly participating in a Cloudflare time honored tradition of dogfooding new services before releasing them to the wild.\nEast 6th Street, Austin Texas\n(A photo I took on a night out with the team while visiting the Cloudflare Austin office)\nDogfooding is when an organization uses its own products. In this case, we dogfed our newest cloud service, Magic Transit, which both protects and accelerates our customers’ entire network infrastructure—not just their web properties or TCP/UDP applications. With Magic Transit, Cloudflare announces your IP prefixes via BGP, attracts (routes) your traffic to our global network edge, blocks bad packets, and delivers good packets to your data centers via Anycast GRE.\nWe decided to use Austin’s network because we wanted to test the new service on a live network with real traffic from real people and apps. With the target identified, we began onboarding the Austin office in an always-on routing topology. \nIn an always-on routing mode, Cloudflare data centers constantly advertise Austin’s prefix, resulting in faster, almost immediate mitigation. As opposed to traditional on-demand scrubbing center solutions with limited networks, Cloudflare operates within 100 milliseconds of 99% of the Internet-connected population in the developed world. For our customers, this means that always-on DDoS mitigation doesn’t sacrifice performance due to suboptimal routing. On the contrary, Magic Transit can actually improve your performance due to our network’s reach.\nCloudflare’s Global Network\nDDoS’ing Austin\nNow that we’ve completed onboarding Austin to Magic Transit, all we needed was a motivated attacker to launch a DDoS attack. Luckily, we found more than a few willing volunteers on our Site Reliability Engineering (SRE) team to execute the attack. While the teams were still assembling in multiple locations around the world, our SRE volunteer started firing packets at our target from an undisclosed location.\nWithout Magic Transit, the Austin office would’ve been hit directly with the packet flood. Two things could have happened in this case (not mutually exclusive): \nAustin’s on-premise equipment (routers, firewalls, servers, etc.) would have been overwhelmed and failed\nAustin’s service providers would have dropped packets that exceeded its bandwidth allowance\nBoth cases would result in a very bad day for everyone.\nCloudflare DDoS Mitigation\nInstead, when our SRE attacker launched the flood the packets were automatically routed via BGP to Cloudflare’s network. The packets reached the closest data center via Anycast and encountered multiple defenses in the form of XDP, eBPF and iptables. Those defenses are populated with pre-configured static firewall rules as well as dynamic rules generated by our DDoS mitigation systems. \nStatic rules can vary from straightforward IP blocking and rate-limiting to more sophisticated expressions that match against specific packet attributes. Dynamic rules, on the other hand, are generated automatically in real-time. To play fair with our attacker, we didn’t pre-configure any special rules against the attack. We wanted to give our attacker a fair opportunity to take Austin down. Although due to our multi-layered protection approach, the odds were never actually in their favor.\nSource: https://imgflip.com\nGenerating Dynamic Rules\nAs part of our multi-layered protection approach, Dynamic Rules are generated on-the-fly by analyzing the packets that route through our network. While the packets are being routed, flow data is asynchronously sampled, collected, and analyzed by two main detection systems. The first is called Gatebot and runs across the entire Cloudflare network; the second is our newly deployed DoSD (denial of service daemon) which operates locally within each data center. DoSD is an exciting improvement that we’ve just recently rolled out and we look forward to writing more about its technical details here soon. DoSD samples at a much faster rate (1/100 packets) versus Gatebot which samples at a lower rate (~1/8000 packets), allowing it to detect even more attacks and block them faster.\nThe asynchronous attack detection lifecycle is represented as the dotted lines in the diagram below. Attacks are detected out of path to assure that we don’t add any latency, and mitigation rules are pushed in line and removed as needed.\nMultiple packet attributes and correlations are taken into consideration during analysis and detection. Gatebot and DoSD search for both new network anomalies and already known attacks. Once an attack is detected, rules are automatically generated, propagated, and applied in the optimal location within 10 seconds or less. Just to give you an idea of the scale, we’re talking about hundreds of thousands of dynamic rules that are applied and removed every second across the entire Cloudflare network. \nOne of the beauties of Gatebot and DoSD is that they don’t require a traffic learning period. Once a customer is onboarded, they’re protected immediately. They don’t need to sample traffic for weeks before kicking in. While we can always apply specific firewall rules if requested by the customer, no manual configuration is required by the customer or our teams. It just works.\nWhat this mitigation process looks like in practice\nLet’s look at what happened in Austin when one of our SREs tried to DDoS Austin and failed. During one of the first attempts, before DoSD had rolled out globally, a degradation in audio and video quality was noticed for Austin employees on video calls for a few seconds before Gatebot kicked in. However, as soon as Gatebot kicked in, the quality was immediately restored. If we hadn’t had Magic Transit in-line, the degradation of service would’ve worsened until the point of full denial of service. Austin would have been offline and our Austin colleagues wouldn’t have had a very productive day.\nOn a subsequent attack attempt which took place after DoSD was deployed, our SRE launched a SYN flood on Austin. The attack targeted multiple IP addresses in Austin’s prefix and peaked just above 250,000 packets per second. DoSD detected the attack and blocked it in approximately 3 seconds. DoSD’s quick response resulted in no degradation of service for the Austin team. \nAttack Snapshot\nGreen line = Attack traffic to Cloudflare edge, Yellow line = clean traffic from Cloudflare to origin over GRE\nWhat We Learned\nDogfooding Magic Transit served as a valuable experiment for us with lots of lessons learned both from the engineering and procedural aspects. From the engineering aspect, we fine-tuned our mitigations and optimized routings. From the procedural aspects, we drilled members of multiple teams including the Security Operations Center and Solution Engineering teams to help refine our run-books. By doing so, we reduced the onboarding duration to hours instead of days in order to assure a quick and smooth onboarding experience for our customers.\nWant To Learn More?\nRequest a demo and learn how you can protect and accelerate your network with Cloudflare.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nMagic Transit DDoS Attacks Texas Austin \nRelated Posts\nApril 12, 2022 2:12PM\nDDoS Attack Trends for 2022 Q1\nWelcome to our first DDoS report of 2022, and the ninth in total so far. This report includes new data points and insights both in the application-layer and network-layer sections — as observed across the global Cloudflare network between January and March 2022...\nBy \nJanuary 10, 2022 1:58PM\nDDoS Attack Trends for Q4 2021\nIn Q4, we observed a 95% increase in L3/4 DDoS attacks and record-breaking levels of Ransom DDoS attacks. The Manufacturing industry was the most targeted alongside a 5,800% increase in SNMP-based DDoS attacks and massive campaigns against VoIP providers around the world...\nBy \nApril 25, 2017 8:45AM\nEcommerce websites on Cloudflare: best practices\nCloudflare provides numerous benefits to ecommerce sites, including advanced DDOS protection and an industry-leading Web Application Firewall (WAF) that helps secure your transactions and protect customers’ private data....\nBy \nNovember 09, 2021 12:59PM\nA Brief History of the Meris Botnet\nOver the past months, we’ve been tracking and analyzing the activity of the Meris botnet....\nBy",
      "markdown": "## Who DDoS'd Austin?\n\n10/24/2019\n\n*   [![Omer Yoachimik](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/06/Screenshot-2023-06-02-at-9.38.13-AM.png)](https://blog.cloudflare.com/author/omer/)\n\n6 min read\n\nIt was a scorching Monday on July 22 as temperatures soared above 37°C (99°F) in Austin, TX, the live music capital of the world. Only hours earlier, the last crowds dispersed from the historic East 6th Street entertainment district. A few blocks away, Cloudflarians were starting to make their way to the office. Little did those early arrivers know that they would soon be unknowingly participating in a Cloudflare time honored tradition of dogfooding new services before releasing them to the wild.\n\n## East 6th Street, Austin Texas\n\n![](https://blog.cloudflare.com/content/images/2019/10/pasted-image-0.png)\n\n(A photo I took on a night out with the team while visiting the Cloudflare Austin office)\n\n[Dogfooding](https://en.wikipedia.org/wiki/Eating_your_own_dog_food) is when an organization uses its own products. In this case, we dogfed our newest cloud service, [Magic Transit,](https://blog.cloudflare.com/magic-transit/) which both protects and accelerates our customers’ entire network infrastructure—not just their [web properties](https://www.cloudflare.com/en-gb/waf/) or [TCP/UDP applications](https://www.cloudflare.com/products/cloudflare-spectrum/). With Magic Transit, Cloudflare announces your IP prefixes via BGP, attracts (routes) your traffic to our [global network](https://www.cloudflare.com/network/) edge, blocks bad packets, and delivers good packets to your data centers via [Anycast GRE](https://blog.cloudflare.com/magic-transit-network-functions/#gre-anycast-magic).\n\n![](https://blog.cloudflare.com/content/images/2019/10/Modern-Arch_Diagram@3x--1-.png)\n\nWe decided to use Austin’s network because we wanted to test the new service on a live network with real traffic from real people and apps. With the target identified, we began onboarding the Austin office in an always-on routing topology.\n\nIn an always-on routing mode, Cloudflare data centers constantly advertise Austin’s prefix, resulting in faster, almost immediate mitigation. As opposed to traditional on-demand scrubbing center solutions with limited networks, Cloudflare operates within 100 milliseconds of 99% of the Internet-connected population in the developed world. For our customers, this means that always-on DDoS mitigation doesn’t sacrifice performance due to suboptimal routing. On the contrary, Magic Transit can actually _improve_ your performance due to our network’s reach.\n\n## Cloudflare’s Global Network\n\n![](https://blog.cloudflare.com/content/images/2019/10/image29.png)\n\n## DDoS’ing Austin\n\nNow that we’ve completed onboarding Austin to Magic Transit, all we needed was a motivated attacker to launch a [DDoS attack](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/). Luckily, we found more than a few willing volunteers on our Site Reliability Engineering (SRE) team to execute the attack. While the teams were still assembling in multiple locations around the world, our SRE volunteer started firing packets at our target from an undisclosed location.\n\n![](https://blog.cloudflare.com/content/images/2019/10/image-7-1.png)\n\nWithout Magic Transit, the Austin office would’ve been hit directly with the packet flood. Two things could have happened in this case (not mutually exclusive):\n\n1.  Austin’s on-premise equipment (routers, firewalls, servers, etc.) would have been overwhelmed and failed\n2.  Austin’s service providers would have dropped packets that exceeded its bandwidth allowance\n\nBoth cases would result in a very bad day for everyone.\n\n## Cloudflare DDoS Mitigation\n\nInstead, when our SRE attacker launched the flood the packets were automatically [routed via BGP](https://blog.cloudflare.com/magic-transit-network-functions/) to Cloudflare’s network. The packets reached the closest data center via [Anycast](https://en.wikipedia.org/wiki/Anycast) and encountered multiple defenses in the form of [XDP, eBPF and iptables](https://blog.cloudflare.com/l4drop-xdp-ebpf-based-ddos-mitigations/). Those defenses are populated with pre-configured static firewall rules as well as dynamic rules generated by our DDoS mitigation systems.\n\nStatic rules can vary from straightforward IP blocking and rate-limiting to more sophisticated expressions that match against specific packet attributes. Dynamic rules, on the other hand, are generated automatically in real-time. To play fair with our attacker, we didn’t pre-configure any special rules against the attack. We wanted to give our attacker a fair opportunity to take Austin down. Although due to our multi-layered protection approach, the odds were never actually in their favor.\n\n![](https://blog.cloudflare.com/content/images/2019/10/pasted-image-0--1-.png)\n\nSource: https://imgflip.com\n\n## Generating Dynamic Rules\n\nAs part of our multi-layered protection approach, Dynamic Rules are generated on-the-fly by analyzing the packets that route through our network. While the packets are being routed, flow data is asynchronously sampled, collected, and analyzed by two main detection systems. The first is called [Gatebot](https://blog.cloudflare.com/meet-gatebot-a-bot-that-allows-us-to-sleep/) and runs across the entire Cloudflare network; the second is our newly deployed DoSD (denial of service daemon) which operates locally within each data center. DoSD is an exciting improvement that we’ve just [recently rolled out](https://www.cloudflare.com/whats-new/) and we look forward to writing more about its technical details here soon. DoSD samples at a much faster rate (1/100 packets) versus Gatebot which samples at a lower rate (~1/8000 packets), allowing it to detect even more attacks and block them faster.\n\nThe asynchronous attack detection lifecycle is represented as the dotted lines in the diagram below. Attacks are detected out of path to assure that we don’t add any latency, and mitigation rules are pushed in line and removed as needed.\n\n![](https://blog.cloudflare.com/content/images/2019/10/pasted-image-0--2-.png)\n\nMultiple packet attributes and correlations are taken into consideration during analysis and detection. Gatebot and DoSD search for both new network anomalies and already known attacks. Once an attack is detected, rules are automatically generated, propagated, and applied in the optimal location within 10 seconds or less. Just to give you an idea of the scale, we’re talking about hundreds of thousands of dynamic rules that are applied and removed every second across the entire Cloudflare network.\n\nOne of the beauties of Gatebot and DoSD is that they don’t require a traffic learning period. Once a customer is onboarded, they’re protected immediately. They don’t need to sample traffic for weeks before kicking in. While we can always apply specific firewall rules if requested by the customer, no manual configuration is required by the customer or our teams. It just works.\n\n## What this mitigation process looks like in practice\n\nLet’s look at what happened in Austin when one of our SREs tried to DDoS Austin and failed. During one of the first attempts, before DoSD had rolled out globally, a degradation in audio and video quality was noticed for Austin employees on video calls for a few seconds before Gatebot kicked in. However, as soon as Gatebot kicked in, the quality was immediately restored. If we hadn’t had Magic Transit in-line, the degradation of service would’ve worsened until the point of full denial of service. Austin would have been offline and our Austin colleagues wouldn’t have had a very productive day.\n\nOn a subsequent attack attempt which took place after DoSD was deployed, our SRE launched a [SYN flood](https://www.cloudflare.com/learning/ddos/syn-flood-ddos-attack/) on Austin. The attack targeted multiple IP addresses in Austin’s prefix and peaked just above 250,000 packets per second. DoSD detected the attack and blocked it in approximately 3 seconds. DoSD’s quick response resulted in no degradation of service for the Austin team.\n\n## Attack Snapshot\n\n![](https://blog.cloudflare.com/content/images/2019/10/image-6-1.png)\n\nGreen line = Attack traffic to Cloudflare edge, Yellow line = clean traffic from Cloudflare to origin over GRE\n\n## What We Learned\n\nDogfooding Magic Transit served as a valuable experiment for us with lots of lessons learned both from the engineering and procedural aspects. From the engineering aspect, we fine-tuned our mitigations and optimized routings. From the procedural aspects, we drilled members of multiple teams including the [Security Operations Center](https://www.cloudflare.com/learning/security/glossary/what-is-a-security-operations-center-soc/) and Solution Engineering teams to help refine our run-books. By doing so, we reduced the onboarding duration to hours instead of days in order to assure a quick and smooth onboarding experience for our customers.\n\n## Want To Learn More?\n\n[Request](https://www.cloudflare.com/magic-transit/) a demo and learn how you can protect and accelerate your network with Cloudflare.  \n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Magic Transit](https://blog.cloudflare.com/tag/magic-transit/) [DDoS](https://blog.cloudflare.com/tag/ddos/) [Attacks](https://blog.cloudflare.com/tag/attacks/) [Texas](https://blog.cloudflare.com/tag/texas/) [Austin](https://blog.cloudflare.com/tag/austin/)\n\nRelated Posts\n\nApril 12, 2022 2:12PM\n\n[\n\n## DDoS Attack Trends for 2022 Q1\n\n](https://blog.cloudflare.com/ddos-attack-trends-for-2022-q1/)\n\nWelcome to our first DDoS report of 2022, and the ninth in total so far. This report includes new data points and insights both in the application-layer and network-layer sections — as observed across the global Cloudflare network between January and March 2022...\n\nBy \n\nJanuary 10, 2022 1:58PM\n\n[\n\n## DDoS Attack Trends for Q4 2021\n\n](https://blog.cloudflare.com/ddos-attack-trends-for-2021-q4/)\n\nIn Q4, we observed a 95% increase in L3/4 DDoS attacks and record-breaking levels of Ransom DDoS attacks. The Manufacturing industry was the most targeted alongside a 5,800% increase in SNMP-based DDoS attacks and massive campaigns against VoIP providers around the world...\n\nBy \n\nApril 25, 2017 8:45AM\n\n[\n\n## Ecommerce websites on Cloudflare: best practices\n\n](https://blog.cloudflare.com/ecommerce-best-practices/)\n\nCloudflare provides numerous benefits to ecommerce sites, including advanced DDOS protection and an industry-leading Web Application Firewall (WAF) that helps secure your transactions and protect customers’ private data....\n\nBy \n\nNovember 09, 2021 12:59PM\n\n[\n\n## A Brief History of the Meris Botnet\n\n](https://blog.cloudflare.com/meris-botnet/)\n\nOver the past months, we’ve been tracking and analyzing the activity of the Meris botnet....\n\nBy"
    },
    {
      "url": "https://blog.cloudflare.com/how-the-us-paused-shopping-and-browsing-for-thanksgiving/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/how-the-us-paused-shopping-and-browsing-for-thanksgiving/",
        "loadedTime": "2023-12-05T02:35:38.151Z",
        "referrerUrl": "https://blog.cloudflare.com/do-hackers-eat-turkey-and-other-thanksgiving-internet-trends/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/how-the-us-paused-shopping-and-browsing-for-thanksgiving/",
        "title": "How the US paused shopping (and browsing) for Thanksgiving",
        "description": "When we announced Cloudflare Radar, back in September 2020, we explained how Internet use follows patterns that humans create. Throughout the pandemic we saw different trends caused by people being more at home than usual, but Internet patterns also change at specific times of the year (like when students go back to school or when it’s colder outside) or on some holidays like Thanksgiving.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/26/2021\n4 min read\nSo, if you like to keep up with the tradition in the United States you and your family yesterday (November 25, 2021) celebrated Thanksgiving. So on a special day, with family gatherings for many and with a lot of cooking if you’re into the tradition (roast turkey, stuffing and pumpkin pie), it makes sense that different Internet patterns show up on Cloudflare Radar.\nFirst, let’s look at shopping habits. After a busy Monday, Tuesday and Wednesday, online shopping paused for Thanksgiving Day and dipped at lunchtime. So in a very good week for e-Commerce, Thanksgiving was an exception, especially at the extended lunchtime.\nNow, let’s focus on Internet traffic at the time of the Thanksgiving Dinner. First, what time is that? Every family is different, but a 2018 survey of US consumers showed that for 42% early afternoon (between 13:00 and 15:00 is the preferred time to sit at the table and start to dig in). But 16:00 seems to be the “correct time” — The Atlantic explains why.\nCloudflare Radar shows that Internet traffic in the US increased this past seven days, compared with the previous period, and that makes sense given that it’s traditionally a good week for online shopping. But we can also see in the next chart that the time of Thanksgiving dinner in the continental US was a clear exception.\nThe circle in red in the chart clearly shows us that yesterday afternoon in the US the Internet traffic was a lot slower than the previous days at the same time and that’s more evident between 21:00 and 01:00 UTC (we use that as a standard timezone in Radar). That time period is “translated” for the East Coast between 16:00 and 20:00 EST and for the West Coast the time between 13:00 to 17:00 PST.\nInternet traffic is going up\nWe can also use Cloudflare Radar to see that in the last two weeks Internet traffic in the US has been increasing (compared with the same period of the previous month). In a time of the year when temperatures go down, Internet traffic was definitely going up. That’s more evident this week after Sunday, November 21.\nThe biggest spike of the last 30 days, so far (maybe today, Black Friday, November 26, will change that — you can see the live trends using Radar), was definitely in the evening of Monday, November 22 (~02:00 UTC, November 23). This past Tuesday night, November 23, was the second day of the month with the highest traffic in the US, and the third day was actually the next day (Wednesday, November 24).\nWe can also see on Radar (represented in the next chart) that after several days of the Internet traffic peaks being reached at around 02:00 UTC (which “translates” to 21:00 EST and 18:00 PST), this Thanksgiving Day it was reached later, at about 03:00 UTC (22:00 EST and 19:00 PST).\nMobile traffic goes up in the busiest online week of the month\nAnother interesting trend regarding Thanksgiving week in the US is how there are more people this week using mobile devices to access the Internet than in the previous weeks.\nYesterday, November 25, mobile traffic represented 54% of the Internet traffic in the US. That’s 8% more than the usual 46% of mobile traffic percentage in the US that we registered in the last 30 days — in the last seven days that number goes up to 49%.\nActually, back in October when we blogged about the popularity of mobile traffic in the world, only 42% of the Internet traffic in the US was made using mobile devices.\nWe can actually see this more clearly when we only focus on mobile traffic. This past Thursday, Thanksgiving Day, the mobile traffic percentage in the US increased by about 6% compared to the previous week:\nConclusion\nWhen we announced Cloudflare Radar, back in September 2020, we explained how Internet use follows patterns that humans create. Throughout the pandemic we saw different trends caused by people being more at home than usual, but Internet patterns also change at specific times of the year (like when students go back to school or when it’s colder outside) or on some holidays like Thanksgiving.\nLike we saw in the US yesterday, a holiday can affect Internet traffic as a whole, but also the time of the day we are online most, the devices we use to access the Internet and the types of websites we visit (e-commerce websites are getting an increase in traffic this week).\nAnd remember: you can keep an eye on Cloudflare Radar to monitor how we see Internet traffic globally and in every country.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nCloudflare Radar Thanksgiving Internet Traffic \nRelated Posts\nApril 12, 2022 2:12PM\nDDoS Attack Trends for 2022 Q1\nWelcome to our first DDoS report of 2022, and the ninth in total so far. This report includes new data points and insights both in the application-layer and network-layer sections — as observed across the global Cloudflare network between January and March 2022...\nBy \nJanuary 10, 2022 1:58PM\nDDoS Attack Trends for Q4 2021\nIn Q4, we observed a 95% increase in L3/4 DDoS attacks and record-breaking levels of Ransom DDoS attacks. The Manufacturing industry was the most targeted alongside a 5,800% increase in SNMP-based DDoS attacks and massive campaigns against VoIP providers around the world...\nBy \nNovember 09, 2021 12:59PM\nA Brief History of the Meris Botnet\nOver the past months, we’ve been tracking and analyzing the activity of the Meris botnet....\nBy \nNovember 04, 2021 12:58PM\nDDoS Attack Trends for Q3 2021\nIn Q3, 2021 we saw and mitigated record-setting HTTP DDoS attacks, terabit-strong network layer attacks, one of the largest botnets ever deployed (Meris), and more recently, ransom attacks on Voice-over-IP (VoIP) service providers....\nBy",
      "markdown": "11/26/2021\n\n*   [![João Tomé](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/V0x3WKfJ_400x400-1.jpeg)](https://blog.cloudflare.com/author/joao-tome/)\n\n4 min read\n\nSo, if you like to keep up with the tradition in the United States you and your family yesterday (November 25, 2021) celebrated [Thanksgiving](https://en.wikipedia.org/wiki/Thanksgiving_(United_States)). So on a special day, with family gatherings for many and with a lot of cooking if you’re into the tradition (roast turkey, stuffing and pumpkin pie), it makes sense that different Internet patterns show up on [Cloudflare Radar](https://radar.cloudflare.com/us).\n\nFirst, let’s look at shopping habits. After a busy Monday, Tuesday and Wednesday, online shopping paused for Thanksgiving Day and dipped at lunchtime. So in a very good week for [e-Commerce](https://www.cloudflare.com/ecommerce/), Thanksgiving was an exception, especially at the extended lunchtime.\n\n![](https://blog.cloudflare.com/content/images/2021/11/cloudflare-radar-special-event-blackfriday-2021-2021-11-26T15_45_09.863Z.png)\n\nNow, let’s focus on Internet traffic at the time of the Thanksgiving Dinner. First, what time is that? Every family is different, but a 2018 survey of US consumers showed that for 42% early afternoon (between 13:00 and 15:00 is the preferred time to sit at the table and start to dig in). But 16:00 seems to be the “correct time” — [The Atlantic](https://www.theatlantic.com/family/archive/2018/11/when-thanksgiving-dinner/576274/) explains why.\n\nCloudflare Radar shows that Internet traffic in the US increased this past seven days, compared with the previous period, and that makes sense given that it’s traditionally a good week for online shopping. But we can also see in the next chart that the time of Thanksgiving dinner in the continental US was a clear exception.\n\n![](https://blog.cloudflare.com/content/images/2021/11/image.png)\n\nThe circle in red in the chart clearly shows us that yesterday afternoon in the US the Internet traffic was a lot slower than the previous days at the same time and that’s more evident between 21:00 and 01:00 UTC (we use that as a standard timezone in Radar). That time period is “translated” for the East Coast between 16:00 and 20:00 EST and for the West Coast the time between 13:00 to 17:00 PST.\n\n### Internet traffic is going up\n\nWe can also use Cloudflare Radar to see that in the last two weeks Internet traffic in the US has been increasing (compared with the same period of the previous month). In a time of the year when temperatures go down, Internet traffic was definitely going up. That’s more evident this week after Sunday, November 21.\n\n![](https://blog.cloudflare.com/content/images/2021/11/image5-16.png)\n\nThe biggest spike of the last 30 days, so far (maybe today, Black Friday, November 26, will change that — you can see the live trends using [Radar](https://radar.cloudflare.com/us)), was definitely in the evening of Monday, November 22 (~02:00 UTC, November 23). This past Tuesday night, November 23, was the second day of the month with the highest traffic in the US, and the third day was actually the next day (Wednesday, November 24).\n\n![](https://blog.cloudflare.com/content/images/2021/11/image6-15.png)\n\nWe can also see on Radar (represented in the next chart) that after several days of the Internet traffic peaks being reached at around 02:00 UTC (which “translates” to 21:00 EST and 18:00 PST), this Thanksgiving Day it was reached later, at about 03:00 UTC (22:00 EST and 19:00 PST).\n\n![](https://blog.cloudflare.com/content/images/2021/11/image7-16.png)\n\n### Mobile traffic goes up in the busiest online week of the month\n\nAnother interesting trend regarding Thanksgiving week in the US is how there are more people this week using mobile devices to access the Internet than in the previous weeks.\n\nYesterday, November 25, mobile traffic represented 54% of the Internet traffic in the US. That’s 8% more than the usual 46% of mobile traffic percentage in the US that we registered in the last 30 days — in the last seven days that number goes up to 49%.\n\nActually, back in October when we [blogged about the popularity of mobile traffic in the world](https://blog.cloudflare.com/where-mobile-traffic-more-and-less-popular/), only 42% of the Internet traffic in the US was made using mobile devices.\n\n![](https://blog.cloudflare.com/content/images/2021/11/image2-31.png)\n\nWe can actually see this more clearly when we only focus on mobile traffic. This past Thursday, Thanksgiving Day, the mobile traffic percentage in the US increased by about 6% compared to the previous week:\n\n![](https://blog.cloudflare.com/content/images/2021/11/image1-74.png)\n\n### Conclusion\n\nWhen we announced Cloudflare Radar, back in September 2020, we [explained how](https://blog.cloudflare.com/introducing-cloudflare-radar/) Internet use follows patterns that humans create. Throughout the pandemic we [saw different trends](https://blog.cloudflare.com/cloudflare-radar-2020-year-in-review/) caused by people being more at home than usual, but Internet patterns also change at specific times of the year (like when students go [back to school](https://blog.cloudflare.com/when-students-go-back-to-school-mobile-usage-goes-down/) or when it’s colder outside) or on some holidays like Thanksgiving.\n\nLike we saw in the US yesterday, a holiday can affect Internet traffic as a whole, but also the time of the day we are online most, the devices we use to access the Internet and the types of websites we visit (e-commerce websites are getting an increase in traffic this week).\n\nAnd remember: you can keep an eye on [Cloudflare Radar](https://radar.cloudflare.com/) to monitor how we see Internet traffic globally and in every country.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Cloudflare Radar](https://blog.cloudflare.com/tag/cloudflare-radar/) [Thanksgiving](https://blog.cloudflare.com/tag/thanksgiving/) [Internet Traffic](https://blog.cloudflare.com/tag/internet-traffic/)\n\nRelated Posts\n\nApril 12, 2022 2:12PM\n\n[\n\n## DDoS Attack Trends for 2022 Q1\n\n](https://blog.cloudflare.com/ddos-attack-trends-for-2022-q1/)\n\nWelcome to our first DDoS report of 2022, and the ninth in total so far. This report includes new data points and insights both in the application-layer and network-layer sections — as observed across the global Cloudflare network between January and March 2022...\n\nBy \n\nJanuary 10, 2022 1:58PM\n\n[\n\n## DDoS Attack Trends for Q4 2021\n\n](https://blog.cloudflare.com/ddos-attack-trends-for-2021-q4/)\n\nIn Q4, we observed a 95% increase in L3/4 DDoS attacks and record-breaking levels of Ransom DDoS attacks. The Manufacturing industry was the most targeted alongside a 5,800% increase in SNMP-based DDoS attacks and massive campaigns against VoIP providers around the world...\n\nBy \n\nNovember 09, 2021 12:59PM\n\n[\n\n## A Brief History of the Meris Botnet\n\n](https://blog.cloudflare.com/meris-botnet/)\n\nOver the past months, we’ve been tracking and analyzing the activity of the Meris botnet....\n\nBy \n\nNovember 04, 2021 12:58PM\n\n[\n\n## DDoS Attack Trends for Q3 2021\n\n](https://blog.cloudflare.com/ddos-attack-trends-for-2021-q3/)\n\nIn Q3, 2021 we saw and mitigated record-setting HTTP DDoS attacks, terabit-strong network layer attacks, one of the largest botnets ever deployed (Meris), and more recently, ransom attacks on Voice-over-IP (VoIP) service providers....\n\nBy"
    },
    {
      "url": "https://blog.cloudflare.com/a-thanksgiving-2020-reading-list/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/a-thanksgiving-2020-reading-list/",
        "loadedTime": "2023-12-05T02:35:44.948Z",
        "referrerUrl": "https://blog.cloudflare.com/do-hackers-eat-turkey-and-other-thanksgiving-internet-trends/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/a-thanksgiving-2020-reading-list/",
        "title": "A Thanksgiving 2020 Reading List",
        "description": "If you want to relax in an active and learning way this Thanksgiving weekend, here are some of the topics we’ve covered on the Cloudflare blog this past week that you may find interesting.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/28/2020\n3 min read\nWhile our colleagues in the US are celebrating Thanksgiving this week and taking a long weekend off, there is a lot going on at Cloudflare. The EMEA team is having a full day on CloudflareTV with a series of live shows celebrating #CloudflareCareersDay.\nSo if you want to relax in an active and learning way this weekend, here are some of the topics we’ve covered on the Cloudflare blog this past week that you may find interesting.\nImproving Performance and Search Rankings with Cloudflare for Fun and Profit\nMaking things fast is one of the things we do at Cloudflare. More responsive websites, apps, APIs, and networks directly translate into improved conversion and user experience. On November 10, Google announced that Google Search will directly take web performance and page experience data into account when ranking results on their search engine results pages (SERPs), beginning in May 2021.\nRustam Lalkaka and Rita Kozlov explain in this blog post how Google Search will prioritize results based on how pages score on Core Web Vitals, a measurement methodology Cloudflare has worked closely with Google to establish, and we have implemented support for in our analytics tools. Read the full blog post.\nGetting to the Core: Benchmarking Cloudflare’s Latest Server Hardware\nAt the Cloudflare Core, we process logs to analyze attacks and compute analytics. In 2020, our Core servers were in need of a refresh, so we decided to redesign the hardware to be more in line with our Gen X edge servers. We designed two major server variants for the core. The first is Core Compute 2020, an AMD-based server for analytics and general-purpose compute paired with solid-state storage drives. The second is Core Storage 2020, an Intel-based server with twelve spinning disks to run database workloads. This is a refresh of the hardware that Cloudflare uses to run analytics provided big efficiency improvements.\nRead the full blog post by Brian Bassett\nMoving Quicksilver into production\nWe previously explained how and why we built Quicksilver. Quicksilver is the data store responsible for storing and distributing the billions of KV pairs used to configure the millions of sites and Internet services which use Cloudflare. This second blog post is about the long journey to production which culminates with Kyoto Tycoon removal from Cloudflare infrastructure and points to the first signs of obsolescence.\nGeoffrey Plouviez takes you through the entire story of real-world engineering challenges and what it’s like to replace one of Cloudflare’s oldest critical components: read the full blog post here.\nBuilding Black Friday e-commerce experiences with JAMstack and Cloudflare Workers\nIn this blog post, we explore how Cloudflare Workers continues to excel as a JAMstack deployment platform, and how it can be used to power e-commerce experiences, integrating with familiar tools like Stripe, as well as new technologies like Nuxt.js, and Sanity.io.\nRead the full blog post and get all the details and open-source code from Kristian Freeman.\nA Byzantine failure in the real world\nWhen we review design documents at Cloudflare, we are always on the lookout for Single Points of Failure (SPOFs). In this post, we present a timeline of a real-world incident, and how an interesting failure mode known as a Byzantine fault played a role in a cascading series of events.\nTom Lianza and Chris Snook’s full blog post describes the consequences of a malfunctioning switch on a system built for reliability.\nASICs at the Edge\nAt Cloudflare, we pride ourselves in our global network that spans more than 200 cities in over 100 countries. To accelerate all that traffic through our network, there are multiple technologies at play. So let’s have a look at one of the cornerstones that makes all of this work.\nTom Strickx’ epic deep dive into ASICs is here.\nLet us know your thoughts and comments below or feel free to also reach out to us via our social media channels. And because we talked about careers in the beginning of this blog post, check out our available jobs if you are interested to join Cloudflare.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nCloudflare TV Careers Thanksgiving Serverless eCommerce \nRelated Posts\nJanuary 30, 2014 2:00PM\nStories from our recent global data center upgrade\nEach day at CloudFlare is full of surprises. As it turns out, it takes a lot of work to stop massive attacks and to help make the web faster. Over the past six months, our entire team has contributed in every way imaginable to more than double the capacity of our global network....\nBy \nJuly 30, 2021 2:00PM\nBuilding a sustainable workforce, through communities\nAt Cloudflare, we place a lot of value on the importance of diversity, equity and inclusion. Diversity, equity, and inclusion lead to better outcomes through improved decision-making, more innovative teams, stronger financial returns and simply a better place to work for everyone....\nBy \nJune 13, 2022 1:00AM\nCloudflare is redefining employee well-being in Japan\nCloudflare Japan is making a few important changes to our employee benefits...\nBy \nMay 20, 2022 2:00AM\nWendy Komadina: No one excited me more than Cloudflare, so I joined.\nWhen I considered joining Cloudflare, I recall consistently reading the message around “Helping to Build a Better Internet”. At first those words didn’t connect with me, but they sounded like an important mission....\nBy",
      "markdown": "11/28/2020\n\n*   [![Val Vesa](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/07/Ct5xlQ1a_400x400.jpeg)](https://blog.cloudflare.com/author/val/)\n\n3 min read\n\nWhile our colleagues in the US are celebrating Thanksgiving this week and taking a long weekend off, there is a lot going on at Cloudflare. The EMEA team is having a full day on CloudflareTV with a series of live shows celebrating [#CloudflareCareersDay](https://cfl.re/3nT7HHE).\n\nSo if you want to relax in an active and learning way this weekend, here are some of the topics we’ve covered on the Cloudflare blog this past week that you may find interesting.\n\n## Improving Performance and Search Rankings with Cloudflare for Fun and Profit\n\nMaking things fast is one of the things we do at Cloudflare. More responsive websites, apps, APIs, and networks directly translate into improved conversion and user experience. On November 10, Google announced that Google Search will directly take web performance and page experience data into account when ranking results on their search engine results pages (SERPs), beginning in May 2021.\n\nRustam Lalkaka and Rita Kozlov explain in this blog post how Google Search will prioritize results based on how pages score on Core Web Vitals, a measurement methodology Cloudflare has worked closely with Google to establish, and we have implemented support for in our analytics tools. [Read the full blog post](https://blog.cloudflare.com/improving-performance-and-search-rankings-with-cloudflare-for-fun-and-profit/).\n\n## Getting to the Core: Benchmarking Cloudflare’s Latest Server Hardware  \n\nAt the Cloudflare Core, we process logs to analyze attacks and compute analytics. In 2020, our Core servers were in need of a refresh, so we decided to redesign the hardware to be more in line with our Gen X edge servers. We designed two major server variants for the core. The first is Core Compute 2020, an AMD-based server for analytics and general-purpose compute paired with solid-state storage drives. The second is Core Storage 2020, an Intel-based server with twelve spinning disks to run database workloads. This is a refresh of the hardware that Cloudflare uses to run analytics provided big efficiency improvements.\n\n[Read the full blog post](https://blog.cloudflare.com/getting-to-the-core/) by Brian Bassett\n\n## Moving Quicksilver into production\n\nWe [previously](https://blog.cloudflare.com/introducing-quicksilver-configuration-distribution-at-internet-scale/) explained how and why we built Quicksilver. Quicksilver is the data store responsible for storing and distributing the billions of KV pairs used to configure the millions of sites and Internet services which use Cloudflare. This second blog post is about the long journey to production which culminates with Kyoto Tycoon removal from Cloudflare infrastructure and points to the first signs of obsolescence.\n\nGeoffrey Plouviez takes you through the entire story of real-world engineering challenges and what it’s like to replace one of Cloudflare’s oldest critical components: [read the full blog post here](https://blog.cloudflare.com/moving-quicksilver-into-production/).\n\n## Building Black Friday e-commerce experiences with JAMstack and Cloudflare Workers\n\nIn this blog post, we explore how Cloudflare Workers continues to excel as a JAMstack deployment platform, and how it can be used to power [e-commerce experiences](https://www.cloudflare.com/ecommerce/), integrating with familiar tools like Stripe, as well as new technologies like Nuxt.js, and Sanity.io.\n\n[Read the full blog post](https://blog.cloudflare.com/building-black-friday-e-commerce-experiences-with-jamstack-and-cloudflare-workers/) and get all the details and open-source code from Kristian Freeman.\n\n## A Byzantine failure in the real world\n\nWhen we review design documents at Cloudflare, we are always on the lookout for Single Points of Failure (SPOFs). In this post, we present a timeline of a real-world incident, and how an interesting failure mode known as a Byzantine fault played a role in a cascading series of events.\n\nTom Lianza and Chris Snook’s [full blog post](https://blog.cloudflare.com/a-byzantine-failure-in-the-real-world/) describes the consequences of a malfunctioning switch on a system built for reliability.\n\n## ASICs at the Edge\n\nAt Cloudflare, we pride ourselves in our global network that spans more than 200 cities in over 100 countries. To accelerate all that traffic through our network, there are multiple technologies at play. So let’s have a look at one of the cornerstones that makes all of this work.\n\nTom Strickx’ epic deep dive into ASICs is [here](https://blog.cloudflare.com/asics-at-the-edge/).\n\nLet us know your thoughts and comments below or feel free to also reach out to us via our social media channels. And because we talked about careers in the beginning of this blog post, check out our [available jobs](https://www.cloudflare.com/careers/) if you are interested to join Cloudflare.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Cloudflare TV](https://blog.cloudflare.com/tag/cloudflare-tv/) [Careers](https://blog.cloudflare.com/tag/careers/) [Thanksgiving](https://blog.cloudflare.com/tag/thanksgiving/) [Serverless](https://blog.cloudflare.com/tag/serverless/) [eCommerce](https://blog.cloudflare.com/tag/ecommerce/)\n\nRelated Posts\n\nJanuary 30, 2014 2:00PM\n\n[\n\n## Stories from our recent global data center upgrade\n\n](https://blog.cloudflare.com/stories-from-our-recent-global-data-center-upgrade/)\n\nEach day at CloudFlare is full of surprises. As it turns out, it takes a lot of work to stop massive attacks and to help make the web faster. Over the past six months, our entire team has contributed in every way imaginable to more than double the capacity of our global network....\n\nBy \n\nJuly 30, 2021 2:00PM\n\n[\n\n## Building a sustainable workforce, through communities\n\n](https://blog.cloudflare.com/building-a-sustainable-workforce-through-communities/)\n\nAt Cloudflare, we place a lot of value on the importance of diversity, equity and inclusion. Diversity, equity, and inclusion lead to better outcomes through improved decision-making, more innovative teams, stronger financial returns and simply a better place to work for everyone....\n\nBy \n\nJune 13, 2022 1:00AM\n\n[\n\n## Cloudflare is redefining employee well-being in Japan\n\n](https://blog.cloudflare.com/cloudflare-is-redefining-employee-well-being-in-japan/)\n\nCloudflare Japan is making a few important changes to our employee benefits...\n\nBy \n\nMay 20, 2022 2:00AM\n\n[\n\n## Wendy Komadina: No one excited me more than Cloudflare, so I joined.\n\n](https://blog.cloudflare.com/wendy-komadina-no-one-excited-me-more-than-cloudflare-so-i-joined/)\n\nWhen I considered joining Cloudflare, I recall consistently reading the message around “Helping to Build a Better Internet”. At first those words didn’t connect with me, but they sounded like an important mission....\n\nBy"
    },
    {
      "url": "https://blog.cloudflare.com/an-american-story-surviving-the-crush-of-holiday-traffic/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/an-american-story-surviving-the-crush-of-holiday-traffic/",
        "loadedTime": "2023-12-05T02:35:51.767Z",
        "referrerUrl": "https://blog.cloudflare.com/do-hackers-eat-turkey-and-other-thanksgiving-internet-trends/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/an-american-story-surviving-the-crush-of-holiday-traffic/",
        "title": "A Thanksgiving Story: Surviving the Crush of Holiday Traffic",
        "description": "This is the story of two things that are deeply American: post-Thanksgiving shopping and scrappy entrepreneurs.\nOn the fourth Thursday in November, the United States celebrates Thanksgiving. The Friday after, known as Black Friday, is considered the official start of the holiday shopping season.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/24/2011\n4 min read\nThis is the story of two things that are deeply American: post-Thanksgiving shopping and scrappy entrepreneurs.\nAn American Tradition\nOn the fourth Thursday in November, the United States celebrates Thanksgiving. The Friday after Thanksgiving, known as Black Friday, is considered the official start of the holiday shopping season. It is typically the busiest (offline) shopping day of the year. Shopping after Thanksgiving is so deeply engrained in American culture that in 1939 in the midst of the Great Depression, when Thanksgiving was scheduled to fall on the last day of the month, then-President Franklin D. Roosevelt moved the holiday up a week in order to get people out and spending more quickly.\nRetailers today are attuned to this tradition and offer specials to lure customers to their stores. People literally will camp out for a week in front of some retailers in order to be the first in line when the stores open Friday morning. If you're reading this outside the United States and think it sounds crazy, rest assured that most of us here do as well. That said, suffice it to say that there is a high demand among many for the details of the Black Friday deals.\nEnter 2011BlackFridayAds.com\nJust as American as shopping after Thanksgiving is the fact that if the market demands something, scrappy entrepreneurs will step up to meet that demand. In this case, the website 2011BlackFridayAds.com provides a one-stop resource to find out what all the biggest retailers will be offering. The site was created by Ty Price, who has been tracking Black Friday deals since 2008. This year, if you search Google for \"Black Friday 2011,\" among the more than 71 million sites returned, Ty's is the first result.\nTy had been through this before and knew the amount of traffic to his site could spike significantly in the days leading up to Black Friday. So, in late September, he signed up for CloudFlare to help manage the coming load. Because 2011BlackFridayAds.com is hosted through MediaTemple, one of CloudFlare's hosting partners, setting up our service took literally two clicks and less than a minute. The graph below shows the growth in traffic over the last month, peaking in the last 24 hours at nearly 3 million page views a day (about 35 per second).\nCloudFlare Helps Save Christmas Shopping\nTurns out a lot of people want to see Black Friday deals, and we're proud to have helped ensure 2011BlackFridayAds.com stay up and running under crushing load. CloudFlare didn't just make the site safer and protect it from attacks, we also significantly reduced the load on the servers so they could keep up with visitors' requests. In fact, CloudFlare cut the number of requests that needed to be handled by the server by nearly half a billion, or about 75% of the total load it would have experienced without CloudFlare. We also saved the site more than 23 Terabytes, 93% of the bandwidth that would have otherwise been used serving the site's pages.\nAny downtime experienced by a site is quickly punished by Google,\nmeaning that even if you have an initially popular site, if you can't stay online you can't stay on top. CloudFlare helped 2011BlackFridayAds.com stay fast and reliable, even under a crushing load, and the effect has kept it at the top of the rankings and kept the traffic growing. Speed and reliability are the foundation of SEO.\nAs Ty just wrote me via email: \"CloudFlare has done wonders for us this year. It's amazing how fast the site is.\" Helping ensure the sites of entrepreneurs like Ty could perform as well as the big companies' is why we — a scrappy group of entrepreneurs ourselves — built CloudFlare in the first place.\nSurviving Cyber Monday\nWhile Black Friday is the biggest offline retail shopping day, the Monday after Black Friday, a day now known as Cyber Monday, is reportedly the largest for online shopping. Everyone in the U.S. returns to work, still in a shopping mood, and takes any downtime at their desks to scour Internet retailers for gifts. If you're an online business, you literally can't afford to go down. And, if you're a eMerchant who finds yourself worried over the coming days that your site might fail, spend the few minutes it takes to sign up for CloudFlare. Our basic service is free (Ty uses the PRO version for\n$20/month), it doesn't require any technical skill to setup, and it will work regardless of your platform.\nThe biggest Internet retailer in the world is Amazon.com. As of today, CloudFlare powers 5x the traffic of Amazon. In other words, if you're worried about a crush of traffic to your website over the coming holiday season, we're here for you and happy to help.\nFor those camped out in front of a Walmart for Black Friday, stay warm and make sure to check 2011BlackFridayAds.com from your smart phone for a preview of the latest deals. For those watching from the sidelines, check back to these pages after Cyber Monday for reports on Internet traffic patterns we saw.\nAnd, for those in the United States, have a very Happy Thanksgiving.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\neCommerce Thanksgiving Holidays \nRelated Posts\nNovember 24, 2023 3:11PM\nDo hackers eat turkey? And other Thanksgiving Internet trends\nOffline for turkey time: Which US states logged off on Thanksgiving Day? Is there a difference between coastal and central states? Do hackers take a Thanksgiving break? Are food delivery services gaining or losing traffic? We answer those questions and more...\nBy \nNovember 28, 2020 12:00PM\nA Thanksgiving 2020 Reading List\nIf you want to relax in an active and learning way this Thanksgiving weekend, here are some of the topics we’ve covered on the Cloudflare blog this past week that you may find interesting....\nBy \nNovember 25, 2022 6:32PM\nAn early look at Thanksgiving 2022 Internet trends\nYesterday, November 24, 2022, was Thanksgiving Day in the US. Last year, we saw how the US paused shopping (and browsing) for Thanksgiving. So, how was it this year?...\nBy \nNovember 26, 2021 3:51PM\nHow the US paused shopping (and browsing) for Thanksgiving\nThroughout the pandemic we saw different trends caused by people being more at home than usual, but Internet patterns also change at specific times of the year (like when students go back to school or when it’s colder outside) or on some holidays like Thanksgiving....\nBy",
      "markdown": "11/24/2011\n\n*   [![Matthew Prince](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/06/Matthew-Prince-3.jpeg)](https://blog.cloudflare.com/author/matthew-prince/)\n\n4 min read\n\n![A Thanksgiving Story: Surviving the Crush of Holiday\nTraffic](https://blog.cloudflare.com/content/images/black_friday_2010_opening.jpg.scaled500.jpg \"A Thanksgiving Story: Surviving the Crush of Holiday Traffic\")\n\nThis is the story of two things that are deeply American: post-Thanksgiving shopping and scrappy entrepreneurs.\n\n### An American Tradition\n\nOn the fourth Thursday in November, the United States celebrates Thanksgiving. The Friday after Thanksgiving, known as Black Friday, is considered the official start of the holiday shopping season. It is typically the busiest (offline) shopping day of the year. Shopping after Thanksgiving is so deeply engrained in American culture that in 1939 in the midst of the Great Depression, when Thanksgiving was scheduled to fall on the last day of the month, then-President Franklin D. Roosevelt [moved the holiday up a week](https://en.wikipedia.org/wiki/Franksgiving) in order to get people out and spending more quickly.\n\nRetailers today are attuned to this tradition and offer specials to lure customers to their stores. People literally will [camp out for a week](https://www.google.com/search?q=line+up+for+black+friday&num=50&hl=en&newwindow=1&safe=off&prmd=imvnsu&source=univ&tbm=nws&tbo=u&sa=X&ei=EbrNToqxMsTMiQLehOWODA&ved=0CDgQqAI&biw=1093&bih=647) in front of some retailers in order to be the first in line when the stores open Friday morning. If you're reading this outside the United States and think it sounds crazy, rest assured that most of us here do as well. That said, suffice it to say that there is a high demand among many for the details of the Black Friday deals.\n\n### Enter 2011BlackFridayAds.com\n\nJust as American as shopping after Thanksgiving is the fact that if the market demands something, scrappy entrepreneurs will step up to meet that demand. In this case, the website 2011BlackFridayAds.com provides a one-stop resource to find out what all the biggest retailers will be offering. The site was created by Ty Price, who has been tracking Black Friday deals since 2008. This year, if you [search Google](https://www.google.com/search?q=Black+Friday+2011) for \"[Black Friday 2011](http://www.2011blackfridayads.com/),\" among the more than 71 million sites returned, Ty's is the first result.\n\n![A Thanksgiving Story: Surviving the Crush of Holiday Traffic](https://blog.cloudflare.com/content/images/google_result.png.scaled500.png \"A Thanksgiving Story: Surviving the Crush of Holiday Traffic\")\n\nTy had been through this before and knew the amount of traffic to his site could spike significantly in the days leading up to Black Friday. So, in late September, he signed up for CloudFlare to help manage the coming load. Because 2011BlackFridayAds.com is hosted through MediaTemple, [one of CloudFlare's hosting partners](http://mediatemple.net/cloudflare/), setting up our service took literally two clicks and less than a minute. The graph below shows the growth in traffic over the last month, peaking in the last 24 hours at nearly 3 million page views a day (about 35 per second).\n\n![A Thanksgiving Story: Surviving the Crush of Holiday\nTraffic](https://blog.cloudflare.com/content/images/2011blackfridayads.png.scaled500.png \"A Thanksgiving Story: Surviving the Crush of Holiday Traffic\")\n\n### CloudFlare Helps Save Christmas Shopping\n\nTurns out a lot of people want to see Black Friday deals, and we're proud to have helped ensure 2011BlackFridayAds.com stay up and running under crushing load. CloudFlare didn't just make the site safer and protect it from attacks, we also significantly reduced the load on the servers so they could keep up with visitors' requests. In fact, CloudFlare cut the number of requests that needed to be handled by the server by nearly **half a billion**, or about 75% of the total load it would have experienced without CloudFlare. We also saved the site more than **23 Terabytes**, 93% of the bandwidth that would have otherwise been used serving the site's pages.\n\n![A Thanksgiving Story: Surviving the Crush of Holiday\nTraffic](https://blog.cloudflare.com/content/images/black_friday_savings.png.scaled500.png \"A Thanksgiving Story: Surviving the Crush of Holiday Traffic\")\n\nAny downtime experienced by a site is quickly punished by Google,  \nmeaning that even if you have an initially popular site, if you can't stay online you can't stay on top. CloudFlare helped 2011BlackFridayAds.com stay fast and reliable, even under a crushing load, and the effect has kept it at the top of the rankings and kept the traffic growing. Speed and reliability are the foundation of SEO.\n\nAs Ty just wrote me via email: \"CloudFlare has done wonders for us this year. It's amazing how fast the site is.\" Helping ensure the sites of entrepreneurs like Ty could perform as well as the big companies' is why we — a scrappy group of entrepreneurs ourselves — built CloudFlare in the first place.\n\n### Surviving Cyber Monday\n\nWhile Black Friday is the biggest offline retail shopping day, the Monday after Black Friday, a day now known as Cyber Monday, is reportedly the largest for online shopping. Everyone in the U.S. returns to work, still in a shopping mood, and takes any downtime at their desks to scour Internet retailers for gifts. If you're an online business, you literally can't afford to go down. And, if you're a eMerchant who finds yourself worried over the coming days that your site might fail, spend the few minutes it takes to [sign up for CloudFlare](https://www.cloudflare.com/sign-up). Our basic service is free (Ty uses the [PRO version](https://www.cloudflare.com/plans) for  \n$20/month), it doesn't require any technical skill to setup, and it will work regardless of your platform.\n\nThe biggest Internet retailer in the world is Amazon.com. As of today, CloudFlare powers **5x the traffic of Amazon**. In other words, if you're worried about a crush of traffic to your website over the coming holiday season, we're here for you and happy to help.\n\nFor those camped out in front of a Walmart for Black Friday, stay warm and make sure to check [2011BlackFridayAds.com](http://www.2011blackfridayads.com/) from your smart phone for a preview of the latest deals. For those watching from the sidelines, check back to these pages after Cyber Monday for reports on Internet traffic patterns we saw.\n\nAnd, for those in the United States, have a very Happy Thanksgiving.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[eCommerce](https://blog.cloudflare.com/tag/ecommerce/) [Thanksgiving](https://blog.cloudflare.com/tag/thanksgiving/) [Holidays](https://blog.cloudflare.com/tag/holidays/)\n\nRelated Posts\n\nNovember 24, 2023 3:11PM\n\n[\n\n## Do hackers eat turkey? And other Thanksgiving Internet trends\n\n](https://blog.cloudflare.com/do-hackers-eat-turkey-and-other-thanksgiving-internet-trends/)\n\nOffline for turkey time: Which US states logged off on Thanksgiving Day? Is there a difference between coastal and central states? Do hackers take a Thanksgiving break? Are food delivery services gaining or losing traffic? We answer those questions and more...\n\nBy \n\nNovember 28, 2020 12:00PM\n\n[\n\n## A Thanksgiving 2020 Reading List\n\n](https://blog.cloudflare.com/a-thanksgiving-2020-reading-list/)\n\nIf you want to relax in an active and learning way this Thanksgiving weekend, here are some of the topics we’ve covered on the Cloudflare blog this past week that you may find interesting....\n\nBy \n\nNovember 25, 2022 6:32PM\n\n[\n\n## An early look at Thanksgiving 2022 Internet trends\n\n](https://blog.cloudflare.com/an-early-look-at-thanksgiving-2022-internet-trends/)\n\nYesterday, November 24, 2022, was Thanksgiving Day in the US. Last year, we saw how the US paused shopping (and browsing) for Thanksgiving. So, how was it this year?...\n\nBy \n\nNovember 26, 2021 3:51PM\n\n[\n\n## How the US paused shopping (and browsing) for Thanksgiving\n\n](https://blog.cloudflare.com/how-the-us-paused-shopping-and-browsing-for-thanksgiving/)\n\nThroughout the pandemic we saw different trends caused by people being more at home than usual, but Internet patterns also change at specific times of the year (like when students go back to school or when it’s colder outside) or on some holidays like Thanksgiving....\n\nBy"
    },
    {
      "url": "https://blog.cloudflare.com/an-early-look-at-thanksgiving-2022-internet-trends/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/an-early-look-at-thanksgiving-2022-internet-trends/",
        "loadedTime": "2023-12-05T02:35:52.884Z",
        "referrerUrl": "https://blog.cloudflare.com/do-hackers-eat-turkey-and-other-thanksgiving-internet-trends/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/an-early-look-at-thanksgiving-2022-internet-trends/",
        "title": "An early look at Thanksgiving 2022 Internet trends",
        "description": "Yesterday, November 24, 2022, was Thanksgiving Day in the US. Last year, we saw how the US paused shopping (and browsing) for Thanksgiving. So, how was it this year?",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/25/2022\n5 min read\n\"The more you practice the art of thankfulness, the more you have to be thankful for.\" — Norman Vincent Peale, American author\nThe turkey. The sweet potatoes. The stuffing. The pumpkin pie. Yesterday, November 24, 2022, was Thanksgiving Day in the US. A time for families and loved ones to be together and thankful, according to the tradition. Last year, we saw how the US paused shopping (and browsing) for Thanksgiving. So, how was it this year? Not only did we see Internet traffic go down (by 13%) during Thanksgiving dinner, but it was much higher than usual the day before and the day after (the Black Friday effect… so far). There was also a clear, but short, Thanksgiving Day effect on e-commerce DNS trends. \nWe'll have to wait to see what Black Friday looks like.\nLet’s start with Internet traffic at the time of Thanksgiving dinner. Although every family is different, a 2018 survey of US consumers showed that for 42% early afternoon (between 13:00 and 15:00 is the preferred time to sit at the table and start to dig in). But 16:00 seems to be the “correct time” — The Atlantic explains why. \nThat said, Cloudflare Radar shows that between 21:00 and 01:00 UTC (we use that as the standard timezone in Radar) there was a clear drop in Internet traffic, mostly between 21:00 and 22:00 UTC, when traffic dropped 13%, compared with the week before. That time period is “translated” for the East Coast to between 16:00 and 20:00 EST and for the West Coast the time between 13:00 to 17:00 PST. Similar to what we saw last year. \nRadar also allows anyone to focus on the last 24 hours and check the traffic volume change compared with the previous period. The more granular view in the next graph shows not only the 13% drop during Thanksgiving dinner, but also the clear increase after. At around 01:00 EST (22:00 PST), traffic was 15% higher than the day before, and today, November 25, Black Friday morning (08:00 EST, 05:00 PST), was growing ~16% more in traffic at 09:00 EST (06:00 PST).\nIt’s a similar perspective when we look at the last seven days, a filter that also shows the night before Thanksgiving in the US, traffic was 15% higher than the week before at around 01:00-03:00 EST (22:00-00:00 PST). And there’s a general increase in traffic this week, probably related to the fact it is also \"Black Friday Week\" (more on e-commerce trends at the end). \nIn terms of Internet traffic growth (made by humans, not bots) in November, there’s a clear increase throughout the month, but mostly this week. The next chart aggregates traffic by day. So far, Tuesday, November 22, 2022, was the day of the month with most traffic in the US — +13% than what we saw on Tuesday, November 1.\nIt’s also clear in the previous graph that weekends in the US have less traffic, especially Saturdays, but that Thanksgiving Day was the one with less traffic of the past two weeks — 10% less traffic than the same day the week before.\nWe’ve been focused on human Internet traffic. Bots, on the other hand, are not that interested in the Thanksgiving and Black Friday, and there was actually more bot traffic in the US last week than in this one. So far.\nTo wrap up this Internet traffic section, let’s look at mobile device trends. In the last four weeks, we saw an average of 48% of Internet traffic in the US coming from mobile devices. But on Thanksgiving Day that average was 55%. That was actually the day in November when people in the US were most online using their mobile devices.\nHere’s the view that shows the mobile percentage difference from the past two weeks, with an up to 9% increase (compared with the previous week) in mobile devices' predominance in Internet traffic, between 10:00 and 16:00 EST (07:00-13:00 PST).\nE-commerce interest: growing (but with a Thanksgiving dip)\nNow, let’s look at DNS query trends (from our globally used 1.1.1.1 DNS resolver) to e-commerce websites in the US. First, the Thanksgiving Day effect. \nAggregating several e-commerce domains, we can see not only that there are several spikes in the last two weeks, but that during Thanksgiving, there was a clear dip in DNS traffic between 15:00 and 17:00 EST (12:00-14:00 PST). How much? At 17:00 EST, Thanksgiving Day, there was 13% less DNS traffic than in the previous week.\nWe have been following e-commerce trends this week on our Cloudflare Radar Twitter account. And, so far, November 14, 21 and 22, were the days that generated most interest.\nUsing a smoothed seven-day rolling average to those e-commerce domains (only in the US), the growth trend for the past 30 days is even more clear in the past two weeks (after a clear dip in early November). From November 13 to November 22, the rolling average grew ~5%.\nLast year, Cyber Monday was the biggest day for online shopping, in terms of DNS queries that we saw. Next week, we’ll see how it was this year. \nJapan: A different kind of Thanksgiving\nAlso, this week, Japan had its Labor Thanksgiving, an annual public holiday that was celebrated on Wednesday, November 23, 2022. And there was also a clear impact, but because, in Japan, this is a day full of events held throughout the country, there was an increase in traffic during the day. How much? \nThe peak was at around 01:00 UTC (10:00 in local time), when Internet traffic was 60% higher than in the previous week (and it remained high during Labor Thanksgiving Day).\nYou can check Cloudflare Radar, but also our Twitter account where we continue to see country patterns related to the FIFA World Cup in Qatar (Internet traffic does shift, depending on the country, when national teams are playing), but also e-commerce DNS trends.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nCloudflare Radar Thanksgiving Trends \nRelated Posts\nApril 12, 2022 2:12PM\nDDoS Attack Trends for 2022 Q1\nWelcome to our first DDoS report of 2022, and the ninth in total so far. This report includes new data points and insights both in the application-layer and network-layer sections — as observed across the global Cloudflare network between January and March 2022...\nBy \nJanuary 10, 2022 1:58PM\nDDoS Attack Trends for Q4 2021\nIn Q4, we observed a 95% increase in L3/4 DDoS attacks and record-breaking levels of Ransom DDoS attacks. The Manufacturing industry was the most targeted alongside a 5,800% increase in SNMP-based DDoS attacks and massive campaigns against VoIP providers around the world...\nBy \nNovember 09, 2021 12:59PM\nA Brief History of the Meris Botnet\nOver the past months, we’ve been tracking and analyzing the activity of the Meris botnet....\nBy \nNovember 04, 2021 12:58PM\nDDoS Attack Trends for Q3 2021\nIn Q3, 2021 we saw and mitigated record-setting HTTP DDoS attacks, terabit-strong network layer attacks, one of the largest botnets ever deployed (Meris), and more recently, ransom attacks on Voice-over-IP (VoIP) service providers....\nBy",
      "markdown": "11/25/2022\n\n*   [![João Tomé](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/V0x3WKfJ_400x400-1.jpeg)](https://blog.cloudflare.com/author/joao-tome/)\n\n5 min read\n\n> _\"The more you practice the art of thankfulness, the more you have to be thankful for.\"_ — Norman Vincent Peale, American author\n\n![An early look at Thanksgiving 2022 Internet trends](https://blog.cloudflare.com/content/images/2022/11/image9-8.png)\n\nThe turkey. The sweet potatoes. The stuffing. The pumpkin pie. Yesterday, November 24, 2022, was [Thanksgiving Day](https://en.wikipedia.org/wiki/Thanksgiving_(United_States)) in the US. A time for families and loved ones to be together and thankful, according to the tradition. Last year, we saw [how the US paused shopping (and browsing) for Thanksgiving](https://blog.cloudflare.com/how-the-us-paused-shopping-and-browsing-for-thanksgiving/). So, how was it this year? Not only did we see Internet traffic go down (by 13%) during Thanksgiving dinner, but it was much higher than usual the day before and the day after (the Black Friday effect… so far). There was also a clear, but short, Thanksgiving Day effect on e-commerce DNS trends.\n\nWe'll have to wait to see what Black Friday looks like.\n\nLet’s start with Internet traffic at the time of Thanksgiving dinner. Although every family is different, a 2018 survey of US consumers showed that for 42% early afternoon (between 13:00 and 15:00 is the preferred time to sit at the table and start to dig in). But 16:00 seems to be the “correct time” — [The Atlantic](https://www.theatlantic.com/family/archive/2018/11/when-thanksgiving-dinner/576274/) explains why.\n\nThat said, [Cloudflare Radar](https://radar.cloudflare.com/us) shows that between 21:00 and 01:00 UTC (we use that as the standard timezone in Radar) there was a clear drop in Internet traffic, mostly between 21:00 and 22:00 UTC, when traffic dropped 13%, compared with the week before. That time period is “translated” for the East Coast to between 16:00 and 20:00 EST and for the West Coast the time between 13:00 to 17:00 PST. Similar to what we saw last year.\n\n![](https://blog.cloudflare.com/content/images/2022/11/tg22-1.jpeg)\n\nRadar also allows anyone to focus on the last [24 hours](https://radar.cloudflare.com/traffic/us?range=1d) and check the traffic volume change compared with the previous period. The more granular view in the next graph shows not only the 13% drop during Thanksgiving dinner, but also the clear increase after. At around 01:00 EST (22:00 PST), traffic was 15% higher than the day before, and today, November 25, Black Friday morning (08:00 EST, 05:00 PST), was growing ~16% more in traffic at 09:00 EST (06:00 PST).\n\n![](https://blog.cloudflare.com/content/images/2022/11/unnamed-1.jpg)\n\nIt’s a similar perspective when we look at the last seven days, a filter that also shows the night before Thanksgiving in [the US](https://radar.cloudflare.com/traffic/us), traffic was 15% higher than the week before at around 01:00-03:00 EST (22:00-00:00 PST). And there’s a general increase in traffic this week, probably related to the fact it is also \"Black Friday Week\" (more on e-commerce trends at the end).\n\n![](https://blog.cloudflare.com/content/images/2022/11/us-2022-11-25-at-14.16.23.jpg)\n\nIn terms of Internet traffic growth (made by humans, not bots) in November, there’s a clear increase throughout the month, but mostly this week. The next chart aggregates traffic by day. So far, Tuesday, November 22, 2022, was the day of the month with most traffic in the US — +13% than what we saw on Tuesday, November 1.\n\n![](https://blog.cloudflare.com/content/images/2022/11/tg22-4.png)\n\nIt’s also clear in the previous graph that weekends in the US have less traffic, especially Saturdays, but that Thanksgiving Day was the one with less traffic of the past two weeks — 10% less traffic than the same day the week before.\n\nWe’ve been focused on human Internet traffic. Bots, on the other hand, are not that interested in the Thanksgiving and Black Friday, and there was actually more bot traffic in the US last week than in this one. So far.\n\nTo wrap up this Internet traffic section, let’s look at mobile device trends. In the last four weeks, we saw an average of 48% of Internet traffic in the US coming from mobile devices. But on Thanksgiving Day that average was 55%. That was actually the day in November when people in the US were most online using their mobile devices.\n\n![](https://blog.cloudflare.com/content/images/2022/11/tg22-5.png)\n\nHere’s the view that shows the mobile percentage difference from the past two weeks, with an up to 9% increase (compared with the previous week) in mobile devices' predominance in Internet traffic, between 10:00 and 16:00 EST (07:00-13:00 PST).\n\n![](https://blog.cloudflare.com/content/images/2022/11/tg22-6.png)\n\n### E-commerce interest: growing (but with a Thanksgiving dip)\n\nNow, let’s look at DNS query trends (from our globally used [1.1.1.1](https://1.1.1.1/) DNS resolver) to e-commerce websites in the US. First, the Thanksgiving Day effect.\n\nAggregating several e-commerce domains, we can see not only that there are several spikes in the last two weeks, but that during Thanksgiving, there was a clear dip in DNS traffic between 15:00 and 17:00 EST (12:00-14:00 PST). How much? At 17:00 EST, Thanksgiving Day, there was 13% less DNS traffic than in the previous week.\n\n![](https://blog.cloudflare.com/content/images/2022/11/tg22-7.png)\n\nWe have been following e-commerce trends this week on our [Cloudflare Radar Twitter account](https://twitter.com/CloudflareRadar). And, so far, November 14, 21 and 22, were the days that generated [most interest](https://twitter.com/CloudflareRadar/status/1595392643453837314).\n\n![](https://blog.cloudflare.com/content/images/2022/11/tg22-8.png)\n\nUsing a smoothed seven-day rolling average to those e-commerce domains (only in the US), the growth trend for the past 30 days is even more clear in the past two weeks (after a clear dip in early November). From November 13 to November 22, the rolling average grew ~5%.\n\n![](https://blog.cloudflare.com/content/images/2022/11/tg22-9.png)\n\n[Last year](https://blog.cloudflare.com/thanksgivings-biggest-online-shopping-day-was-cyber-monday-but-other-days-were-close-behind/), Cyber Monday was the biggest day for online shopping, in terms of DNS queries that we saw. Next week, we’ll see how it was this year.\n\n### Japan: A different kind of Thanksgiving\n\nAlso, this week, [Japan](https://radar.cloudflare.com/traffic/jp) had its [Labor Thanksgiving](https://en.wikipedia.org/wiki/Labor_Thanksgiving_Day), an annual public holiday that was celebrated on Wednesday, November 23, 2022. And there was also a clear impact, but because, in Japan, this is a day full of events held throughout the country, there was an increase in traffic during the day. How much?\n\nThe peak was at around 01:00 UTC (10:00 in local time), when Internet traffic was 60% higher than in the previous week (and it remained high during Labor Thanksgiving Day).\n\n![](https://blog.cloudflare.com/content/images/2022/11/tg22-10.png)\n\nYou can check Cloudflare Radar, but also our [Twitter account](https://twitter.com/CloudflareRadar/) where we continue to see country patterns related to the [FIFA World Cup in Qatar](https://twitter.com/CloudflareRadar/status/1595860677591171072) (Internet traffic does shift, depending on the country, when national teams are playing), but also [e-commerce](https://twitter.com/CloudflareRadar/status/1595391272986071042) DNS trends.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Cloudflare Radar](https://blog.cloudflare.com/tag/cloudflare-radar/) [Thanksgiving](https://blog.cloudflare.com/tag/thanksgiving/) [Trends](https://blog.cloudflare.com/tag/trends/)\n\nRelated Posts\n\nApril 12, 2022 2:12PM\n\n[\n\n## DDoS Attack Trends for 2022 Q1\n\n](https://blog.cloudflare.com/ddos-attack-trends-for-2022-q1/)\n\nWelcome to our first DDoS report of 2022, and the ninth in total so far. This report includes new data points and insights both in the application-layer and network-layer sections — as observed across the global Cloudflare network between January and March 2022...\n\nBy \n\nJanuary 10, 2022 1:58PM\n\n[\n\n## DDoS Attack Trends for Q4 2021\n\n](https://blog.cloudflare.com/ddos-attack-trends-for-2021-q4/)\n\nIn Q4, we observed a 95% increase in L3/4 DDoS attacks and record-breaking levels of Ransom DDoS attacks. The Manufacturing industry was the most targeted alongside a 5,800% increase in SNMP-based DDoS attacks and massive campaigns against VoIP providers around the world...\n\nBy \n\nNovember 09, 2021 12:59PM\n\n[\n\n## A Brief History of the Meris Botnet\n\n](https://blog.cloudflare.com/meris-botnet/)\n\nOver the past months, we’ve been tracking and analyzing the activity of the Meris botnet....\n\nBy \n\nNovember 04, 2021 12:58PM\n\n[\n\n## DDoS Attack Trends for Q3 2021\n\n](https://blog.cloudflare.com/ddos-attack-trends-for-2021-q3/)\n\nIn Q3, 2021 we saw and mitigated record-setting HTTP DDoS attacks, terabit-strong network layer attacks, one of the largest botnets ever deployed (Meris), and more recently, ransom attacks on Voice-over-IP (VoIP) service providers....\n\nBy"
    },
    {
      "url": "https://blog.cloudflare.com/de-de/workers-ai-update-hello-mistral-7b-de-de/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/de-de/workers-ai-update-hello-mistral-7b-de-de/",
        "loadedTime": "2023-12-05T02:36:00.952Z",
        "referrerUrl": "https://blog.cloudflare.com/workers-ai-update-hello-mistral-7b/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/de-de/workers-ai-update-hello-mistral-7b-de-de/",
        "title": "Workers AI Update: Hallo Mistral 7B",
        "description": "Wir freuen uns, heute ankündigen zu können, dass wir Mistral-7B-v0.1-instruct zu Workers AI hinzugefügt haben",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/21/2023\n7 min read\nWir freuen uns, heute ankündigen zu können, dass wir Mistral-7B-v0.1-instruct zu Workers AI hinzugefügt haben. Mistral 7B ist ein 7,3 Milliarden Parameter-umfassendes Sprachmodell mit einer Reihe von einzigartigen Vorteilen. Mit Hilfe der Gründer von Mistral AI werden wir uns einige der Highlights des Mistral 7B-Modells ansehen und die Gelegenheit nutzen, um tiefer in das Thema „Attention“ (Aufmerksamkeit) und seine Variationen wie Multi-Query-Attention und Grouped-Query-Attention einzutauchen.\nKurz erklärt: Was ist Mistral 7B?\nMistral 7B ist ein 7,3 Milliarden Parameter-umfassendes KI-Modell, das bei Benchmarks beeindruckende Zahlen liefert. Das Modell:\nübertrifft Llama 2 13B bei allen Benchmarks\nübertrifft Llama 1 34B bei vielen Benchmarks,\nerreicht bei Programmieraufgaben fast die Performance von CodeLlama 7B, während es bei Englisch-Aufgaben noch immer gut abschneidet\nDie optimierte, auf Chat-Dialog ausgerichtete Version, die wir eingesetzt haben, übertrifft Llama 2 13B Chat in den von Mistral durchgeführten Benchmarks.\nEs folgt ein Beispiel für das Streamen mit der REST-API:\ncurl -X POST \\ “https://api.cloudflare.com/client/v4/accounts/{account-id}/ai/run/@cf/mistral/mistral-7b-instruct-v0.1” \\ -H “Authorization: Bearer {api-token}” \\ -H “Content-Type:application/json” \\ -d '{ “prompt”: “What is grouped query attention”, “stream”: true }' API Response: { response: “Grouped query attention is a technique used in natural language processing (NLP) and machine learning to improve the performance of models…” } \nUnd hier sehen Sie ein Beispiel für die Verwendung eines Worker-Skripts:\nimport { Ai } from '@cloudflare/ai'; export default { async fetch(request, env) { const ai = new Ai(env.AI); const stream = await ai.run('@cf/mistral/mistral-7b-instruct-v0.1', { prompt: 'What is grouped query attention', stream: true }); return Response.json(stream, { headers: { “content-type”: “text/event-stream” } }); } } \nMistral nutzt die sogenannte Grouped-Query Attention für schnellere Inferenz. Diese kürzlich entwickelte Technik verbessert die Geschwindigkeit der Inferenz, ohne die Qualität der Ergebnisse zu beeinträchtigen. Bei Modellen mit 7 Milliarden Parametern können wir dank der Grouped-Query Attention mit Mistral fast 4x so viele Token pro Sekunde generieren wie mit Llama.\nSie benötigen natürlich keine weiteren Informationen als diese, um Mistral-7B zu nutzen. Sie können es noch heute testen unter ai.cloudflare.com. Wenn Sie mehr über Attention und Grouped-Query-Attention erfahren möchten, lesen Sie einfach weiter!\nWas bedeutet „Attention“, also Aufmerksamkeit, eigentlich im Bereich der KI?\nDer grundlegende Attention-Mechanismus, insbesondere die „Scaled Dot-Product Attention“, wie sie in der bahnbrechenden Forschungsarbeit „Attention Is All You Need“, vorgestellt wurde, ist recht einfach:\nWir nennen unsere spezielle Aufmerksamkeit „Scale Dot-Product Attention“. Der Input besteht aus der Abfrage und den Schlüsseln der Dimension d_k und den Werten der Dimension d_v. Wir berechnen die Dot-Produkte der Abfrage mit allen Schlüsseln, teilen jedes durch sqrt(d_k) und wenden eine Softmax-Funktion an, um die Gewichtung der Werte zu erhalten.\nKonkret sieht das folgendermaßen aus:\nsource\nVereinfacht ausgedrückt, können sich die Modelle so auf wichtige Teile des Inputs konzentrieren. Stellen Sie sich vor, Sie lesen einen Satz und versuchen, ihn zu verstehen. Scaled Dot-Product-Attention ermöglicht es Ihnen, bestimmten Wörtern auf der Grundlage ihrer Relevanz mehr Aufmerksamkeit zu schenken. Dabei wird die Ähnlichkeit zwischen jedem Wort (K) im Satz und einer Abfrage (Q) berechnet. Anschließend werden die Ähnlichkeitswerte durch die Quadratwurzel der Dimension der Abfrage geteilt. Diese Skalierung hilft, sehr kleine oder sehr große Werte zu vermeiden. Anhand dieser skalierten Ähnlichkeitswerte können wir schließlich bestimmen, wie viel Aufmerksamkeit oder Bedeutung jedes Wort erhalten sollte. Dieser Aufmerksamkeitsmechanismus hilft den Modellen, wichtige Informationen (V) zu erkennen und ihre Verständnis- und Übersetzungsfähigkeiten zu verbessern.\nEigentlich ganz einfach, oder? Um von diesem einfachen Mechanismus zu einer KI zu gelangen, der man komplexe Schreibaufträge erteilen kann wie z. B. „Schreibe eine Seinfeld-Folge, in der Jerry den Bubble-Sort-Algorithmus erlernt“, müssen wir ihn jedoch noch komplexer machen. (Tatsächlich hat alles, was wir gerade behandelt haben, nicht einmal gelernte Parameter – konstante Werte, die während des Trainierens des Modells gelernt werden und die die Ausgabe des Attention-Blocks anpassen!)\nAttention-Blöcke im Stile von „Attention is All You Need“ führen hauptsächlich drei Arten von Komplexität ein:\nGelernte Parameter\nGelernte Parameter beziehen sich auf Werte oder Gewichte, die während des Trainingsprozesses eines Modells angepasst werden, um dessen Performance zu verbessern. Diese Parameter werden verwendet, um den Informationsfluss oder die Aufmerksamkeit innerhalb des Modells zu steuern, damit es sich auf die wichtigsten Teile der Inputdaten konzentrieren kann. Einfacher ausgedrückt: Gelernte Parameter sind wie einstellbare Knöpfe an einer Maschine, an denen man drehen kann, um ihren Betrieb zu optimieren.\n„Vertikale“ Stapelung – übereinandergelagerte Attention-Blöcke\nBei der vertikalen Stapelung werden mehrere Aufmerksamkeitsmechanismen übereinander gestapelt, wobei jede Schicht auf dem Ergebnis der vorherigen Schicht aufbaut. Dadurch kann sich das Modell auf verschiedene Teile der Inputdaten auf unterschiedlichen Abstraktionsebenen konzentrieren, was zu einer besseren Performance bei bestimmten Aufgaben führen kann.\nHorizontale Stapelung – auch bekannt als „Multi-Head-Attention“\nDie Abbildung aus der Arbeit zeigt das vollständige Multi-Head-Attention-Modul. Mehrere Attention-Operationen werden parallel durchgeführt, wobei das Q-K-V-Input für jede durch eine eindeutige lineare Projektion der gleichen Input-Daten (definiert durch einen eindeutigen Satz gelernter Parameter) erzeugt wird. Diese parallelen Attention-Blöcke werden als „Attention-Heads“ bezeichnet. Die gewichteten Summen-Outputs aller Attention-Heads werden zu einem einzigen Vektor verkettet und durch eine weitere parametrisierte lineare Transformation geleitet, um das endgültige Output zu erhalten.\nsource\nDieser Mechanismus ermöglicht es einem Modell, sich gleichzeitig auf verschiedene Teile der Inputdaten zu konzentrieren. Stellen Sie sich vor, Sie versuchen, eine komplexe Information zu verstehen, etwa einen Satz oder einen Absatz. Um diesen zu verstehen, müssen Sie gleichzeitig auf verschiedene Teile achten. So müssen Sie beispielsweise gleichzeitig auf das Subjekt des Satzes, das Verb und das Objekt achten, um die Bedeutung des Satzes zu begreifen. Die Multi-Headed-Attention funktioniert sehr ähnlich. Sie ermöglicht es einem Modell, gleichzeitig auf verschiedene Teile der Inputdaten zu achten, indem es mehrere „Bereiche“ der Aufmerksamkeit („Heads of Attention“) verwendet. Jeder Aufmerksamkeitsbereich konzentriert sich auf einen anderen Aspekt der Inputdaten, und die Ergebnisse aller Bereiche werden kombiniert, um das endgültige Ergebnis des Modells zu erhalten.\nArten von Attention\nEs gibt drei gängige Anordnungen von Attention-Blöcken, die von LLMs verwendet werden, die in den letzten Jahren entwickelt wurden: Multi-Head-Attention, Grouped-Query-Attention und Multi-Query-Attention. Sie unterscheiden sich durch die Anzahl der K- und V-Vektoren im Verhältnis zur Anzahl der Abfragevektoren. Multi-Head-Attention verwendet die gleiche Anzahl von K- und V-Vektoren wie Q-Vektoren, in der folgenden Tabelle mit „N“ bezeichnet. Multi-Query-Attention verwendet nur einen einzigen K- und V-Vektor. Grouped-Query-Attention, die Art, die im Mistral 7B-Modell verwendet wird, teilt die Q-Vektoren gleichmäßig in Gruppen mit jeweils „G“ Vektoren auf und verwendet dann einen einzelnen K- und V-Vektor für jede Gruppe, so dass insgesamt N durch G Gruppen von K- und V-Vektoren geteilt werden. Soweit zu den Unterschieden. Wir werden uns weiter unten mit den Auswirkungen dieser Unterschiede befassen.\n\t\nAnzahl der Schlüssel/Wert-Blöcke\n\t\nQualität\n\t\nSpeicher-verbrauch\n\t\nMulti-head attention (MHA)\n\t\nN\n\t\nBeste\n\t\nAm meisten\n\t\nGrouped-query attention (GQA)\n\t\nN / G\n\t\nBesser\n\t\nWeniger\n\t\nMulti-query attention (MQA)\n\t\n1\n\t\nGutartiger\n\t\nAm wenigsten\n\t\nZusammenfassung der Attention-Stile\nDiese Abbildung verdeutlicht den Unterschied zwischen den drei Stilen:\nsource\nMulti-Query-Attention\nMulti-Query Attention wurde 2019 in einer Arbeit von Google beschrieben: „Fast Transformer Decoding: One Write-Head is All You Need“. Die Idee besteht darin, dass für jeden Q-Vektor im Attention-Mechanismus keine separaten K- und V-Einträge erstellt werden, wie bei der obigen Multi-Head-Attention, sondern nur ein einziger K- und V-Vektor für den gesamten Satz von Q-Vektoren verwendet wird. Daher der Name: mehrere Abfragen kombiniert in einem einzigen Attention-Mechanismus. In der Arbeit wurde dies an einer Übersetzungsaufgabe getestet und zeigte die gleiche Performance wie die Multi-Head-Attention bei der Benchmark-Aufgabe.\nUrsprünglich war die Idee, die Gesamtgröße des Speichers zu reduzieren, auf den bei der Durchführung der Inferenz für das Modell zugegriffen wird. Seitdem haben sich verallgemeinerte Modelle herausgebildet und die Anzahl der Parameter ist gestiegen. Der benötigte GPU-Speicher ist oft der Engpass. Hier zeigt sich die Stärke der Multi-Query-Attention, da sie von den drei Attention-Arten den geringsten Beschleunigungsspeicher benötigt. Mit zunehmender Größe und Allgemeingültigkeit der Modelle nahm die Performance der Multi-Query-Attention im Vergleich zur Multi-Head-Attention jedoch ab.\nGrouped-Query-Attention\nDer neueste – und der von Mistral verwendete Ansatz – ist die Grouped-Query-Attention, wie sie in der im Mai 2023 auf arxiv.org veröffentlichten Arbeit GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints beschrieben wurde. Grouped-Query-Attention kombiniert das Beste beider Ansätze: die Qualität von Multi-Head-Attention mit der Geschwindigkeit und dem geringen Speicherbedarf von Multi-Query-Attention. Anstelle eines einzigen Satzes von K- und V-Vektoren oder eines Satzes für jeden Q-Vektor wird ein festes Verhältnis von einem Satz von K- und V-Vektoren für jeden Q-Vektor verwendet, was die Speichernutzung reduziert, aber eine hohe Performance bei vielen Aufgaben sichert.\nBei der Auswahl eines für den laufenden Betrieb zu implementierenden Modells geht es oft nicht nur darum, das beste verfügbare Modell auszuwählen, denn wir müssen Kompromisse zwischen Performance, Speichernutzung, Batch-Größe und verfügbarer Hardware (oder Cloud-Kosten) berücksichtigen. Das Wissen um diese drei Arten der Attention (Aufmerksamkeit) kann uns helfen, diese Entscheidungen zu treffen und zu verstehen, wann wir unter den jeweiligen Umständen ein bestimmtes Modell wählen sollten.\nHier kommt Mistral ins Spiel – testen Sie es noch heute\nAls eines der ersten Large-Language-Modelle, das die Grouped-Query-Attention nutzt und sie mit der Sliding-Window-Attention kombiniert, scheint Mistral die perfekte Lösung gefunden zu haben – niedrige Latenz, hoher Durchsatz und eine sehr gute Performance bei Benchmarks, selbst im Vergleich zu größeren Modellen (13B). Alles, was ich sagen will, ist, dass es für seine Größe sehr viel zu bieten hat, und wir freuen uns sehr, es heute allen Entwicklern über Workers AI zur Verfügung stellen zu können. \nSchauen Sie sich unsere Entwicklerdokumente an, um loszulegen, und wenn Sie Hilfe benötigen, Feedback geben oder Ihre Arbeit mit anderen teilen möchten, besuchen Sie einfach unseren Developer Discord!\nDas Workers AI-Team wächst und stellt neue Mitarbeitende ein. Schauen Sie auf unserer Karriere-Seite nach offenen Stellen, wenn Sie sich für KI-Engineering begeistern und uns beim Aufbau und der Weiterentwicklung unserer globalen, serverlosen GPU-gestützten Inferenzplattform unterstützen möchten.\nWir schützen ganze Firmennetzwerke, helfen Kunden dabei, Internet-Anwendungen effizient zu entwickeln, jede Website oder Internetanwendung zu beschleunigen, DDoS-Angriffe abzuwehren, Hacker in Schach zu halten und unterstützen Sie bei Ihrer Umstellung auf Zero-Trust. \nBesuchen Sie 1.1.1.1 von einem beliebigen Gerät aus und nutzen Sie unsere kostenlose App, die Ihr Internet schneller und sicherer macht. \nWeitere Informationen über unsere Mission, ein besseres Internet zu schaffen, finden Sie hier. Sie möchten sich beruflich neu orientieren? Dann werfen Sie doch einen Blick auf unsere offenen Stellen. \nWorkers AI (DE) Mistral (DE) Cloudflare Workers (DE) Deutsch",
      "markdown": "11/21/2023\n\n*   [![Jesse Kipp](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2020/12/jesse-headshot-square.jpg)](https://blog.cloudflare.com/author/jesse/)\n*   [![Isaac Rehg](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/11/Isaac-Rehg.jpeg)](https://blog.cloudflare.com/author/isaac-rehg/)\n\n7 min read\n\n![Workers AI Update: Hello Mistral 7B](https://blog.cloudflare.com/content/images/2023/11/Mistral-1.png)\n\nWir freuen uns, heute ankündigen zu können, dass wir Mistral-7B-v0.1-instruct zu Workers AI hinzugefügt haben. Mistral 7B ist ein 7,3 Milliarden Parameter-umfassendes Sprachmodell mit einer Reihe von einzigartigen Vorteilen. Mit Hilfe der Gründer von Mistral AI werden wir uns einige der Highlights des Mistral 7B-Modells ansehen und die Gelegenheit nutzen, um tiefer in das Thema „Attention“ (Aufmerksamkeit) und seine Variationen wie Multi-Query-Attention und Grouped-Query-Attention einzutauchen.\n\n## Kurz erklärt: Was ist Mistral 7B?\n\nMistral 7B ist ein 7,3 Milliarden Parameter-umfassendes KI-Modell, das [bei Benchmarks beeindruckende Zahlen](https://mistral.ai/news/announcing-mistral-7b/) liefert. Das Modell:\n\n*   übertrifft Llama 2 13B bei allen Benchmarks\n*   übertrifft Llama 1 34B bei vielen Benchmarks,\n*   erreicht bei Programmieraufgaben fast die Performance von CodeLlama 7B, während es bei Englisch-Aufgaben noch immer gut abschneidet\n*   Die optimierte, auf Chat-Dialog ausgerichtete Version, die wir eingesetzt haben, übertrifft Llama 2 13B Chat in den von Mistral durchgeführten Benchmarks.\n\nEs folgt ein Beispiel für das Streamen mit der [REST-API](https://developers.cloudflare.com/workers-ai/get-started/rest-api/):\n\n```\ncurl -X POST \\\n“https://api.cloudflare.com/client/v4/accounts/{account-id}/ai/run/@cf/mistral/mistral-7b-instruct-v0.1” \\\n-H “Authorization: Bearer {api-token}” \\\n-H “Content-Type:application/json” \\\n-d '{ “prompt”: “What is grouped query attention”, “stream”: true }'\n\nAPI Response: { response: “Grouped query attention is a technique used in natural language processing  (NLP) and machine learning to improve the performance of models…” }\n```\n\nUnd hier sehen Sie ein Beispiel für die Verwendung eines Worker-Skripts:\n\n```\nimport { Ai } from '@cloudflare/ai';\nexport default {\n    async fetch(request, env) {\n        const ai = new Ai(env.AI);\n        const stream = await ai.run('@cf/mistral/mistral-7b-instruct-v0.1', {\n            prompt: 'What is grouped query attention',\n            stream: true\n        });\n        return Response.json(stream, { headers: { “content-type”: “text/event-stream” } });\n    }\n}\n```\n\nMistral nutzt die sogenannte [Grouped-Query Attention](https://arxiv.org/abs/2305.13245) für schnellere Inferenz. Diese kürzlich entwickelte Technik verbessert die Geschwindigkeit der Inferenz, ohne die Qualität der Ergebnisse zu beeinträchtigen. Bei Modellen mit 7 Milliarden Parametern können wir dank der Grouped-Query Attention mit Mistral fast 4x so viele Token pro Sekunde generieren wie mit Llama.\n\nSie benötigen natürlich keine weiteren Informationen als diese, um Mistral-7B zu nutzen. Sie können es noch heute testen unter [ai.cloudflare.com](https://ai.cloudflare.com/). Wenn Sie mehr über Attention und Grouped-Query-Attention erfahren möchten, lesen Sie einfach weiter!\n\n## Was bedeutet „Attention“, also Aufmerksamkeit, eigentlich im Bereich der KI?\n\nDer grundlegende Attention-Mechanismus, insbesondere die „Scaled Dot-Product Attention“, wie sie in der bahnbrechenden Forschungsarbeit „[Attention Is All You Need](https://arxiv.org/abs/1706.03762)“, vorgestellt wurde, ist recht einfach:\n\n> Wir nennen unsere spezielle Aufmerksamkeit „Scale Dot-Product Attention“. Der Input besteht aus der Abfrage und den Schlüsseln der Dimension d\\_k und den Werten der Dimension d\\_v. Wir berechnen die Dot-Produkte der Abfrage mit allen Schlüsseln, teilen jedes durch sqrt(d\\_k) und wenden eine Softmax-Funktion an, um die Gewichtung der Werte zu erhalten.\n\nKonkret sieht das folgendermaßen aus:\n\n![](https://blog.cloudflare.com/content/images/2023/11/Screenshot-2023-11-21-at-09.12.30.png)\n\n[source](https://arxiv.org/abs/1706.03762)\n\nVereinfacht ausgedrückt, können sich die Modelle so auf wichtige Teile des Inputs konzentrieren. Stellen Sie sich vor, Sie lesen einen Satz und versuchen, ihn zu verstehen. Scaled Dot-Product-Attention ermöglicht es Ihnen, bestimmten Wörtern auf der Grundlage ihrer Relevanz mehr Aufmerksamkeit zu schenken. Dabei wird die Ähnlichkeit zwischen jedem Wort (K) im Satz und einer Abfrage (Q) berechnet. Anschließend werden die Ähnlichkeitswerte durch die Quadratwurzel der Dimension der Abfrage geteilt. Diese Skalierung hilft, sehr kleine oder sehr große Werte zu vermeiden. Anhand dieser skalierten Ähnlichkeitswerte können wir schließlich bestimmen, wie viel Aufmerksamkeit oder Bedeutung jedes Wort erhalten sollte. Dieser Aufmerksamkeitsmechanismus hilft den Modellen, wichtige Informationen (V) zu erkennen und ihre Verständnis- und Übersetzungsfähigkeiten zu verbessern.\n\nEigentlich ganz einfach, oder? Um von diesem einfachen Mechanismus zu einer KI zu gelangen, der man komplexe Schreibaufträge erteilen kann wie z. B. „Schreibe eine Seinfeld-Folge, in der Jerry den Bubble-Sort-Algorithmus erlernt“, müssen wir ihn jedoch noch komplexer machen. (Tatsächlich hat alles, was wir gerade behandelt haben, nicht einmal gelernte Parameter – konstante Werte, die während des Trainierens des Modells gelernt werden und die die Ausgabe des Attention-Blocks anpassen!)\n\nAttention-Blöcke im Stile von „_Attention is All You Need_“ führen hauptsächlich drei Arten von Komplexität ein:\n\n### Gelernte Parameter\n\nGelernte Parameter beziehen sich auf Werte oder Gewichte, die während des Trainingsprozesses eines Modells angepasst werden, um dessen Performance zu verbessern. Diese Parameter werden verwendet, um den Informationsfluss oder die Aufmerksamkeit innerhalb des Modells zu steuern, damit es sich auf die wichtigsten Teile der Inputdaten konzentrieren kann. Einfacher ausgedrückt: Gelernte Parameter sind wie einstellbare Knöpfe an einer Maschine, an denen man drehen kann, um ihren Betrieb zu optimieren.\n\n### „Vertikale“ Stapelung – übereinandergelagerte Attention-Blöcke\n\nBei der vertikalen Stapelung werden mehrere Aufmerksamkeitsmechanismen übereinander gestapelt, wobei jede Schicht auf dem Ergebnis der vorherigen Schicht aufbaut. Dadurch kann sich das Modell auf verschiedene Teile der Inputdaten auf unterschiedlichen Abstraktionsebenen konzentrieren, was zu einer besseren Performance bei bestimmten Aufgaben führen kann.\n\n### Horizontale Stapelung – auch bekannt als „Multi-Head-Attention“\n\nDie Abbildung aus der Arbeit zeigt das vollständige Multi-Head-Attention-Modul. Mehrere Attention-Operationen werden parallel durchgeführt, wobei das Q-K-V-Input für jede durch eine eindeutige lineare Projektion der gleichen Input-Daten (definiert durch einen eindeutigen Satz gelernter Parameter) erzeugt wird. Diese parallelen Attention-Blöcke werden als „Attention-Heads“ bezeichnet. Die gewichteten Summen-Outputs aller Attention-Heads werden zu einem einzigen Vektor verkettet und durch eine weitere parametrisierte lineare Transformation geleitet, um das endgültige Output zu erhalten.\n\n![](https://blog.cloudflare.com/content/images/2023/11/Screenshot-2023-11-21-at-09.13.49.png)\n\n[source](https://arxiv.org/abs/1706.03762)\n\nDieser Mechanismus ermöglicht es einem Modell, sich gleichzeitig auf verschiedene Teile der Inputdaten zu konzentrieren. Stellen Sie sich vor, Sie versuchen, eine komplexe Information zu verstehen, etwa einen Satz oder einen Absatz. Um diesen zu verstehen, müssen Sie gleichzeitig auf verschiedene Teile achten. So müssen Sie beispielsweise gleichzeitig auf das Subjekt des Satzes, das Verb und das Objekt achten, um die Bedeutung des Satzes zu begreifen. Die Multi-Headed-Attention funktioniert sehr ähnlich. Sie ermöglicht es einem Modell, gleichzeitig auf verschiedene Teile der Inputdaten zu achten, indem es mehrere „Bereiche“ der Aufmerksamkeit („Heads of Attention“) verwendet. Jeder Aufmerksamkeitsbereich konzentriert sich auf einen anderen Aspekt der Inputdaten, und die Ergebnisse aller Bereiche werden kombiniert, um das endgültige Ergebnis des Modells zu erhalten.\n\n## Arten von Attention\n\nEs gibt drei gängige Anordnungen von Attention-Blöcken, die von LLMs verwendet werden, die in den letzten Jahren entwickelt wurden: Multi-Head-Attention, Grouped-Query-Attention und Multi-Query-Attention. Sie unterscheiden sich durch die Anzahl der K- und V-Vektoren im Verhältnis zur Anzahl der Abfragevektoren. **Multi-Head-Attention** verwendet die gleiche Anzahl von K- und V-Vektoren wie Q-Vektoren, in der folgenden Tabelle mit „N“ bezeichnet. **Multi-Query-Attention** verwendet nur einen einzigen K- und V-Vektor. **Grouped-Query-Attention**, die Art, die im Mistral 7B-Modell verwendet wird, teilt die Q-Vektoren gleichmäßig in Gruppen mit jeweils „G“ Vektoren auf und verwendet dann einen einzelnen K- und V-Vektor für jede Gruppe, so dass insgesamt N durch G Gruppen von K- und V-Vektoren geteilt werden. Soweit zu den Unterschieden. Wir werden uns weiter unten mit den Auswirkungen dieser Unterschiede befassen.\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|     | **Anzahl der Schlüssel/Wert-Blöcke** | **Qualität** | **Speicher-verbrauch** |\n| **Multi-head attention (MHA)** | N   | Beste | Am meisten |\n| **Grouped-query attention (GQA)** | N / G | Besser | Weniger |\n| **Multi-query attention (MQA)** | 1   | Gutartiger | Am wenigsten |\n\nZusammenfassung der Attention-Stile\n\nDiese Abbildung verdeutlicht den Unterschied zwischen den drei Stilen:\n\n![](https://blog.cloudflare.com/content/images/2023/11/image1-6.png)\n\n[source](https://arxiv.org/pdf/2305.13245.pdf)\n\n### Multi-Query-Attention\n\nMulti-Query Attention wurde 2019 in einer Arbeit von Google beschrieben: „[Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/abs/1911.02150)“. Die Idee besteht darin, dass für jeden Q-Vektor im Attention-Mechanismus keine separaten K- und V-Einträge erstellt werden, wie bei der obigen Multi-Head-Attention, sondern nur ein einziger K- und V-Vektor für den gesamten Satz von Q-Vektoren verwendet wird. Daher der Name: mehrere Abfragen kombiniert in einem einzigen Attention-Mechanismus. In der Arbeit wurde dies an einer Übersetzungsaufgabe getestet und zeigte die gleiche Performance wie die Multi-Head-Attention bei der Benchmark-Aufgabe.\n\nUrsprünglich war die Idee, die Gesamtgröße des Speichers zu reduzieren, auf den bei der Durchführung der Inferenz für das Modell zugegriffen wird. Seitdem haben sich verallgemeinerte Modelle herausgebildet und die Anzahl der Parameter ist gestiegen. Der benötigte GPU-Speicher ist oft der Engpass. Hier zeigt sich die Stärke der Multi-Query-Attention, da sie von den drei Attention-Arten den geringsten Beschleunigungsspeicher benötigt. Mit zunehmender Größe und Allgemeingültigkeit der Modelle nahm die Performance der Multi-Query-Attention im Vergleich zur Multi-Head-Attention jedoch ab.\n\n### Grouped-Query-Attention\n\nDer neueste – und der von Mistral verwendete Ansatz – ist die Grouped-Query-Attention, wie sie in der im Mai 2023 auf arxiv.org veröffentlichten Arbeit [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/abs/2305.13245) beschrieben wurde. Grouped-Query-Attention kombiniert das Beste beider Ansätze: die Qualität von Multi-Head-Attention mit der Geschwindigkeit und dem geringen Speicherbedarf von Multi-Query-Attention. Anstelle eines einzigen Satzes von K- und V-Vektoren oder eines Satzes für jeden Q-Vektor wird ein festes Verhältnis von einem Satz von K- und V-Vektoren für jeden Q-Vektor verwendet, was die Speichernutzung reduziert, aber eine hohe Performance bei vielen Aufgaben sichert.\n\nBei der Auswahl eines für den laufenden Betrieb zu implementierenden Modells geht es oft nicht nur darum, das beste verfügbare Modell auszuwählen, denn wir müssen Kompromisse zwischen Performance, Speichernutzung, Batch-Größe und verfügbarer Hardware (oder Cloud-Kosten) berücksichtigen. Das Wissen um diese drei Arten der Attention (Aufmerksamkeit) kann uns helfen, diese Entscheidungen zu treffen und zu verstehen, wann wir unter den jeweiligen Umständen ein bestimmtes Modell wählen sollten.\n\n## Hier kommt Mistral ins Spiel – testen Sie es noch heute\n\nAls eines der ersten Large-Language-Modelle, das die Grouped-Query-Attention nutzt und sie mit der Sliding-Window-Attention kombiniert, scheint Mistral die perfekte Lösung gefunden zu haben – niedrige Latenz, hoher Durchsatz und eine sehr gute Performance bei Benchmarks, selbst im Vergleich zu größeren Modellen (13B). Alles, was ich sagen will, ist, dass es für seine Größe sehr viel zu bieten hat, und wir freuen uns sehr, es heute allen Entwicklern über Workers AI zur Verfügung stellen zu können.\n\nSchauen Sie sich unsere [Entwicklerdokumente](https://developers.cloudflare.com/workers-ai/models/text-generation/) an, um loszulegen, und wenn Sie Hilfe benötigen, Feedback geben oder Ihre Arbeit mit anderen teilen möchten, besuchen Sie einfach unseren [Developer Discord](https://discord.com/invite/cloudflaredev)!\n\nDas Workers AI-Team wächst und stellt neue Mitarbeitende ein. Schauen Sie auf unserer [Karriere-Seite](https://www.cloudflare.com/careers/jobs/) nach offenen Stellen, wenn Sie sich für KI-Engineering begeistern und uns beim Aufbau und der Weiterentwicklung unserer globalen, serverlosen GPU-gestützten Inferenzplattform unterstützen möchten.\n\nWir schützen [ganze Firmennetzwerke](https://www.cloudflare.com/de-de/network-services/), helfen Kunden dabei, [Internet-Anwendungen effizient zu entwickeln](https://workers.cloudflare.com/), jede [Website oder Internetanwendung zu beschleunigen,](https://www.cloudflare.com/de-de/performance/accelerate-internet-applications/) [DDoS-Angriffe abzuwehren](https://www.cloudflare.com/ddos/), [Hacker in Schach zu halten](https://www.cloudflare.com/de-de/application-security/) und unterstützen Sie bei [Ihrer Umstellung auf Zero-Trust](https://www.cloudflare.com/de-de/products/zero-trust/).\n\nBesuchen Sie [1.1.1.1](https://1.1.1.1/) von einem beliebigen Gerät aus und nutzen Sie unsere kostenlose App, die Ihr Internet schneller und sicherer macht.\n\nWeitere Informationen über unsere Mission, ein besseres Internet zu schaffen, finden Sie [hier](https://www.cloudflare.com/de-de/learning/what-is-cloudflare/). Sie möchten sich beruflich neu orientieren? Dann werfen Sie doch einen Blick auf [unsere offenen Stellen](https://cloudflare.com/de-de/careers).\n\n[Workers AI (DE)](https://blog.cloudflare.com/tag/workers-ai-de/) [Mistral (DE)](https://blog.cloudflare.com/tag/mistral-de/) [Cloudflare Workers (DE)](https://blog.cloudflare.com/tag/cloudflare-workers-de/) [Deutsch](https://blog.cloudflare.com/tag/german-de/)"
    },
    {
      "url": "https://blog.cloudflare.com/fr-fr/workers-ai-update-hello-mistral-7b-fr-fr/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/fr-fr/workers-ai-update-hello-mistral-7b-fr-fr/",
        "loadedTime": "2023-12-05T02:36:02.919Z",
        "referrerUrl": "https://blog.cloudflare.com/workers-ai-update-hello-mistral-7b/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/fr-fr/workers-ai-update-hello-mistral-7b-fr-fr",
        "title": "Du nouveau dans Workers AI : bonjour Mistral 7B",
        "description": "Nous sommes heureux d'annoncer aujourd'hui l'ajout dans Workers AI du grand modèle de langage Mistral-7B-v0.1-instruct",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/21/2023\n8 min read\nNous sommes heureux d'annoncer aujourd'hui l'ajout dans Workers AI du grand modèle de langage Mistral-7B-v0.1-instruct. Il s'agit d'un modèle à 7,3 milliards de paramètres qui compte un grand nombre d'avantages uniques. Avec l'aide des fondateurs de Mistral AI, nous allons présenter certains points forts du modèle Mistral 7B. Nous profiterons également de l'occasion pour étudier en détail le concept d'attention et ses variantes telles que l'attention multi-requête (MQA pour multi-query attention) et l'attention par requête groupée (GQA pour grouped-query attention).\nMistral 7B tl;dr:\nMistral 7B est un modèle à 7,3 milliards de paramètres qui affiche des résultats impressionnants lors des évaluations des performances. Le modèle :\nobtient de meilleurs résultats que Llama 2 13B dans toutes les évaluations\nobtient de meilleurs résultats que Llama 1 34B dans toutes les évaluations,\nparvient à des performances presque identiques à celles de CodeLlama 7B en ce qui concerne le code, tout en restant performant dans les tâches liées à l'anglais et\nla version plus avancée du chat que nous avons déployée donne de meilleurs résultats que celui de Llama 2 13B dans les évaluations fournies par Mistral.\nVoici un exemple d'utilisation de la diffusion en continu avec l'API REST :\ncurl -X POST \\ “https://api.cloudflare.com/client/v4/accounts/{account-id}/ai/run/@cf/mistral/mistral-7b-instruct-v0.1” \\ -H “Authorization: Bearer {api-token}” \\ -H “Content-Type:application/json” \\ -d '{ “prompt”: “What is grouped query attention”, “stream”: true }' API Response: { response: “Grouped query attention is a technique used in natural language processing (NLP) and machine learning to improve the performance of models…” } \nEt voici un exemple avec un script Workers :\nimport { Ai } from '@cloudflare/ai'; export default { async fetch(request, env) { const ai = new Ai(env.AI); const stream = await ai.run('@cf/mistral/mistral-7b-instruct-v0.1', { prompt: 'What is grouped query attention', stream: true }); return Response.json(stream, { headers: { “content-type”: “text/event-stream” } }); } } \nMistral met à profit l'attention par requête groupée pour accélérer l'inférence. Cette technique récemment développée améliore la vitesse d'inférence sans compromettre la qualité du résultat. Pour des modèles à 7 milliards de paramètres, nous pouvons générer près de quatre fois plus de jetons par seconde avec Mistral qu'avec Llama, grâce à l'attention par requête groupée.\nÀ ce stade, vous n'avez pas besoin de plus d'informations pour commencer à utiliser Mistral-7B, vous pouvez le tester dès aujourd'hui ai.cloudflare.com. Pour en savoir plus sur l'attention en général et l'attention par requête groupée, poursuivez la lecture !\nAvant toute chose, qu'est-ce que « l'attention » ?\nLe mécanisme de base de l'attention, plus précisément « Scaled Dot-Product Attention » (Attention à produit scalaire pondéré) tel qu'il est présenté dans l'article qui fut déterminant Attention Is All You Need, est relativement simple :\nNous appelons notre attention particulière « Scale Dot-Product Attention » (Attention à produit scalaire pondéré). Les données d'entrée sont constituées d'une requête et des clés de dimension d_k, ainsi que des valeurs de dimension d_v. Nous calculons les produits scalaires de la requête avec toutes les clés, avant de les diviser chacun par sqrt(d_k) et d'appliquer une fonction softmax pour obtenir la pondération des valeurs.\nPlus concrètement, voici à quoi cela ressemble :\nsource\nEn termes plus simples, ce type d'attention permet aux modèles de se concentrer sur des éléments essentiels de l'entrée. Imaginez-vous en train de lire une phrase et d'essayer de la comprendre. Avec l'attention à produit scalaire pondéré, vous lisez en priorité certains mots en fonction de leur pertinence. Pour ce faire, l'attention calcule la similarité entre chaque mot (K) de la phrase et une requête (Q). Ensuite, les scores de similarités sont divisés par la racine carrée de la dimension de la requête. Cette mise à l'échelle permet d'éviter les valeurs très petites ou très grandes. Enfin, ces scores de similarité ainsi divisés nous permettent d'établir l'attention ou l'importance à accorder à chaque mot. Grâce à ce mécanisme d'attention, les modèles peuvent identifier les informations qui sont cruciales (V) et améliorer leurs capacités de compréhension et de traduction.\nC'est simple non ? Pour passer de ce simple mécanisme à une intelligence artificielle capable d'écrire un épisode de Seinfeld dans lequel Jerry apprend l'algorithme du tri à bulles, il nous faut le rendre plus complexe. (En réalité, dans tout ce que nous avons présenté, il n'existe pas encore de paramètres appris ; des valeurs constantes apprises au cours de l'entraînement du modèle et qui permettent de personnaliser le résultat du bloc d'attention !)\nLes blocs d'attention dans le style de ceux de l'article Attention is All You Need ajoutent essentiellement trois types de complexité :\nParamètres appris\nLes paramètres appris correspondent aux valeurs ou aux poids qui sont ajustés pendant le processus d'entraînement d'un modèle dans le but d'en améliorer les performances. Ces paramètres servent à contrôler le flux d'information ou d'attention au sein d'un modèle, afin de le concentrer sur les éléments les plus pertinents des données d'entrées. En d'autres termes, les paramètres appris sont comparables à des boutons sur une machine, qu'il est possible de régler pour en optimiser le fonctionnement.\nEmpilement vertical - blocs d'attention en couches\nL'empilement en couches vertical est une façon d'empiler de multiples mécanismes d'attention les uns au-dessus des autres, chaque couche reprenant le résultat de la couche précédente. Le modèle est ainsi en mesure de centrer ses efforts sur différentes parties des données entrées à différents niveaux d'abstraction, ce qui améliore les performances pour certaines tâches.\nL'empilement parallèle, également appelé attention multi-tête\nLa figure illustrant l'article présente le modèle d'attention multi-tête. De multiples opérations d'attention sont exécutées en parallèle, l'entrée Q-K-V de chacune étant générée par une projection linéaire unique des mêmes données d'entrée (définie par un ensemble unique de paramètres appris). Les blocs d'attention parallèle sont appelés « têtes d'attention ». Les résultats pondérés de toutes les têtes d'attention sont réunis dans un vecteur unique et subissent une autre transformation linéaire paramétrée avant de produire le résultat final.\nsource\nAvec ce mécanisme le modèle se concentre sur différents éléments des données d'entrée en même temps. Imaginez que vous essayez de comprendre une information complexe telle qu'une phrase ou un paragraphe. Pour la comprendre, vous devez examiner plusieurs éléments différents en même temps. Vous devez par exemple faire attention au sujet de la phrase, au verbe et à l'objet, le tout simultanément pour comprendre le sens de la phrase. L'attention multi-tête fonctionne de la même manière. Elle permet tau modèle de se concentrer sur différents éléments des données d'entrée en même temps, à l'aide des multiples « têtes » d'attention. Chaque tête d'attention centre ses efforts sur différents éléments des données d'entrée et le résultat de toutes les têtes est combiné pour produire le résultat final du modèle.\nTypes d'attention\nTrois arrangements courants pour les blocs d'attention sont utilisés par les grands modèles de langage développés au cours des dernières années, l'attention multi-tête (MHA), l'attention par requête groupée (GQA) et l'attention multi-requête (MQA). Ils diffèrent dans le nombre de vecteurs K et V relatif au nombre de vecteurs de requêtes. L'attention multi-tête utilise le même nombre de vecteurs K et V que de vecteurs Q, indiqué par la lettre « N » dans le tableau ci-dessous. L'attention multi-requête utilise un vecteur K et V unique. L'attention par requête groupée, le type utilisé dans le modèle Mistral 7B, divise les vecteurs Q en parts égales contenant « G » vecteurs chacunes, elle utilise ensuit un vecteur K et V unique pour chaque groupe, pour un total de N divisé par G ensembles de vecteurs K et V. Le tableau 1 ci-dessous présente un récapitulatif des différences, et nous allons expliquer ensuite ce qu'elles impliquent.\n\n\t\nNombre de blocs clé/valeur\n\t\nQualité\n\t\nUtilisation de mémoire\n\t\nAttention multi-tête (MHA)\n\t\nN\n\t\nLa meilleure\n\t\nLa plus importante\n\t\nAttention par requête groupée (GQA)\n\t\nN / G\n\t\nMeilleure\n\t\nInférieure\n\t\nAttention multi-requête (MQA)\n\t\n1\n\t\nLégitime\n\t\nLa plus faible\n\t\nRécapitulatif des styles d'attention\nCe diagramme illustre les différences entre les trois styles :\nSource\nAttention multi-requête\nL'attention multi-requête a été présentée en 2019 dans la publication de Google : Fast Transformer Decoding: One Write-Head is All You Need. L'idée est qu'au lieu de créer des entrées K et V séparées pour chaque vecteur Q dans le mécanisme d'attention, ce qui correspond à l'attention multi-tête mentionnée précédemment, un seul vecteur K et V est utilisé pour l'ensemble des vecteurs Q. Comme le nom le suggère, de multiples requêtes sont combinées dans un mécanisme d'attention unique. Dans la publication, l'évaluation comparative dudit mécanisme se faisait sur une tâche de traduction et ses performances étaient équivalentes à celles de l'attention multi-tête.\nÀ l'origine, l'idée était de réduire la quantité totale de mémoire à laquelle il était nécessaire d'accéder pendant l'exécution de l'inférence pour le modèle. Depuis, des modèles généralisés sont apparus, avec des paramètres toujours plus nombreux, en conséquence, la mémoire processeur nécessaire est souvent le goulot d'étranglement et c'est ce qui fait la force de l'attention multi-requête. C'est, parmi les trois types d'attention, celle qui exige le moins de mémoire d'accélération. Cependant, à mesure que les modèles gagnaient en volume et en généralité, les performances de l'attention multi-requête ont régressé par rapport à celles de l'attention multi-tête.\nAttention par requête groupée\nLa plus récente des trois (celle utilisée par Mistral) est l'attention par requête groupée, telle qu'elle est décrite dans la publication GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints sortie sur arxiv.org en mai 2023. L'attention par requête groupée combine le meilleur des deux autres versions : la qualité de l'attention multi-tête d'une part et la vitesse et faible consommation de mémoire de l'attention multi-requête d'autre part. Au lieu d'avoir soit un ensemble unique de vecteurs K et V, soit un ensemble pour chaque vecteur Q, un rapport fixe d'un ensemble de vecteurs K et V pour chaque vecteur Q est utilisé, ce qui réduit la consommation de mémoire sans nuire aux performances pour de nombreuses tâches.\nSouvent, la sélection d'un modèle pour une tâche de production ne se résume pas à simplement choisir le meilleur modèle disponible car il convient de trouver le bon équilibre entre performances, utilisation de la mémoire, taille des lots et le matériel disponible (ou les coûts de cloud). Il peut être utile de comprendre le fonctionnement de ces trois styles d'attention au moment de prendre une décision, afin de savoir pourquoi il est préférable de choisir un modèle particulier en fonction des circonstances.\nDécouvrez Mistral — essayez-le dès aujourd'hui\nS'agissant d'un des premiers grands modèles de langage à exploiter l'attention par requête groupée et à la combiner avec l'attention à fenêtre coulissante, Mistral semble avoir atteint un idéal : un modèle à faible latence, avec un haut débit et qui obtient un très bon classement dans les évaluations comparatives, même en face de modèles plus importants (13B). Autant dire qu'il est impressionnant pour sa taille et que nous sommes on ne peut plus heureux de le mettre à disposition de tous les développeurs aujourd'hui, dans le cadre de Workers AI. \nConsultez nos documents pour développeurs afin de vous lancer, et si vous avez besoin d'aide ou si vous souhaitez nous faire part de vos commentaires ou nous expliquer ce que vous êtes en train de développer, contactez-nous sur Developer Discord !\nL'équipe Workers AI se développe et recrute ; consultez les postes vacants sur notre page de recrutement si vous êtes passionné par l'ingénierie de l'intelligence artificielle et que vous souhaitez participer à la création et à l'évolution d'une plateforme d'inférence serverless reposant sur des processeurs graphiques.\nNous protégeons des réseaux d'entreprise entiers, aidons nos clients à développer efficacement des applications à l'échelle d'Internet, accélérons n'importe quel site web ou application Internet, repoussons les attaques DDoS, maintenons les pirates à distance et pouvons vous aider dans votre parcours vers le Zero Trust. \nRendez-vous sur 1.1.1.1 depuis n'importe quel appareil pour commencer à utiliser notre application gratuite, qui rend votre navigation Internet plus rapide et plus sûre. \nPour en savoir plus sur notre mission visant à bâtir un meilleur Internet, cliquez ici. Si vous cherchez de nouvelles perspectives professionnelles, consultez nos postes vacants. \nWorkers AI (FR) Mistral (FR) Cloudflare Workers (FR) Français",
      "markdown": "11/21/2023\n\n*   [![Jesse Kipp](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2020/12/jesse-headshot-square.jpg)](https://blog.cloudflare.com/author/jesse/)\n*   [![Isaac Rehg](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/11/Isaac-Rehg.jpeg)](https://blog.cloudflare.com/author/isaac-rehg/)\n\n8 min read\n\n![](https://blog.cloudflare.com/content/images/2023/11/Mistral-2-2.png)\n\nNous sommes heureux d'annoncer aujourd'hui l'ajout dans Workers AI du grand modèle de langage Mistral-7B-v0.1-instruct. Il s'agit d'un modèle à 7,3 milliards de paramètres qui compte un grand nombre d'avantages uniques. Avec l'aide des fondateurs de Mistral AI, nous allons présenter certains points forts du modèle Mistral 7B. Nous profiterons également de l'occasion pour étudier en détail le concept d'attention et ses variantes telles que l'attention multi-requête (MQA pour multi-query attention) et l'attention par requête groupée (GQA pour grouped-query attention).\n\n## Mistral 7B tl;dr:\n\nMistral 7B est un modèle à 7,3 milliards de paramètres qui affiche des résultats [impressionnants lors des évaluations des performances](https://mistral.ai/news/announcing-mistral-7b/). Le modèle :\n\n*   obtient de meilleurs résultats que Llama 2 13B dans toutes les évaluations\n*   obtient de meilleurs résultats que Llama 1 34B dans toutes les évaluations,\n*   parvient à des performances presque identiques à celles de CodeLlama 7B en ce qui concerne le code, tout en restant performant dans les tâches liées à l'anglais et\n*   la version plus avancée du chat que nous avons déployée donne de meilleurs résultats que celui de Llama 2 13B dans les évaluations fournies par Mistral.\n\nVoici un exemple d'utilisation de la diffusion en continu avec [l'API REST](https://developers.cloudflare.com/workers-ai/get-started/rest-api/) :\n\n```\ncurl -X POST \\\n“https://api.cloudflare.com/client/v4/accounts/{account-id}/ai/run/@cf/mistral/mistral-7b-instruct-v0.1” \\\n-H “Authorization: Bearer {api-token}” \\\n-H “Content-Type:application/json” \\\n-d '{ “prompt”: “What is grouped query attention”, “stream”: true }'\n\nAPI Response: { response: “Grouped query attention is a technique used in natural language processing  (NLP) and machine learning to improve the performance of models…” }\n```\n\nEt voici un exemple avec un script Workers :\n\n```\nimport { Ai } from '@cloudflare/ai';\nexport default {\n    async fetch(request, env) {\n        const ai = new Ai(env.AI);\n        const stream = await ai.run('@cf/mistral/mistral-7b-instruct-v0.1', {\n            prompt: 'What is grouped query attention',\n            stream: true\n        });\n        return Response.json(stream, { headers: { “content-type”: “text/event-stream” } });\n    }\n}\n```\n\nMistral met à profit l'attention par requête [groupée pour accélérer l'inférence](https://arxiv.org/abs/2305.13245). Cette technique récemment développée améliore la vitesse d'inférence sans compromettre la qualité du résultat. Pour des modèles à 7 milliards de paramètres, nous pouvons générer près de quatre fois plus de jetons par seconde avec Mistral qu'avec Llama, grâce à l'attention par requête groupée.\n\nÀ ce stade, vous n'avez pas besoin de plus d'informations pour commencer à utiliser Mistral-7B, vous pouvez le tester dès aujourd'hui [ai.cloudflare.com](https://ai.cloudflare.com/). Pour en savoir plus sur l'attention en général et l'attention par requête groupée, poursuivez la lecture !\n\n## Avant toute chose, qu'est-ce que « l'attention » ?\n\nLe mécanisme de base de l'attention, plus précisément « Scaled Dot-Product Attention » (Attention à produit scalaire pondéré) tel qu'il est présenté dans l'article qui fut déterminant [Attention Is All You Need](https://arxiv.org/abs/1706.03762), est relativement simple :\n\n> Nous appelons notre attention particulière « Scale Dot-Product Attention » (Attention à produit scalaire pondéré). Les données d'entrée sont constituées d'une requête et des clés de dimension d\\_k, ainsi que des valeurs de dimension d\\_v. Nous calculons les produits scalaires de la requête avec toutes les clés, avant de les diviser chacun par sqrt(d\\_k) et d'appliquer une fonction softmax pour obtenir la pondération des valeurs.\n\nPlus concrètement, voici à quoi cela ressemble :\n\n![](https://blog.cloudflare.com/content/images/2023/11/Scaled-dot-product-attention.png)\n\n[source](https://arxiv.org/abs/1706.03762)\n\nEn termes plus simples, ce type d'attention permet aux modèles de se concentrer sur des éléments essentiels de l'entrée. Imaginez-vous en train de lire une phrase et d'essayer de la comprendre. Avec l'attention à produit scalaire pondéré, vous lisez en priorité certains mots en fonction de leur pertinence. Pour ce faire, l'attention calcule la similarité entre chaque mot (K) de la phrase et une requête (Q). Ensuite, les scores de similarités sont divisés par la racine carrée de la dimension de la requête. Cette mise à l'échelle permet d'éviter les valeurs très petites ou très grandes. Enfin, ces scores de similarité ainsi divisés nous permettent d'établir l'attention ou l'importance à accorder à chaque mot. Grâce à ce mécanisme d'attention, les modèles peuvent identifier les informations qui sont cruciales (V) et améliorer leurs capacités de compréhension et de traduction.\n\nC'est simple non ? Pour passer de ce simple mécanisme à une intelligence artificielle capable d'écrire un épisode de Seinfeld dans lequel Jerry apprend l'algorithme du tri à bulles, il nous faut le rendre plus complexe. (En réalité, dans tout ce que nous avons présenté, il n'existe pas encore de paramètres appris ; des valeurs constantes apprises au cours de l'entraînement du modèle et qui permettent de personnaliser le résultat du bloc d'attention !)\n\nLes blocs d'attention dans le style de ceux de l'article Attention is All You Need ajoutent essentiellement trois types de complexité :\n\n### Paramètres appris\n\nLes paramètres appris correspondent aux valeurs ou aux poids qui sont ajustés pendant le processus d'entraînement d'un modèle dans le but d'en améliorer les performances. Ces paramètres servent à contrôler le flux d'information ou d'attention au sein d'un modèle, afin de le concentrer sur les éléments les plus pertinents des données d'entrées. En d'autres termes, les paramètres appris sont comparables à des boutons sur une machine, qu'il est possible de régler pour en optimiser le fonctionnement.\n\n### Empilement vertical - blocs d'attention en couches\n\nL'empilement en couches vertical est une façon d'empiler de multiples mécanismes d'attention les uns au-dessus des autres, chaque couche reprenant le résultat de la couche précédente. Le modèle est ainsi en mesure de centrer ses efforts sur différentes parties des données entrées à différents niveaux d'abstraction, ce qui améliore les performances pour certaines tâches.\n\n### L'empilement parallèle, également appelé attention multi-tête\n\nLa figure illustrant l'article présente le modèle d'attention multi-tête. De multiples opérations d'attention sont exécutées en parallèle, l'entrée Q-K-V de chacune étant générée par une projection linéaire unique des mêmes données d'entrée (définie par un ensemble unique de paramètres appris). Les blocs d'attention parallèle sont appelés « têtes d'attention ». Les résultats pondérés de toutes les têtes d'attention sont réunis dans un vecteur unique et subissent une autre transformation linéaire paramétrée avant de produire le résultat final.\n\n![](https://blog.cloudflare.com/content/images/2023/11/Multi-head-attention.png)\n\n[source](https://arxiv.org/abs/1706.03762)\n\nAvec ce mécanisme le modèle se concentre sur différents éléments des données d'entrée en même temps. Imaginez que vous essayez de comprendre une information complexe telle qu'une phrase ou un paragraphe. Pour la comprendre, vous devez examiner plusieurs éléments différents en même temps. Vous devez par exemple faire attention au sujet de la phrase, au verbe et à l'objet, le tout simultanément pour comprendre le sens de la phrase. L'attention multi-tête fonctionne de la même manière. Elle permet tau modèle de se concentrer sur différents éléments des données d'entrée en même temps, à l'aide des multiples « têtes » d'attention. Chaque tête d'attention centre ses efforts sur différents éléments des données d'entrée et le résultat de toutes les têtes est combiné pour produire le résultat final du modèle.\n\n## Types d'attention\n\nTrois arrangements courants pour les blocs d'attention sont utilisés par les grands modèles de langage développés au cours des dernières années, l'attention multi-tête (MHA), l'attention par requête groupée (GQA) et l'attention multi-requête (MQA). Ils diffèrent dans le nombre de vecteurs K et V relatif au nombre de vecteurs de requêtes. **L'attention multi-tête** utilise le même nombre de vecteurs K et V que de vecteurs Q, indiqué par la lettre « N » dans le tableau ci-dessous. **L'attention multi-requête** utilise un vecteur K et V unique. **L'attention par requête groupée**, le type utilisé dans le modèle Mistral 7B, divise les vecteurs Q en parts égales contenant « G » vecteurs chacunes, elle utilise ensuit un vecteur K et V unique  pour chaque groupe, pour un total de N divisé par G ensembles de vecteurs K et V. Le tableau 1 ci-dessous présente un récapitulatif des différences, et nous allons expliquer ensuite ce qu'elles impliquent.\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|     | Nombre de blocs clé/valeur | Qualité | Utilisation de mémoire |\n| Attention multi-tête (MHA) | N   | La meilleure | La plus importante |\n| Attention par requête groupée (GQA) | N / G | Meilleure | Inférieure |\n| Attention multi-requête (MQA) | 1   | Légitime | La plus faible |\n\nRécapitulatif des styles d'attention\n\nCe diagramme illustre les différences entre les trois styles :\n\n![](https://blog.cloudflare.com/content/images/2023/11/Grouped-query.png)\n\n[Source](https://arxiv.org/pdf/2305.13245.pdf)\n\n### Attention multi-requête\n\nL'attention multi-requête a été présentée en 2019 dans la publication de Google : [Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/abs/1911.02150). L'idée est qu'au lieu de créer des entrées K et V séparées pour chaque vecteur Q dans le mécanisme d'attention, ce qui correspond à l'attention multi-tête mentionnée précédemment, un seul vecteur K et V est utilisé pour l'ensemble des vecteurs Q. Comme le nom le suggère, de multiples requêtes sont combinées dans un mécanisme d'attention unique. Dans la publication, l'évaluation comparative dudit mécanisme se faisait sur une tâche de traduction et ses performances étaient équivalentes à celles de l'attention multi-tête.\n\nÀ l'origine, l'idée était de réduire la quantité totale de mémoire à laquelle il était nécessaire d'accéder pendant l'exécution de l'inférence pour le modèle. Depuis, des modèles généralisés sont apparus, avec des paramètres toujours plus nombreux, en conséquence, la mémoire processeur nécessaire est souvent le goulot d'étranglement et c'est ce qui fait la force de l'attention multi-requête. C'est, parmi les trois types d'attention, celle qui exige le moins de mémoire d'accélération. Cependant, à mesure que les modèles gagnaient en volume et en généralité, les performances de l'attention multi-requête ont régressé par rapport à celles de l'attention multi-tête.\n\n### Attention par requête groupée\n\nLa plus récente des trois (celle utilisée par Mistral) est l'attention par requête groupée, telle qu'elle est décrite dans la publication [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/abs/2305.13245) sortie sur arxiv.org en mai 2023. L'attention par requête groupée combine le meilleur des deux autres versions : la qualité de l'attention multi-tête d'une part et la vitesse et faible consommation de mémoire de l'attention multi-requête d'autre part. Au lieu d'avoir soit un ensemble unique de vecteurs K et V, soit un ensemble pour chaque vecteur Q, un rapport fixe d'un ensemble de vecteurs K et V pour chaque vecteur Q est utilisé, ce qui réduit la consommation de mémoire sans nuire aux performances pour de nombreuses tâches.\n\nSouvent, la sélection d'un modèle pour une tâche de production ne se résume pas à simplement choisir le meilleur modèle disponible car il convient de trouver le bon équilibre entre performances, utilisation de la mémoire, taille des lots et le matériel disponible (ou les coûts de cloud). Il peut être utile de comprendre le fonctionnement de ces trois styles d'attention au moment de prendre une décision, afin de savoir pourquoi il est préférable de choisir un modèle particulier en fonction des circonstances.\n\n## Découvrez Mistral — essayez-le dès aujourd'hui\n\nS'agissant d'un des premiers grands modèles de langage à exploiter l'attention par requête groupée et à la combiner avec l'attention à fenêtre coulissante, Mistral semble avoir atteint un idéal : un modèle à faible latence, avec un haut débit et qui obtient un très bon classement dans les évaluations comparatives, même en face de modèles plus importants (13B). Autant dire qu'il est impressionnant pour sa taille et que nous sommes on ne peut plus heureux de le mettre à disposition de tous les développeurs aujourd'hui, dans le cadre de Workers AI.\n\nConsultez nos [documents pour développeurs](https://developers.cloudflare.com/workers-ai/models/text-generation/) afin de vous lancer, et si vous avez besoin d'aide ou si vous souhaitez nous faire part de vos commentaires ou nous expliquer ce que vous êtes en train de développer, contactez-nous sur [Developer Discord](https://discord.com/invite/cloudflaredev) !\n\nL'équipe Workers AI se développe et recrute ; consultez les postes vacants sur notre [page de recrutement](https://www.cloudflare.com/fr-fr/careers/jobs/) si vous êtes passionné par l'ingénierie de l'intelligence artificielle et que vous souhaitez participer à la création et à l'évolution d'une plateforme d'inférence serverless reposant sur des processeurs graphiques.\n\nNous protégeons [des réseaux d'entreprise entiers](https://www.cloudflare.com/fr-fr/network-services/), aidons nos clients à développer [efficacement des applications à l'échelle d'Internet](https://workers.cloudflare.com/), accélérons n'importe quel [site web ou application Internet,](https://www.cloudflare.com/fr-fr/performance/accelerate-internet-applications/) repoussons [les attaques DDoS](https://www.cloudflare.com/fr-fr/ddos/), maintenons [les pirates à distance](https://www.cloudflare.com/fr-fr/application-security/) et pouvons vous aider dans [votre parcours vers le Zero Trust](https://www.cloudflare.com/fr-fr/products/zero-trust/).\n\nRendez-vous sur [1.1.1.1](https://1.1.1.1/) depuis n'importe quel appareil pour commencer à utiliser notre application gratuite, qui rend votre navigation Internet plus rapide et plus sûre.\n\nPour en savoir plus sur notre mission visant à bâtir un meilleur Internet, cliquez [ici](https://www.cloudflare.com/fr-fr/learning/what-is-cloudflare/). Si vous cherchez de nouvelles perspectives professionnelles, consultez [nos postes vacants](https://cloudflare.com/fr-fr/careers).\n\n[Workers AI (FR)](https://blog.cloudflare.com/tag/workers-ai-fr/) [Mistral (FR)](https://blog.cloudflare.com/tag/mistral-fr/) [Cloudflare Workers (FR)](https://blog.cloudflare.com/tag/cloudflare-workers-fr/) [Français](https://blog.cloudflare.com/tag/french-fr/)"
    },
    {
      "url": "https://blog.cloudflare.com/writing-poems-using-llama-2-on-workers-ai/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/writing-poems-using-llama-2-on-workers-ai/",
        "loadedTime": "2023-12-05T02:36:03.665Z",
        "referrerUrl": "https://blog.cloudflare.com/workers-ai-update-stable-diffusion-code-llama-workers-ai-in-100-cities/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/writing-poems-using-llama-2-on-workers-ai/",
        "title": "Writing poems using LLama 2 on Workers AI",
        "description": "Matthew and Michelle, co-founders of Cloudflare, published their annual founders’ letter today. The letter ends with a poem written by an AI running using Workers AI on Cloudflare’s global network. \n\nHere’s the code that wrote the poem.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "09/27/2023\n3 min read\nMatthew and Michelle, co-founders of Cloudflare, published their annual founders’ letter today. The letter ends with a poem written by an AI running using Workers AI on Cloudflare’s global network.\nHere’s the code that wrote the poem. It uses Workers AI and the Meta Llama 2 model with 7B parameters and 8-bit integers. Just 14 lines of code running on the Cloudflare global network, and you’ve got your very own AI to chat with.\nimport { Ai } from \"@cloudflare/ai\"; export default { async fetch(request: Request, env: Env): Promise<Response> { const body = await request.json(); const ai = new Ai(env.AI); const response = await ai.run(\"@cf/meta/llama-2-7b-chat-int8\", body); return new Response(JSON.stringify(response)); }, }; export interface Env { AI: any; } \nThat was deployed on Workers AI and all I had to do was ask for poems. Here’s my terminal output (with just the domain name changed).\n% curl -X POST https://example.com/ -d '{\"prompt\":\"Write a poem \\ that talks about the connectivity cloud\"}' | jq -r .response \nCloud computing provides a connectivity that's unmatched,\nA bridge that spans the globe with ease and grace.\nIt brings us closer, no matter where we are,\nAnd makes the world a smaller, more intimate place.\nWith just a click, we can connect and share,\nOur thoughts and ideas, our hopes and dreams.\nThe cloud unites us, in a way that's fair,\nAnd makes the world a more harmonious theme.\nSo let us embrace this connectivity,\nAnd use it to build a better world for all.\nFor in the cloud, we find a unity,\nThat can help us reach new heights, and stand tall.\n% curl -X POST https://example.com/ -d '{\"prompt\":\"Write a poem \\ that talks about the connectivity cloud and contrast it with the \\ captivity cloud\"}' | jq -r .response \nThe connectivity cloud, a wondrous sight to see,\nWhere data flows and knowledge is set free,\nA place where minds meet and ideas take flight,\nA world of endless possibility in sight.\nBut then there's the captivity cloud, a darker place,\nWhere thoughts are trapped and minds are in disgrace,\nA prison of the mind, where freedom's lost,\nA world of endless sorrow and cost.\nSo let us choose the connectivity cloud,\nAnd let our minds be free and allowed,\nTo soar and glide and reach for the sky,\nAnd never be held back, but always fly.\n% curl -X POST https://example.com/ -d '{\"prompt\":\"Write a poem \\ that talks about the connectivity cloud and contrast it with the \\ captivity cloud using Elizabethan English\"}' | jq -r .response \nIn the connectivity cloud, I find my peace,\nWhere thoughts and dreams and hopes do cease to flee.\nThe captivity cloud doth but darken my day,\nWith chains of sorrow that will not away.\nIn the connectivity cloud, I find my home,\nWhere love and joy and laughter do make their moan.\nThe captivity cloud doth but lock me in,\nWith sorrow's shackles that will not give in.\nIn the connectivity cloud, I find my fate,\nWhere fortune smiles and misery doth wait.\nThe captivity cloud doth but bring me woe,\nWith darkness that will not let me go.\nIn the connectivity cloud, I find my heart,\nWhere love and hope and joy do play their part.\nThe captivity cloud doth but tear it apart,\nWith sorrow's hand that will not let it start.\n% curl -X POST https://example.com/ -d '{\"prompt\":\"Write a poem \\ that talks about the connectivity cloud and contrast it with the \\ captivity cloud in the style of Lord Byron\"}' | jq -r .response \nIn the realm of the cloud, where data reigns supreme,\nA new divide has emerged, a digital dream,\nThe connectivity cloud, a place of delight,\nWhere knowledge and wisdom take flight.\nBut then there's the captivity cloud, a darker hue,\nWhere privacy and security are hard to pursue,\nA prison of sorts, where freedom's light is lost,\nAnd data's secrets are forever crossed.\nYet still, we cling to the captivity cloud,\nFor in its depths, we find a measure of proof,\nThat though the connectivity cloud may be bright,\nThe captivity cloud holds a certain allure in sight.\nSo let us embrace both, the light and the dark,\nFor in their union, we find a digital spark,\nThat illuminates the path to a brighter day,\nWhere knowledge and privacy reign in a digital way.\nWhat are you going to build on Workers AI? It’s ready and waiting. We’ll help you go from idea to deployed in minutes. \nIf you want to know exactly how to deploy something like this read the Workers AI announcement blog.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nBirthday Week AI Workers AI Cloudflare Workers",
      "markdown": "09/27/2023\n\n*   [![John Graham-Cumming](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2017/03/url-2.jpg)](https://blog.cloudflare.com/author/john-graham-cumming/)\n\n3 min read\n\n![Writing poems using LLama 2 on Workers AI](https://blog.cloudflare.com/content/images/2023/09/image1-30.png)\n\nMatthew and Michelle, co-founders of Cloudflare, published their [annual founders’ letter today](https://blog.cloudflare.com/cloudflares-annual-founders-letter-2023/). The letter ends with a poem written by an AI running using Workers AI on Cloudflare’s global network.\n\nHere’s the code that wrote the poem. It uses [Workers AI](https://blog.cloudflare.com/workers-ai/) and the Meta [Llama 2 model with 7B parameters and 8-bit integers](https://ai.meta.com/llama/). Just 14 lines of code running on the Cloudflare global network, and you’ve got your very own [AI](https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/) to chat with.\n\n```\nimport { Ai } from \"@cloudflare/ai\";\n\nexport default {\n    async fetch(request: Request, env: Env): Promise<Response> {\n        const body = await request.json();\n        const ai = new Ai(env.AI);\n        const response = await ai.run(\"@cf/meta/llama-2-7b-chat-int8\", body);\n        return new Response(JSON.stringify(response));\n    },\n};\n\nexport interface Env {\n    AI: any;\n}\n```\n\nThat was deployed on Workers AI and all I had to do was ask for poems. Here’s my terminal output (with just the domain name changed).\n\n```\n% curl -X POST https://example.com/ -d '{\"prompt\":\"Write a poem \\\nthat talks about the connectivity cloud\"}' | jq -r .response\n```\n\n`Cloud computing provides a connectivity that's unmatched,`  \n`A bridge that spans the globe with ease and grace.`  \n`It brings us closer, no matter where we are,`  \n`And makes the world a smaller, more intimate place.`\n\n`With just a click, we can connect and share,`  \n`Our thoughts and ideas, our hopes and dreams.`  \n`The cloud unites us, in a way that's fair,`  \n`And makes the world a more harmonious theme.`\n\n`So let us embrace this connectivity,`  \n`And use it to build a better world for all.`  \n`For in the cloud, we find a unity,`  \n`That can help us reach new heights, and stand tall.`\n\n```\n% curl -X POST https://example.com/ -d '{\"prompt\":\"Write a poem \\\nthat talks about the connectivity cloud and contrast it with the  \\\ncaptivity cloud\"}' | jq -r .response\n```\n\n`The connectivity cloud, a wondrous sight to see,`  \n`Where data flows and knowledge is set free,`  \n`A place where minds meet and ideas take flight,`  \n`A world of endless possibility in sight.`\n\n`But then there's the captivity cloud, a darker place,`  \n`Where thoughts are trapped and minds are in disgrace,`  \n`A prison of the mind, where freedom's lost,`  \n`A world of endless sorrow and cost.`\n\n`So let us choose the connectivity cloud,`  \n`And let our minds be free and allowed,`  \n`To soar and glide and reach for the sky,`  \n`And never be held back, but always fly.`\n\n```\n% curl -X POST https://example.com/ -d '{\"prompt\":\"Write a poem \\\nthat talks about the connectivity cloud and contrast it with the  \\\ncaptivity cloud using Elizabethan English\"}' | jq -r .response\n```\n\n`In the connectivity cloud, I find my peace,`  \n`Where thoughts and dreams and hopes do cease to flee.`  \n`The captivity cloud doth but darken my day,`  \n`With chains of sorrow that will not away.`\n\n`In the connectivity cloud, I find my home,`  \n`Where love and joy and laughter do make their moan.`  \n`The captivity cloud doth but lock me in,`  \n`With sorrow's shackles that will not give in.`\n\n`In the connectivity cloud, I find my fate,`  \n`Where fortune smiles and misery doth wait.`  \n`The captivity cloud doth but bring me woe,`  \n`With darkness that will not let me go.`\n\n`In the connectivity cloud, I find my heart,`  \n`Where love and hope and joy do play their part.`  \n`The captivity cloud doth but tear it apart,`  \n`With sorrow's hand that will not let it start.`\n\n```\n% curl -X POST https://example.com/ -d '{\"prompt\":\"Write a poem \\\nthat talks about the connectivity cloud and contrast it with the  \\\ncaptivity cloud in the style of Lord Byron\"}' | jq -r .response\n```\n\n`In the realm of the cloud, where data reigns supreme,`  \n`A new divide has emerged, a digital dream,`  \n`The connectivity cloud, a place of delight,`  \n`Where knowledge and wisdom take flight.`\n\n`But then there's the captivity cloud, a darker hue,`  \n`Where privacy and security are hard to pursue,`  \n`A prison of sorts, where freedom's light is lost,`  \n`And data's secrets are forever crossed.`\n\n`Yet still, we cling to the captivity cloud,`  \n`For in its depths, we find a measure of proof,`  \n`That though the connectivity cloud may be bright,`  \n`The captivity cloud holds a certain allure in sight.`\n\n`So let us embrace both, the light and the dark,`  \n`For in their union, we find a digital spark,`  \n`That illuminates the path to a brighter day,`  \n`Where knowledge and privacy reign in a digital way.`\n\nWhat are you going to build on Workers AI? It’s ready and waiting. We’ll help you go from idea to deployed in minutes.\n\nIf you want to know exactly how to deploy something like this [read the Workers AI announcement blog](https://blog.cloudflare.com/workers-ai/).\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Birthday Week](https://blog.cloudflare.com/tag/birthday-week/) [AI](https://blog.cloudflare.com/tag/ai/) [Workers AI](https://blog.cloudflare.com/tag/workers-ai/) [Cloudflare Workers](https://blog.cloudflare.com/tag/workers/)"
    },
    {
      "url": "https://blog.cloudflare.com/nl-nl/workers-ai-update-hello-mistral-7b-nl-nl/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/nl-nl/workers-ai-update-hello-mistral-7b-nl-nl/",
        "loadedTime": "2023-12-05T02:36:13.345Z",
        "referrerUrl": "https://blog.cloudflare.com/workers-ai-update-hello-mistral-7b/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/nl-nl/workers-ai-update-hello-mistral-7b-nl-nl/",
        "title": "Workers AI Update: dit is Mistral 7B",
        "description": "Vandaag kunnen we met trots aankondigen dat we de Mistral-7B-v0.1-instructie hebben toegevoegd aan Workers AI",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/21/2023\n7 min read\nVandaag kunnen we met trots aankondigen dat we de Mistral-7B-v0.1-instructie hebben toegevoegd aan Workers AI. Mistral 7B is een taalmodel met 7,3 miljard parameters en een aantal unieke voordelen. Met wat hulp van de oprichters van Mistral AI bekijken we enkele van de hoogtepunten van het Mistral 7B-model en maken we van de gelegenheid gebruik om dieper in te gaan op \"attention\" en variaties daarop, zoals multi-query attention en grouped-query attention.\nMistral 7B tl;dr:\nMistral 7B is een model met 7,3 miljard parameters dat indrukwekkende scores behaalt op benchmarks. Het model:\nPresteert beter dan Llama 2 13B op alle benchmarks\nPresteert beter dan Llama 1 34B op veel benchmarks,\nBenadert de prestaties van CodeLlama 7B op code, zonder afbreuk aan het Engelse taalvermogen, en\nDe versie met gespecialiseerde chat die we hebben ingezet presteert beter dan Llama 2 13B chat op de benchmarks die Mistral heeft geleverd.\nHier is een voorbeeld van het gebruik van streaming met de REST API:\ncurl -X POST \\ “https://api.cloudflare.com/client/v4/accounts/{account-id}/ai/run/@cf/mistral/mistral-7b-instruct-v0.1” \\ -H “Authorization: Bearer {api-token}” \\ -H “Content-Type:application/json” \\ -d '{ “prompt”: “What is grouped query attention”, “stream”: true }' API Response: { response: “Grouped query attention is a technique used in natural language processing (NLP) and machine learning to improve the performance of models…” } \nEn hier is een voorbeeld met een Worker-script:\nimport { Ai } from '@cloudflare/ai'; export default { async fetch(request, env) { const ai = new Ai(env.AI); const stream = await ai.run('@cf/mistral/mistral-7b-instruct-v0.1', { prompt: 'What is grouped query attention', stream: true }); return Response.json(stream, { headers: { “content-type”: “text/event-stream” } }); } } \nMistral maakt gebruik van grouped-query attention voor snellere inferentie. Deze recent ontwikkelde techniek verbetert de snelheid van inferentie zonder afbreuk te doen aan de uitvoerkwaliteit. Voor modellen met 7 miljard parameters kunnen we met Mistral bijna 4x zoveel tokens per seconde genereren als met Llama, dankzij Grouped-Query attention.\nU heeft verder geen informatie nodig om Mistral-7B te gaan gebruiken, u kunt het vandaag nog uitproberen op ai.cloudflare.com. Lees verder voor meer informatie over attention en grouped-query attention!\nWat is “attention” eigenlijk?\nHet basismechanisme van attention, specifiek “Scaled Dot-Product Attention” zoals geïntroduceerd in het baanbrekende artikel Attention Is All You Need, is vrij eenvoudig:\nWe noemen onze bijzondere attention “Scale Dot-Product Attention”. De invoer bestaat uit een query en sleutels van dimensie d_k, en waarden van dimensie d_v. We berekenen de puntproducten van de query met alle sleutels, delen elk door sqrt(d_k) en passen een softmax-functie toe om de gewichten op de waarden te verkrijgen.\nConcreet ziet dit er als volgt uit:\nsource\nEenvoudiger gezegd: hierdoor kunnen modellen zich richten op belangrijke delen van de invoer. Stel je voor dat je een zin leest en probeert te begrijpen. Met Scaled dot product attention kun je meer aandacht besteden aan bepaalde woorden op basis van hun relevantie. Het werkt door de gelijkenis tussen elk woord (K) in de zin en een query (Q) te berekenen. Vervolgens worden de similariteitsscores geschaald door ze te delen door de vierkantswortel van de dimensie van de query. Deze schaling helpt om erg kleine of erg grote waarden te vermijden. Tot slot kunnen we met behulp van deze geschaalde similariteitsscores bepalen hoeveel aandacht of belang elk woord zou moeten krijgen. Dit attention-mechanisme helpt modellen cruciale informatie (V) te identificeren en hun begrip en vertaalvermogen te verbeteren.\nSimpel, toch? Om van dit eenvoudige mechanisme naar een AI te komen die een “Seinfeld-aflevering” kan schrijven waarin Jerry het bubble sort-algoritme leert, moeten we het complexer maken. In feite heeft alles wat we net hebben behandeld niet eens geleerde parameters - constante waarden die tijdens de modeltraining worden geleerd en die de uitvoer van het attention-blok aanpassen!\nAttention blocks in the style of Attention is All You Need add mainly three types of complexity:\nAangeleerde parameters\nAangeleerde parameters verwijzen naar waarden of gewichten die worden aangepast tijdens het trainingsproces van een model om de prestaties ervan te verbeteren. Deze parameters worden gebruikt om de informatiestroom of aandacht binnen het model te regelen, zodat het model zich kan richten op de meest relevante delen van de invoergegevens. Eenvoudiger gezegd, zijn geleerde parameters als instelbare knoppen op een machine waaraan gedraaid kan worden om de werking te optimaliseren.\nVerticaal stapelen - gelaagde attention-blokken\nVerticaal stapelen in lagen is een manier om meerdere attention-mechanismen op elkaar te stapelen, waarbij elke laag voortbouwt op de uitvoer van de vorige laag. Hierdoor kan het model zich richten op verschillende delen van de invoergegevens op verschillende abstractieniveaus, wat kan leiden tot betere prestaties bij bepaalde taken.\nHorizontaal stapelen - ook wel bekend als Multi-Head Attention\nDe figuur uit de paper toont de volledige multi-head attention-module. Meerdere attention-operaties worden parallel uitgevoerd, waarbij de Q-K-V invoer voor elke operatie wordt gegenereerd door een unieke lineaire projectie van dezelfde invoergegevens (gedefinieerd door een unieke set aangeleerde parameters). Deze parallelle attention-blokken worden “attention heads” genoemd. De gewogen-som outputs van alle attention heads worden samengevoegd tot een enkele vector en door een andere geparametriseerde lineaire transformatie gehaald om de uiteindelijke output te krijgen.\nsource\nDankzij dit mechanisme kan een model zich tegelijkertijd op verschillende delen van de invoergegevens richten. Stel je voor dat je een complex stuk informatie probeert te begrijpen, zoals een zin of een alinea. Om het te begrijpen, moet je aandacht besteden aan verschillende delen tegelijk. Je moet bijvoorbeeld aandacht besteden aan het onderwerp van de zin, het werkwoord en het lijdend voorwerp, allemaal tegelijk, om de betekenis van de zin te begrijpen. Multi-headed attention werkt op dezelfde manier. Het stelt een model in staat om aandacht te besteden aan verschillende delen van de invoergegevens op hetzelfde moment, door gebruik te maken van meerdere “attention heads”. Elk attention-head richt zich op een ander aspect van de invoergegevens en de uitvoer van alle heads wordt gecombineerd tot de uiteindelijke uitvoer van het model.\nSoorten attention\nEr zijn drie veelvoorkomende opstellingen van attention-blocks die worden gebruikt door grote taalmodellen die de afgelopen jaren zijn ontwikkeld: multi-head attention, grouped-query attention en multi-query attention. Ze verschillen in het aantal K- en V-vectoren ten opzichte van het aantal queryvectoren. Multi-head attention gebruikt hetzelfde aantal K- en V-vetoren als Q-vectoren, aangegeven als “N” in de onderstaande tabel. Multi-query attention gebruikt slechts een enkele K- en V-vector. Grouped-query attention, het type dat gebruikt wordt in het Mistral 7B model, verdeelt de Q-vectoren gelijkmatig in groepen die elk “G” vectoren bevatten en gebruikt dan een enkele K- en V-vector voor elke groep voor een totaal van N gedeeld door G sets van K- en V-vectoren. Dit is een samenvatting van de verschillen en we gaan hieronder in de implicaties hiervan.\n\t\nAantal key/value-blokken\n\t\nKwaliteit\n\t\nGeheugengebruik\n\t\nMulti-head attention (MHA)\n\t\nN\n\t\nBeste\n\t\nMeeste\n\t\nGrouped-query attention (GQA)\n\t\nN / G\n\t\nBeter\n\t\nMinder\n\t\nMulti-query attention (MQA)\n\t\n1\n\t\nGoed\n\t\nMinste\n\t\nSamenvatting van soorten attention\nEn dit diagram helpt het verschil tussen de drie soorten te illustreren:\nsource\nMulti-Query Attention\nMulti-query attention werd in 2019 beschreven in het paper van Google: Fast Transformer Decoding: One Write-Head is All You Need. Het idee is dat in plaats van aparte K- en V-items te maken voor elke Q-vector in het attention-mechanisme, zoals in de bovengenoemde multi-head attention, er slechts één enkele K- en V-vector wordt gebruikt voor de hele reeks Q-vectoren. Vandaar de naam, meerdere query's gecombineerd in een enkel aandachtsmechanisme. In de paper werd dit gebenchmarkt op een vertaaltaak en toonde het prestaties gelijk aan multi-head aandacht op de benchmarktaak.\nOorspronkelijk was het idee om de totale grootte van het geheugen dat wordt gebruikt bij het uitvoeren van inferentie voor het model te verkleinen. Sindsdien, met de opkomst van gegeneraliseerde modellen en het toenemen van het aantal parameters, is het benodigde GPU-geheugen vaak de bottleneck. Dit is de kracht van multi-query aandacht, omdat het van de drie soorten aandacht het minste versnellergeheugen vereist. Naarmate de modellen echter groter en algemener werden, daalde de prestatie van multi-query aandacht ten opzichte van multi-head aandacht.\nGrouped-Query Attention\nDe jongste van het stel, en de attention die gebruikt wordt door Mistral, is grouped-query attention, zoals beschreven in de paper GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints dat in mei 2023 werd gepubliceerd op arxiv.org. Gegroepeerde-query aandacht combineert het beste van twee werelden: de kwaliteit van meervoudige aandacht met de snelheid en het lage geheugengebruik van multi-query attention. In plaats van een enkele set van K- en V-vectoren of een set voor elke Q-vector, wordt een vaste verhouding van 1 set van K- en V-vectoren voor elke Q-vector gebruikt, waardoor minder geheugen wordt gebruikt maar de hoge prestaties op veel taken behouden blijven.\nVaak gaat het bij het kiezen van een model voor een productietaak niet alleen om het kiezen van het beste model dat beschikbaar is, omdat we afwegingen moeten maken tussen prestaties, geheugengebruik, batchgrootte en beschikbare hardware (of cloudkosten). Inzicht in deze drie stijlen van aandacht kan helpen bij het nemen van deze beslissingen en begrijpen wanneer we, gegeven onze omstandigheden, voor een bepaald model zouden kunnen kiezen.\nWerk met Mistral — probeer het vandaag nog\nAls een van de eerste grote taalmodellen die gebruik maakt van grouped-query attention en deze combineert met 'sliding window'-attention, lijkt Mistral het optimale compromis gevonden te hebben: het heeft een lage latentie, een hoge doorvoer en het presteert echt goed op benchmarks, zelfs in vergelijking met grotere modellen (13B). Dit alles wil zeggen dat het een kracht heeft voor zijn grootte. Met veel trots maken we het vandaag beschikbaar voor alle ontwikkelaars, via Workers AI. \nGa nu naar onze developer docs om te beginnen, en als u hulp nodig heeft, feedback wilt geven, of wilt delen wat u aan het bouwen bent, kom gewoon langs op onze Developer Discord!\nHet Workers AI-team is ook op zoek naar nieuwe medewerkers, zie onze jobs-pagina voor openstaande functies als je gepassioneerd bent over AI-engineering en ons wilt helpen bij het bouwen en ontwikkelen van ons wereldwijde, serverloze GPU-aangedreven inferentieplatform.\nWorkers AI (NL) Mistral (NL) Cloudflare Workers (NL) Nederlands",
      "markdown": "11/21/2023\n\n*   [![Jesse Kipp](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2020/12/jesse-headshot-square.jpg)](https://blog.cloudflare.com/author/jesse/)\n*   [![Isaac Rehg](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/11/Isaac-Rehg.jpeg)](https://blog.cloudflare.com/author/isaac-rehg/)\n\n7 min read\n\n![Workers AI Update: Hello Mistral 7B](https://blog.cloudflare.com/content/images/2023/11/Mistral-1.png)\n\nVandaag kunnen we met trots aankondigen dat we de Mistral-7B-v0.1-instructie hebben toegevoegd aan Workers AI. Mistral 7B is een taalmodel met 7,3 miljard parameters en een aantal unieke voordelen. Met wat hulp van de oprichters van Mistral AI bekijken we enkele van de hoogtepunten van het Mistral 7B-model en maken we van de gelegenheid gebruik om dieper in te gaan op \"attention\" en variaties daarop, zoals multi-query attention en grouped-query attention.\n\n## Mistral 7B tl;dr:\n\nMistral 7B is een model met 7,3 miljard parameters dat [indrukwekkende scores behaalt op benchmarks](https://mistral.ai/news/announcing-mistral-7b/). Het model:\n\n*   Presteert beter dan Llama 2 13B op alle benchmarks\n*   Presteert beter dan Llama 1 34B op veel benchmarks,\n*   Benadert de prestaties van CodeLlama 7B op code, zonder afbreuk aan het Engelse taalvermogen, en\n*   De versie met gespecialiseerde chat die we hebben ingezet presteert beter dan Llama 2 13B chat op de benchmarks die Mistral heeft geleverd.\n\nHier is een voorbeeld van het gebruik van streaming met de [REST API](https://developers.cloudflare.com/workers-ai/get-started/rest-api/):\n\n```\ncurl -X POST \\\n“https://api.cloudflare.com/client/v4/accounts/{account-id}/ai/run/@cf/mistral/mistral-7b-instruct-v0.1” \\\n-H “Authorization: Bearer {api-token}” \\\n-H “Content-Type:application/json” \\\n-d '{ “prompt”: “What is grouped query attention”, “stream”: true }'\n\nAPI Response: { response: “Grouped query attention is a technique used in natural language processing  (NLP) and machine learning to improve the performance of models…” }\n```\n\nEn hier is een voorbeeld met een Worker-script:\n\n```\nimport { Ai } from '@cloudflare/ai';\nexport default {\n    async fetch(request, env) {\n        const ai = new Ai(env.AI);\n        const stream = await ai.run('@cf/mistral/mistral-7b-instruct-v0.1', {\n            prompt: 'What is grouped query attention',\n            stream: true\n        });\n        return Response.json(stream, { headers: { “content-type”: “text/event-stream” } });\n    }\n}\n```\n\nMistral maakt gebruik van [grouped-query attention](https://arxiv.org/abs/2305.13245) voor snellere inferentie. Deze recent ontwikkelde techniek verbetert de snelheid van inferentie zonder afbreuk te doen aan de uitvoerkwaliteit. Voor modellen met 7 miljard parameters kunnen we met Mistral bijna 4x zoveel tokens per seconde genereren als met Llama, dankzij Grouped-Query attention.\n\nU heeft verder geen informatie nodig om Mistral-7B te gaan gebruiken, u kunt het vandaag nog uitproberen op [ai.cloudflare.com](https://ai.cloudflare.com/). Lees verder voor meer informatie over attention en grouped-query attention!\n\n## Wat is “attention” eigenlijk?\n\nHet basismechanisme van attention, specifiek “Scaled Dot-Product Attention” zoals geïntroduceerd in het baanbrekende artikel [Attention Is All You Need](https://arxiv.org/abs/1706.03762), is vrij eenvoudig:\n\n> We noemen onze bijzondere attention “Scale Dot-Product Attention”. De invoer bestaat uit een query en sleutels van dimensie d\\_k, en waarden van dimensie d\\_v. We berekenen de puntproducten van de query met alle sleutels, delen elk door sqrt(d\\_k) en passen een softmax-functie toe om de gewichten op de waarden te verkrijgen.\n\nConcreet ziet dit er als volgt uit:\n\n![](https://blog.cloudflare.com/content/images/2023/11/Screenshot-2023-11-21-at-09.12.30.png)\n\n[source](https://arxiv.org/abs/1706.03762)\n\nEenvoudiger gezegd: hierdoor kunnen modellen zich richten op belangrijke delen van de invoer. Stel je voor dat je een zin leest en probeert te begrijpen. Met Scaled dot product attention kun je meer aandacht besteden aan bepaalde woorden op basis van hun relevantie. Het werkt door de gelijkenis tussen elk woord (K) in de zin en een query (Q) te berekenen. Vervolgens worden de similariteitsscores geschaald door ze te delen door de vierkantswortel van de dimensie van de query. Deze schaling helpt om erg kleine of erg grote waarden te vermijden. Tot slot kunnen we met behulp van deze geschaalde similariteitsscores bepalen hoeveel aandacht of belang elk woord zou moeten krijgen. Dit attention-mechanisme helpt modellen cruciale informatie (V) te identificeren en hun begrip en vertaalvermogen te verbeteren.\n\nSimpel, toch? Om van dit eenvoudige mechanisme naar een AI te komen die een “Seinfeld-aflevering” kan schrijven waarin Jerry het bubble sort-algoritme leert, moeten we het complexer maken. In feite heeft alles wat we net hebben behandeld niet eens geleerde parameters - constante waarden die tijdens de modeltraining worden geleerd en die de uitvoer van het attention-blok aanpassen!\n\nAttention blocks in the style of _Attention is All You Need_ add mainly three types of complexity:\n\n### Aangeleerde parameters\n\nAangeleerde parameters verwijzen naar waarden of gewichten die worden aangepast tijdens het trainingsproces van een model om de prestaties ervan te verbeteren. Deze parameters worden gebruikt om de informatiestroom of aandacht binnen het model te regelen, zodat het model zich kan richten op de meest relevante delen van de invoergegevens. Eenvoudiger gezegd, zijn geleerde parameters als instelbare knoppen op een machine waaraan gedraaid kan worden om de werking te optimaliseren.\n\n### Verticaal stapelen - gelaagde attention-blokken\n\nVerticaal stapelen in lagen is een manier om meerdere attention-mechanismen op elkaar te stapelen, waarbij elke laag voortbouwt op de uitvoer van de vorige laag. Hierdoor kan het model zich richten op verschillende delen van de invoergegevens op verschillende abstractieniveaus, wat kan leiden tot betere prestaties bij bepaalde taken.\n\n### Horizontaal stapelen - ook wel bekend als Multi-Head Attention\n\nDe figuur uit de paper toont de volledige multi-head attention-module. Meerdere attention-operaties worden parallel uitgevoerd, waarbij de Q-K-V invoer voor elke operatie wordt gegenereerd door een unieke lineaire projectie van dezelfde invoergegevens (gedefinieerd door een unieke set aangeleerde parameters). Deze parallelle attention-blokken worden “attention heads” genoemd. De gewogen-som outputs van alle attention heads worden samengevoegd tot een enkele vector en door een andere geparametriseerde lineaire transformatie gehaald om de uiteindelijke output te krijgen.\n\n![](https://blog.cloudflare.com/content/images/2023/11/Screenshot-2023-11-21-at-09.13.49.png)\n\n[source](https://arxiv.org/abs/1706.03762)\n\nDankzij dit mechanisme kan een model zich tegelijkertijd op verschillende delen van de invoergegevens richten. Stel je voor dat je een complex stuk informatie probeert te begrijpen, zoals een zin of een alinea. Om het te begrijpen, moet je aandacht besteden aan verschillende delen tegelijk. Je moet bijvoorbeeld aandacht besteden aan het onderwerp van de zin, het werkwoord en het lijdend voorwerp, allemaal tegelijk, om de betekenis van de zin te begrijpen. Multi-headed attention werkt op dezelfde manier. Het stelt een model in staat om aandacht te besteden aan verschillende delen van de invoergegevens op hetzelfde moment, door gebruik te maken van meerdere “attention heads”. Elk attention-head richt zich op een ander aspect van de invoergegevens en de uitvoer van alle heads wordt gecombineerd tot de uiteindelijke uitvoer van het model.\n\n## Soorten attention\n\nEr zijn drie veelvoorkomende opstellingen van attention-blocks die worden gebruikt door grote taalmodellen die de afgelopen jaren zijn ontwikkeld: multi-head attention, grouped-query attention en multi-query attention. Ze verschillen in het aantal K- en V-vectoren ten opzichte van het aantal queryvectoren. **Multi-head attention** gebruikt hetzelfde aantal K- en V-vetoren als Q-vectoren, aangegeven als “N” in de onderstaande tabel. **Multi-query attention** gebruikt slechts een enkele K- en V-vector. **Grouped-query attention**, het type dat gebruikt wordt in het Mistral 7B model, verdeelt de Q-vectoren gelijkmatig in groepen die elk “G” vectoren bevatten en gebruikt dan een enkele K- en V-vector voor elke groep voor een totaal van N gedeeld door G sets van K- en V-vectoren. Dit is een samenvatting van de verschillen en we gaan hieronder in de implicaties hiervan.\n\n|     |     |     |     |\n| --- | --- | --- | --- |\n|     | **Aantal key/value-blokken** | **Kwaliteit** | **Geheugengebruik** |\n| **Multi-head attention (MHA)** | N   | Beste | Meeste |\n| **Grouped-query attention (GQA)** | N / G | Beter | Minder |\n| **Multi-query attention (MQA)** | 1   | Goed | Minste |\n\nSamenvatting van soorten attention\n\nEn dit diagram helpt het verschil tussen de drie soorten te illustreren:\n\n![](https://blog.cloudflare.com/content/images/2023/11/image1-6.png)\n\n[source](https://arxiv.org/pdf/2305.13245.pdf)\n\n### Multi-Query Attention\n\nMulti-query attention werd in 2019 beschreven in het paper van Google: [Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/abs/1911.02150). Het idee is dat in plaats van aparte K- en V-items te maken voor elke Q-vector in het attention-mechanisme, zoals in de bovengenoemde multi-head attention, er slechts één enkele K- en V-vector wordt gebruikt voor de hele reeks Q-vectoren. Vandaar de naam, meerdere query's gecombineerd in een enkel aandachtsmechanisme. In de paper werd dit gebenchmarkt op een vertaaltaak en toonde het prestaties gelijk aan multi-head aandacht op de benchmarktaak.\n\nOorspronkelijk was het idee om de totale grootte van het geheugen dat wordt gebruikt bij het uitvoeren van inferentie voor het model te verkleinen. Sindsdien, met de opkomst van gegeneraliseerde modellen en het toenemen van het aantal parameters, is het benodigde GPU-geheugen vaak de bottleneck. Dit is de kracht van multi-query aandacht, omdat het van de drie soorten aandacht het minste versnellergeheugen vereist. Naarmate de modellen echter groter en algemener werden, daalde de prestatie van multi-query aandacht ten opzichte van multi-head aandacht.\n\n### Grouped-Query Attention\n\nDe jongste van het stel, en de attention die gebruikt wordt door Mistral, is grouped-query attention, zoals beschreven in de paper [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/abs/2305.13245) dat in mei 2023 werd gepubliceerd op arxiv.org. Gegroepeerde-query aandacht combineert het beste van twee werelden: de kwaliteit van meervoudige aandacht met de snelheid en het lage geheugengebruik van multi-query attention. In plaats van een enkele set van K- en V-vectoren of een set voor elke Q-vector, wordt een vaste verhouding van 1 set van K- en V-vectoren voor elke Q-vector gebruikt, waardoor minder geheugen wordt gebruikt maar de hoge prestaties op veel taken behouden blijven.\n\nVaak gaat het bij het kiezen van een model voor een productietaak niet alleen om het kiezen van het beste model dat beschikbaar is, omdat we afwegingen moeten maken tussen prestaties, geheugengebruik, batchgrootte en beschikbare hardware (of cloudkosten). Inzicht in deze drie stijlen van aandacht kan helpen bij het nemen van deze beslissingen en begrijpen wanneer we, gegeven onze omstandigheden, voor een bepaald model zouden kunnen kiezen.\n\n## Werk met Mistral — probeer het vandaag nog\n\nAls een van de eerste grote taalmodellen die gebruik maakt van grouped-query attention en deze combineert met 'sliding window'-attention, lijkt Mistral het optimale compromis gevonden te hebben: het heeft een lage latentie, een hoge doorvoer en het presteert echt goed op benchmarks, zelfs in vergelijking met grotere modellen (13B). Dit alles wil zeggen dat het een kracht heeft voor zijn grootte. Met veel trots maken we het vandaag beschikbaar voor alle ontwikkelaars, via Workers AI.\n\nGa nu naar onze [developer docs](https://developers.cloudflare.com/workers-ai/models/text-generation/) om te beginnen, en als u hulp nodig heeft, feedback wilt geven, of wilt delen wat u aan het bouwen bent, kom gewoon langs op onze [Developer Discord](https://discord.com/invite/cloudflaredev)!\n\nHet Workers AI-team is ook op zoek naar nieuwe medewerkers, zie onze [jobs-pagina](https://www.cloudflare.com/careers/jobs/) voor openstaande functies als je gepassioneerd bent over AI-engineering en ons wilt helpen bij het bouwen en ontwikkelen van ons wereldwijde, serverloze GPU-aangedreven inferentieplatform.\n\n[Workers AI (NL)](https://blog.cloudflare.com/tag/workers-ai-nl/) [Mistral (NL)](https://blog.cloudflare.com/tag/mistral-nl/) [Cloudflare Workers (NL)](https://blog.cloudflare.com/tag/cloudflare-workers-nl/) [Nederlands](https://blog.cloudflare.com/tag/nederlands/)"
    },
    {
      "url": "https://blog.cloudflare.com/cloudflare-zero-trust-for-galileo-and-athenian/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/cloudflare-zero-trust-for-galileo-and-athenian/",
        "loadedTime": "2023-12-05T02:36:13.844Z",
        "referrerUrl": "https://blog.cloudflare.com/2024-the-year-of-elections/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/cloudflare-zero-trust-for-galileo-and-athenian/",
        "title": "Cloudflare Zero Trust for Project Galileo and the Athenian Project",
        "description": "Starting today, we are making the Cloudflare One Zero Trust suite available to teams that qualify for Projects Galileo or Athenian at no cost.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Cloudflare Zero Trust for Project Galileo and the Athenian Project\n12/12/2022\n6 min read\nThis post is also available in 日本語, Deutsch, Français, Español.\nThe organizations served by Projects Galileo and Athenian face the same security challenges as some of the world’s largest companies, but lack the budget to protect themselves. Sophisticated phishing campaigns attempt to compromise user credentials. Bad actors find ways to disrupt connectivity to critical resources. However, the tools to defend against these threats have historically only been available to the largest enterprises.\nWe’re excited to help fix that. Starting today, we are making the Cloudflare One Zero Trust suite available to teams that qualify for Project Galileo or Athenian at no cost. Cloudflare One includes the same Zero Trust security and connectivity solutions used by over 10,000 customers today to connect their users and safeguard their data.\nSame problem, different missions\nAthenian Project candidates work to safeguard elections in the United States. Project Galileo applicants launched their causes to support journalists, encourage artistic expression, or protect persecuted groups. They each set out to fix difficult and painful problems. None of the applications to our programs wrote their mission statement to deal with phishing attacks or internal data loss.\nHowever, security problems plague these teams. Instead of being able to focus on their unique mission, these groups spend money, time, and energy attempting to defend from attacks. The headaches range from expensive distractions to outright breaches. Even the mundane work to connect employees to important tools continues to be a headache. Every chore or incident takes away from the ability of these organizations to advance their cause.\nWe built Cloudflare One to solve the common security problems that can derail any team. Our mission is to help build a better Internet and, in doing so, we create tools that allow the groups served by the Athenian Project and Project Galileo spend as much of their day solving their own unique challenges.\nThe products we are making available today provide security against a broad, and growing, range of attacks that target how a team works together on the Internet. Project Galileo and Athenian candidates can choose to start in any place depending on their existing security challenges. If you need a guide on where to get started, we’ve broken down three common first steps that we recommend.\n1) Stop phishing attacks\nMany phishing attacks start with a malicious link buried in a single email from a sender that seems trustworthy. A user in your organization clicks on that link, believing it to be from a teammate or manager, and lands on a website that looks almost identical to your identity provider or one of the web applications they use every day. They input their username and password, sending their credentials directly to the attacker.\nCloudflare One’s email security, our Area 1 product, is our first line of phishing defense. Area 1 scans the emails headed to your organization for the presence of potential phishing campaigns and other types of security attacks. Malicious messages never arrive without interrupting the emails that your team should receive. You can deploy Area 1 in minutes with a few changes to your DNS records to safeguard your Microsoft 365, Gmail, or nearly any other email deployment.\nAs part of today’s announcement, we are making Area 1 available to Project Galileo and Athenian organizations at no cost. The same level of protection trusted by large corporations from Werner Enterprises to Fortune 500 consumer packaged goods firms is now available to your team.\nIn some cases, an email evades detection or the phishing link reaches your users through other channels. Cloudflare One can still help. When your team members navigate the Internet, they rely on DNS queries made by their device in order to translate the hostname of a website to the IP address of the server. Their device sends those queries to a DNS resolver.\nCloudflare runs the world’s fastest DNS resolver, 1.1.1.1, and we offer a security version that also filters DNS queries made to destinations that are known to be malicious. If a user accidentally clicks on a link from a text message or in a website, their device first sends that DNS query to Cloudflare. If dangerous, we stop the query before the malicious destination can load. If benign, we’ll respond with the destination faster than other resolvers.\nCloudflare’s DNS filtering keeps the US Federal Government safe, but can be deployed by teams of any size. You can secure entire office networks with the change of one router setting or deploy our roaming agent to keep your users safe wherever they work. Together with email protection, your team can filter out phishing attacks in a defense-in-depth approach.\n2) Connect employees and partners\nMany teams that qualify for Project Galileo had to find ways to work across geographies long before the pandemic sent employees home from other companies. These teams typically deployed a legacy virtual private network (VPN) to allow team members from across the world to reach the tools they needed to collect data, file stories, or submit research. At best, those VPN deployments slowed down user connectivity and introduced maintenance headaches. At worst, they gave anyone on the network overly broad access to nearly any resource.\nWith Cloudflare One, your team can operate in any location and still reach your internal tools while controlling exactly who can access which application or service. Organizations that need to operate a traditional private network can run one on Cloudflare by deploying our device client (WARP) on user endpoints and establishing outbound connections to our global network via Cloudflare Tunnel. Users enjoy the performance and availability of Cloudflare’s network while administrators can build granular permissions without the need for additional application development.\nWe also know that many Galileo and Athenian organizations work alongside hundreds or thousands of partners and volunteers. Those users need to also reach internal resources but are not willing or able to install software on their personal devices.\nTo solve that challenge, Cloudflare One can be deployed in a fully clientless mode that can use multiple identity providers including consumer options like Google, Facebook, and LinkedIn. Users authenticate with the single-sign on option they already use from any mobile or desktop device. Administrators control which users can reach specific applications while logging every attempt.\n3) Secure your team’s path to the Internet\nBeyond phishing attacks, bad actors target organizations with other types of threats like malware hidden in downloads. Researchers and journalists exploring a topic with untrusted sources can bring ransomware back into the entire organization. Team members connecting to the Internet from a hotel Wi-Fi network can have unencrypted DNS queries monitored and reported.\nCloudflare One provides every member of your team with an encrypted, secured on-ramp to the entire Internet. Powered by the same Cloudflare WARP agent that helps millions of users enjoy a more private Internet connection, Cloudflare’s Secure Web Gateway filters all Internet-bound for hidden threats.\nWhen users inadvertently connect to a malicious destination, Cloudflare One will block the attempt and present them with a page explaining what just happened. In the other direction, Cloudflare’s network scans downloads for malware and blocks the download before the user can open it.\nThe same filtering can be extended to keep sensitive data from leaving your organization. You can build rules that flag file uploads that contain personal information or patterns that are unique to your team or focus area. With just a few clicks, you can create policies that prevent the accidental or malicious loss of data while also restricting uploads to approved destinations.\nAll without the need for an enterprise IT department\nToday’s announcement makes the security technology deployed by the world’s largest enterprises available to organizations of any size. And, despite the broad impact of Athenian and Galileo organizations, that size tends to be smaller.\nThe teams supported by Project Galileo focus limited resources on advancing journalism, artistic expression, human rights, and other causes. The state and local governments who qualify for the Athenian Project spend their days protecting democracy in the United States. Both groups tend to lack the resources of a Fortune 500 to staff and operate a large IT department.\nWe built Cloudflare One as a service that a team could configure and deploy in a matter of hours and still benefit from comprehensive Zero Trust security. We’ve published a Zero Trust Roadmap that your team can use to determine how to get started with guidelines for the time required at each step.\nHow to get started\nWe’re excited to extend Projects Galileo and Athenian to include Cloudflare One. Are you an existing qualified organization or interested in applying? Follow the link here and here to get started.\nIf you are not part of Project Galileo or Athenian, but still want to begin deploying Cloudflare One, we make the service available at no cost to teams of up to 50 users. Click here to sign up.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nImpact Week Project Galileo Athenian Project",
      "markdown": "## Cloudflare Zero Trust for Project Galileo and the Athenian Project\n\n12/12/2022\n\n*   [![Sam Rhea](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/05/Screenshot-2023-05-24-at-9.15.29-AM.png)](https://blog.cloudflare.com/author/sam/)\n\n6 min read\n\n_This post is also available in [日本語](https://blog.cloudflare.com/ja-jp/cloudflare-zero-trust-for-galileo-and-athenian-ja-jp/), [Deutsch](https://blog.cloudflare.com/de-de/cloudflare-zero-trust-for-galileo-and-athenian-de-de/), [Français](https://blog.cloudflare.com/fr-fr/cloudflare-zero-trust-for-galileo-and-athenian-fr-fr/), [Español](https://blog.cloudflare.com/es-es/cloudflare-zero-trust-for-galileo-and-athenian-es-es/)._\n\n![](https://blog.cloudflare.com/content/images/2022/12/image1-1.png)\n\nThe organizations served by [Projects Galileo](https://www.cloudflare.com/galileo/) and [Athenian](https://www.cloudflare.com/athenian/) face the same security challenges as some of the world’s largest companies, but lack the budget to protect themselves. Sophisticated phishing campaigns attempt to compromise user credentials. Bad actors find ways to disrupt connectivity to critical resources. However, the tools to defend against these threats have historically only been available to the largest enterprises.\n\nWe’re excited to help fix that. Starting today, we are making the [Cloudflare One](https://www.cloudflare.com/cloudflare-one/) Zero Trust suite available to teams that qualify for Project Galileo or Athenian at no cost. Cloudflare One includes the same [Zero Trust security](https://www.cloudflare.com/learning/security/glossary/what-is-zero-trust/) and connectivity solutions used by over 10,000 customers today to connect their users and safeguard their data.\n\n## Same problem, different missions\n\nAthenian Project candidates work to safeguard elections in the United States. [Project Galileo](https://www.cloudflare.com/galileo/) applicants launched their causes to support journalists, encourage artistic expression, or protect persecuted groups. They each set out to fix difficult and painful problems. None of the applications to our programs wrote their mission statement to deal with phishing attacks or internal data loss.\n\nHowever, security problems plague these teams. Instead of being able to focus on their unique mission, these groups spend money, time, and energy attempting to defend from attacks. The headaches range from expensive distractions to outright breaches. Even the mundane work to connect employees to important tools continues to be a headache. Every chore or incident takes away from the ability of these organizations to advance their cause.\n\nWe built Cloudflare One to solve the common security problems that can derail any team. Our mission is to help build a better Internet and, in doing so, we create tools that allow the groups served by the Athenian Project and Project Galileo spend as much of their day solving their own unique challenges.\n\nThe products we are making available today provide security against a broad, and growing, range of attacks that target how a team works together on the Internet. Project Galileo and Athenian candidates can choose to start in any place depending on their [existing security challenges](https://zerotrustroadmap.org/). If you need a guide on where to get started, we’ve broken down three common first steps that we recommend.\n\n### 1) Stop phishing attacks\n\nMany phishing attacks start with a malicious link buried in a single email from a sender that seems trustworthy. A user in your organization clicks on that link, believing it to be from a teammate or manager, and lands on a website that looks almost identical to your identity provider or one of the web applications they use every day. They input their username and password, sending their credentials directly to the attacker.\n\nCloudflare One’s email security, our [Area 1 product](https://www.cloudflare.com/products/zero-trust/email-security/), is our first line of phishing defense. Area 1 scans the emails headed to your organization for the presence of potential phishing campaigns and other types of security attacks. Malicious messages never arrive without interrupting the emails that your team should receive. You can deploy Area 1 in minutes with [a few changes to your DNS records](https://developers.cloudflare.com/email-security/deployment/inline/) to safeguard your Microsoft 365, Gmail, or nearly any other email deployment.\n\nAs part of today’s announcement, we are making Area 1 available to Project Galileo and Athenian organizations at no cost. The same level of protection trusted by large corporations from [Werner Enterprises](https://www.cloudflare.com/case-studies/werner-enterprises/) to [Fortune 500 consumer packaged goods](https://www.cloudflare.com/case-studies/consumer-goods-leader/) firms is now available to your team.\n\nIn some cases, an email evades detection or the phishing link reaches your users through other channels. Cloudflare One can still help. When your team members navigate the Internet, they rely on DNS queries made by their device in order to translate the hostname of a website to the IP address of the server. Their device sends those queries to a DNS resolver.\n\nCloudflare runs the world’s fastest DNS resolver, [1.1.1.1](https://1.1.1.1/), and we offer a security version that also filters DNS queries made to destinations that are known to be malicious. If a user accidentally clicks on a link from a text message or in a website, their device first sends that DNS query to Cloudflare. If dangerous, we stop the query before the malicious destination can load. If benign, we’ll respond with the destination faster than other resolvers.\n\nCloudflare’s DNS filtering [keeps the US Federal Government safe](https://blog.cloudflare.com/helping-keep-governments-safe-and-secure/), but can be deployed by teams of any size. You can secure entire [office networks](https://developers.cloudflare.com/cloudflare-one/connections/connect-devices/agentless/dns/) with the change of one router setting or deploy our [roaming agent](https://developers.cloudflare.com/cloudflare-one/connections/connect-devices/warp/) to keep your users safe wherever they work. Together with email protection, your team can filter out phishing attacks in a defense-in-depth approach.\n\n### 2) Connect employees and partners\n\nMany teams that qualify for Project Galileo had to find ways to work across geographies long before the pandemic sent employees home from other companies. These teams typically deployed a legacy [virtual private network (VPN)](https://www.cloudflare.com/learning/access-management/what-is-a-vpn/) to allow team members from across the world to reach the tools they needed to collect data, file stories, or submit research. At best, those VPN deployments slowed down user connectivity and introduced maintenance headaches. At worst, they gave anyone on the network overly broad access to nearly any resource.\n\nWith Cloudflare One, your team can operate in any location and still reach your internal tools while controlling exactly who can access which [application or service](https://developers.cloudflare.com/cloudflare-one/applications/). Organizations that need to operate a traditional private network can run one on Cloudflare by deploying our device client (WARP) on user endpoints and establishing outbound connections to our global network via [Cloudflare Tunnel](https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/). Users enjoy the performance and availability of Cloudflare’s network while administrators can build granular permissions without the need for additional application development.\n\nWe also know that many Galileo and Athenian organizations work alongside hundreds or thousands of partners and volunteers. Those users need to also reach internal resources but are not willing or able to install software on their personal devices.\n\nTo solve that challenge, Cloudflare One can be deployed in a [fully clientless mode](https://developers.cloudflare.com/cloudflare-one/applications/configure-apps/) that can use multiple identity providers including consumer options like Google, Facebook, and LinkedIn. Users authenticate with the single-sign on option they already use from any mobile or desktop device. Administrators control which users can reach specific applications while logging every attempt.\n\n### 3) Secure your team’s path to the Internet\n\nBeyond phishing attacks, bad actors target organizations with other types of threats like malware hidden in downloads. Researchers and journalists exploring a topic with untrusted sources can bring ransomware back into the entire organization. Team members connecting to the Internet from a hotel Wi-Fi network can have unencrypted DNS queries monitored and reported.\n\nCloudflare One provides every member of your team with an [encrypted, secured on-ramp](https://developers.cloudflare.com/cloudflare-one/connections/connect-devices/) to the entire Internet. Powered by the same Cloudflare WARP agent that helps millions of users enjoy a more private Internet connection, Cloudflare’s [Secure Web Gatewa](https://www.cloudflare.com/learning/access-management/what-is-a-secure-web-gateway/)y filters all Internet-bound for hidden threats.\n\nWhen users inadvertently connect to a malicious destination, Cloudflare One will [block the attempt](https://developers.cloudflare.com/cloudflare-one/policies/filtering/http-policies/) and present them with a page explaining what just happened. In the other direction, Cloudflare’s network scans downloads for malware and blocks the download before the user can open it.\n\nThe same filtering can be extended [to keep sensitive data](https://developers.cloudflare.com/cloudflare-one/policies/filtering/http-policies/data-loss-prevention/) from leaving your organization. You can build rules that flag file uploads that contain personal information or patterns that are unique to your team or focus area. With just a few clicks, you can create policies that prevent the accidental or malicious loss of data while also restricting uploads to approved destinations.\n\n## All without the need for an enterprise IT department\n\nToday’s announcement makes the security technology deployed by the world’s largest enterprises available to organizations of any size. And, despite the broad impact of Athenian and Galileo organizations, that size tends to be smaller.\n\nThe teams supported by [Project Galileo](https://www.cloudflare.com/galileo/) focus limited resources on advancing journalism, artistic expression, human rights, and other causes. The state and local governments who qualify for the Athenian Project spend their days protecting democracy in the United States. Both groups tend to lack the resources of a Fortune 500 to staff and operate a large IT department.\n\nWe built Cloudflare One as a service that a team could configure and deploy in a matter of hours and still benefit from comprehensive Zero Trust security. We’ve published a [Zero Trust Roadmap](https://zerotrustroadmap.org/) that your team can use to determine how to get started with guidelines for the time required at each step.\n\n## How to get started\n\nWe’re excited to extend Projects Galileo and Athenian to include Cloudflare One. Are you an existing qualified organization or interested in applying? Follow the link [here](https://www.cloudflare.com/athenian/) and [here](https://www.cloudflare.com/galileo/) to get started.\n\nIf you are not part of Project Galileo or Athenian, but still want to begin deploying Cloudflare One, we make the service available at no cost to teams of up to 50 users. Click [here](https://dash.cloudflare.com/sign-up/teams) to sign up.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Impact Week](https://blog.cloudflare.com/tag/impact-week/) [Project Galileo](https://blog.cloudflare.com/tag/project-galileo/) [Athenian Project](https://blog.cloudflare.com/tag/athenian-project/)"
    },
    {
      "url": "https://blog.cloudflare.com/introducing-cloudflare-for-campaigns/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/introducing-cloudflare-for-campaigns/",
        "loadedTime": "2023-12-05T02:36:15.149Z",
        "referrerUrl": "https://blog.cloudflare.com/2024-the-year-of-elections/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/introducing-cloudflare-for-campaigns/",
        "title": "Introducing Cloudflare for Campaigns",
        "description": "During the past year, we saw nearly 2 billion global citizens go to the polls to vote in democratic elections. There were major elections in more than 50 countries, including India, Nigeria, and the United Kingdom, as well as elections for the European Parliament.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Introducing Cloudflare for Campaigns\n01/15/2020\n7 min read\nDuring the past year, we saw nearly 2 billion global citizens go to the polls to vote in democratic elections. There were major elections in more than 50 countries, including India, Nigeria, and the United Kingdom, as well as elections for the European Parliament. In 2020, we will see a similar number of elections in countries from Peru to Myanmar. In November, U.S citizens will cast their votes for the 46th President, 435 seats in the U.S House of Representatives, 35 of the 100 seats in the U.S. Senate, and many state and local elections. \nRecognizing the importance of maintaining public access to election information, Cloudflare launched the Athenian Project in 2017, providing U.S. state and local government entities with the tools needed to secure their election websites for free. As we’ve seen, however, political parties and candidates for office all over the world are also frequent targets for cyberattack. Cybersecurity needs for campaign websites and internal tools are at an all time high.\nAlthough Cloudflare has helped improve the security and performance of political parties and candidates for office all over the world for years, we’ve long felt that we could do more. So today, we’re announcing Cloudflare for Campaigns, a suite of Cloudflare services tailored to campaign needs. Cloudflare for Campaigns is designed to make it easier for all political campaigns and parties, especially those with small teams and limited resources, to get access to cybersecurity services.\nRisks faced by political campaigns\nSince Russians attempted to use cyberattacks to interfere in the U.S. Presidential election in 2016, the news has been filled with reports of cyber threats against political campaigns, in both the United States and around the world. Hackers targeted the Presidential campaigns of Emmanuel Macron in France and Angela Merkel in Germany with phishing attacks, the main political parties in the UK with DDoS attacks, and congressional campaigns in California with a combination of malware, DDoS attacks and brute force login attempts. \nBoth because of our services to state and local government election websites through the Athenian Project and because a significant number of political parties and candidates for office use our services, Cloudflare has seen many attacks on election infrastructure and political campaigns firsthand.\nDuring the 2020 U.S. election cycle, Cloudflare has provided services to 18 major presidential campaigns, as well as a range of congressional campaigns. On a typical day, Cloudflare blocks 400,000 attacks against political campaigns, and, on a busy day, Cloudflare blocks more than 40 million attacks against campaigns.\nWhat is Cloudflare for Campaigns?\nCloudflare for Campaigns is a suite of Cloudflare products focused on the needs of political campaigns, particularly smaller campaigns that don’t have the resources to bring significant cybersecurity resources in house. To ensure the security of a campaign website, the Cloudflare for Campaigns package includes Business-level service, as well as security tools particularly helpful for political campaigns websites, such as the web application firewall, rate limiting, load balancing, Enterprise level “I am Under Attack Support”, bot management, and multi-user account enablement.\nTo ensure the security of internal campaign teams, the Cloudflare for Campaigns service will also provide tools for campaigns to ensure the security of their internal teams with Cloudflare Access, allowing for campaigns to secure, authenticate, and monitor user access to any domain, application, or path on Cloudflare, without using a VPN. Along with Access, we will be providing Cloudflare Gateway with DNS-based filtering at multiple locations to protect campaign staff as they navigate the Internet by keeping malicious content off the campaign’s network using DNS filtering, helping prevent users from running into phishing scams or malware sites. Campaigns can use Gateway after the product’s public release.\nCloudflare for Campaigns also includes Cloudflare reliability and security guide, which lists a best practice guide for political campaigns to maintain their campaign site and secure their internal teams.\nRegulatory Challenges\nAlthough there is widespread agreement that campaigns and political parties face threats of cyberattack, there is less consensus on how best to get political campaigns the help they need. Many political campaigns and political parties operate under resource constraints, without the technological capability and financial resources to dedicate to cybersecurity. At the same time, campaigns around the world are the subject of a variety of different regulations intended to prevent corruption of democratic processes. As a practical matter, that means that, although campaigns may not have the resources needed to access cybersecurity services, donation of cybersecurity services to campaigns may not always be allowed.\nIn the U.S., campaign finance regulations prohibit corporations from providing any contributions of either money or services to federal candidates or political party organizations. These rules prevent companies from offering free or discounted services if those services are not provided on the same terms and conditions to similarly situated members of the general public. The Federal Elections Commission (FEC), which enforces U.S. campaign finance laws, has struggled with the issue of how best to apply those rules to the provision of free or discounted cybersecurity services to campaigns. In consideration of a number of advisory opinions, they have publicly wrestled with the competing priorities of securing campaigns from cyberattack while not opening a backdoor to donation of goods services that are intended to curry favors with particular candidates.\nThe FEC has issued two advisory opinions to tech companies seeking to provide free or discounted cybersecurity services to campaigns. In 2018, the FEC approved a request by Microsoft to offer a package of enhanced online account security protections for “election-sensitive” users. The FEC reasoned that Microsoft was offering the services to its paid users “based on commercial rather than political considerations, in the ordinary course of its business and not merely for promotional consideration or to generate goodwill.” In July 2019, the FEC approved a request by a cybersecurity company to provide low-cost anti-phishing services to campaigns because those services would be provided in the ordinary course of business and on the same terms and conditions as offered to similarly situated non-political clients.\nIn September 2018, a month after Microsoft submitted its request, Defending Digital Campaigns (DDC), a nonprofit established with the mission to “secure our democratic campaign process by providing eligible campaigns and political parties, committees, and related organizations with knowledge, training, and resources to defend themselves from cyber threats,” submitted a request to the FEC to offer free or reduced-cost cybersecurity services, including from technology corporations, to federal candidates and parties. Over the following months, the FEC issued and requested comment on multiple draft opinions on whether the donation was permissible and, if so, on what basis. As described by the FEC, to support its position, DDC represented that “federal candidates and parties are singularly ill-equipped to counteract these threats.” The FEC’s advisory opinion to DDC noted:\n“You [DDC] state that presidential campaign committees and national party committees require expert guidance on cybersecurity and you contend that the 'vast majority of campaigns' cannot afford full-time cybersecurity staff and that 'even basic cybersecurity consulting software and services' can overextend the budgets of most congressional campaigns. AOR004. For instance, you note that a congressional candidate in California reported a breach to the Federal Bureau of Investigation (FBI) in March of this year but did not have the resources to hire a professional cybersecurity firm to investigate the attack, or to replace infected computers. AOR003.”\nIn May 2019, the FEC approved DDC’s request to partner with technology companies to provide free and discounted cybersecurity services “[u]nder the unusual and exigent circumstances” presented by the request and “in light of the demonstrated, currently enhanced threat of foreign cyberattacks against party and candidate committees.”\nAll of these opinions demonstrate the FEC’s desire to allow campaigns to access affordable cybersecurity services because of the heightened threat of cyberattack, while still being cautious to ensure that those services are offered transparently and consistent with the goals of campaign finance laws.\nPartnering with DDC to Provide Free Services to US Candidates\nWe share the view of both DDC and the FEC that political campaigns -- which are central to our democracy -- must have the tools to protect themselves against foreign cyberattack. Cloudflare is therefore excited to announce a new partnership with DDC to provide Cloudflare for Campaigns for free to candidates and parties that meet DDC’s criteria.\nTo receive free services under DDC, political campaigns must meet the following criteria, as the DDC laid out to the FEC:\nA House candidate’s committee that has at least 100,000 in receipts for the current election cycle;\nA House or Senate candidate’s committee for candidates who have qualified for the general election ballot in their respective elections; or\nAny presidential candidate’s committee whose candidate is polling above five percent in national polls.\nFor more information on eligibility for these services under DDC and the next steps, please visit cloudflare.com/campaigns/usa. \nElection package\nAlthough political campaigns are regulated differently all around the world, Cloudflare believes that the integrity of all political campaigns should be protected against powerful adversaries. With this in mind, Cloudflare will therefore also be offering Cloudflare for Campaigns as a paid service, designed to help campaigns all around the world as we attempt to address regulatory hurdles. For more information on how to sign up for the Cloudflare election package, please visit cloudflare.com/campaigns.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nProduct News Election Security Security Speed & Reliability",
      "markdown": "## Introducing Cloudflare for Campaigns\n\n01/15/2020\n\n*   [![Alissa Starzak](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2020/12/image--1-.png)](https://blog.cloudflare.com/author/alissa-starzak/)\n*   [![Jocelyn Woolbright](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2019/12/0-1.jpg)](https://blog.cloudflare.com/author/jocelyn/)\n\n7 min read\n\n![](https://blog.cloudflare.com/content/images/2020/01/CloudflareforCampaigns_Blog---Purple-BG.png)\n\nDuring the past year, we [saw](https://interactive.aljazeera.com/aje/2019/how-the-world-votes-2019/index.html) nearly 2 billion global citizens go to the polls to vote in democratic elections. There were major elections in more than 50 countries, including India, Nigeria, and the United Kingdom, as well as elections for the European Parliament. In 2020, we will see a similar number of elections in countries from Peru to Myanmar. In November, U.S citizens will cast their votes for the 46th President, 435 seats in the U.S House of Representatives, 35 of the 100 seats in the U.S. Senate, and many state and local elections.\n\nRecognizing the importance of maintaining public access to election information, Cloudflare launched the [Athenian Project](https://www.cloudflare.com/athenian/) in 2017, providing U.S. state and local government entities with the tools needed to secure their election websites for free. As we’ve seen, however, political parties and candidates for office all over the world are also frequent targets for cyberattack. [Cybersecurity](https://www.cloudflare.com/learning/security/what-is-cyber-security/) needs for campaign websites and internal tools are at an all time high.\n\nAlthough Cloudflare has helped improve the security and performance of political parties and candidates for office all over the world for years, we’ve long felt that we could do more. So today, we’re announcing Cloudflare for Campaigns, a suite of Cloudflare services tailored to campaign needs. Cloudflare for Campaigns is designed to make it easier for all political campaigns and parties, especially those with small teams and limited resources, to get access to cybersecurity services.\n\n### Risks faced by political campaigns\n\nSince Russians [attempted to use cyberattacks](https://time.com/5565991/russia-influence-2016-election/) to interfere in the U.S. Presidential election in 2016, the news has been filled with reports of cyber threats against political campaigns, in both the United States and around the world. Hackers [targeted](https://www.reuters.com/article/us-france-election-macron-cyber/macron-campaign-was-target-of-cyber-attacks-by-spy-linked-group-idUSKBN17Q200) the Presidential campaigns of Emmanuel Macron in France and Angela Merkel in Germany with phishing attacks, the [main political parties in the UK](https://www.reuters.com/article/us-britain-election-labour-cyber/hackers-hit-uk-political-parties-with-back-to-back-cyberattacks-idUSKBN1XM19I) with DDoS attacks, and [congressional campaigns in California](https://thehill.com/policy/cybersecurity/410229-primary-season-cyberattacks-illuminate-campaign-vulnerabilities) with a combination of malware, DDoS attacks and brute force login attempts.\n\nBoth because of our services to state and local government election websites through the Athenian Project and because a significant number of political parties and candidates for office use our services, Cloudflare has seen many attacks on election infrastructure and political campaigns firsthand.\n\nDuring the 2020 U.S. election cycle, Cloudflare has provided services to 18 major presidential campaigns, as well as a range of congressional campaigns. On a typical day, Cloudflare blocks 400,000 attacks against political campaigns, and, on a busy day, Cloudflare blocks more than 40 million attacks against campaigns.\n\n### What is Cloudflare for Campaigns?\n\nCloudflare for Campaigns is a suite of Cloudflare products focused on the needs of political campaigns, particularly smaller campaigns that don’t have the resources to bring significant cybersecurity resources in house. To ensure the security of a campaign website, the Cloudflare for Campaigns package includes Business-level service, as well as security tools particularly helpful for political campaigns websites, such as the [web application firewall](https://www.cloudflare.com/learning/ddos/glossary/web-application-firewall-waf/), rate limiting, load balancing, Enterprise level “I am Under Attack Support”, bot management, and multi-user account enablement.\n\n![](https://blog.cloudflare.com/content/images/2020/01/image2-6.png)\n\nTo ensure the security of internal campaign teams, the Cloudflare for Campaigns service will also provide tools for campaigns to ensure the security of their internal teams with [Cloudflare Access](https://teams.cloudflare.com/access/index.html), allowing for campaigns to secure, authenticate, and monitor user access to any domain, application, or path on Cloudflare, without using a VPN. Along with Access, we will be providing [Cloudflare Gateway](https://teams.cloudflare.com/gateway/index.html) with DNS-based filtering at multiple locations to protect campaign staff as they navigate the Internet by keeping malicious content off the campaign’s network using DNS filtering, helping prevent users from running into phishing scams or malware sites. Campaigns can use Gateway after the product’s public release.\n\nCloudflare for Campaigns also includes Cloudflare reliability and security guide, which lists a best practice guide for political campaigns to maintain their campaign site and secure their internal teams.\n\n### Regulatory Challenges\n\nAlthough there is widespread agreement that campaigns and political parties face threats of cyberattack, there is less consensus on how best to get political campaigns the help they need.  Many political campaigns and political parties operate under resource constraints, without the technological capability and financial resources to dedicate to cybersecurity. At the same time, campaigns around the world are the subject of a variety of different regulations intended to prevent corruption of democratic processes. As a practical matter, that means that, although campaigns may not have the resources needed to access cybersecurity services, donation of cybersecurity services to campaigns may not always be allowed.\n\nIn the U.S., campaign finance regulations prohibit corporations from providing any contributions of either money or services to federal candidates or political party organizations. These rules prevent companies from offering free or discounted services if those services are not provided on the same terms and conditions to similarly situated members of the general public. The Federal Elections Commission (FEC), which enforces U.S. campaign finance laws, has struggled with the issue of how best to apply those rules to the provision of free or discounted cybersecurity services to campaigns. In consideration of a number of advisory opinions, they have publicly wrestled with the competing priorities of securing campaigns from cyberattack while not opening a backdoor to donation of goods services that are intended to curry favors with particular candidates.\n\nThe FEC has issued two advisory opinions to tech companies seeking to provide free or discounted cybersecurity services to campaigns. In 2018, the FEC [approved](https://www.fec.gov/files/legal/aos/2018-11/2018-11.pdf) a request by Microsoft to offer a package of enhanced online account security protections for “election-sensitive” users. The FEC reasoned that Microsoft was offering the services to its paid users “based on commercial rather than political considerations, in the ordinary course of its business and not merely for promotional consideration or to generate goodwill.” In July 2019, the FEC [approved](https://www.fec.gov/files/legal/aos/2019-12/2019-12.pdf) a request by a cybersecurity company to provide low-cost anti-phishing services to campaigns because those services would be provided in the ordinary course of business and on the same terms and conditions as offered to similarly situated non-political clients.\n\nIn September 2018, a month after Microsoft submitted its request, Defending Digital Campaigns (DDC), a nonprofit established with the mission to “secure our democratic campaign process by providing eligible campaigns and political parties, committees, and related organizations with knowledge, training, and resources to defend themselves from cyber threats,” submitted a request to the FEC to offer free or reduced-cost cybersecurity services, including from technology corporations, to federal candidates and parties. Over the following months, the FEC issued and requested comment on multiple draft opinions on whether the donation was permissible and, if so, on what basis. As [described by the FEC](https://www.fec.gov/files/legal/aos/2018-12/2018-12.pdf), to support its position, DDC represented that “federal candidates and parties are singularly ill-equipped to counteract these threats.” The FEC’s advisory opinion to DDC noted:\n\n_“You \\[DDC\\] state that presidential campaign committees and national party committees require expert guidance on cybersecurity and you contend that the 'vast majority of campaigns' cannot afford full-time cybersecurity staff and that 'even basic cybersecurity consulting software and services' can overextend the budgets of most congressional campaigns. AOR004. For instance, you note that a congressional candidate in California reported a breach to the Federal Bureau of Investigation (FBI) in March of this year but did not have the resources to hire a professional cybersecurity firm to investigate the attack, or to replace infected computers. AOR003.”_  \n\nIn May 2019, the FEC [approved](https://www.fec.gov/files/legal/aos/2018-12/2018-12.pdf) DDC’s request to partner with technology companies to provide free and discounted cybersecurity services “\\[u\\]nder the unusual and exigent circumstances” presented by the request and “in light of the demonstrated, currently enhanced threat of foreign cyberattacks against party and candidate committees.”\n\nAll of these opinions demonstrate the FEC’s desire to allow campaigns to access affordable cybersecurity services because of the heightened threat of cyberattack, while still being cautious to ensure that those services are offered transparently and consistent with the goals of campaign finance laws.\n\n### Partnering with DDC to Provide Free Services to US Candidates\n\nWe share the view of both DDC and the FEC that political campaigns -- which are central to our democracy -- must have the tools to protect themselves against foreign cyberattack. Cloudflare is therefore excited to announce a new partnership with DDC to provide Cloudflare for Campaigns for free to candidates and parties that meet DDC’s criteria.\n\n![](https://blog.cloudflare.com/content/images/2020/01/image1.jpg)\n\nTo receive free services under DDC, political campaigns must meet the following criteria, as the DDC laid out to the FEC:\n\n*   A House candidate’s committee that has at least 100,000 in receipts for the current election cycle;\n*   A House or Senate candidate’s committee for candidates who have qualified for the general election ballot in their respective elections; or\n*   Any presidential candidate’s committee whose candidate is polling above five percent in national polls.\n\nFor more information on eligibility for these services under DDC and the next steps, please visit [cloudflare.com/campaigns/usa](https://cloudflare.com/campaigns/usa/).\n\n### Election package\n\nAlthough political campaigns are regulated differently all around the world, Cloudflare believes that the integrity of all political campaigns should be protected against powerful adversaries. With this in mind, Cloudflare will therefore also be offering Cloudflare for Campaigns as a paid service, designed to help campaigns all around the world as we attempt to address regulatory hurdles. For more information on how to sign up for the Cloudflare election package, please visit [cloudflare.com/campaigns](https://cloudflare.com/campaigns/).\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Product News](https://blog.cloudflare.com/tag/product-news/) [Election Security](https://blog.cloudflare.com/tag/election-security/) [Security](https://blog.cloudflare.com/tag/security/) [Speed & Reliability](https://blog.cloudflare.com/tag/speed-and-reliability/)"
    },
    {
      "url": "https://blog.cloudflare.com/how-the-brazilian-presidential-elections-affected-internet-traffic/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/how-the-brazilian-presidential-elections-affected-internet-traffic/",
        "loadedTime": "2023-12-05T02:36:25.744Z",
        "referrerUrl": "https://blog.cloudflare.com/2024-the-year-of-elections/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/how-the-brazilian-presidential-elections-affected-internet-traffic/",
        "title": "How the Brazilian Presidential elections affected Internet traffic",
        "description": "What happens to the Internet traffic of a country when an important election happens. In Brazil, the first round of the Presidential elections brought a 10% decrease in Internet traffic, but the runoff on Sunday had a bigger: 21% drop",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/03/2022\n8 min read\nThis post is also available in 简体中文 and 繁體中文.\nBrasil, sei lá\nOu o meu coração se engana\nOu uma terra igual não há\n— From Tom Jobim’s song, Brasil Nativo\nBrazil’s recent presidential election got significant attention from both global and national media outlets, not only because of the size of the country, but also because of premature allegations of electoral fraud. The first round of the Brazilian 2022 general election was held on October 2, and the runoff was held on Sunday, October 30. With 124 million votes counted, former president Lula da Silva (2003-2010) won with 50.9% of the votes, beating incumbent Jair Bolsonaro, who had 49.1% of the votes. \nThe final results of the elections as published by the official Tribunal Superior Eleitoral, with more than 124 million votes counted.)\nUsing Cloudflare’s data, we can explore the impact that this election had on Internet traffic patterns in Brazil, as well as interest in content from election-related websites, news organizations, social media platforms, and video platforms.\nHere are a few highlights: while the runoff generated much more interest to election related websites (we actually have a view to DNS queries, a proxy to websites), the first round showed bigger increases in traffic to news organizations. \nFor the candidate’s domains, Lula’s win had the higher impact.\nAlso: official results came earlier on the runoff than the first round, and spikes in traffic were higher earlier that day (October 30).\n(Note: we’re using local times — that means UTC-3, that is related to the more populated regions of Brazil — in this blog, although some charts have x-axis UTC).\nLet’s start by looking at general Internet traffic in Brazil.\nOn election days, traffic goes down (during the day)\nUsing Cloudflare Radar, we can see something that has also been observed in other countries that hold Sunday elections: when most people are getting outside to vote, Internet traffic goes down (in comparison with previous Sundays). We saw this in the two rounds of the Presidential elections in France back in April 2022, in Portugal’s legislative elections in January 2022 and now, in Brazil.\nWe can also compare Sundays in October. There were five weekends. The two that had elections show the same pattern of lower traffic during the day, as seen in the previous chart. Comparing the two election days, there was a bigger drop in traffic on October 30 (down 21% at around 18:00 local time), than on October 2 (down 10% at around 20:00). Related or not, there was a bigger turnout on the runoff (124 million votes) than on the first round (123 million). Here’s the view on October 30:\nAnd here’s October 2:\nA more clear view in comparing the October weekends, and where you can see how the October 2 and 30 Sundays have the same pattern and different from the others three of the month, is this one (bear in mind that the x-axis is showing UTC time, it’s -3 hours in Brazil):\nIf we look at the main network providers (ASNs) in Brazil, the trend is the same. Claro (AS28573) also shows the drop in traffic on October 30, as does Telefonica (AS27699):\nHere’s Telefonica:\nWe observed a similar impact from the October 30 runoff election to traffic from different states in Brazil, including São Paulo, Rio de Janeiro, Rio Grande do Norte, Minas Gerais, and Bahia. \nMobile device usage greater on weekends (and on election days)\nWhen we look at the share of Brazil’s Internet traffic from mobile devices during October, we find that the highest percentages were on October 2 (first round of the elections, 66.3%), October 9 (66.4%) and October 30 (runoff election, 65%). We’ve seen this in other elections, an increase in mobile device traffice, so this seems to follow the same trend. \nThis chart also shows how mobile device usage in Brazil is at its highest on the weekends (all the main spikes for percentage of mobile devices are over the weekend, and more on Sundays).\nNow, let’s look at anonymized and aggregated DNS traffic data from our 1.1.1.1 resolver. This data provides a proxy for traffic to, and thus interest in, different categories of sites from users in Brazil around the election.\nBrazil has government websites related to elections, but also its own Tribunal Superior Eleitoral (Electoral Superior Court) that includes a website and app with live updates on the results of the elections for everyone to check. Looking at those related domains and using mean hourly traffic in September as a baseline, we can see that the October 2 first round spiked to 16x more DNS queries at 20:00 local time. However, DNS query traffic during the runoff election peaked at 18:00 local time on October 30 with 17.4x more DNS traffic as compared to the September baseline.\nWe can look more closely at each one of those two election days. On October 2, traffic had its first significant increase at around 17:00 local time, reaching 15x more requests to election-related domains as compared to the September baseline. This initial peak occurred at the same time the polling stations were closing. However, the peak that day, at 16x above baseline, was reached at 20:00 local time, as seen in the figure below.\nOn Sunday, October 30, 2022, the pattern is similar, although the peak was reached earlier, given that results started to arrive earlier than on the first round. The peak was reached at around 18:00 local time, with request traffic 17.4x above baseline.\nAs seen in the figure below, Lula first led in the official results at 18:45 local time, with votes from 67% of the polling stations counted at that time. Around 20:00 Lula was considered the winner (the peak seen in the previous chart was at that time).\nCandidate websites: in the end, winner takes all?\nFor Lula-related domains, there are clear spikes around the first round of elections on October 2. A 13x spike was observed on October 1 at around 21:00 local time. Two notable spikes were observed on October 2 — one at 16.7x above baseline at 09:00 local time, and the other at 10.7x above baseline at 21:00 local time. During the October 30 runoff election, only one clear spike was observed. The spike, at 16.7x above baseline, occurred at around 20:00, coincident with the time Lula was being announced as the winner.\nFor Bolsonaro-related domains, we observed a different pattern. Increased traffic as compared to the baseline is visible in the days leading up to the first round election, reaching 10x on September 30. On October 2, a 8x spike above baseline was seen at 18:00 local time. However, the two most significant spikes seen over the course of the month were observed on October 16, at 20x above baseline, a few hours after the first Lula-Bolsonaro television debate, and on October 25, at around 20:00, at 22x above baseline. That was the last week of campaigning before the October 30 runoff and when several polling predictions were announced. The second and last Bolsonaro-Lula debate was on October 28, and there’s a spike at 22:00 to Lula’s websites, and a smaller but also clear one at 21:00 to Bolsonaro’s websites).\nNews websites: more interest in the first round\nWith official election results being available more rapidly, DNS traffic for Brazilian news organization websites peaked much earlier in the evening than what we saw in France, for example, where more definitive election results arrived much later on election day. But another interesting trend here is how the first round, on October 2, had 9.1x more DNS traffic (compared with the September baseline), than what we saw during the runoff on October 30 (6.1x). \nThe way the results arrived faster also had an impact on the time of the peak, occurring at around 19:00 local time on October 30, as compared to around 20:00 on October 2. \nAt 19:45 local time on October 30, Lula was already the winner with more than 98% of the votes counted. After 20:00 there was a clear drop in DNS traffic to news organizations.\nOn October 2, it was only around 22:00 that it became official that there would be a runoff between Lula and Bolsonaro. Peak request volume was reached at 20:00 (9x), but traffic remained high (8x) at around 21:00 and until 22:00, like the following chart shows:\nConclusion: Real world events impact the Internet\nCloudflare Radar, our tool for Internet insights, can provide a unique perspective on how major global or national events impact the Internet. It is interesting to not only see that a real world event can impact Internet traffic (and different types of websites) for a whole country, but also see how much that impact is represented at specific times. It’s all about human behavior at relevant moments in time, like elections as a collective event is. \nPast examples of this include important presidential elections, the Super Bowl, the Oscars, Eurovision, never before seen views of the universe from a telescope , the holiday shopping season, or religious events such as Ramadan.\nYou can keep an eye on these trends using Cloudflare Radar.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nCloudflare Radar Internet Traffic Trends Brazil Election Security",
      "markdown": "11/03/2022\n\n*   [![João Tomé](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/V0x3WKfJ_400x400-1.jpeg)](https://blog.cloudflare.com/author/joao-tome/)\n\n8 min read\n\n_This post is also available in [简体中文](https://blog.cloudflare.com/zh-cn/how-the-brazilian-presidential-elections-affected-internet-traffic-zh-cn/) and [繁體中文](https://blog.cloudflare.com/zh-tw/how-the-brazilian-presidential-elections-affected-internet-traffic-zh-tw/)._\n\n![](https://lh5.googleusercontent.com/8bUx4Ez83TdAOmSz_WglK7ccLiFO3cXvj0fYBMx8Vz2frKVzmzL2S3F5fA7AQiZKk52ecxPx8dY1FrDR5xAKbf51Tz4qLwUFq-GpaDG0K3XjAQbQxQ69Gh7UbxGJpNrP_2aUZZRXcxBZB73iXOxZlzv93Hu421LEqAqzT7JhUUPtBxwAbeMEFLQbQMsv9g)\n\n> _Brasil, sei lá_  \n> _Ou o meu coração se engana_  \n> _Ou uma terra igual não há_  \n> — From Tom Jobim’s song, Brasil Nativo\n\nBrazil’s recent presidential election got significant attention from both global and national media outlets, not only because of the size of the country, but also because of premature allegations of electoral fraud. The first round of the Brazilian 2022 general election was held on October 2, and the runoff was held on Sunday, October 30. With 124 million votes counted, former president Lula da Silva (2003-2010) won with 50.9% of the votes, beating incumbent Jair Bolsonaro, who had 49.1% of the votes.\n\n![The final results of the elections as published by the official Tribunal Super Eleitoral, with more than 124 million votes counted.)](https://blog.cloudflare.com/content/images/2022/11/2.png)\n\nThe final results of the elections as published by the official [Tribunal Superior Eleitoral](https://resultados.tse.jus.br/oficial/app/index.html#/eleicao/resultados), with more than 124 million votes counted.)\n\nUsing Cloudflare’s data, we can explore the impact that this election had on Internet traffic patterns in Brazil, as well as interest in content from election-related websites, news organizations, social media platforms, and video platforms.\n\nHere are a few highlights: while the runoff generated much more interest to election related websites (we actually have a view to DNS queries, a proxy to websites), the first round showed bigger increases in traffic to news organizations.\n\nFor the candidate’s domains, Lula’s win had the higher impact.\n\nAlso: official results came earlier on the runoff than the first round, and spikes in traffic were higher earlier that day (October 30).\n\n(Note: we’re using local times — that means UTC-3, that is related to the more populated regions of Brazil — in this blog, although some charts have x-axis UTC).\n\nLet’s start by looking at general Internet traffic in Brazil.\n\n### On election days, traffic goes down (during the day)\n\nUsing [Cloudflare Radar](https://radar.cloudflare.com/br), we can see something that has also been observed in other countries that hold Sunday elections: when most people are getting outside to vote, Internet traffic goes down (in comparison with previous Sundays). We saw this in the two rounds of the Presidential [elections in France](https://blog.cloudflare.com/french-elections-2022-runoff/) back in April 2022, in Portugal’s legislative elections in January 2022 and now, in Brazil.\n\n![](https://blog.cloudflare.com/content/images/2022/11/3.png)\n\nWe can also compare Sundays in October. There were five weekends. The two that had elections show the same pattern of lower traffic during the day, as seen in the previous chart. Comparing the two election days, there was a bigger drop in traffic on October 30 (down 21% at around 18:00 local time), than on October 2 (down 10% at around 20:00). Related or not, there was a bigger turnout on the runoff (124 million votes) than on the first round (123 million). Here’s the view on October 30:\n\n![](https://blog.cloudflare.com/content/images/2022/11/4.png)\n\nAnd here’s October 2:\n\n![](https://blog.cloudflare.com/content/images/2022/11/5.png)\n\nA more clear view in comparing the October weekends, and where you can see how the October 2 and 30 Sundays have the same pattern and different from the others three of the month, is this one (bear in mind that the x-axis is showing UTC time, it’s -3 hours in Brazil):\n\n![](https://blog.cloudflare.com/content/images/2022/11/6.png)\n\nIf we look at the main network providers ([ASNs](https://www.cloudflare.com/learning/network-layer/what-is-an-autonomous-system/)) in [Brazil](https://radar.cloudflare.com/br), the trend is the same. Claro ([AS28573](https://radar.cloudflare.com/as28573)) also shows the drop in traffic on October 30, as does Telefonica ([AS27699](https://radar.cloudflare.com/as27699)):\n\n![](https://blog.cloudflare.com/content/images/2022/11/7.png)\n\nHere’s Telefonica:\n\n![](https://blog.cloudflare.com/content/images/2022/11/8.png)\n\nWe observed a similar impact from the October 30 runoff election to traffic from different states in Brazil, including São Paulo, Rio de Janeiro, Rio Grande do Norte, Minas Gerais, and Bahia.\n\n### Mobile device usage greater on weekends (and on election days)\n\nWhen we look at the share of Brazil’s Internet traffic from mobile devices during October, we find that the highest percentages were on October 2 (first round of the elections, 66.3%), October 9 (66.4%) and October 30 (runoff election, 65%). We’ve seen this in other elections, an increase in mobile device traffice, so this seems to follow the same trend.\n\n![](https://blog.cloudflare.com/content/images/2022/11/9.png)\n\nThis chart also shows how mobile device usage in Brazil is at its highest on the weekends (all the main spikes for percentage of mobile devices are over the weekend, and more on Sundays).\n\nNow, let’s look at anonymized and aggregated DNS traffic data from our [1.1.1.1](https://1.1.1.1/) resolver. This data provides a proxy for traffic to, and thus interest in, different categories of sites from users in Brazil around the election.\n\nBrazil has government websites related to elections, but also its own Tribunal Superior Eleitoral (Electoral Superior Court) that includes a [website](https://resultados.tse.jus.br/oficial/app/index.html#/eleicao/resultados) and [app](https://www.tse.jus.br/eleicoes/eleicoes-2022/divulgacao-dos-resultados-das-eleicoes-2022) with live updates on the results of the elections for everyone to check. Looking at those related domains and using mean hourly traffic in September as a baseline, we can see that the October 2 first round spiked to 16x more DNS queries at 20:00 local time. However, DNS query traffic during the runoff election peaked at 18:00 local time on October 30 with 17.4x more DNS traffic as compared to the September baseline.\n\n![](https://blog.cloudflare.com/content/images/2022/11/10.png)\n\nWe can look more closely at each one of those two election days. On October 2, traffic had its first significant increase at around 17:00 local time, reaching 15x more requests to election-related domains as compared to the September baseline. This initial peak occurred at the same time the polling stations were closing. However, the peak that day, at 16x above baseline, was reached at 20:00 local time, as seen in the figure below.\n\n![](https://blog.cloudflare.com/content/images/2022/11/11.png)\n\nOn Sunday, October 30, 2022, the pattern is similar, although the peak was reached earlier, given that results started to arrive earlier than on the first round. The peak was reached at around 18:00 local time, with request traffic 17.4x above baseline.\n\n![](https://blog.cloudflare.com/content/images/2022/11/12.png)\n\nAs seen in the figure below, Lula first led in the official results at 18:45 local time, with votes from 67% of the polling stations counted at that time. Around 20:00 Lula was considered the winner (the peak seen in the previous chart was at that time).\n\n![](https://blog.cloudflare.com/content/images/2022/11/Screenshot-2022-11-03-at-12.36.52-PM.png)\n\n### Candidate websites: in the end, winner takes all?\n\nFor Lula-related domains, there are clear spikes around the first round of elections on October 2. A 13x spike was observed on October 1 at around 21:00 local time. Two notable spikes were observed on October 2 — one at 16.7x above baseline at 09:00 local time, and the other at 10.7x above baseline at 21:00 local time. During the October 30 runoff election, only one clear spike was observed. The spike, at 16.7x above baseline, occurred at around 20:00, coincident with the time Lula was being announced as the winner.\n\n![](https://blog.cloudflare.com/content/images/2022/11/14.png)\n\nFor Bolsonaro-related domains, we observed a different pattern. Increased traffic as compared to the baseline is visible in the days leading up to the first round election, reaching 10x on September 30. On October 2, a 8x spike above baseline was seen at 18:00 local time. However, the two most significant spikes seen over the course of the month were observed on October 16, at 20x above baseline, a few hours after the first Lula-Bolsonaro television debate, and on October 25, at around 20:00, at 22x above baseline. That was the last week of campaigning before the October 30 runoff and when several polling predictions were announced. The second and last Bolsonaro-Lula debate was on October 28, and there’s a spike at 22:00 to Lula’s websites, and a smaller but also clear one at 21:00 to Bolsonaro’s websites).\n\n![](https://blog.cloudflare.com/content/images/2022/11/15.png)\n\n### News websites: more interest in the first round\n\nWith official election results being available more rapidly, DNS traffic for Brazilian news organization websites peaked much earlier in the evening than what we saw in [France](https://blog.cloudflare.com/french-elections-2022-runoff/), for example, where more definitive election results arrived much later on election day. But another interesting trend here is how the first round, on October 2, had 9.1x more DNS traffic (compared with the September baseline), than what we saw during the runoff on October 30 (6.1x).  \n\n![](https://blog.cloudflare.com/content/images/2022/11/16.png)\n\nThe way the results arrived faster also had an impact on the time of the peak, occurring at around 19:00 local time on October 30, as compared to around 20:00 on October 2.\n\nAt 19:45 local time on October 30, Lula was already the winner with more than 98% of the votes counted. After 20:00 there was a clear drop in DNS traffic to news organizations.\n\n![](https://blog.cloudflare.com/content/images/2022/11/17.png)\n\nOn October 2, it was only around 22:00 that it became official that there would be a runoff between Lula and Bolsonaro. Peak request volume was reached at 20:00 (9x), but traffic remained high (8x) at around 21:00 and until 22:00, like the following chart shows:\n\n![](https://blog.cloudflare.com/content/images/2022/11/18.png)\n\n### Conclusion: Real world events impact the Internet\n\nCloudflare Radar, our tool for Internet insights, can provide a unique perspective on how major global or national events impact the Internet. It is interesting to not only see that a real world event can impact Internet traffic (and different types of websites) for a whole country, but also see how much that impact is represented at specific times. It’s all about human behavior at relevant moments in time, like elections as a collective event is.\n\nPast examples of this include important [presidential elections](https://blog.cloudflare.com/elections-france-2022/), the [Super Bowl](https://blog.cloudflare.com/who-won-super-bowl-lvi-a-look-at-internet-traffic-during-the-big-game/), the [Oscars](https://blog.cloudflare.com/oscars-2022-impact/), [Eurovision](https://blog.cloudflare.com/eurovision-2022-internet-trends/), never before seen views of the universe from a [telescope](https://blog.cloudflare.com/how-the-james-webb-telescopes-cosmic-pictures-impacted-the-internet/) , the holiday shopping season, or religious events such as [Ramadan](https://blog.cloudflare.com/how-ramadan-shows-up-in-internet-trends/).\n\nYou can keep an eye on these trends using [Cloudflare Radar](https://radar.cloudflare.com/).  \n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Cloudflare Radar](https://blog.cloudflare.com/tag/cloudflare-radar/) [Internet Traffic](https://blog.cloudflare.com/tag/internet-traffic/) [Trends](https://blog.cloudflare.com/tag/trends/) [Brazil](https://blog.cloudflare.com/tag/brazil/) [Election Security](https://blog.cloudflare.com/tag/election-security/)"
    },
    {
      "url": "https://blog.cloudflare.com/cloudflares-athenian-project-expands-internationally/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/cloudflares-athenian-project-expands-internationally/",
        "loadedTime": "2023-12-05T02:36:21.748Z",
        "referrerUrl": "https://blog.cloudflare.com/2024-the-year-of-elections/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/cloudflares-athenian-project-expands-internationally/",
        "title": "Cloudflare's Athenian Project Expands Internationally",
        "description": "Due to the growing trend of cyberattacks targeting election infrastructure, election security is not a US-specific issue. Today, we are extending our security tools to a range of election entities around the world.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "07/29/2021\n9 min read\nOver the course of the past few years, we’ve seen a wide variety of different kinds of online threats to democratically-held elections around the world. These threats range from attempts to restrict the availability of information, to efforts to control the dialogue around elections, to full disruptions of the voting process. \nSome countries have shut down the Internet completely during elections. In 2020, Access Now’s #KeepItOn Campaign reported at least 155 Internet shutdowns in 29 countries such as Togo, Republic of the Congo, Niger and Benin. In 2021, Uganda's government ordered the \"Suspension Of The Operation Of Internet Gateways\" the day before the country's general election. \nEven outside a full Internet shutdown, election reporting and registration websites can face attacks from other nations and from parties seeking to disrupt the administration of the election or undermine trust in the electoral process. These cyberattacks target not only electronic voting or election technologies, but access to information and communications tools such as voter registration and websites that host election results. In 2014, a series of cyberattacks including DDoS, malware and phishing attacks were launched against Ukraine’s Central Election Commission ahead of the presidential election. These sophisticated attacks attempted to infiltrate the internal voting system and spread malware to deliver fake election results. Similar attacks were seen again in 2019 as Ukraine accused Russia of launching a DDoS attack against the CEC a month before the presidential election. These types of attacks that target electoral management agencies’ communication tools and public facing websites have been on the rise in countries ranging from Indonesia, North Macedonia, Georgia, and Estonia. \nThree and a half years ago, Cloudflare launched the Athenian Project to provide free Enterprise level services to state and local election websites in the United States. Through this project we have protected over 292 websites with information about voter registration, voting and polling places, as well as sites publishing final results across 30 states at no cost to the entities administering them. However, due to the growing trend of cyberattacks targeting election infrastructure, election security is not a US-specific issue, and since we launched the Athenian Project in the United States many people have asked: why don’t you extend these cybersecurity protections to election entities around the world?\nChallenges, Solutions and Partnerships\nThe short answer is we weren’t entirely sure whether Cloudflare, a US based company, could provide a free set of upgraded security services to foreign election entities. Cloudflare is a global company with 16 offices around the world and a global network that spans over 100 countries to provide security and performance tools. We are proud to create new and innovative products to enhance user privacy and security, but understanding the intricacies of local elections, the regulatory environment, and political players is complicated, to say the least. \nWhen we started the Athenian Project in 2017, we understood the environment and gaps in coverage for state and local governments in the United States. The United States has a decentralized election administrative system, which means that local election administrators may conduct elections differently in every state. Because of the funding challenges that come with a decentralized system, state and local governments in all 50 states could benefit from free Enterprise-level services. Fast-forward to more than three years after we launched the project, we have learned a great deal about what types of threats election entities face, what products election entities need for securing their web infrastructure, and how to build trust with state and local governments in need of these types of protections. \nAs the Athenian Project and Cloudflare for Campaigns grew in the United States, we received inquiries from foreign election bodies, political parties and campaigns on whether they were eligible for protection under one of these projects. We turned to our Project Galileo partners for their advice and guidance. \nUnder Project Galileo, we partner with more than 40 civil society organizations to protect a range of sensitive players on the Internet including human rights organizations, journalism and independent media, and organizations that focus on strengthening democracy in 111 countries. Many of these civil society partners work on election-related matters such as capacity building, strengthening democratic institutions, supporting civil society organizations to equipping these groups with the tools they need to be safe and secure online. These partners, many of whom have local representatives on the ground, understand the intricacies of the election landscape and delicate nature of trust building between local election administrations, political parties and organizations with personnel directly on the ground in many of these regions to provide direct support and expertise when it comes to safeguarding elections. \nAfter many discussions and years in the making, we are excited to announce our collaboration with The International Foundation for Electoral Systems, National Democratic Institute, the International Republican Institute and to provide free Enterprise Cloudflare services to groups working on election reporting and to election management agencies to provide the tools, resources and expertise to help them stay online in the face of large scale cyber attacks.\nPartnership with International Foundation for Electoral Systems\nAs we work with civil society organizations on issues in the election space and extending protections outside the United States, we frequently heard organizations bring up IFES, the International Foundation for Electoral Systems, due to their expertise in promoting and protecting democracy. The International Foundation for Electoral Systems is a nonpartisan, nonprofit organization that has worked in more than 145 countries, from developing to mature democracies. \nFounded in 1987, IFES’ work in promoting democracy and genuine elections has evolved to meet the challenges of today and tomorrow. IFES offers research, innovation and technical assistance to support democratic elections, human rights, combat corruption, promote equal political participation, and ensure that information and technology advance, not undermine, democracy and elections.\nOne of the many reasons we wanted to work with IFES on expanding our election offering was due to the organizations’ unique position in terms of technical expertise, understanding of the political landscapes in which they operate, and fundamental knowledge of the types of protections these election management bodies (EMBs) need in preparing and conducting elections. Building trust in the election space is critical when providing support to EMBs. Due to years of hard work from IFES assisting with the implementation of election operations as well as direct assistance to support democratic developments, and the trust IFES has correspondingly developed with EMBs, they were a logical partner. \nIFES’ Center for Technology & Democracy, in collaboration with IFES program teams worldwide, provides cybersecurity and ICT assistance to EMBs and civil society organizations (CSOs). IFES uses leading cybersecurity and ICT practices and standards incorporated into its Holistic Exposure and Adaptation Testing (HEAT) methodology with the aim of increasing EMBs and CSOs digital transformation while mitigating associated risks.\n“Cloudflare has played an integral role in helping EMBs and CSOs protect their websites, prevent website defacement, and ensure that they are accessible during peak traffic spikes. This has allowed EMBs and CSOs to build internal and external stakeholder confidence while gaining access and building local capacity on cutting-edge cybersecurity solutions and good practices.” \n— Stephen Boyce, Senior Global Election Technology & Cybersecurity Advisor at IFES. \nAs part of the partnership with IFES, Cloudflare provides its highest level of services to EMBs working with IFES and equips them with the cybersecurity tools for their web infrastructure and internal teams to promote electoral integrity and stronger democracies. Along with cybersecurity tools, Cloudflare will work closely with IFES on training and direct assistance to these election bodies, so they have the knowledge and expertise to conduct a free, fair, and safe elections. In the past, Cloudflare has worked with IFES to provide services in support of elections in Georgia, and we look forward to extending these protections to other EMBs in the future. \nPartnership with National Democratic Institute, the International Republican Institute and the Design 4 Democracy Coalition\nThe National Democratic Institute and The International Republican Institute are two of the many Project Galileo partners that we have worked with to provide cybersecurity tools to organizations that work building and strengthening democratic institutions and increasing civic participation all around the world. As we worked together on Project Galileo, our conversations often focused on the best way to extend these types of security tools to groups in the election space. \nCloudflare is excited to announce that we are partnering with the National Democractic Institute (NDI), the International Republican Institute (IRI) and the Design 4 Democracy Coalition (D4D) to expand our election support efforts. Through this initiative, Cloudflare will provide free service to vulnerable groups working on elections, as identified by NDI and IRI. Our combined expertise in cybersecurity and elections administration will enable us to be mutually beneficial in navigating this space. As part of protecting a new set of election groups, Cloudflare will work with NDI and IRI to understand the global threats faced by democratic election institutions. \n“Elections are being undermined by a wide range of malign actors. Through our partnership with Cloudflare, IRI has been able to ensure that the civil society and independent media organizations we support globally are able to defend themselves against cyber attacks and massive increases in web traffic - keeping them safe and online at the most critical moments for democratic integrity. We are excited to be working with Cloudflare, NDI, and the D4D Coalition to expand those offerings to election management bodies, political parties, and political campaigns - a critical step toward ensuring that political competition is fought in the sphere of policy and governance delivery, and not through information and cyber warfare.” \n— Amy Studdart, Senior Advisor for Digital Democracy, Center for Global Impact at the International Republican Institute.\nAs part of our new initiative, when Cloudflare tests new products which would be particularly useful for election groups we will work with NDI, IRI and D4D to encourage these groups to adopt the new services. This might include passing along information and documentation on how to deploy them, offering webinars, and providing other specialized support. Piloting new products with this audience will also provide us with the opportunity to learn about needs and pain points for these groups.\n“Safe, reliable access to the internet is fundamental to a free, open, and democratic electoral process in the modern era. Cloudflare’s sophisticated protections against various forms of cyberattack have provided invaluable support to at-risk campaigns and civic organizations through NDI and the D4D Coalition. This new initiative will go further to supporting one of the most fundamental of human rights: the vote.”\n— Chris Doten, Chief Innovation Officer at the National Democratic Institute. \nExtending Protection to State Parties in the United States with Defending Digital Campaigns\nWe didn’t forget our friends in the United States. I am excited to announce that we are extending our support to provide a suite of Cloudflare products to eligible state parties in the United States with our partnership with Defending Digital Campaigns (DDC). In January 2020, we announced our partnership with Defending Digital Campaigns, a nonprofit, nonpartisan organization that provides access to cybersecurity products, services, and information to eligible federal campaigns. \nWe have reported on the regulatory challenges of providing free or discounted services to political campaigns in the past. Due to campaign finance regulations in the United States, private corporations are prohibited from providing any contributions of either money or services to federal candidates or political party organizations. We partnered with DDC, who was granted special permission by the Federal Election Commission to provide eligible federal campaigns with free or reduced-cost cybersecurity services due to the enhanced threat of foreign cyberattacks against party and candidate committees.\nSince the start of our partnership, we have provided products to protect Presidential, Senate and House campaigns with tools like DDoS protection, web application firewall, SSL encryption, and bot protection. We have also offered campaigns cybersecurity tools to protect their internal networks, offering Cloudflare Access and Gateway to more than 75 campaigns in the 2020 U.S. election. \nAfter the 2020 U.S. election, DDC extended their offering to protect state parties in select states. \n“One of DDC’s core recommendations for any campaign or an organization like a State Party is protecting their websites from attacks or defacements,” said Michael Kaiser, President and CEO of Defending Digital Campaigns. “Our partnership with Cloudflare is critical to bringing this core protection to eligible entities and protecting our democracy.” \nWe are excited to be furthering our partnership with Defendering Digital Campaigns to provide our free suite of services to eligible state parties to better secure themselves from cyber attacks. \nFor more information on eligibility for these services under DDC and the next steps, please visit cloudflare.com/campaigns/usa.\nTo the future…\nRecognizing the global nature of cyberthreats targeting election-related technologies, we are excited to be working with these groups to help players in the election space stay secure online. In addition to the goals already laid out, Cloudflare intends to build on these partnerships in the future. Eventually, we hope to assist with each of these partners’ programs as mentors and trainers, perhaps directly participating in assessments and training around critical elections. These groups' expertise makes them fantastic partners in this space, and we look forward to the opportunity to expand our work with their guidance.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nImpact Week Athenian Project",
      "markdown": "07/29/2021\n\n*   [![Jocelyn Woolbright](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2019/12/0-1.jpg)](https://blog.cloudflare.com/author/jocelyn/)\n\n9 min read\n\n![Athnian International Update](https://blog.cloudflare.com/content/images/2021/07/Athenian-Expansion.png)\n\nOver the course of the past few years, we’ve seen a wide variety of different kinds of online threats to democratically-held elections around the world. These threats range from attempts to restrict the availability of information, to efforts to control the dialogue around elections, to full disruptions of the voting process.\n\nSome countries have shut down the Internet completely during elections. In 2020, Access Now’s [#KeepItOn](https://www.accessnow.org/keepiton/#problem) Campaign reported at least 155 Internet shutdowns in 29 countries such as Togo, Republic of the Congo, Niger and Benin. In 2021, Uganda's government [ordered](https://blog.cloudflare.com/uganda-january-13-2021-internet-shut-down/) the \"Suspension Of The Operation Of Internet Gateways\" the day before the country's general election.\n\nEven outside a full Internet shutdown, election reporting and registration websites can face attacks from other nations and from parties seeking to disrupt the administration of the election or undermine trust in the electoral process. These cyberattacks target not only electronic voting or election technologies, but access to information and communications tools such as voter registration and websites that host election results. In 2014, a [series of cyberattacks](https://www.csmonitor.com/World/Passcode/2014/0617/Ukraine-election-narrowly-avoided-wanton-destruction-from-hackers) including DDoS, malware and phishing attacks were launched against Ukraine’s Central Election Commission ahead of the presidential election. These sophisticated attacks attempted to infiltrate the internal voting system and spread malware to deliver fake election results. Similar attacks were seen again in 2019 as Ukraine accused Russia of [launching a DDoS attack](https://www.cyberscoop.com/ukraines-president-accuses-russia-launching-cyberattack-election-commission/) against the CEC a month before the presidential election. These types of attacks that target electoral management agencies’ communication tools and public facing websites have been on the rise in countries ranging from Indonesia, North Macedonia, Georgia, and Estonia.  \n\nThree and a half years ago, Cloudflare [launched the Athenian Project](https://blog.cloudflare.com/the-athenian-project/) to provide free Enterprise level services to state and local election websites in the United States. Through this project we have protected over 292 websites with information about voter registration, voting and polling places, as well as sites publishing final results across 30 states at no cost to the entities administering them. However, due to the growing trend of cyberattacks targeting election infrastructure, election security is not a US-specific issue, and since we launched the Athenian Project in the United States many people have asked: why don’t you extend these cybersecurity protections to election entities around the world?\n\n### Challenges, Solutions and Partnerships\n\nThe short answer is we weren’t entirely sure whether Cloudflare, a US based company, could provide a free set of upgraded security services to foreign election entities. Cloudflare is a global company with 16 offices around the world and a global network that spans over 100 countries to provide security and performance tools. We are proud to create new and innovative products to enhance user privacy and security, but understanding the intricacies of local elections, the regulatory environment, and political players is complicated, to say the least.\n\nWhen we started the Athenian Project in 2017, we understood the environment and gaps in coverage for state and local governments in the United States. The United States has a decentralized election administrative system, which means that local election administrators may conduct elections differently in every state. Because of the funding challenges that come with a decentralized system, state and local governments in all 50 states could benefit from free Enterprise-level services. Fast-forward to more than three years after we launched the project, we have learned a great deal about what types of threats election entities face, what products election entities need for securing their web infrastructure, and how to build trust with state and local governments in need of these types of protections.\n\nAs the Athenian Project and Cloudflare for Campaigns grew in the United States, we received inquiries from foreign election bodies, political parties and campaigns on whether they were eligible for protection under one of these projects. We turned to our Project Galileo partners for their advice and guidance.\n\nUnder Project Galileo, we partner with more than 40 civil society organizations to protect a range of sensitive players on the Internet including human rights organizations, journalism and independent media, and organizations that focus on strengthening democracy in 111 countries. Many of these civil society partners work on election-related matters such as capacity building, strengthening democratic institutions, supporting civil society organizations to equipping these groups with the tools they need to be safe and secure online. These partners, many of whom have local representatives on the ground, understand the intricacies of the election landscape and delicate nature of trust building between local election administrations, political parties and organizations with personnel directly on the ground in many of these regions to provide direct support and expertise when it comes to safeguarding elections.\n\nAfter many discussions and years in the making, we are excited to announce our collaboration with The International Foundation for Electoral Systems, National Democratic Institute, the International Republican Institute and to provide free Enterprise Cloudflare services to groups working on election reporting and to election management agencies to provide the tools, resources and expertise to help them stay online in the face of large scale cyber attacks.\n\n### Partnership with International Foundation for Electoral Systems\n\n![](https://blog.cloudflare.com/content/images/2021/07/Screenshot-2021-07-28-at-16.52.05.png)\n\nAs we work with civil society organizations on issues in the election space and extending protections outside the United States, we frequently heard organizations bring up IFES, the International Foundation for Electoral Systems, due to their expertise in promoting and protecting democracy. The International Foundation for Electoral Systems is a nonpartisan, nonprofit organization that has worked in more than 145 countries, from developing to mature democracies.\n\nFounded in 1987, IFES’ work in promoting democracy and genuine elections has evolved to meet the challenges of today and tomorrow. IFES offers research, innovation and technical assistance to support democratic elections, human rights, combat corruption, promote equal political participation, and ensure that information and technology advance, not undermine, democracy and elections.\n\nOne of the many reasons we wanted to work with IFES on expanding our election offering was due to the organizations’ unique position in terms of technical expertise, understanding of the political landscapes in which they operate, and fundamental knowledge of the types of protections these election management bodies (EMBs) need in preparing and conducting elections. Building trust in the election space is critical when providing support to EMBs. Due to years of hard work from IFES assisting with the implementation of election operations as well as direct assistance to support democratic developments, and the trust IFES has correspondingly developed with EMBs, they were a logical partner.\n\nIFES’ Center for Technology & Democracy, in collaboration with IFES program teams worldwide, provides cybersecurity and ICT assistance to EMBs and civil society organizations (CSOs). IFES uses leading cybersecurity and ICT practices and standards incorporated into its Holistic Exposure and Adaptation Testing (HEAT) methodology with the aim of increasing EMBs and CSOs digital transformation while mitigating associated risks.\n\n> _“Cloudflare has played an integral role in helping EMBs and CSOs protect their websites, prevent website defacement, and ensure that they are accessible during peak traffic spikes. This has allowed EMBs and CSOs to build internal and external stakeholder confidence while gaining access and building local capacity on cutting-edge cybersecurity solutions and good practices.”_  \n> — **Stephen Boyce**, Senior Global Election Technology & Cybersecurity Advisor at IFES.\n\nAs part of the partnership with IFES, Cloudflare provides its highest level of services to EMBs working with IFES and equips them with the cybersecurity tools for their web infrastructure and internal teams to promote electoral integrity and stronger democracies. Along with cybersecurity tools, Cloudflare will work closely with IFES on training and direct assistance to these election bodies, so they have the knowledge and expertise to conduct a free, fair, and safe elections.  In the past, Cloudflare has worked with IFES to provide services in support of elections in Georgia, and we look forward to extending these protections to other EMBs in the future.\n\n### Partnership with National Democratic Institute, the International Republican Institute and the Design 4 Democracy Coalition\n\n![](https://blog.cloudflare.com/content/images/2021/07/Screenshot-2021-07-28-at-16.53.12.png)\n\nThe National Democratic Institute and The International Republican Institute are two of the many Project Galileo partners that we have worked with to provide cybersecurity tools to organizations that work building and strengthening democratic institutions and increasing civic participation all around the world. As we worked together on Project Galileo, our conversations often focused on the best way to extend these types of security tools to groups in the election space.\n\nCloudflare is excited to announce that we are partnering with the National Democractic Institute (NDI), the International Republican Institute (IRI) and the Design 4 Democracy Coalition (D4D) to expand our election support efforts. Through this initiative, Cloudflare will provide free service to vulnerable groups working on elections, as identified by NDI and IRI. Our combined expertise in cybersecurity and elections administration will enable us to be mutually beneficial in navigating this space. As part of protecting a new set of election groups, Cloudflare will work with NDI and IRI to understand the global threats faced by democratic election institutions.\n\n> _“Elections are being undermined by a wide range of malign actors. Through our partnership with Cloudflare, IRI has been able to ensure that the civil society and independent media organizations we support globally are able to defend themselves against cyber attacks and massive increases in web traffic - keeping them safe and online at the most critical moments for democratic integrity. We are excited to be working with Cloudflare, NDI, and the D4D Coalition to expand those offerings to election management bodies, political parties, and political campaigns - a critical step toward ensuring that political competition is fought in the sphere of policy and governance delivery, and not through information and cyber warfare.”  \n> _— **Amy Studdart**, Senior Advisor for Digital Democracy, Center for Global Impact at the International Republican Institute.\n\nAs part of our new initiative, when Cloudflare tests new products which would be particularly useful for election groups we will work with NDI, IRI and D4D to encourage these groups to adopt the new services. This might include passing along information and documentation on how to deploy them, offering webinars, and providing other specialized support. Piloting new products with this audience will also provide us with the opportunity to learn about needs and pain points for these groups.\n\n> _“Safe, reliable access to the internet is fundamental to a free, open, and democratic electoral process in the modern era. Cloudflare’s sophisticated protections against various forms of cyberattack have provided invaluable support to at-risk campaigns and civic organizations through NDI and the D4D Coalition. This new initiative will go further to supporting one of the most fundamental of human rights: the vote.”_  \n> — **Chris Doten**,  Chief Innovation Officer at the National Democratic Institute.\n\n### Extending Protection to State Parties in the United States with Defending Digital Campaigns\n\n![](https://blog.cloudflare.com/content/images/2021/07/Screenshot-2021-07-28-at-16.54.14.png)\n\nWe didn’t forget our friends in the United States. I am excited to announce that we are extending our support to provide a suite of Cloudflare products to eligible state parties in the United States with our partnership with Defending Digital Campaigns (DDC). In January 2020, we [announced](https://blog.cloudflare.com/introducing-cloudflare-for-campaigns/) our partnership with [Defending Digital Campaigns](https://www.defendcampaigns.org/), a nonprofit, nonpartisan organization that provides access to cybersecurity products, services, and information to eligible federal campaigns.\n\nWe have reported on the regulatory challenges of providing free or discounted services to political campaigns in the past. Due to campaign finance regulations in the United States, private corporations are prohibited from providing any contributions of either money or services to federal candidates or political party organizations. We partnered with DDC, who was granted special permission by the Federal Election Commission to provide eligible federal campaigns with free or reduced-cost cybersecurity services due to the enhanced threat of foreign cyberattacks against party and candidate committees.\n\nSince the start of our partnership, we have provided products to protect Presidential, Senate and House campaigns with tools like DDoS protection, web application firewall, SSL encryption, and bot protection. We have also offered campaigns cybersecurity tools to protect their internal networks, offering Cloudflare Access and Gateway to more than 75 campaigns in the 2020 U.S. election.\n\nAfter the 2020 U.S. election, DDC extended their offering to protect state parties in select states.\n\n“One of DDC’s core recommendations for any campaign or an organization like a State Party is protecting their websites from attacks or defacements,” said Michael Kaiser, President and CEO of Defending Digital Campaigns. “Our partnership with Cloudflare is critical to bringing this core protection to eligible entities and protecting our democracy.”\n\nWe are excited to be furthering our partnership with Defendering Digital Campaigns to provide our free suite of services to eligible state parties to better secure themselves from cyber attacks.\n\nFor more information on eligibility for these services under DDC and the next steps, please visit [cloudflare.com/campaigns/usa](https://cloudflare.com/campaigns/usa/).\n\n### To the future…\n\nRecognizing the global nature of cyberthreats targeting election-related technologies, we are excited to be working with these groups to help players in the election space stay secure online. In addition to the goals already laid out, Cloudflare intends to build on these partnerships in the future. Eventually, we hope to assist with each of these partners’ programs as mentors and trainers, perhaps directly participating in assessments and training around critical elections. These groups' expertise makes them fantastic partners in this space, and we look forward to the opportunity to expand our work with their guidance.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Impact Week](https://blog.cloudflare.com/tag/impact-week/) [Athenian Project](https://blog.cloudflare.com/tag/athenian-project/)"
    },
    {
      "url": "https://blog.cloudflare.com/2022-us-midterm-elections-attack-analysis/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/2022-us-midterm-elections-attack-analysis/",
        "loadedTime": "2023-12-05T02:36:38.540Z",
        "referrerUrl": "https://blog.cloudflare.com/2024-the-year-of-elections/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/2022-us-midterm-elections-attack-analysis/",
        "title": "2022 US midterm elections attack analysis",
        "description": "For Athenian Project and Cloudflare for Campaigns participant websites, overall traffic volume ramped as Election Day approached",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/11/2022\n6 min read\nThrough Cloudflare’s Impact programs, we provide cyber security products to help protect access to authoritative voting information and the security of sensitive voter data. Two core programs in this space are the Athenian Project, dedicated to protecting state and local governments that run elections, and Cloudflare for Campaigns, a project with a suite of Cloudflare products to secure political campaigns’ and state parties’ websites and internal teams. \nHowever, the weeks ahead of the elections, and Election Day itself, were not entirely devoid of attacks. Using data from Cloudflare Radar, which showcases global Internet traffic, attack, and technology trends and insights, we can explore traffic patterns, attack types, and top attack sources associated with both Athenian Project and Cloudflare for Campaigns participants. \nFor both programs, overall traffic volume unsurprisingly ramped up as Election Day approached. SQL Injection (SQLi) and HTTP Anomaly attacks were the two largest categories of attacks mitigated by Cloudflare’s Web Application Firewall (WAF), and the United States was the largest source of observed attacks — see more on this last point below.\nBelow, we explore the trends seen across both customer sets from October 1, 2022, through Election Day on November 8.\nAthenian Project\nThroughout October, daily peak traffic volumes effectively doubled over the course of the month, with a weekday/weekend pattern also clearly visible. However, significant traffic growth is visible on Monday, November 7, and Tuesday, November 8 (Election Day), with Monday’s peak just under 2x October’s peaks, while Tuesday saw two peaks, one just under 4x higher than October peaks, while the other was just over 4x higher. Zooming in, the first peak was at 1300 UTC (0800 Eastern time, 0500 Pacific time), while the second was at 0400 UTC (2300 Eastern time, 2000 Pacific time). The first one appears to be aligned with the polls opening on the East Coast, while the second appears to be aligned with the time that the polls closed on the West Coast.\nHowever, aggregating the traffic here presents a somewhat misleading picture. While both spikes were due to increased traffic across multiple customer sites, the second one was exacerbated by a massive increase in traffic for a single customer. Regardless, the increased traffic clearly shows that voters turned to local government sites around Election Day.\nDespite this increase in overall traffic, attack traffic mitigated by Cloudflare’s Web Application Firewall (WAF) remained remarkably consistent throughout October and into November, as seen in the graph below. The obvious exception was an attack that occurred on Monday, October 10. This attack targeted a single Athenian Project participant, and was mitigated by rate limiting the requests. \nSQL injection (SQLi) attacks saw significant growth in volume in the week and a half ahead of Election Day, along with an earlier significant spike on October 24. While the last weekend in October (October 29 and 30) saw significant SQLi attack activity, the weekend of November 5 and 6 was comparatively quiet. However, those attacks ramped up again heading into and on Election Day, as seen in the graph below.\nAttempted attacks mitigated with the HTTP Anomaly ruleset also ramped up in the week ahead of Election Day, though to a much lesser extent than SQLi attacks. As the graph below shows, the biggest spikes were seen on October 31/November 1, and just after midnight UTC on November 4 (late afternoon to early evening in the US). Related request volume also grew heading into Election Day, but without significant short-duration spikes. There is also a brief but significant attack clearly visible on the graph on October 10. However, it occurred several hours after the rate limited attack referenced above — it is not clear if the two are related.\nThe distribution of attacks over the surveyed period from October 1 through November 9 shows that those categorized as SQLi and HTTP Anomaly were responsible for just over two-thirds of WAF-mitigated requests. Nearly 14% were categorized as “Software Specific,” which includes attacks related to specific CVEs. The balance of the attacks were mitigated by WAF rules in categories including File Inclusion, XSS (Cross Site Scripting), Directory Traversal, and Command Injection.\nMedia reports suggest that foreign adversaries actively try to interfere with elections in the United States. While this may be the case, analysis of the mitigated attacks targeting Athenian Project customers found that over 95% of the mitigated requests (attacks) came from IP addresses that geolocate to the United States. However, that does not mean that the attackers themselves are necessarily located in the country, but rather that they appear to be using compromised systems and proxies within the United States to launch their attacks against these sites protected by Cloudflare.\nCloudflare for Campaigns\nIn contrast to Athenian Project participants, traffic to candidate sites that are participants in Cloudflare for Campaigns began to grow several weeks ahead of Election Day. The graph below shows a noticeable increase (~50%) in peak traffic volumes starting on October 12, with an additional growth (50-100%) starting a week later. Traffic to these sites appeared to quiet a bit toward the end of October, but saw significant growth again heading into, and during, Election Day.\nHowever, once again, this aggregate traffic data presents something of a misleading picture, as one candidate site saw multiple times more traffic than the other participating sites. While those other sites saw similar shifts in traffic as well, they were dwarfed by those experienced by the outlier site.\nThe WAF-mitigated traffic trend for campaign sites followed a similar pattern to the overall traffic. As the graph below shows, attack traffic also began to increase around October 19, with a further ramp near the end of the month. The October 27 spike visible in the graph was due to an attack targeting a single customer’s site, and was addressed using “Security Level” mitigation techniques, which uses IP reputation information to decide if and how to present challenges for incoming requests.\nThe top two rule categories, HTTP Anomaly and SQLi, together accounted for nearly three-quarters of the mitigated requests, and Directory Traversal attacks were just under 10% of mitigated requests for this customer set. The HTTP Anomaly and Directory Traversal percentages were higher than those for attacks targeting Athenian Project participants, while the SQLi percentage was slightly lower.\nOnce again, a majority of the WAF-mitigated attacks came from IP addresses in the United States. However, among Cloudflare for Campaigns participants, the United States only accounted for 55% of attacks, significantly lower than the 95% seen for Athenian Project participants. The balance is spread across a long tail of countries, with allies including Germany, Canada, and the United Kingdom among the top five. As noted above, however, the attackers may be elsewhere, and are using botnets or other compromised systems in these countries to launch attacks.\nImproving security with data\nWe are proud to be trusted by local governments, campaigns, state parties, and voting rights organizations to protect their websites and provide uninterrupted access to information and trusted election results. Sharing information about the threats facing these websites helps us further support their valuable work by enabling them, and other participants in the election space, to take proactive steps to improve site security.\nLearn more about how to apply to the Athenian Project, and check out Cloudflare Radar for real-time insights into Internet traffic, attack trends, and more.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nCloudflare Radar Election Security Trends Security",
      "markdown": "11/11/2022\n\n*   [![David Belson](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/11/David-Belson.jpeg)](https://blog.cloudflare.com/author/david-belson/)\n\n6 min read\n\n![Protecting election groups during the 2022 US midterm elections.](https://blog.cloudflare.com/content/images/2022/11/1-1.png)\n\nThrough Cloudflare’s [Impact](https://www.cloudflare.com/impact/) programs, we provide cyber security products to help protect access to authoritative voting information and the security of sensitive voter data. Two core programs in this space are the [Athenian Project](https://www.cloudflare.com/athenian/), dedicated to protecting state and local governments that run elections, and [Cloudflare for Campaigns](https://www.cloudflare.com/campaigns/usa/), a project with a suite of Cloudflare products to secure political campaigns’ and state parties’ websites and internal teams.\n\nHowever, the weeks ahead of the elections, and Election Day itself, were not entirely devoid of attacks. Using data from [Cloudflare Radar](https://radar.cloudflare.com/), which showcases global Internet traffic, attack, and technology trends and insights, we can explore traffic patterns, attack types, and top attack sources associated with both Athenian Project and Cloudflare for Campaigns participants.\n\nFor both programs, overall traffic volume unsurprisingly ramped up as Election Day approached. SQL Injection (SQLi) and HTTP Anomaly attacks were the two largest categories of attacks mitigated by Cloudflare’s Web Application Firewall (WAF), and the United States was the largest source of observed attacks — see more on this last point below.\n\nBelow, we explore the trends seen across both customer sets from October 1, 2022, through Election Day on November 8.\n\n### Athenian Project\n\nThroughout October, daily peak traffic volumes effectively doubled over the course of the month, with a weekday/weekend pattern also clearly visible. However, significant traffic growth is visible on Monday, November 7, and Tuesday, November 8 (Election Day), with Monday’s peak just under 2x October’s peaks, while Tuesday saw two peaks, one just under 4x higher than October peaks, while the other was just over 4x higher. Zooming in, the first peak was at 1300 UTC (0800 Eastern time, 0500 Pacific time), while the second was at 0400 UTC (2300 Eastern time, 2000 Pacific time). The first one appears to be aligned with the polls opening on the East Coast, while the second appears to be aligned with the time that the polls closed on the West Coast.\n\nHowever, aggregating the traffic here presents a somewhat misleading picture. While both spikes were due to increased traffic across multiple customer sites, the second one was exacerbated by a massive increase in traffic for a single customer. Regardless, the increased traffic clearly shows that voters turned to local government sites around Election Day.\n\n![](https://blog.cloudflare.com/content/images/2022/11/athenian---all-traffic---crop.png)\n\nDespite this increase in overall traffic, attack traffic mitigated by Cloudflare’s Web Application Firewall ([WAF](https://www.cloudflare.com/learning/ddos/glossary/web-application-firewall-waf/)) remained remarkably consistent throughout October and into November, as seen in the graph below. The obvious exception was an attack that occurred on Monday, October 10. This attack targeted a single Athenian Project participant, and was mitigated by [rate limiting](https://www.cloudflare.com/learning/bots/what-is-rate-limiting/) the requests.\n\n![](https://blog.cloudflare.com/content/images/2022/11/athenian---WAF-traffic---crop.png)\n\n[SQL injection (SQLi) attacks](https://www.cloudflare.com/learning/security/threats/sql-injection/) saw significant growth in volume in the week and a half ahead of Election Day, along with an earlier significant spike on October 24. While the last weekend in October (October 29 and 30) saw significant SQLi attack activity, the weekend of November 5 and 6 was comparatively quiet. However, those attacks ramped up again heading into and on Election Day, as seen in the graph below.\n\n![](https://blog.cloudflare.com/content/images/2022/11/athenian---SQLi---crop.png)\n\nAttempted attacks mitigated with the [HTTP Anomaly](https://blog.cloudflare.com/application-security/) ruleset also ramped up in the week ahead of Election Day, though to a much lesser extent than SQLi attacks. As the graph below shows, the biggest spikes were seen on October 31/November 1, and just after midnight UTC on November 4 (late afternoon to early evening in the US). Related request volume also grew heading into Election Day, but without significant short-duration spikes. There is also a brief but significant attack clearly visible on the graph on October 10. However, it occurred several hours after the rate limited attack referenced above — it is not clear if the two are related.\n\n![](https://blog.cloudflare.com/content/images/2022/11/athenian---http-anomaly---crop.png)\n\nThe distribution of attacks over the surveyed period from October 1 through November 9 shows that those categorized as SQLi and HTTP Anomaly were responsible for just over two-thirds of WAF-mitigated requests. Nearly 14% were categorized as [“Software Specific,”](https://blog.cloudflare.com/searchresults/#q=cve&sort=date%20descending&f:@language=[English]) which includes attacks related to specific [CVEs](https://www.cve.org/). The balance of the attacks were mitigated by WAF rules in categories including [File Inclusion](http://projects.webappsec.org/w/page/13246955/Remote%20File%20Inclusion), [XSS (Cross Site Scripting)](https://www.cloudflare.com/learning/security/threats/cross-site-scripting/), [Directory Traversal](https://portswigger.net/web-security/file-path-traversal), and [Command Injection](https://owasp.org/www-community/attacks/Command_Injection).\n\n![](https://blog.cloudflare.com/content/images/2022/11/athenian---WAF-mitigated-request-percentages.png)\n\nMedia reports [suggest](https://www.axios.com/2022/11/08/possible-interference-from-beijing-looms-over-elections) that foreign adversaries actively try to interfere with elections in the United States. While this may be the case, analysis of the mitigated attacks targeting Athenian Project customers found that over 95% of the mitigated requests (attacks) came from IP addresses that geolocate to the United States. However, that does not mean that the attackers themselves are necessarily located in the country, but rather that they appear to be using compromised systems and proxies within the United States to launch their attacks against these sites protected by Cloudflare.\n\n![](https://blog.cloudflare.com/content/images/2022/11/athenian---waf-mitigated-by-client-country.png)\n\n### Cloudflare for Campaigns\n\nIn contrast to Athenian Project participants, traffic to candidate sites that are participants in Cloudflare for Campaigns began to grow several weeks ahead of Election Day. The graph below shows a noticeable increase (~50%) in peak traffic volumes starting on October 12, with an additional growth (50-100%) starting a week later. Traffic to these sites appeared to quiet a bit toward the end of October, but saw significant growth again heading into, and during, Election Day.\n\nHowever, once again, this aggregate traffic data presents something of a misleading picture, as one candidate site saw multiple times more traffic than the other participating sites. While those other sites saw similar shifts in traffic as well, they were dwarfed by those experienced by the outlier site.\n\n![](https://blog.cloudflare.com/content/images/2022/11/campaigns---all-traffic---crop.png)\n\nThe WAF-mitigated traffic trend for campaign sites followed a similar pattern to the overall traffic. As the graph below shows, attack traffic also began to increase around October 19, with a further ramp near the end of the month. The October 27 spike visible in the graph was due to an attack targeting a single customer’s site, and was addressed using “[Security Level](https://support.cloudflare.com/hc/en-us/articles/200170056-Understanding-the-Cloudflare-Security-Level)” mitigation techniques, which uses IP reputation information to decide if and how to present challenges for incoming requests.\n\n![](https://blog.cloudflare.com/content/images/2022/11/campaigns---WAF-traffic---crop-1.png)\n\nThe top two rule categories, HTTP Anomaly and SQLi, together accounted for nearly three-quarters of the mitigated requests, and Directory Traversal attacks were just under 10% of mitigated requests for this customer set. The HTTP Anomaly and Directory Traversal percentages were higher than those for attacks targeting Athenian Project participants, while the SQLi percentage was slightly lower.\n\n![](https://blog.cloudflare.com/content/images/2022/11/campaigns---WAF-mitigated-request-percentages.png)\n\nOnce again, a majority of the WAF-mitigated attacks came from IP addresses in the United States. However, among Cloudflare for Campaigns participants, the United States only accounted for 55% of attacks, significantly lower than the 95% seen for Athenian Project participants. The balance is spread across a long tail of countries, with allies including Germany, Canada, and the United Kingdom among the top five. As noted above, however, the attackers may be elsewhere, and are using botnets or other compromised systems in these countries to launch attacks.\n\n![](https://blog.cloudflare.com/content/images/2022/11/campaigns---waf-mitigated-by-client-country.png)\n\n### Improving security with data\n\nWe are proud to be trusted by local governments, campaigns, state parties, and voting rights organizations to protect their websites and provide uninterrupted access to information and trusted election results. Sharing information about the threats facing these websites helps us further support their valuable work by enabling them, and other participants in the election space, to take proactive steps to improve site security.\n\nLearn more about [how to apply to the Athenian Project](https://www.cloudflare.com/athenian/), and check out [Cloudflare Radar](https://radar.cloudflare.com/) for real-time insights into Internet traffic, attack trends, and more.\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Cloudflare Radar](https://blog.cloudflare.com/tag/cloudflare-radar/) [Election Security](https://blog.cloudflare.com/tag/election-security/) [Trends](https://blog.cloudflare.com/tag/trends/) [Security](https://blog.cloudflare.com/tag/security/)"
    },
    {
      "url": "https://blog.cloudflare.com/why-the-100th-anniversary-of-womens-right-to-vote-in-the-u-s-is-important-to-celebrate-on-international-womens-day/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/why-the-100th-anniversary-of-womens-right-to-vote-in-the-u-s-is-important-to-celebrate-on-international-womens-day/",
        "loadedTime": "2023-12-05T02:36:36.348Z",
        "referrerUrl": "https://blog.cloudflare.com/2024-the-year-of-elections/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/why-the-100th-anniversary-of-womens-right-to-vote-in-the-u-s-is-important-to-celebrate-on-international-womens-day/",
        "title": "Why the 100th Anniversary of Women’s Right to Vote in the U.S. is Important to Celebrate on International Women’s Day",
        "description": "Yellow roses were worn by supporters of Women's Suffrage Movement in the early 20th century in the United States. This week, we celebrate the achievements of women.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "03/07/2020\n4 min read\nSeven months ago, I joined Cloudflare to work on the Public Policy Team focusing on our democracy projects such as Project Galileo, Athenian Project and Cloudflare for Campaigns. Since I joined the team, I have learned a lot about how important cybersecurity protections are for organizations that are the target of sophisticated cyberattacks, while also learning about the complex election security environment in the United States and abroad. \nIt seems fitting that on International Women’s Day, a day people throughout the world are celebrating the achievements of women, we also celebrate the Centennial Anniversary of the Women’s Suffrage Movement which was the tipping point that gave many women voting rights in the United States.\nSince I have been working on Cloudflare’s election security projects, this day means something extra special to me and many of my colleagues who believe that voting is the cornerstone of democracy and that having access to information regarding voting and elections is essential. \nHere are five reflections that I want to share on International Women’s Day and the Centennial Anniversary of the Nineteenth Amendment which granted women the right to vote in the United States:\n1. The Women’s Suffrage Movement in the United States was a decades-long battle\nThe Women’s Suffrage movement burst into view in the United States in 1848 at the Seneca Falls Convention, where participants introduced the notion that women deserved their own political identity and that a righteous government cannot exist without equal rights for all. These organizers passed the torch to the Congressional Union for Woman Suffrage, founded in 1913, which raised awareness through distributing pamphlets at street meetings, organizing parades, speaking tours, and petitioning Congress to pass legislation on the movement. In 1919, the Senate passed the Nineteenth Amendment and it was officially ratified on August 26, 1920.\n2. Due to racial inequality, many women of color in the United States were not granted the right to vote until 1965 \nWith the ratification of the Nineteenth Amendment in 1920, it technically granted women the right to vote. However, due to widespread inequality within the ranks of the women’s suffrage movement who primarily focused on white middle-class interests, many African Americans, Asian Americans, Hispanics, and American Indian women did not receive the right to vote until later in the century. African American women were not guaranteed the right to vote until the Voting Right Acts of 1965. During the height of the civil rights movement, The Act was signed into law by President Lyndon Johnson to prohibit racial discrimination in voting.\n3. There has been a historical, global increase of women in political power\nMuch has changed since the ratification of the Nineteenth Amendment. The Center for American Women and Politics in the United States reports that in every presidential election since 1964, the number of female voters has exceeded the number of male voters. \nThere has also been a historical increase of women in elected offices around the world. This is evident with the highest number of women ever elected to the U.S. Congress in 2018, Slovakia electing the first female president, the United Kingdom electing 220 female MPs to the House of Commons, women making up 49% of Senate of the Republic of Mexico and female Prime Ministers in Denmark, Norway, and Finland. Foundationally, the right to vote is a nonpartisan issue that benefits the interest of the country, strengthens our democracy, and with more women in office, it promotes diversity of thought and experience. \n4. The spread of voting and election information has changed\nThe way we share information has evolved dramatically from distributing pamphlets in 1913 to millions of people sharing information on the Internet across the world in 2020. State and local governments now use their election websites as the primary source to provide up to date announcements and information on how to register to vote, find designated polling stations, and access election results. Political campaigns use their digital infrastructure to release information about their policies, accept donations, recruit volunteers and give updates on the campaign to increase supporters’ engagement.\n5. Access to election information is essential to voter turnout and democracy.\nVoting is a crucial tenet of our democratic system and regardless of circumstance, individuals should have access to the information necessary to exercise their rights without outside interference. At Cloudflare, our mission is to build a better Internet and part of that is ensuring that users have access to accurate, trusted information, in a safe environment. With many upcoming elections in 2020, it is important that we have confidence in the democratic processes and that starts with ensuring their website infrastructure and internal teams are secure against malicious efforts to take them offline and shake voter’s faith in democracy. \nCloudflare has made election security a priority, investing our time in the Athenian Project and Cloudflare for Campaigns as political campaigns and state and local government election websites are the first line of defense in election security. In 2016, it was reported by the Department of Homeland Security that state and local government election infrastructure in all 50 states were targeted during the Presidential election. Fast forward to 2020, we are protecting more than 170 state and local government election websites and providing services to 18 of the 32 U.S. Presidential campaigns.\nTherefore, it seems fitting that we celebrate the Centennial anniversary of the Nineteenth Amendment and International Women’s Day, highlighting the achievement of women throughout history and the importance of voter confidence in the democratic institutions that many fought to participate and have their voices heard. \nWorking at Cloudflare has allowed me to learn how important access to information is to Internet users, and voters across the world, and I am proud to work for a company that supports strengthening democracy. \nIf you are interested in learning more about our election project, please visit cloudflare.com/athenian/ & cloudflare.com/campaigns/usa/.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nWomenflare Employee Resource Groups Election Security Diversity Legal \nRelated Posts\nJanuary 27, 2022 5:21PM\nProtecting Holocaust educational websites\nCloudflare’s Project Galileo provides free protection to at-risk groups across the world including Holocaust educational and remembrance websites...\nBy \nJuly 30, 2021 2:00PM\nBuilding a sustainable workforce, through communities\nAt Cloudflare, we place a lot of value on the importance of diversity, equity and inclusion. Diversity, equity, and inclusion lead to better outcomes through improved decision-making, more innovative teams, stronger financial returns and simply a better place to work for everyone....\nBy \nMarch 08, 2023 2:05PM\nEmbrace equity on International Women’s Day (and every day)\nHappy International Women’s Day! The global theme for 2023 is #EmbraceEquity, which is part of an ongoing effort to raise awareness around “Why equal opportunities are no longer enough.” Today is a time to highlight achievements made by women, but also an opportunity to become better informed....\nBy \nMarch 08, 2022 1:55PM\nInternational Women’s Day 2022\nWelcome to International Women’s Day 2022! Here at Cloudflare, we are happy to celebrate it with you! Our celebration is not only this blog post, but many events prepared for the month of March: our way of honoring Women’s History Month by showcasing women’s empowerment...\nBy",
      "markdown": "03/07/2020\n\n*   [![Jocelyn Woolbright](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/https://blog-cloudflare-com-assets.storage.googleapis.com/2019/12/0-1.jpg)](https://blog.cloudflare.com/author/jocelyn/)\n\n4 min read\n\n![](https://blog.cloudflare.com/content/images/2020/03/Screen-Shot-2020-02-27-at-3.51.08-PM-1.png)\n\nSeven months ago, I joined Cloudflare to work on the Public Policy Team focusing on our democracy projects such as [Project Galileo,](https://www.cloudflare.com/galileo/) [Athenian Project](https://www.cloudflare.com/athenian/) and [Cloudflare for Campaigns](https://www.cloudflare.com/campaigns/). Since I joined the team, I have learned a lot about how important [cybersecurity protections](https://www.cloudflare.com/learning/security/what-is-cyber-security/) are for organizations that are the target of sophisticated cyberattacks, while also learning about the complex election security environment in the United States and abroad.\n\nIt seems fitting that on International Women’s Day, a day people throughout the world are celebrating the achievements of women, we also celebrate the Centennial Anniversary of the Women’s Suffrage Movement which was the tipping point that gave many women voting rights in the United States.\n\nSince I have been working on Cloudflare’s election security projects, this day means something extra special to me and many of my colleagues who believe that voting is the cornerstone of democracy and that having access to information regarding voting and elections is essential.\n\n![](https://blog.cloudflare.com/content/images/2020/03/19th-amendment-option-2@2x-1.png)\n\nHere are five reflections that I want to share on International Women’s Day and the Centennial Anniversary of the Nineteenth Amendment which granted women the right to vote in the United States:\n\n### 1\\. The Women’s Suffrage Movement in the United States was a decades-long battle\n\nThe Women’s Suffrage movement burst into view in the United States in 1848 at the Seneca Falls Convention, where participants introduced the notion that women deserved their own political identity and that a righteous government cannot exist without equal rights for all. These organizers passed the torch to the Congressional Union for Woman Suffrage, founded in 1913, which raised awareness through distributing pamphlets at street meetings, organizing parades, speaking tours, and petitioning Congress to pass legislation on the movement. In 1919, the Senate passed the Nineteenth Amendment and it was officially ratified on August 26, 1920.\n\n### 2\\. Due to racial inequality, many women of color in the United States were not granted the right to vote until 1965\n\nWith the ratification of the Nineteenth Amendment in 1920, it technically granted women the right to vote. However, due to widespread inequality within the ranks of the women’s suffrage movement who [primarily focused on white middle-class interests](https://www.npr.org/2011/07/13/137681070/for-stanton-all-women-were-not-created-equal), many African Americans, Asian Americans, Hispanics, and American Indian women did not receive the right to vote until later in the century. African American women were not guaranteed the right to vote until the Voting Right Acts of 1965. During the height of the civil rights movement, The Act was signed into law by President Lyndon Johnson to prohibit racial discrimination in voting.\n\n![National League of Women Voters hold up signs reading, 'VOTE', Sept. 17, 1924.](https://blog.cloudflare.com/content/images/2020/03/women-suffrage.jpg)\n\n### 3\\. There has been a historical, global increase of women in political power\n\nMuch has changed since the ratification of the Nineteenth Amendment. The Center for American Women and Politics in the United States reports that in every presidential election since 1964, [the number of female voters has exceeded the number of male voters](http://cawp.rutgers.edu/sites/default/files/resources/genderdiff.pdf).\n\nThere has also been a historical increase of women in elected offices around the world. This is evident with the highest number of women ever elected to the U.S. Congress in 2018, Slovakia electing the [first female president](https://www.theguardian.com/world/2019/mar/31/slovakia-elects-zuzana-caputova-first-female-president), the United Kingdom electing 220 female MPs to the [House of Commons](https://www.theguardian.com/politics/2019/dec/13/uk-elects-record-number-of-female-mps), women making up 49% of [Senate of the Republic of Mexic](https://www.unwomen.org/en/news/stories/2018/7/statement-ed-phumzile-womens-political-participation-mexico)o and female Prime Ministers in Denmark, Norway, and Finland. Foundationally, the right to vote is a nonpartisan issue that benefits the interest of the country, strengthens our democracy, and with more women in office, it promotes diversity of thought and experience.\n\n### 4\\. The spread of voting and election information has changed\n\nThe way we share information has evolved dramatically from distributing pamphlets in 1913 to millions of people sharing information on the Internet across the world in 2020. State and local governments now use their election websites as the primary source to provide up to date announcements and information on how to register to vote, find designated polling stations, and access election results. Political campaigns use their digital infrastructure to release information about their policies, accept donations, recruit volunteers and give updates on the campaign to increase supporters’ engagement.\n\n### 5\\. Access to election information is essential to voter turnout and democracy.\n\nVoting is a crucial tenet of our democratic system and regardless of circumstance, individuals should have access to the information necessary to exercise their rights without outside interference. At Cloudflare, our mission is to build a better Internet and part of that is ensuring that users have access to accurate, trusted information, in a safe environment. With many upcoming elections in 2020, it is important that we have confidence in the democratic processes and that starts with ensuring their website infrastructure and internal teams are secure against malicious efforts to take them offline and shake voter’s faith in democracy.\n\nCloudflare has made election security a priority, investing our time in the Athenian Project and Cloudflare for Campaigns as political campaigns and state and local government election websites are the first line of defense in election security. In 2016, it was [reported by the Department of Homeland Security](https://www.intelligence.senate.gov/sites/default/files/documents/Report_Volume1.pdf) that state and local government election infrastructure in all 50 states were targeted during the Presidential election. Fast forward to 2020, we are protecting more than 170 state and local government election websites and providing services to 18 of the 32 U.S. Presidential campaigns.\n\nTherefore, it seems fitting that we celebrate the Centennial anniversary of the Nineteenth Amendment and International Women’s Day, highlighting the achievement of women throughout history and the importance of voter confidence in the democratic institutions that many fought to participate and have their voices heard.\n\nWorking at Cloudflare has allowed me to learn how important access to information is to Internet users, and voters across the world, and I am proud to work for a company that supports strengthening democracy.\n\nIf you are interested in learning more about our election project, please visit [cloudflare.com/athenian/](https://www.cloudflare.com/athenian/) & [cloudflare.com/campaigns/usa/](https://www.cloudflare.com/campaigns/usa/).\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[Womenflare](https://blog.cloudflare.com/tag/womenflare/) [Employee Resource Groups](https://blog.cloudflare.com/tag/employee-resource-groups/) [Election Security](https://blog.cloudflare.com/tag/election-security/) [Diversity](https://blog.cloudflare.com/tag/diversity/) [Legal](https://blog.cloudflare.com/tag/legal/)\n\nRelated Posts\n\nJanuary 27, 2022 5:21PM\n\n[\n\n## Protecting Holocaust educational websites\n\n](https://blog.cloudflare.com/protecting-holocaust-educational-websites/)\n\nCloudflare’s Project Galileo provides free protection to at-risk groups across the world including Holocaust educational and remembrance websites...\n\nBy \n\nJuly 30, 2021 2:00PM\n\n[\n\n## Building a sustainable workforce, through communities\n\n](https://blog.cloudflare.com/building-a-sustainable-workforce-through-communities/)\n\nAt Cloudflare, we place a lot of value on the importance of diversity, equity and inclusion. Diversity, equity, and inclusion lead to better outcomes through improved decision-making, more innovative teams, stronger financial returns and simply a better place to work for everyone....\n\nBy \n\nMarch 08, 2023 2:05PM\n\n[\n\n## Embrace equity on International Women’s Day (and every day)\n\n](https://blog.cloudflare.com/international-womens-day-2023/)\n\nHappy International Women’s Day! The global theme for 2023 is #EmbraceEquity, which is part of an ongoing effort to raise awareness around “Why equal opportunities are no longer enough.” Today is a time to highlight achievements made by women, but also an opportunity to become better informed....\n\nBy \n\nMarch 08, 2022 1:55PM\n\n[\n\n## International Women’s Day 2022\n\n](https://blog.cloudflare.com/international-womens-day-2022/)\n\nWelcome to International Women’s Day 2022! Here at Cloudflare, we are happy to celebrate it with you! Our celebration is not only this blog post, but many events prepared for the month of March: our way of honoring Women’s History Month by showcasing women’s empowerment...\n\nBy"
    },
    {
      "url": "https://blog.cloudflare.com/french-elections-2022-runoff/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/french-elections-2022-runoff/",
        "loadedTime": "2023-12-05T02:36:46.629Z",
        "referrerUrl": "https://blog.cloudflare.com/2024-the-year-of-elections/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/french-elections-2022-runoff/",
        "title": "Two voting days, a debate and a polling rule in France impacts the Internet",
        "description": "We blogged previously about some trends concerning the first round of the 2022 French presidential election, held on April 10. Here we take a look at the run-off election this Sunday, April 24, that ended up re-electing Emmanuel Macron as President of France",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "04/26/2022\n6 min read\nThis post is also available in Français.\nWe blogged previously about some trends concerning the first round of the 2022 French presidential election, held on April 10. Here we take a look at the run-off election this Sunday, April 24, that ended up re-electing Emmanuel Macron as President of France. \nFirst, the two main trends: French-language news sites outside France were clearly impacted by the local rule that states that exit polls can only be published after 20:00.\nAnd Internet traffic was similar on both the election days (April 10 and 24) and that includes the increase in use of mobile devices and interest in news websites — there we also saw a clear interest in the Macron-Le Pen debate on April 20.\nWe have discussed before that election days usually don’t have a major impact on overall Internet traffic. Let’s compare April 10 with 24, the two Sundays when the elections were held. The trends throughout the day are incredibly similar (with a slight increase in traffic on April 24), even with a two-week gap between them. \nAnother election-day trend is the use of mobile devices to access the Internet, mainly at night. The largest spikes in number of requests made using mobile devices in France during April seemed to be all election-related:\n#1. April 10 (first round of the election), 21:00 local time. 58% of traffic by mobile devices.\n#2. April 24 (second round of the election), 22:00. 57% mobile traffic.\n#3. April 20 (presidential debate), 22:00. 56% mobile traffic.\nNot only did both the election Sundays (after the polling stations were closed) have an impact on mobile traffic in France, but the presidential debate (Wednesday, April 20) had the same type of impact, increasing requests from mobile devices.\nThe TV debate was seen by 15.6 million viewers in France and lasted between 21:00 and 22:45, local time; at the same time mobile traffic was higher than in any other Wednesday and was the #3 spike of April, with 10% more mobile requests than in the previous Wednesday at the same time. \nThe special case of French-language news sites\nFor the elections, local rules state that French media is barred from publishing partial results or polls of any kind until 20:00, the time when voting stations in metropolitan France officially close. So, that means that French news outlets have to wait for the allotted hour to give official projections. \nGiven that, we looked at French-language news websites from French-speaking countries like Switzerland and Belgium. They aren’t bound by French law and can show information about exit polls earlier (bear in mind that in most French cities polling stations close at 19:00 and only in the bigger cities does it go on until 20:00). \nFor example, the Swiss Le Temps published exit polls at 19:30.\nWe can clearly see that requests to French-language news sites outside France clearly spiked earlier than those in France. News websites in France had spikes after 20:00 local time on both elections days, but Belgian and Swiss news sites had major increases in traffic at 19:00 on April 10 (1857% more than the previous Sunday!). For the runoff elections on April 24, the biggest spike of the month was at 18:00 (3100% more requests than the previous Sunday), but it was also higher than on previous days one hour later, at 19:00 (3080% higher). \nThere are no spikes at all related to the French debate (April 20), so that seems to show that those Belgian and Swiss news sites had a huge increase of French citizens eager to see the polls before 20:00.\nElection results change online patterns\nWe saw two weeks ago that official election websites had a clear spike in requests on April 10, the first round of the elections. Here we’re looking at DNS request trends to get a sense of traffic to Internet properties. \nOfficial French election-related websites had an increase in traffic throughout the week prior to the first round, after Monday, April 4, but it’s no surprise that the two major spikes were on both the elections' day. How much? Here is the breakdown by bigger spikes in traffic:\n#1. April 10 (first round of the election), 00:00 local time. 925% more requests than the previous Sunday (at the same time).\n#2. April 24 (second round of the election), 20:00. 707% more requests.\n#3. April 10 (first round of the election), 20:00. 370% more requests.\n#3. April 11, 10:00. 115% more requests than the previous Monday.\n(there’s a draw at these last two spikes)\nNews sites go up after polling stations close\nRegarding the main French news websites, as we saw two weeks ago, 20:00 local time, after the polling stations are all closed, and the first major polls are revealed continues to be the time of the biggest spikes of the whole month. \nThe biggest spike of the month in our aggregate DNS chart, that shows trends from 12 news websites, was definitely on April 10, the first round election day, around 20:00 local time, when those domains had 116% more traffic than at the same time on the previous Sunday. And the second-biggest spike was the runoff election day, on April 24, at the same time (20:00 local time), with an increase of 142% in traffic compared to the previous Sunday at the same time.\nVery close to those two spikes is Monday morning, April 11, after the first round of the elections. At 10:00 local time requests were 45% higher than in the previous Monday. The Macron-Le Pen debate on Wednesday, April 20, also had a spike. At 21:00, when it was starting, requests were 56% higher than on the previous Wednesday. \nThe same trend is seen on the major French TV station websites, with a clear isolated spike on April 10 (the first round election day) at 20:00 local time, with a 472% increase in traffic compared to the previous Sunday, when the main exit polls were announced. Something similar, at the same time (20:00), on April 24, with a 375% increase in requests compared to the previous Sunday.\nThat’s only matched, again, by the April 20 debate. At 21:00 traffic was 308% higher than the previous Wednesday, so people were clearly taking notice of the debate and checking news outlets and TV station websites — there were French sites like france.tv that transmitted via streaming.\nConclusion\nWhen people are really eager to see something as important as election results, they go and search where the first polls are (in this case, before 20:00 local time, they are outside France). \nAlso, in two different election moments in France separated by two weeks, there are clear similarities in Internet trends that show the way people use the Internet during election periods. That’s more clear when results start to arrive, but also a debate as important for a presidential election as the Le Pen-Macron one, also impacts not only the Internet traffic but also the attention to news and TV websites. \nYou can keep an eye on these trends using Cloudflare Radar.\nWe protect entire corporate networks, help customers build Internet-scale applications efficiently, accelerate any website or Internet application, ward off DDoS attacks, keep hackers at bay, and can help you on your journey to Zero Trust.\nVisit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.\nTo learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.\nFrance Election Security Cloudflare Radar Trends \nRelated Posts\nJanuary 30, 2014 2:00PM\nStories from our recent global data center upgrade\nEach day at CloudFlare is full of surprises. As it turns out, it takes a lot of work to stop massive attacks and to help make the web faster. Over the past six months, our entire team has contributed in every way imaginable to more than double the capacity of our global network....\nBy \nJune 15, 2022 5:08PM\nCloudflare Zaraz launches new privacy features in response to French CNIL standards\nLast week, the French National Data Protection Authority, CNIL, published guidelines for a GDPR-compliant way of loading Google Analytics. Today, Zaraz is launching a new set of features to help our customers use Google Analytics and similar tools, while meeting those strict standards...\nBy \nNovember 07, 2019 8:00AM\nQuoi de neuf en Francophonie?\nCloudflare en France, Belgique et Suisse, ce sont plus d’une centaine de clients Enterprise, plusieurs milliers d’organisations sur les plans en self-service et une équipe de plus de quinze personnes pour accompagner nos clients francophones dans la gestion technique et commerciale de leur compte...\nBy \nNovember 30, 2016 1:00AM\nNot one, not two, but three undersea cables cut in Jersey\nSometime before midnight Monday (UK local time) a ship dropped its anchor and broke, not one, not two, but three undersea cables serving the island of Jersey in the English Channel....\nBy",
      "markdown": "04/26/2022\n\n*   [![João Tomé](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/V0x3WKfJ_400x400-1.jpeg)](https://blog.cloudflare.com/author/joao-tome/)\n\n6 min read\n\n_This post is also available in [Français](https://blog.cloudflare.com/fr-fr/french-elections-2022-runoff-fr-fr/)._\n\n![](https://blog.cloudflare.com/content/images/2022/04/image2-17.png)\n\nWe blogged previously about some trends concerning the first round of the 2022 French [presidential election](https://en.wikipedia.org/wiki/2022_French_presidential_election), held on April 10. Here we take a look at the run-off election this Sunday, April 24, that ended up re-electing Emmanuel Macron as President of France.\n\nFirst, the two main trends: French-language news sites outside France were clearly impacted by the local rule that states that exit polls can only be published after 20:00.\n\nAnd Internet traffic was similar on both the election days (April 10 and 24) and that includes the increase in use of mobile devices and interest in news websites — there we also saw a clear interest in the Macron-Le Pen debate on April 20.\n\nWe have discussed before that [election days](https://blog.cloudflare.com/elections-france-2022/) usually don’t have a major impact on overall Internet traffic. Let’s compare April 10 with 24, the two Sundays when the elections were held. The trends throughout the day are incredibly similar (with a slight increase in traffic on April 24), even with a two-week gap between them.\n\n![](https://blog.cloudflare.com/content/images/2022/04/image7-8.png)\n\nAnother election-day trend is the use of mobile devices to access the Internet, mainly at night. The largest spikes in number of requests made using mobile devices in France during April seemed to be all election-related:\n\n![](https://blog.cloudflare.com/content/images/2022/04/image1-23.png)\n\n#1. April 10 (first round of the election), 21:00 local time. 58% of traffic by mobile devices.\n\n#2. April 24 (second round of the  election), 22:00. 57% mobile traffic.\n\n#3. April 20 (presidential debate), 22:00. 56% mobile traffic.\n\nNot only did both the election Sundays (after the polling stations were closed) have an impact on mobile traffic in France, but the presidential debate (Wednesday, April 20) had the same type of impact, increasing requests from mobile devices.\n\nThe [TV debate](https://www.bbc.com/news/world-europe-61166601) was seen by 15.6 million viewers in France and lasted between 21:00 and 22:45, local time; at the same time mobile traffic was higher than in any other Wednesday and was the #3 spike of April, with 10% more mobile requests than in the previous Wednesday at the same time.\n\n#### The special case of French-language news sites\n\nFor the elections, [local rules state](https://www.cnccep.fr/pdf-cp8.html) that French media is barred from publishing partial results or polls of any kind until 20:00, the time when voting stations in metropolitan France officially close. So, that means that French news outlets have to wait for the allotted hour to give official [projections](https://www.france24.com/en/france/20220424-not-just-exit-polls-why-french-election-projections-are-almost-always-correct).\n\nGiven that, we looked at French-language news websites from French-speaking countries like Switzerland and Belgium. They aren’t bound by French law and can show information about exit polls earlier (bear in mind that in most French cities polling stations close at 19:00 and only in the bigger cities does it go on until 20:00).\n\nFor example, the Swiss Le Temps [published exit polls](https://www.letemps.ch/monde/resultats-presidentielle-francaise-emmanuel-macron-reelu-plus-58-voix) at 19:30.\n\n![](https://blog.cloudflare.com/content/images/2022/04/french.png)\n\nWe can clearly see that requests to French-language news sites outside France clearly spiked earlier than those in France. News websites in France had spikes after 20:00 local time on both elections days, but Belgian and Swiss news sites had major increases in traffic at 19:00 on April 10 (1857% more than the previous Sunday!). For the runoff elections on April 24, the biggest spike of the month was at 18:00 (3100% more requests than the previous Sunday), but it was also higher than on previous days one hour later, at 19:00 (3080% higher).\n\nThere are no spikes at all related to the French debate (April 20), so that seems to show that those Belgian and Swiss news sites had a huge increase of French citizens eager to see the polls before 20:00.\n\n### **Election results change online patterns**\n\n[We saw two weeks ago](https://blog.cloudflare.com/elections-france-2022/) that official election websites had a clear spike in requests on April 10, the first round of the elections. Here we’re looking at DNS request trends to get a sense of traffic to Internet properties.\n\nOfficial French election-related websites had an increase in traffic throughout the week prior to the first round, after Monday, April 4, but it’s no surprise that the two major spikes were on both the elections' day. How much? Here is the breakdown by bigger spikes in traffic:\n\n![](https://blog.cloudflare.com/content/images/2022/04/image6-9.png)\n\n#1. April 10 (first round of the election), 00:00 local time. 925% more requests than the previous Sunday (at the same time).\n\n#2. April 24  (second round of the election), 20:00. 707% more requests.\n\n#3. April 10 (first round of the election), 20:00. 370% more requests.\n\n#3. April 11, 10:00. 115% more requests than the previous Monday.\n\n_(there’s a draw at these last two spikes)_\n\n#### News sites go up after polling stations close\n\nRegarding the main French news websites, as we saw [two weeks ago](https://blog.cloudflare.com/elections-france-2022/), 20:00 local time, after the polling stations are all closed, and the first major polls are revealed continues to be the time of the biggest spikes of the whole month.\n\nThe biggest spike of the month in our aggregate DNS chart, that shows trends from 12 news websites, was definitely on April 10, the first round election day, around 20:00 local time, when those domains had 116% more traffic than at the same time on the previous Sunday. And the second-biggest spike was the runoff election day, on April 24, at the same time (20:00 local time), with an increase of 142% in traffic compared to the previous Sunday at the same time.  \n\n![](https://blog.cloudflare.com/content/images/2022/04/image4-12.png)\n\nVery close to those two spikes is Monday morning, April 11, after the first round of the elections. At 10:00 local time requests were 45% higher than in the previous Monday. The Macron-Le Pen debate on Wednesday, April 20, also had a spike. At 21:00, when it was starting, requests were 56% higher than on the previous Wednesday.\n\nThe same trend is seen on the major French TV station websites, with a clear isolated spike on April 10 (the first round election day) at 20:00 local time, with a 472% increase in traffic compared to the previous Sunday, when the main exit polls were announced. Something similar, at the same time (20:00), on April 24, with a 375% increase in requests compared to the previous Sunday.\n\n![](https://blog.cloudflare.com/content/images/2022/04/image5-16.png)\n\nThat’s only matched, again, by the April 20 debate. At 21:00 traffic was 308% higher than the previous Wednesday, so people were clearly taking notice of the debate and checking news outlets and TV station websites — there were French sites like france.tv that transmitted via [streaming](https://www.france.tv/france-5/c-dans-l-air/3264448-emission-du-mercredi-20-avril-2022.html).\n\n### **Conclusion**\n\nWhen people are really eager to see something as important as election results, they go and search where the first polls are (in this case, before 20:00 local time, they are outside France).\n\nAlso, in two different election moments in France separated by two weeks, there are clear similarities in Internet trends that show the way people use the Internet during election periods. That’s more clear when results start to arrive, but also a debate as important for a presidential election as the Le Pen-Macron one, also impacts not only the Internet traffic but also the attention to news and TV websites.\n\nYou can keep an eye on these trends using [Cloudflare Radar](https://radar.cloudflare.com/).\n\nWe protect [entire corporate networks](https://www.cloudflare.com/network-services/), help customers build [Internet-scale applications efficiently](https://workers.cloudflare.com/), accelerate any [website or Internet application](https://www.cloudflare.com/performance/accelerate-internet-applications/), [ward off DDoS attacks](https://www.cloudflare.com/ddos/), keep [hackers at bay](https://www.cloudflare.com/application-security/), and can help you on [your journey to Zero Trust](https://www.cloudflare.com/products/zero-trust/).\n\nVisit [1.1.1.1](https://1.1.1.1/) from any device to get started with our free app that makes your Internet faster and safer.\n\nTo learn more about our mission to help build a better Internet, [start here](https://www.cloudflare.com/learning/what-is-cloudflare/). If you're looking for a new career direction, check out [our open positions](https://cloudflare.com/careers).\n\n[France](https://blog.cloudflare.com/tag/france/) [Election Security](https://blog.cloudflare.com/tag/election-security/) [Cloudflare Radar](https://blog.cloudflare.com/tag/cloudflare-radar/) [Trends](https://blog.cloudflare.com/tag/trends/)\n\nRelated Posts\n\nJanuary 30, 2014 2:00PM\n\n[\n\n## Stories from our recent global data center upgrade\n\n](https://blog.cloudflare.com/stories-from-our-recent-global-data-center-upgrade/)\n\nEach day at CloudFlare is full of surprises. As it turns out, it takes a lot of work to stop massive attacks and to help make the web faster. Over the past six months, our entire team has contributed in every way imaginable to more than double the capacity of our global network....\n\nBy \n\nJune 15, 2022 5:08PM\n\n[\n\n## Cloudflare Zaraz launches new privacy features in response to French CNIL standards\n\n](https://blog.cloudflare.com/zaraz-privacy-features-in-response-to-cnil/)\n\nLast week, the French National Data Protection Authority, CNIL, published guidelines for a GDPR-compliant way of loading Google Analytics. Today, Zaraz is launching a new set of features to help our customers use Google Analytics and similar tools, while meeting those strict standards...\n\nBy \n\nNovember 07, 2019 8:00AM\n\n[\n\n## Quoi de neuf en Francophonie?\n\n](https://blog.cloudflare.com/fr-fr/quoi-de-neuf-en-francophonie-fr-fr/)\n\nCloudflare en France, Belgique et Suisse, ce sont plus d’une centaine de clients Enterprise, plusieurs milliers d’organisations sur les plans en self-service et une équipe de plus de quinze personnes pour accompagner nos clients francophones dans la gestion technique et commerciale de leur compte...\n\nBy \n\nNovember 30, 2016 1:00AM\n\n[\n\n## Not one, not two, but three undersea cables cut in Jersey\n\n](https://blog.cloudflare.com/not-one-not-two-but-three-undersea-cables-cut-in-jersey/)\n\nSometime before midnight Monday (UK local time) a ship dropped its anchor and broke, not one, not two, but three undersea cables serving the island of Jersey in the English Channel....\n\nBy"
    },
    {
      "url": "https://blog.cloudflare.com/zh-cn/introducing-advanced-session-audit-capabilities-in-cloudflare-one-zh-cn/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/zh-cn/introducing-advanced-session-audit-capabilities-in-cloudflare-one-zh-cn/",
        "loadedTime": "2023-12-05T02:36:50.625Z",
        "referrerUrl": "https://blog.cloudflare.com/introducing-advanced-session-audit-capabilities-in-cloudflare-one/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/zh-cn/introducing-advanced-session-audit-capabilities-in-cloudflare-one-zh-cn/",
        "title": "在 Cloudflare One 中引入高级会话审计功能",
        "description": "管理员现在可以轻松审核其 Cloudflare One 政策使用的所有活动用户会话和相关数据。这可以实现两全其美的效果：极其精细的控制，同时保持更高的故障排除和诊断能力。",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Zero Trust 的基础是为每个应用程序、用户和设备定义精细的控制和授权政策。拥有足够精细的系统来做到这一点对于同时满足监管和安全要求至关重要。但如此多的控制有一个潜在的缺点：为了解决用户问题，管理员必须考虑应用程序、用户身份和设备信息等变量的复杂组合，这可能需要费力地筛选日志。\n我们认为有一种更好的方法，正因如此，从今天开始，管理员可以轻松审核其 Cloudflare One 政策使用的所有活动用户会话和相关数据。这可以实现两全其美的效果：极其精细的控制，同时在单个简单的控制面板中保持更高的故障排除和诊断 Zero Trust 部署的能力。管理员现在可以使用以前存在于用户浏览器中或动态更改的信息，而无需打扰最终用户或深入挖掘日志。\n应用程序身份验证和授权快速入门\n身份验证和授权是 Zero Trust 政策在允许用户访问资源之前评估的两个组件。\n身份验证是验证用户、设备或系统身份的过程。常见的身份验证方法包括输入用户名和密码、出示数字证书，以及指纹或面部扫描等生物识别技术。多因素身份验证 (MFA) 需要两种或更多独立的身份验证方法来增强安全性，例如硬件密钥与密码相结合。\n授权是在实体成功通过身份验证后授予或拒绝对特定资源或权限的访问的过程。它定义了经过身份验证的实体在系统中可以做什么和不能做什么。\n应用程序身份验证/授权机制\n我们将重点介绍 Web 应用程序，此类应用程序通常使用 HTTP cookie 来处理身份验证和授权。\n身份验证：\n登录：当用户通过输入用户名和密码登录 Web 应用程序时，应用程序会根据其数据库或身份提供商 (IdP) 验证这些凭证。还可以应用其他形式的身份验证来实现多因素身份验证。如果它们匹配，服务器或外部安全服务（例如 Cloudflare Access）就会认为用户已通过身份验证。\nCookie/令牌创建：然后，服务器以 Cookie 或 JSON Web 令牌的形式为用户创建会话。Cookie 在一段时间内有效，之后用户必须重新进行身份验证。\n发送和存储 Cookie：服务器向用户的浏览器发送响应，其中包括会话 ID 和 Cookie 中有关用户的其他识别信息。然后，浏览器会存储这个 Cookie。此 Cookie 用于在用户的后续请求中识别该用户。\n授权：\n后续请求：对于对 Web 应用程序的所有后续请求，用户的浏览器会自动在请求中包含 cookie（带有会话 ID 和其他识别信息）。\n服务器端验证：服务器从 Cookie 中获取用户数据并检查会话是否有效。如果有效，服务器还会检索用户的详细信息及其与该会话 ID 相关的访问权限。\n授权决定：根据用户的访问权限，服务器决定用户是否有权执行请求的操作或访问请求的资源。\n这样，用户在登录后，其所有后续请求都保持经过身份验证的状态（并且可以检查其授权），直到会话到期或他们退出帐户。\n在现代 Web 应用程序中，此会话状态通常以 JSON Web 令牌 (JWT) 的形式存储。\n调试基于 JWT 的身份验证\n许多现代 Web 应用程序和 Cloudflare Access 等 Zero Trust 网络访问 (ZTNA) 解决方案都使用 JWT 来进行身份验证和授权。JWT 包含一个有效负载，该有效负载对有关用户的信息和可能的其他数据进行编码，并且由服务器对其进行签名以防止篡改。JWT 通常以无状态方式使用，这意味着服务器不会保留每个 JWT 的副本，它只是在其随着请求传入时对其进行验证和解码。JWT 的无状态性质意味着您不必依赖中央系统来处理用户会话管理，这可以避免随着访问系统的用户数量增加而产生可扩展性问题。\n但是，由于 JWT 的这种无状态性质，如果不从用户处获得特定的 JWT，则很难调试基于 JWT 的身份验证。原因如下：\n1. 令牌特定性：每个 JWT 都特定于一个用户和一个会话。它包含有关用户、颁发机构、令牌的颁发时间、到期时间以及可能的其他数据的信息（声明）。因此，要调试问题，您通常需要获得导致问题的确切 JWT。\n2. 无服务器端记录：由于 JWT 是无状态的，因此服务器默认不存储会话。它无法查找过去的令牌或其关联状态，除非它是专门设计用来记录它们的，但出于隐私和数据最小化考虑，通常情况并非如此。\n3. 暂时性问题：JWT 的问题可能是暂时性的，它们可能与使用令牌的具体时刻有关。例如，如果用户尝试使用令牌时，该令牌已过期，则需要该特定令牌来调试问题。\n4. 隐私和安全：JWT 可能包含敏感信息，因此应谨慎处理。从用户那里获取 JWT 可能会将他们的个人信息或安全凭证暴露给调试问题的人员。此外，如果用户通过不安全的渠道将 JWT 发送给开发人员或 IT 帮助台，则可能会被拦截（Cloudflare 最近发布了免费的 HAR Sanitizer 来帮助缓解这一问题）。\n这些因素使得在没有相关特定令牌的情况下，很难对基于 JWT 的身份验证问题进行故障排除。\n调试身份问题的更好方法\n我们着手构建一种更好的方法来调试与 Cloudflare Zero Trust 中用户身份相关的问题，而无需来回分享 JWT 或 HAR 文件。管理员现在可以查看用户的注册表身份（用于 Gateway 政策）和所有活动 Access 会话。\n此会话信息包括 Zero Trust 评估的完整身份，包括 IdP 声明、设备态势信息、网络背景信息等。通过利用 Cloudflare Workers KV，我们能够在不对 Access 的身份验证逻辑进行任何额外负载的情况下构建此功能。当用户使用 Access 进行身份验证时，其关联的身份会立即保存到 Workers KV 中的键/值对中。这一切都发生在用户身份验证事件的上下文中，这意味着延迟影响或对外部服务的依赖极小。\n所有 Zero Trust 计划的所有客户都可以使用此功能。如果您想开始使用 Cloudflare Zero Trust，请立即注册一个最多可容纳 50 位用户的免费帐户！或者，与 Cloudflare 专家合作，讨论适合贵组织的 SSE 或 SASE，一步一步地解决您的 Zero Trust 用例。",
      "markdown": "![](https://blog.cloudflare.com/content/images/2023/11/image5.png)\n\nZero Trust 的基础是为每个应用程序、用户和设备定义精细的控制和授权政策。拥有足够精细的系统来做到这一点对于同时满足监管和安全要求至关重要。但如此多的控制有一个潜在的缺点：为了解决用户问题，管理员必须考虑应用程序、用户身份和设备信息等变量的复杂组合，这可能需要费力地筛选日志。\n\n我们认为有一种更好的方法，正因如此，从今天开始，管理员可以轻松审核其 Cloudflare One 政策使用的所有活动用户会话和相关数据。这可以实现两全其美的效果：极其精细的控制，同时在单个简单的控制面板中保持更高的故障排除和诊断 [Zero Trust](https://www.cloudflare.com/learning/security/glossary/what-is-zero-trust/) 部署的能力。管理员现在可以使用以前存在于用户浏览器中或动态更改的信息，而无需打扰最终用户或深入挖掘日志。\n\n![](https://blog.cloudflare.com/content/images/2023/11/image4.png)\n\n### **应用程序身份验证和授权快速入门**\n\n_身份验证_和_授权_是 Zero Trust 政策在允许用户访问资源之前评估的两个组件。\n\n**身份验证**是验证用户、设备或系统身份的过程。常见的[身份验证](https://www.cloudflare.com/learning/access-management/what-is-authentication/)方法包括输入用户名和密码、出示数字证书，以及指纹或面部扫描等生物识别技术。[多因素身份验证 (MFA)](https://www.cloudflare.com/learning/access-management/what-is-multi-factor-authentication/) 需要两种或更多独立的身份验证方法来增强安全性，例如硬件密钥与密码相结合。\n\n![](https://blog.cloudflare.com/content/images/2023/11/image6.png)\n\n**授权**是在实体成功通过身份验证后授予或拒绝对特定资源或权限的访问的过程。它定义了经过身份验证的实体在系统中可以做什么和不能做什么。\n\n![](https://blog.cloudflare.com/content/images/2023/11/image1-3.png)\n\n### **应用程序身份验证/授权机制**\n\n我们将重点介绍 Web 应用程序，此类应用程序通常使用 HTTP cookie 来处理身份验证和授权。\n\n**身份验证：**\n\n1.  **登录**：当用户通过输入用户名和密码登录 Web 应用程序时，应用程序会根据其数据库或[身份提供商 (IdP)](https://www.cloudflare.com/learning/access-management/what-is-an-identity-provider/) 验证这些凭证。还可以应用其他形式的身份验证来实现多因素身份验证。如果它们匹配，服务器或外部安全服务（例如 Cloudflare Access）就会认为用户已通过身份验证。\n2.  **Cookie/令牌创建**：然后，服务器以 Cookie 或 JSON Web 令牌的形式为用户创建会话。Cookie 在一段时间内有效，之后用户必须重新进行身份验证。\n3.  **发送和存储 Cookie**：服务器向用户的浏览器发送响应，其中包括会话 ID 和 Cookie 中有关用户的其他识别信息。然后，浏览器会存储这个 Cookie。此 Cookie 用于在用户的后续请求中识别该用户。\n\n**授权：**\n\n1.  **后续请求**：对于对 Web 应用程序的所有后续请求，用户的浏览器会自动在请求中包含 cookie（带有会话 ID 和其他识别信息）。\n2.  **服务器端验证**：服务器从 Cookie 中获取用户数据并检查会话是否有效。如果有效，服务器还会检索用户的详细信息及其与该会话 ID 相关的访问权限。\n3.  **授权决定**：根据用户的访问权限，服务器决定用户是否有权执行请求的操作或访问请求的资源。\n\n这样，用户在登录后，其所有后续请求都保持经过身份验证的状态（并且可以检查其授权），直到会话到期或他们退出帐户。\n\n在现代 Web 应用程序中，此会话状态通常以 JSON Web 令牌 (JWT) 的形式存储。\n\n![](https://blog.cloudflare.com/content/images/2023/11/image8.png)\n\n### **调试基于 JWT 的身份验证**\n\n许多现代 Web 应用程序和 Cloudflare Access 等 [Zero Trust 网络访问 (ZTNA)](https://www.cloudflare.com/learning/access-management/what-is-ztna/) 解决方案都使用 JWT 来进行身份验证和授权。JWT 包含一个有效负载，该有效负载对有关用户的信息和可能的其他数据进行编码，并且由服务器对其进行签名以防止篡改。JWT 通常以无状态方式使用，这意味着服务器不会保留每个 JWT 的副本，它只是在其随着请求传入时对其进行验证和解码。JWT 的无状态性质意味着您不必依赖中央系统来处理用户会话管理，这可以避免随着访问系统的用户数量增加而产生可扩展性问题。\n\n![](https://blog.cloudflare.com/content/images/2023/11/image2-2.png)\n\n但是，由于 JWT 的这种无状态性质，如果不从用户处获得特定的 JWT，则很难调试基于 JWT 的身份验证。原因如下：\n\n1\\. **令牌特定性**：每个 JWT 都特定于一个用户和一个会话。它包含有关用户、颁发机构、令牌的颁发时间、到期时间以及可能的其他数据的信息（声明）。因此，要调试问题，您通常需要获得导致问题的确切 JWT。\n\n2\\. **无服务器端记录**：由于 JWT 是无状态的，因此服务器默认不存储会话。它无法查找过去的令牌或其关联状态，除非它是专门设计用来记录它们的，但出于隐私和数据最小化考虑，通常情况并非如此。\n\n3\\. **暂时性问题**：JWT 的问题可能是暂时性的，它们可能与使用令牌的具体时刻有关。例如，如果用户尝试使用令牌时，该令牌已过期，则需要该特定令牌来调试问题。\n\n4\\. **隐私和安全**：JWT 可能包含敏感信息，因此应谨慎处理。从用户那里获取 JWT 可能会将他们的个人信息或安全凭证暴露给调试问题的人员。此外，如果用户通过不安全的渠道将 JWT 发送给开发人员或 IT 帮助台，则可能会被拦截（Cloudflare 最近发布了免费的 [HAR Sanitizer](https://blog.cloudflare.com/introducing-har-sanitizer-secure-har-sharing/) 来帮助缓解这一问题）。\n\n这些因素使得在没有相关特定令牌的情况下，很难对基于 JWT 的身份验证问题进行故障排除。\n\n![](https://blog.cloudflare.com/content/images/2023/11/image3.png)\n\n### **调试身份问题的更好方法**\n\n我们着手构建一种更好的方法来调试与 Cloudflare Zero Trust 中用户身份相关的问题，而无需来回分享 JWT 或 HAR 文件。管理员现在可以查看用户的注册表身份（用于 Gateway 政策）和所有活动 Access 会话。\n\n![](https://blog.cloudflare.com/content/images/2023/11/image7.png)\n\n此会话信息包括 Zero Trust 评估的完整身份，包括 IdP 声明、设备态势信息、网络背景信息等。通过利用 Cloudflare Workers KV，我们能够在不对 Access 的身份验证逻辑进行任何额外负载的情况下构建此功能。当用户使用 Access 进行身份验证时，其关联的身份会立即保存到 Workers KV 中的键/值对中。这一切都发生在用户身份验证事件的上下文中，这意味着延迟影响或对外部服务的依赖极小。\n\n所有 Zero Trust 计划的所有客户都可以使用此功能。如果您想开始使用 Cloudflare Zero Trust，请立即[注册](https://dash.cloudflare.com/sign-up/teams)一个最多可容纳 50 位用户的[免费帐户](https://dash.cloudflare.com/sign-up/teams)！或者，与 [Cloudflare 专家合作](https://www.cloudflare.com/products/zero-trust/plans/enterprise/)，讨论适合贵组织的 SSE 或 SASE，一步一步地解决您的 Zero Trust 用例。"
    },
    {
      "url": "https://blog.cloudflare.com/zh-tw/introducing-advanced-session-audit-capabilities-in-cloudflare-one-zh-tw/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/zh-tw/introducing-advanced-session-audit-capabilities-in-cloudflare-one-zh-tw/",
        "loadedTime": "2023-12-05T02:36:55.246Z",
        "referrerUrl": "https://blog.cloudflare.com/introducing-advanced-session-audit-capabilities-in-cloudflare-one/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/zh-tw/introducing-advanced-session-audit-capabilities-in-cloudflare-one-zh-tw/",
        "title": "在 Cloudflare One 中引入進階工作階段稽核功能",
        "description": "管理員現在可以輕鬆稽核其 Cloudflare One 原則使用的所有作用中使用者工作階段和相關資料。這可以實現兩全其美的效果：極其精細的控制，同時保持更高的疑難排解和診斷能力。",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Zero Trust 的基礎是為每個應用程式、使用者和裝置定義精細的控制和授權原則。擁有足夠精細的系統來做到這一點對於同時滿足監管和安全要求至關重要。但如此多的控制有一個潛在的缺點：為了解決使用者問題，管理員必須考慮應用程式、使用者身分和裝置資訊等變數的複雜組合，這可能需要費力地篩選記錄。\n我們認為有一種更好的方法，正因如此，從今天開始，管理員可以輕鬆稽核其 Cloudflare One 原則使用的所有作用中使用者工作階段和相關資料。這可以實現兩全其美的效果：極其精細的控制，同時在單個簡單的主控台中保持更高的疑難排解和診斷 Zero Trust 部署的能力。管理員現在可以使用以前存在於使用者瀏覽器中或動態變更的資訊，而無需打擾終端使用者或深入挖掘記錄。\n應用程式驗證和授權快速入門\n驗證和授權是 Zero Trust 原則在允許使用者存取資源之前評估的兩個元件。\n驗證是驗證使用者、裝置或系統身分的過程。常見的驗證方法包括輸入使用者名稱和密碼、出示數位憑證，以及指紋或面部掃描等生物特徵辨識技術。多重要素驗證 (MFA) 需要兩種或更多獨立的驗證方法來增強安全性，例如硬體金鑰與密碼相結合。\n授權是在實體成功通過驗證後授予或拒絕對特定資源或權限的存取的過程。它定義了經過驗證的實體在系統中可以做什麼和不能做什麼。\n應用程式驗證/授權機制\n我們將重點介紹 Web 應用程式，此類應用程式通常使用 HTTP Cookie 來處理驗證和授權。\n驗證：\n登入：當使用者透過輸入使用者名稱和密碼登入 Web 應用程式時，應用程式會根據其資料庫或身分識別提供者 (IdP) 驗證這些憑證。還可以套用其他形式的驗證來實現多重要素驗證。如果它們匹配，伺服器或外部安全服務（例如 Cloudflare Access）就會認為使用者已通過驗證。\nCookie/權杖建立：然後，伺服器以 Cookie 或 JSON Web 權杖的形式為使用者建立工作階段。Cookie 在一段時間內有效，之後使用者必須重新進行驗證。\n傳送和儲存 Cookie：伺服器向使用者的瀏覽器傳送回應，其中包括工作階段 ID 和 Cookie 中有關使用者的其他識別資訊。然後，瀏覽器會儲存這個 Cookie。此 Cookie 用於在使用者的後續請求中識別該使用者。\n授權：\n後續請求：對於對 Web 應用程式的所有後續請求，使用者的瀏覽器會自動在請求中包含 Cookie（帶有工作階段 ID 和其他識別資訊）。\n伺服器端驗證：伺服器從 Cookie 中獲取使用者資料並檢查工作階段是否有效。如果有效，伺服器還會擷取使用者的詳細資訊及其與該工作階段 ID 相關的存取權限。\n授權決定：根據使用者的存取權限，伺服器決定使用者是否有權執行請求的操作或存取請求的資源。\n這樣，使用者在登入後，其所有後續請求都保持經過驗證的狀態（並且可以檢查其授權），直到工作階段到期或他們登出帳戶。\n在現代 Web 應用程式中，此工作階段狀態通常以 JSON Web 權杖 (JWT) 的形式儲存。\n偵錯基於 JWT 的驗證\n許多現代 Web 應用程式和 Cloudflare Access 等 Zero Trust 網路存取 (ZTNA) 解決方案都使用 JWT 來進行驗證和授權。JWT 包含一個負載，該負載對有關使用者的資訊和可能的其他資料進行編碼，並且由伺服器對其進行簽名以防止篡改。JWT 通常以無狀態方式使用，這意味著伺服器不會保留每個 JWT 的復本，它只是在其隨著請求傳入時對其進行驗證和解碼。JWT 的無狀態性質意味著您不必依賴中央系統來處理使用者工作階段管理，這可以避免隨著存取系統的使用者數量增加而產生可擴展性問題。\n但是，由於 JWT 的這種無狀態性質，如果不從使用者處獲得特定的 JWT，則很難對基於 JWT 的驗證進行偵錯。原因如下：\n1. 權杖特定性：每個 JWT 都特定於一個使用者和一個工作階段。它包含有關使用者、頒發機構、權杖的頒發時間、到期時間以及可能的其他資料的資訊（聲明）。因此，要偵錯問題，您通常需要獲得導致問題的確切 JWT。\n2. 無伺服器端記錄：由於 JWT 是無狀態的，因此伺服器預設不儲存工作階段。它無法查找過去的權杖或其關聯狀態，除非它是專門設計用來記錄它們的，但出於隱私和資料縮製考慮，通常情況並非如此。\n3. 暫時性問題：JWT 的問題可能是暫時性的，它們可能與使用權杖的具體時刻有關。例如，如果使用者嘗試使用權杖時，該權杖已過期，則需要該特定權杖來偵錯問題。\n4. 隱私和安全：JWT 可能包含敏感資訊，因此應謹慎處理。從使用者那裡獲取 JWT 可能會將他們的個人資訊或安全憑證暴露給偵錯問題的人員。此外，如果使用者透過不安全的通道將 JWT 傳送給開發人員或 IT 服務台，則可能會被攔截（Cloudflare 最近發佈了免費的 HAR Sanitizer 來幫助緩解這一問題）。\n這些因素使得在沒有相關特定權杖的情況下，很難對基於 JWT 的驗證問題進行疑難排解。\n偵錯身分問題的更好方法\n我們著手建置一種更好的方法來偵錯與 Cloudflare Zero Trust 中使用者身分相關的問題，而無需來回分享 JWT 或 HAR 檔案。管理員現在可以檢視使用者的登錄身分（用於 Gateway 原則）和所有作用中 Access 工作階段。\n此工作階段資訊包括 Zero Trust 評估的完整身分，包括 IdP 聲明、裝置狀態資訊、網路背景資訊等。透過利用 Cloudflare Workers KV，我們能夠在不對 Access 的驗證邏輯進行任何額外負載的情況下建置此功能。當使用者使用 Access 進行驗證時，其關聯的身分會立即儲存到 Workers KV 中的鍵/值組中。這一切都發生在使用者驗證事件的上下文中，這意味著延遲影響或對外部服務的依賴極小。\n所有 Zero Trust 方案的所有客戶都可以使用此功能。如果您想開始使用 Cloudflare Zero Trust，請立即註冊一個最多可容納 50 位使用者的免費帳戶！或者，與 Cloudflare 專家合作，討論適合貴組織的 SSE 或 SASE，一步一步地解決您的 Zero Trust 用例。",
      "markdown": "![](https://blog.cloudflare.com/content/images/2023/11/image5.png)\n\nZero Trust 的基礎是為每個應用程式、使用者和裝置定義精細的控制和授權原則。擁有足夠精細的系統來做到這一點對於同時滿足監管和安全要求至關重要。但如此多的控制有一個潛在的缺點：為了解決使用者問題，管理員必須考慮應用程式、使用者身分和裝置資訊等變數的複雜組合，這可能需要費力地篩選記錄。\n\n我們認為有一種更好的方法，正因如此，從今天開始，管理員可以輕鬆稽核其 Cloudflare One 原則使用的所有作用中使用者工作階段和相關資料。這可以實現兩全其美的效果：極其精細的控制，同時在單個簡單的主控台中保持更高的疑難排解和診斷 [Zero Trust](https://www.cloudflare.com/learning/security/glossary/what-is-zero-trust/) 部署的能力。管理員現在可以使用以前存在於使用者瀏覽器中或動態變更的資訊，而無需打擾終端使用者或深入挖掘記錄。\n\n![](https://blog.cloudflare.com/content/images/2023/11/image4.png)\n\n### **應用程式驗證和授權快速入門**\n\n_驗證_和_授權_是 Zero Trust 原則在允許使用者存取資源之前評估的兩個元件。\n\n**驗證**是驗證使用者、裝置或系統身分的過程。常見的[驗證](https://www.cloudflare.com/learning/access-management/what-is-authentication/)方法包括輸入使用者名稱和密碼、出示數位憑證，以及指紋或面部掃描等生物特徵辨識技術。[多重要素驗證 (MFA)](https://www.cloudflare.com/learning/access-management/what-is-multi-factor-authentication/) 需要兩種或更多獨立的驗證方法來增強安全性，例如硬體金鑰與密碼相結合。\n\n![](https://blog.cloudflare.com/content/images/2023/11/image6.png)\n\n**授權**是在實體成功通過驗證後授予或拒絕對特定資源或權限的存取的過程。它定義了經過驗證的實體在系統中可以做什麼和不能做什麼。\n\n![](https://blog.cloudflare.com/content/images/2023/11/image1-3.png)\n\n### **應用程式驗證/授權機制**\n\n我們將重點介紹 Web 應用程式，此類應用程式通常使用 HTTP Cookie 來處理驗證和授權。\n\n**驗證：**\n\n1.  **登入**：當使用者透過輸入使用者名稱和密碼登入 Web 應用程式時，應用程式會根據其資料庫或[身分識別提供者 (IdP)](https://www.cloudflare.com/learning/access-management/what-is-an-identity-provider/) 驗證這些憑證。還可以套用其他形式的驗證來實現多重要素驗證。如果它們匹配，伺服器或外部安全服務（例如 Cloudflare Access）就會認為使用者已通過驗證。\n2.  **Cookie/權杖建立**：然後，伺服器以 Cookie 或 JSON Web 權杖的形式為使用者建立工作階段。Cookie 在一段時間內有效，之後使用者必須重新進行驗證。\n3.  **傳送和儲存 Cookie**：伺服器向使用者的瀏覽器傳送回應，其中包括工作階段 ID 和 Cookie 中有關使用者的其他識別資訊。然後，瀏覽器會儲存這個 Cookie。此 Cookie 用於在使用者的後續請求中識別該使用者。\n\n**授權：**\n\n1.  **後續請求**：對於對 Web 應用程式的所有後續請求，使用者的瀏覽器會自動在請求中包含 Cookie（帶有工作階段 ID 和其他識別資訊）。\n2.  **伺服器端驗證**：伺服器從 Cookie 中獲取使用者資料並檢查工作階段是否有效。如果有效，伺服器還會擷取使用者的詳細資訊及其與該工作階段 ID 相關的存取權限。\n3.  **授權決定**：根據使用者的存取權限，伺服器決定使用者是否有權執行請求的操作或存取請求的資源。\n\n這樣，使用者在登入後，其所有後續請求都保持經過驗證的狀態（並且可以檢查其授權），直到工作階段到期或他們登出帳戶。\n\n在現代 Web 應用程式中，此工作階段狀態通常以 JSON Web 權杖 (JWT) 的形式儲存。\n\n![](https://blog.cloudflare.com/content/images/2023/11/image8.png)\n\n### **偵錯基於 JWT 的驗證**\n\n許多現代 Web 應用程式和 Cloudflare Access 等 [Zero Trust 網路存取 (ZTNA)](https://www.cloudflare.com/learning/access-management/what-is-ztna/) 解決方案都使用 JWT 來進行驗證和授權。JWT 包含一個負載，該負載對有關使用者的資訊和可能的其他資料進行編碼，並且由伺服器對其進行簽名以防止篡改。JWT 通常以無狀態方式使用，這意味著伺服器不會保留每個 JWT 的復本，它只是在其隨著請求傳入時對其進行驗證和解碼。JWT 的無狀態性質意味著您不必依賴中央系統來處理使用者工作階段管理，這可以避免隨著存取系統的使用者數量增加而產生可擴展性問題。\n\n![](https://blog.cloudflare.com/content/images/2023/11/image2-2.png)\n\n但是，由於 JWT 的這種無狀態性質，如果不從使用者處獲得特定的 JWT，則很難對基於 JWT 的驗證進行偵錯。原因如下：\n\n**1\\. 權杖特定性：**每個 JWT 都特定於一個使用者和一個工作階段。它包含有關使用者、頒發機構、權杖的頒發時間、到期時間以及可能的其他資料的資訊（聲明）。因此，要偵錯問題，您通常需要獲得導致問題的確切 JWT。\n\n**2\\. 無伺服器端記錄：**由於 JWT 是無狀態的，因此伺服器預設不儲存工作階段。它無法查找過去的權杖或其關聯狀態，除非它是專門設計用來記錄它們的，但出於隱私和資料縮製考慮，通常情況並非如此。\n\n**3\\. 暫時性問題：**JWT 的問題可能是暫時性的，它們可能與使用權杖的具體時刻有關。例如，如果使用者嘗試使用權杖時，該權杖已過期，則需要該特定權杖來偵錯問題。\n\n**4\\. 隱私和安全：**JWT 可能包含敏感資訊，因此應謹慎處理。從使用者那裡獲取 JWT 可能會將他們的個人資訊或安全憑證暴露給偵錯問題的人員。此外，如果使用者透過不安全的通道將 JWT 傳送給開發人員或 IT 服務台，則可能會被攔截（Cloudflare 最近發佈了免費的 [HAR Sanitizer](https://blog.cloudflare.com/introducing-har-sanitizer-secure-har-sharing/) 來幫助緩解這一問題）。\n\n這些因素使得在沒有相關特定權杖的情況下，很難對基於 JWT 的驗證問題進行疑難排解。\n\n![](https://blog.cloudflare.com/content/images/2023/11/image3.png)\n\n### **偵錯身分問題的更好方法**\n\n我們著手建置一種更好的方法來偵錯與 Cloudflare Zero Trust 中使用者身分相關的問題，而無需來回分享 JWT 或 HAR 檔案。管理員現在可以檢視使用者的登錄身分（用於 Gateway 原則）和所有作用中 Access 工作階段。\n\n![](https://blog.cloudflare.com/content/images/2023/11/image7.png)\n\n此工作階段資訊包括 Zero Trust 評估的完整身分，包括 IdP 聲明、裝置狀態資訊、網路背景資訊等。透過利用 Cloudflare Workers KV，我們能夠在不對 Access 的驗證邏輯進行任何額外負載的情況下建置此功能。當使用者使用 Access 進行驗證時，其關聯的身分會立即儲存到 Workers KV 中的鍵/值組中。這一切都發生在使用者驗證事件的上下文中，這意味著延遲影響或對外部服務的依賴極小。\n\n所有 Zero Trust 方案的所有客戶都可以使用此功能。如果您想開始使用 Cloudflare Zero Trust，請立即[註冊](https://dash.cloudflare.com/sign-up/teams)一個最多可容納 50 位使用者的[免費帳戶](https://dash.cloudflare.com/sign-up/teams)！或者，[與 Cloudflare 專家合作](https://www.cloudflare.com/products/zero-trust/plans/enterprise/)，討論適合貴組織的 SSE 或 SASE，一步一步地解決您的 Zero Trust 用例。"
    },
    {
      "url": "https://blog.cloudflare.com/ja-jp/introducing-advanced-session-audit-capabilities-in-cloudflare-one-ja-jp/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/ja-jp/introducing-advanced-session-audit-capabilities-in-cloudflare-one-ja-jp/",
        "loadedTime": "2023-12-05T02:36:55.426Z",
        "referrerUrl": "https://blog.cloudflare.com/introducing-advanced-session-audit-capabilities-in-cloudflare-one/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/ja-jp/introducing-advanced-session-audit-capabilities-in-cloudflare-one-ja-jp/",
        "title": "Cloudflare Oneの高度なセッション監査機能紹介",
        "description": "管理者による、Cloudflare Oneポリシーで使用されるすべてのアクティブなユーザーセッションと関連データの監査が簡単にできるようになりました。これにより、トラブルシューティングと診断性能を向上、そして極度にきめ細かな制御の両方の長所が同時に活着ることとなります。",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "Zero Trustでは、アプリケーション、ユーザー、デバイスごとにきめ細かな制御と認可ポリシーを定義することが基本となります。このためには、規制要件とセキュリティ要件の両方を満たすため、十分な粒度を持つシステムを用意することが重要になります。しかし、制御が多くなれば、潜在的にデメリットが増えることになります。ユーザーの問題のトラブルシューティングにおいて、管理者はアプリケーション、ユーザー ID、およびデバイス情報全体の変数の複雑な組み合わせを考慮することが必要になり、ログを丹念にふるいにかける必要が出てくることになります。\n当社では、この点が改善されるべきと考えました。そして、この度Cloudflare Oneポリシーで使用されるすべてのアクティブなユーザーセッションと関連データを管理者が簡単に監査できるようにしました。これにより、単一のシンプルなコントロールパネルでZero Trustの実装におけるトラブルシューティングと診断を行う性能を維持しながら、非常にきめ細かな制御を実現し、両方の長所が活きることとなりました。以前はユーザーのブラウザに存在していた情報や動的に変更されていた情報が、エンドユーザーを煩わしたりログを掘り下げたりすることなく、管理者に利用できるようになったのです。\nアプリケーションの認証と認可に関する簡易的説明\n認証と認可は、Zero Trustポリシーにおいてリソースへのユーザーアクセスを許可する前に評価される2つのコンポーネントです。\n認証は、ユーザー、デバイス、またはシステムの識別を検証するプロセスです。一般的な認証の方法には、ユーザー名とパスワードの入力、デジタル証明書の提示、さらには指紋や顔スキャンなどの生体認証などがあります。多要素認証（MFA）では、セキュリティ強化のため、ハードウェアキーとパスワードの組み合わせなど、2つ以上の個別の認証方法が必要になります。\n認可は、エンティティが正常に認証された後に、特定のリソースまたはアクセスの許可または拒否を行うプロセスです。ここでは、認証されたエンティティがシステム内でできることとできないことを定義します。\nアプリケーションの認証/認可メカニズム\nこの項で取り上げるWebアプリケーションでは、通常、次のようにHTTP Cookieを使用して認証と認可の両方を処理します。\n認証：\nログイン：ユーザーがユーザー名とパスワードを入力してWebアプリケーションにログインすると、アプリケーションはこれらの資格情報をデータベースまたはIDプロバイダー（IDP）と照合して検証します。追加の認証形式を適用し、複数の認証要素を設けることもできます。これらが一致した場合、サーバーまたは外部セキュリティサービス（Cloudflare Accessなど）は、ユーザーが認証されたと見なします。\nCookie/トークンの作成：次に、サーバーがCookieまたはJSON Webトークンの形式でユーザーのセッションを作成します。Cookieは、ユーザーが再認証を行う必要があるタイミングまで、一定期間有効となります。\nCookieの送信と保存：サーバーは、セッションIDとCookie内のユーザーに関するその他の識別情報を含む応答をユーザーのブラウザに送り返します。その後、ブラウザはこのCookieを保存します。このCookieは、後に続く要求でユーザーを認識する際に用いられることになります。\n認可：\n以降の要求：Web アプリケーションに対し以降発生するすべての要求について、ユーザーのブラウザは自動的に要求にCookie（セッション ID およびその他の識別情報を含む）を含めます。\nサーバー側の検証：サーバーはCookieからユーザーデータを取得し、セッションが有効かどうかを確認します。有効な場合、サーバーはユーザーの詳細とそのセッションIDに関連付けられているアクセス許可も取得します。\n認可の判断：サーバーは、ユーザーのアクセス許可に基づき、要求された操作の実行または要求されたリソースへのアクセスをユーザーに許可するかどうかを決定します。\nこうして、ユーザーはログイン後、セッションの有効期限が切れるかログアウトするまで、以降続くすべての要求に対し認可されたまま（および毎回承認が確認される）になります。\n最新のWebアプリケーションでは、このセッション状態はJSON Webトークン（JWT）の形式で格納されるのが最も一般的です。\nJWTベースの認証のデバッグ\n多くの最新のWebアプリやCloudflare AccessなどのZero Trustネットワークアクセス（ZTNA）ソリューションでは、JWTが認証と認可に用いられています。JWTには、ユーザーに関する情報やその他のデータをエンコードする悪意のあるペイロードが含まれており、改ざんを防ぐためにサーバーによって署名されます。JWTはステートレスな方法で使用されることが多く、サーバーは各JWTのコピーを保持するのではなく、リクエストが届いたときに検証してデコードするだけで済みます。JWTのステートレスな性質は、ユーザーセッション管理の処理のために中央システムに依存する必要がないことを意味し、システムにアクセスするユーザーの数が増えてもスケーラビリティの問題の発生を回避できます。\nただし、このJWTのステートレスな性質により、ユーザーから特定のJWTを取得しない限りJWTベースの認証のデバッグは難しくなります。その理由は、次のとおりです:\n1. トークンの特異性：各JWTは、ユーザーとセッションに固有となっています。ユーザー、発行権限者、トークン発行時刻、有効期限、および場合によってはその他のデータに関する情報（要求）がここに含まれます。したがって多くの場合、問題をデバッグする際に問題の原因となっている正確なJWTが必要となります。\n2. サーバー側のレコードの不在：JWTはステートレスであるため、サーバーはデフォルトでセッションを保存しません。過去のトークンや関連する状態についての情報は、ログに記録するように特別に設計されていない限り、検索できません。さらに、プライバシーとデータ最小化の目的により、通常は実現しません。\n3. 一時的な問題：JWTに関する問題は一時的なものとなる場合があり、トークンが使用された特定の瞬間にのみ関連するものとなり得ます。たとえば、ユーザーがトークンを使用しようとしたときにトークンの有効期限が切れていた場合、問題をデバッグするにはその特定のトークンが必要になります。\n4. プライバシーとセキュリティ：JWTには機密情報が含まれている可能性があるため、取り扱いには注意が必要です。ユーザーからJWTを取得すると、問題をデバッグしているユーザーに個人情報やセキュリティ資格情報が公開される可能性があります。さらに、ユーザーが安全でないチャネルを介して開発者やITヘルプデスクにJWTを送信すると、傍受される可能性があります（Cloudflareは最近、この懸念の軽減を目的に無料でご利用いただけるHARサニタイザーをリリースしています)。\nこれらの要因により、問題となる特定のトークンがないと、JWTベースの認証に関する問題のトラブルシューティングが困難になります。\n識別における問題をデバッグするためのより優れた方法\n当社では、JWTやHARファイルをやり取りすることなく、Cloudflare Zero Trustでユーザー識別に関連する問題のデバッグにおけるより優れた方法の構築に着手しました。これにより、ユーザーのレジストリID（Gatewayポリシーに使用）とすべてのアクティブなアクセスセッションを管理者が確認できるようになりました。\nこのセッション情報には、IdP要求、デバイスポスチャー情報、ネットワークコンテクストなどZero Trustによる完全識別評価が含まれます。この情報は、Cloudflare Workers KVを活用し、Acessの認証ロジックで一切付加的な負荷をかけずに小袿できます。ユーザーがAccessで認証する時点で、関連する識別情報は即座にWorkers KVのKey/Valueペアに保存されます。これらはすべて、ユーザーの認証イベントのコンテクスト内で行われ、遅延の影響または外部サービスへの依存が最小限となることを意味しています。\nこの機能は、すべてのZero Trustプランのすべてのお客様が利用できます。Cloudflare Zero Trustを使い始めるには、最大50ユーザーまで利用できる無料アカウントにサインアップしてください。または、Cloudflareのエキスパートとともに貴社のSSEまたはSASEについてレビューを行い、貴社のZero Trustのユースケースに合わせ一歩ずつ進めていくことも可能です。",
      "markdown": "![](https://blog.cloudflare.com/content/images/2023/11/image5.png)\n\nZero Trustでは、アプリケーション、ユーザー、デバイスごとにきめ細かな制御と認可ポリシーを定義することが基本となります。このためには、規制要件とセキュリティ要件の両方を満たすため、十分な粒度を持つシステムを用意することが重要になります。しかし、制御が多くなれば、潜在的にデメリットが増えることになります。ユーザーの問題のトラブルシューティングにおいて、管理者はアプリケーション、ユーザー ID、およびデバイス情報全体の変数の複雑な組み合わせを考慮することが必要になり、ログを丹念にふるいにかける必要が出てくることになります。\n\n当社では、この点が改善されるべきと考えました。そして、この度Cloudflare Oneポリシーで使用されるすべてのアクティブなユーザーセッションと関連データを管理者が簡単に監査できるようにしました。これにより、単一のシンプルなコントロールパネルで[Zero Trust](https://www.cloudflare.com/learning/security/glossary/what-is-zero-trust/)の実装におけるトラブルシューティングと診断を行う性能を維持しながら、非常にきめ細かな制御を実現し、両方の長所が活きることとなりました。以前はユーザーのブラウザに存在していた情報や動的に変更されていた情報が、エンドユーザーを煩わしたりログを掘り下げたりすることなく、管理者に利用できるようになったのです。\n\n![](https://blog.cloudflare.com/content/images/2023/11/image4.png)\n\n### **アプリケーションの認証と認可に関する簡易的説明**\n\n_認証_と_認可_は、Zero Trustポリシーにおいてリソースへのユーザーアクセスを許可する前に評価される2つのコンポーネントです。\n\n**認証は**、ユーザー、デバイス、またはシステムの識別を検証するプロセスです。一般的な[認証](https://www.cloudflare.com/learning/access-management/what-is-authentication/)の方法には、ユーザー名とパスワードの入力、デジタル証明書の提示、さらには指紋や顔スキャンなどの生体認証などがあります。[多要素認証（MFA）](https://www.cloudflare.com/learning/access-management/what-is-multi-factor-authentication/)では、セキュリティ強化のため、ハードウェアキーとパスワードの組み合わせなど、2つ以上の個別の認証方法が必要になります。\n\n![](https://blog.cloudflare.com/content/images/2023/11/image6.png)\n\n**認可は**、エンティティが正常に認証された後に、特定のリソースまたはアクセスの許可または拒否を行うプロセスです。ここでは、認証されたエンティティがシステム内でできることとできないことを定義します。\n\n![](https://blog.cloudflare.com/content/images/2023/11/image1-3.png)\n\n### **アプリケーションの認証/認可メカニズム**\n\nこの項で取り上げるWebアプリケーションでは、通常、次のようにHTTP Cookieを使用して認証と認可の両方を処理します。\n\n**認証：**\n\n1.  **ログイン**：ユーザーがユーザー名とパスワードを入力してWebアプリケーションにログインすると、アプリケーションはこれらの資格情報をデータベースまたは[IDプロバイダー（IDP）](https://www.cloudflare.com/learning/access-management/what-is-an-identity-provider/)と照合して検証します。追加の認証形式を適用し、複数の認証要素を設けることもできます。これらが一致した場合、サーバーまたは外部セキュリティサービス（Cloudflare Accessなど）は、ユーザーが認証されたと見なします。\n2.  **Cookie/トークンの作成：**次に、サーバーがCookieまたはJSON Webトークンの形式でユーザーのセッションを作成します。Cookieは、ユーザーが再認証を行う必要があるタイミングまで、一定期間有効となります。\n3.  **Cookieの送信と保存：**サーバーは、セッションIDとCookie内のユーザーに関するその他の識別情報を含む応答をユーザーのブラウザに送り返します。その後、ブラウザはこのCookieを保存します。このCookieは、後に続く要求でユーザーを認識する際に用いられることになります。\n\n**認可：**\n\n1.  **以降の要求：**Web アプリケーションに対し以降発生するすべての要求について、ユーザーのブラウザは自動的に要求にCookie（セッション ID およびその他の識別情報を含む）を含めます。\n2.  **サーバー側の検証：**サーバーはCookieからユーザーデータを取得し、セッションが有効かどうかを確認します。有効な場合、サーバーはユーザーの詳細とそのセッションIDに関連付けられているアクセス許可も取得します。\n3.  **認可の判断：**サーバーは、ユーザーのアクセス許可に基づき、要求された操作の実行または要求されたリソースへのアクセスをユーザーに許可するかどうかを決定します。\n\nこうして、ユーザーはログイン後、セッションの有効期限が切れるかログアウトするまで、以降続くすべての要求に対し認可されたまま（および毎回承認が確認される）になります。\n\n最新のWebアプリケーションでは、このセッション状態はJSON Webトークン（JWT）の形式で格納されるのが最も一般的です。\n\n![](https://blog.cloudflare.com/content/images/2023/11/image8.png)\n\n### **JWTベースの認証のデバッグ**\n\n多くの最新のWebアプリやCloudflare Accessなどの[Zero Trustネットワークアクセス（ZTNA）](https://www.cloudflare.com/learning/access-management/what-is-ztna/)ソリューションでは、JWTが認証と認可に用いられています。JWTには、ユーザーに関する情報やその他のデータをエンコードする悪意のあるペイロードが含まれており、改ざんを防ぐためにサーバーによって署名されます。JWTはステートレスな方法で使用されることが多く、サーバーは各JWTのコピーを保持するのではなく、リクエストが届いたときに検証してデコードするだけで済みます。JWTのステートレスな性質は、ユーザーセッション管理の処理のために中央システムに依存する必要がないことを意味し、システムにアクセスするユーザーの数が増えてもスケーラビリティの問題の発生を回避できます。\n\n![](https://blog.cloudflare.com/content/images/2023/11/image2-2.png)\n\nただし、このJWTのステートレスな性質により、ユーザーから特定のJWTを取得しない限りJWTベースの認証のデバッグは難しくなります。その理由は、次のとおりです:\n\n**1\\. トークンの特異性：**各JWTは、ユーザーとセッションに固有となっています。ユーザー、発行権限者、トークン発行時刻、有効期限、および場合によってはその他のデータに関する情報（要求）がここに含まれます。したがって多くの場合、問題をデバッグする際に問題の原因となっている正確なJWTが必要となります。\n\n**2\\. サーバー側のレコードの不在：**JWTはステートレスであるため、サーバーはデフォルトでセッションを保存しません。過去のトークンや関連する状態についての情報は、ログに記録するように特別に設計されていない限り、検索できません。さらに、プライバシーとデータ最小化の目的により、通常は実現しません。\n\n**3\\. 一時的な問題：**JWTに関する問題は一時的なものとなる場合があり、トークンが使用された特定の瞬間にのみ関連するものとなり得ます。たとえば、ユーザーがトークンを使用しようとしたときにトークンの有効期限が切れていた場合、問題をデバッグするにはその特定のトークンが必要になります。\n\n**4\\. プライバシーとセキュリティ：**JWTには機密情報が含まれている可能性があるため、取り扱いには注意が必要です。ユーザーからJWTを取得すると、問題をデバッグしているユーザーに個人情報やセキュリティ資格情報が公開される可能性があります。さらに、ユーザーが安全でないチャネルを介して開発者やITヘルプデスクにJWTを送信すると、傍受される可能性があります（Cloudflareは最近、この懸念の軽減を目的に無料でご利用いただける[HARサニタイザー](https://blog.cloudflare.com/introducing-har-sanitizer-secure-har-sharing/)をリリースしています)。\n\nこれらの要因により、問題となる特定のトークンがないと、JWTベースの認証に関する問題のトラブルシューティングが困難になります。\n\n![](https://blog.cloudflare.com/content/images/2023/11/image3.png)\n\n### **識別における問題をデバッグするためのより優れた方法**\n\n当社では、JWTやHARファイルをやり取りすることなく、Cloudflare Zero Trustでユーザー識別に関連する問題のデバッグにおけるより優れた方法の構築に着手しました。これにより、ユーザーのレジストリID（Gatewayポリシーに使用）とすべてのアクティブなアクセスセッションを管理者が確認できるようになりました。\n\n![](https://blog.cloudflare.com/content/images/2023/11/image7.png)\n\nこのセッション情報には、IdP要求、デバイスポスチャー情報、ネットワークコンテクストなどZero Trustによる完全識別評価が含まれます。この情報は、Cloudflare Workers KVを活用し、Acessの認証ロジックで一切付加的な負荷をかけずに小袿できます。ユーザーがAccessで認証する時点で、関連する識別情報は即座にWorkers KVのKey/Valueペアに保存されます。これらはすべて、ユーザーの認証イベントのコンテクスト内で行われ、遅延の影響または外部サービスへの依存が最小限となることを意味しています。\n\nこの機能は、すべてのZero Trustプランのすべてのお客様が利用できます。Cloudflare Zero Trustを使い始めるには、最大50ユーザーまで利用できる[無料アカウントにサインアップ](https://dash.cloudflare.com/sign-up/teams)してください。または、[Cloudflareのエキスパート](https://www.cloudflare.com/products/zero-trust/plans/enterprise/)とともに貴社のSSEまたはSASEについてレビューを行い、貴社のZero Trustのユースケースに合わせ一歩ずつ進めていくことも可能です。"
    },
    {
      "url": "https://blog.cloudflare.com/fr-fr/introducing-advanced-session-audit-capabilities-in-cloudflare-one-fr-fr/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/fr-fr/introducing-advanced-session-audit-capabilities-in-cloudflare-one-fr-fr/",
        "loadedTime": "2023-12-05T02:37:04.128Z",
        "referrerUrl": "https://blog.cloudflare.com/introducing-advanced-session-audit-capabilities-in-cloudflare-one/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/fr-fr/introducing-advanced-session-audit-capabilities-in-cloudflare-one-fr-fr/",
        "title": "Introduction de fonctionnalités avancées d'audit de session dans Cloudflare One",
        "description": "Les administrateurs peuvent désormais auditer facilement toutes les sessions d'utilisateurs actifs, ainsi que les données associées utilisées par leurs politiques Cloudflare One",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/16/2023\n7 min read\nLe principe fondamental de la sécurité Zero Trust est la définition de contrôles granulaires et de politiques d'autorisation pour chaque application, chaque utilisateur et chaque appareil. La conformité aux exigences réglementaires et de sécurité exige de disposer d'un système doté d'une granularité suffisante. Cependant, un tel nombre de contrôles comporte un inconvénient potentiel : pour résoudre les incidents affectant les utilisateurs, un administrateur doit tenir compte d'un ensemble complexe de variables englobant les applications, l'identité des utilisateurs et les informations sur les appareils. Cette approche peut, par conséquent, nécessiter un examen particulièrement minutieux des journaux.\nNous pensons qu'il existe une meilleure approche : c'est pourquoi, à partir d'aujourd'hui, les administrateurs peuvent auditer facilement toutes les sessions d'utilisateurs actifs, ainsi que les données associées utilisées par leurs politiques Cloudflare One. Cette approche réunit tous les avantages : des contrôles extrêmement granulaires, ainsi qu'une capacité améliorée de résolution d'incidents et de diagnostic des déploiements Zero Trust depuis un panneau de contrôle unique et simple. Les informations qui, auparavant, résidaient dans le navigateur de l'utilisateur ou changeaient dynamiquement sont désormais accessibles aux administrateurs, sans contraindre ces derniers à déranger les utilisateurs finaux ou à examiner longuement des journaux.\nQuelques notions fondamentales sur l'authentification et l'autorisation des applications\nL'authentification et l'autorisation sont les deux composantes évaluées par une politique Zero Trust avant d'autoriser un utilisateur à accéder à une ressource.\nL'authentification est le processus de vérification de l'identité d'un utilisateur, d'un appareil ou d'un système. Les méthodes d'authentification courantes incluent notamment la saisie de noms d'utilisateur et de mots de passe, la présentation d'un certificat numérique ou même l'utilisation de données biométriques, telles que l'analyse des empreintes digitales ou l'analyse faciale. L'authentification multifactorielle (MFA, « Multi-Factor Authentication ») fait appel à deux méthodes d'authentification distinctes ou plus pour offrir une sécurité renforcée (par exemple, l'association d'une clé physique et d'un mot de passe).\nL'autorisation est le processus consistant à accorder ou à refuser l'accès à des ressources ou des autorisations spécifiques lorsqu'une entité a été authentifiée avec succès. Elle définit les opérations que l'entité authentifiée est ou non autorisée à effectuer dans le système.\nMécanismes d'authentification/d'autorisation d'applications\nLes applications web, sur lesquelles nous allons nous concentrer, utilisent généralement des cookies HTTP pour gérer l'authentification et l'autorisation.\nAuthentification:\nConnexion : lorsqu'un utilisateur se connecte à une application web en saisissant son nom d'utilisateur et son mot de passe, l'application vérifie ces informations d'identification dans sa base de données ou auprès d'un fournisseur d'identité. D'autres formes d'authentification peuvent également être mises en œuvre, afin d'obtenir plusieurs facteurs d'authentification. Si ces facteurs correspondent, le serveur ou le service de sécurité externe (par exemple, Cloudflare Access) considère que l'utilisateur est authentifié.\nCréation d'un cookie ou d'un jeton : le serveur crée ensuite une session pour l'utilisateur sous la forme d'un cookie ou d'un jeton Web JSON Token (JWT). Le cookie reste valable un certain temps, jusqu'à ce que l'utilisateur doive s'authentifier à nouveau.\nEnvoi et stockage de cookies : le serveur renvoie au navigateur de l'utilisateur une réponse contenant l'identifiant de session et d'autres informations d'identification de l'utilisateur dans le cookie. Le navigateur stocke ce cookie, qui est ensuite utilisé pour reconnaître l'utilisateur lorsque celui-ci transmet des requêtes ultérieures.\nAutorisation :\nRequêtes ultérieures : pour toutes les requêtes ultérieures transmises à l'application web, le navigateur de l'utilisateur inclut automatiquement le cookie (avec l'identifiant de session et d'autres informations d'identification) dans la requête.\nVérification côté serveur : le serveur extrait les données de l'utilisateur du cookie et vérifie si la session est valide. Si elle est valide, le serveur récupère également les coordonnées de l'utilisateur, ainsi que les autorisations d'accès associées à cet identifiant de session.\nDécision d'autorisation : en fonction des autorisations d'accès de l'utilisateur, le serveur décide si l'utilisateur est autorisé à effectuer l'opération demandée ou à accéder à la ressource demandée.\nAinsi, l'utilisateur reste authentifié (et son autorisation peut être vérifiée) pour toutes les requêtes ultérieures après sa connexion, jusqu'à ce que la session expire ou que l'utilisateur se déconnecte.\nDans les applications web modernes, cet état de session est, le plus souvent stocké, sous la forme d'un jeton JSON Web Token (JWT).\nDébogage de l'authentification basée sur les jetons JWT\nLes jetons JWT sont utilisés dans de nombreuses applications web modernes et dans des solutions Zero Trust Network Access (ZTNA), telles que Cloudflare Access, aux fins de l'authentification et de l'autorisation. Un jeton JWT inclut une charge utile assurant l'encodage des informations sur l'utilisateur, ainsi que d'autres données ; il est signé par le serveur, afin d'empêcher toute falsification. Les jetons JWT sont souvent utilisés sans état, ce qui signifie que le serveur ne conserve pas une copie de chaque jeton JWT ; il vérifie et décode simplement les jetons au fur et à mesure qu'il les reçoit avec les requêtes. En raison de la nature sans état des jetons JWT, la gestion des sessions d'utilisateurs ne dépend pas d'un système central, ce qui évite les problèmes d'évolutivité en cas d'augmentation du nombre d'utilisateurs accédant à un système.\nCependant, la nature sans état des jetons JWT rend difficile le débogage de l'authentification basée sur les jetons JWT en l'absence d'accès au jeton JWT spécifique d'un utilisateur. Voici pourquoi :\n1. Spécificité des jetons : chaque jeton JWT est spécifique à un utilisateur et une session. Il contient des informations (affirmations) sur l'utilisateur, l'autorité émettrice, la date et l'heure d'émission du jeton et sa date d'expiration, ainsi que d'autres données, éventuellement. Par conséquent, le débogage d'un incident nécessite souvent d'avoir accès au jeton JWT exact à l'origine du problème.\n2. Absence d'enregistrements côté serveur : les jetons JWT étant sans état, le serveur ne stocke pas les sessions par défaut. Il ne peut pas rechercher de jetons antérieurs, ni leur état associé, à moins d'avoir été spécifiquement conçu pour assurer leur journalisation ; toutefois, ce n'est généralement pas le cas, en raison de considérations liées à la confidentialité et à la minimisation des données.\n3. Problèmes transitoires : les problèmes liés aux jetons JWT peuvent être transitoires ; ils peuvent être liés à l'instant précis auquel le jeton a été utilisé. Par exemple, si un jeton a expiré alors qu'un utilisateur essayait de l'utiliser, vous devrez avoir accès à ce jeton spécifique pour effectuer le débogage de l'incident.\n4. Confidentialité et sécurité : les jetons JWT peuvent contenir des informations sensibles et doivent donc être manipulés avec précaution. L'obtention d'un jeton JWT auprès d'un utilisateur peut exposer ses informations personnelles ou ses identifiants de sécurité à la personne effectuant le débogage de l'incident. En outre, si un utilisateur transmet son jeton JWT à un développeur ou un service d'assistance informatique via un canal non sécurisé, le jeton JWT peut être intercepté (Cloudflare a récemment publié une application gratuite d'assainissement de fichiers HAR afin d'aider à atténuer cette préoccupation).\nEn raison de ces facteurs, il est difficile de résoudre les problèmes liés à l'authentification par jeton JWT sans avoir accès au jeton en question.\nUne meilleure façon de déboguer les incidents liés à l'identité\nNous avons entrepris de développer une meilleure manière de déboguer les incidents liés à l'identité d'un utilisateur dans Cloudflare Zero Trust, qui ne nécessite pas la transmission de jetons JWT ou de fichiers HAR, dans un sens ou dans l'autre. Les administrateurs peuvent désormais visualiser l'identité de registre (« Registry Identity ») d'un utilisateur (utilisée pour les politiques de passerelle), ainsi que toutes les sessions Access actives.\nCes informations sur les sessions incluent l'identité complète évaluée par la solution Zero Trust, notamment les affirmations de fournisseurs d'identité, les informations sur la stratégie de sécurité de l'appareil, le contexte réseau et bien davantage. Nous avons pu développer cette fonctionnalité sans ajouter une charge supplémentaire à la logique d'authentification d'Access, en utilisant Cloudflare Workers KV. Lorsqu'un utilisateur s'authentifie auprès d'Access, l'identité qui lui est associée est immédiatement enregistrée dans une paire clé/valeur dans Workers KV. Tout cela se déroule dans le contexte de l'événement d'authentification de l'utilisateur, permettant ainsi de minimiser l'impact en termes de latence ou la dépendance vis-à-vis d'un service externe.\nCette fonctionnalité est disponible pour tous les clients de toutes les offres Zero Trust. Si vous souhaitez faire vos premiers pas avec Cloudflare Zero Trust, inscrivez-vous dès aujourd'hui pour créer un compte gratuit pour jusqu'à 50 utilisateurs. Vous pouvez également contacter les experts de Cloudflare pour parler de l'approche SSE ou SASE pour votre entreprise et examiner vos scénarios d'utilisation de la sécurité Zero Trust, étape par étape.\nNous protégeons des réseaux d'entreprise entiers, aidons nos clients à développer efficacement des applications à l'échelle d'Internet, accélérons n'importe quel site web ou application Internet, repoussons les attaques DDoS, maintenons les pirates à distance et pouvons vous aider dans votre parcours vers le Zero Trust. \nRendez-vous sur 1.1.1.1 depuis n'importe quel appareil pour commencer à utiliser notre application gratuite, qui rend votre navigation Internet plus rapide et plus sûre. \nPour en savoir plus sur notre mission visant à bâtir un meilleur Internet, cliquez ici. Si vous cherchez de nouvelles perspectives professionnelles, consultez nos postes vacants. \nSASE (FR) Cloudflare Zero Trust (FR) Product News (FR) Cloudflare One (FR) Cloudflare Workers KV (FR)",
      "markdown": "11/16/2023\n\n*   [![Kenny Johnson](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2020/12/0DA903DE-E315-43D5-B0DE-39870448303D.jpeg)](https://blog.cloudflare.com/author/kenny/)\n\n7 min read\n\n![](https://blog.cloudflare.com/content/images/2023/11/image5.png)\n\nLe principe fondamental de la sécurité Zero Trust est la définition de contrôles granulaires et de politiques d'autorisation pour chaque application, chaque utilisateur et chaque appareil.  La conformité aux exigences réglementaires et de sécurité exige de disposer d'un système doté d'une granularité suffisante. Cependant, un tel nombre de contrôles comporte un inconvénient potentiel : pour résoudre les incidents affectant les utilisateurs, un administrateur doit tenir compte d'un ensemble complexe de variables englobant les applications, l'identité des utilisateurs et les informations sur les appareils. Cette approche peut, par conséquent, nécessiter un examen particulièrement minutieux des journaux.\n\nNous pensons qu'il existe une meilleure approche : c'est pourquoi, à partir d'aujourd'hui, les administrateurs peuvent auditer facilement toutes les sessions d'utilisateurs actifs, ainsi que les données associées utilisées par leurs politiques Cloudflare One. Cette approche réunit tous les avantages : des contrôles extrêmement granulaires, ainsi qu'une capacité améliorée de résolution d'incidents et de diagnostic des déploiements [Zero Trust](https://www.cloudflare.com/learning/security/glossary/what-is-zero-trust/) depuis un panneau de contrôle unique et simple. Les informations qui, auparavant, résidaient dans le navigateur de l'utilisateur ou changeaient dynamiquement sont désormais accessibles aux administrateurs, sans contraindre ces derniers à déranger les utilisateurs finaux ou à examiner longuement des journaux.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image4.png)\n\n### **Quelques notions fondamentales sur l'authentification et l'autorisation des applications**\n\nL'_authentification_ et l'_autorisation_ sont les deux composantes évaluées par une politique Zero Trust avant d'autoriser un utilisateur à accéder à une ressource.\n\n**L'authentification** est le processus de vérification de l'identité d'un utilisateur, d'un appareil ou d'un système. Les méthodes d'[authentification](https://www.cloudflare.com/learning/access-management/what-is-authentication/) courantes incluent notamment la saisie de noms d'utilisateur et de mots de passe, la présentation d'un certificat numérique ou même l'utilisation de données biométriques, telles que l'analyse des empreintes digitales ou l'analyse faciale. L'[authentification multifactorielle (MFA, « Multi-Factor Authentication »)](https://www.cloudflare.com/learning/access-management/what-is-multi-factor-authentication/) fait appel à deux méthodes d'authentification distinctes ou plus pour offrir une sécurité renforcée (par exemple, l'association d'une clé physique et d'un mot de passe).\n\n![](https://blog.cloudflare.com/content/images/2023/11/image6.png)\n\nL'**autorisation** est le processus consistant à accorder ou à refuser l'accès à des ressources ou des autorisations spécifiques lorsqu'une entité a été authentifiée avec succès. Elle définit les opérations que l'entité authentifiée est ou non autorisée à effectuer dans le système.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image1-3.png)\n\n### **Mécanismes d'authentification/d'autorisation d'applications**\n\nLes applications web, sur lesquelles nous allons nous concentrer, utilisent généralement des cookies HTTP pour gérer l'authentification et l'autorisation.\n\n**Authentification:**\n\n1.  **Connexion** : lorsqu'un utilisateur se connecte à une application web en saisissant son nom d'utilisateur et son mot de passe, l'application vérifie ces informations d'identification dans sa base de données ou auprès d'un [fournisseur d'identité](https://www.cloudflare.com/learning/access-management/what-is-an-identity-provider/). D'autres formes d'authentification peuvent également être mises en œuvre, afin d'obtenir plusieurs facteurs d'authentification. Si ces facteurs correspondent, le serveur ou le service de sécurité externe (par exemple, Cloudflare Access) considère que l'utilisateur est authentifié.\n2.  **Création d'un cookie ou d'un jeton** : le serveur crée ensuite une session pour l'utilisateur sous la forme d'un cookie ou d'un jeton Web JSON Token (JWT). Le cookie reste valable un certain temps, jusqu'à ce que l'utilisateur doive s'authentifier à nouveau.\n3.  **Envoi et stockage de cookies** : le serveur renvoie au navigateur de l'utilisateur une réponse contenant l'identifiant de session et d'autres informations d'identification de l'utilisateur dans le cookie. Le navigateur stocke ce cookie, qui est ensuite utilisé pour reconnaître l'utilisateur lorsque celui-ci transmet des requêtes ultérieures.\n\n**Autorisation** :\n\n1.  **Requêtes ultérieures** : pour toutes les requêtes ultérieures transmises à l'application web, le navigateur de l'utilisateur inclut automatiquement le cookie (avec l'identifiant de session et d'autres informations d'identification) dans la requête.\n2.  **Vérification côté serveur** : le serveur extrait les données de l'utilisateur du cookie et vérifie si la session est valide. Si elle est valide, le serveur récupère également les coordonnées de l'utilisateur, ainsi que les autorisations d'accès associées à cet identifiant de session.\n3.  **Décision d'autorisation :** en fonction des autorisations d'accès de l'utilisateur, le serveur décide si l'utilisateur est autorisé à effectuer l'opération demandée ou à accéder à la ressource demandée.\n\nAinsi, l'utilisateur reste authentifié (et son autorisation peut être vérifiée) pour toutes les requêtes ultérieures après sa connexion, jusqu'à ce que la session expire ou que l'utilisateur se déconnecte.\n\nDans les applications web modernes, cet état de session est, le plus souvent stocké, sous la forme d'un jeton JSON Web Token (JWT).\n\n![](https://blog.cloudflare.com/content/images/2023/11/image8.png)\n\n### **Débogage de l'authentification basée sur les jetons JWT**\n\nLes jetons JWT sont utilisés dans de nombreuses applications web modernes et dans des solutions [Zero Trust Network Access (ZTNA)](https://www.cloudflare.com/learning/access-management/what-is-ztna/), telles que Cloudflare Access, aux fins de l'authentification et de l'autorisation. Un jeton JWT inclut une charge utile assurant l'encodage des informations sur l'utilisateur, ainsi que d'autres données ; il est signé par le serveur, afin d'empêcher toute falsification. Les jetons JWT sont souvent utilisés sans état, ce qui signifie que le serveur ne conserve pas une copie de chaque jeton JWT ; il vérifie et décode simplement les jetons au fur et à mesure qu'il les reçoit avec les requêtes. En raison de la nature sans état des jetons JWT, la gestion des sessions d'utilisateurs ne dépend pas d'un système central, ce qui évite les problèmes d'évolutivité en cas d'augmentation du nombre d'utilisateurs accédant à un système.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image2-2.png)\n\nCependant, la nature sans état des jetons JWT rend difficile le débogage de l'authentification basée sur les jetons JWT en l'absence d'accès au jeton JWT spécifique d'un utilisateur. Voici pourquoi :\n\n**1\\. Spécificité des jetons :** chaque jeton JWT est spécifique à un utilisateur et une session. Il contient des informations (affirmations) sur l'utilisateur, l'autorité émettrice, la date et l'heure d'émission du jeton et sa date d'expiration, ainsi que d'autres données, éventuellement. Par conséquent, le débogage d'un incident nécessite souvent d'avoir accès au jeton JWT exact à l'origine du problème.\n\n**2\\. Absence d'enregistrements côté serveur  :** les jetons JWT étant sans état, le serveur ne stocke pas les sessions par défaut. Il ne peut pas rechercher de jetons antérieurs, ni leur état associé, à moins d'avoir été spécifiquement conçu pour assurer leur journalisation ; toutefois, ce n'est généralement pas le cas, en raison de considérations liées à la confidentialité et à la minimisation des données.\n\n**3\\. Problèmes transitoires** : les problèmes liés aux jetons JWT peuvent être transitoires ; ils peuvent être liés à l'instant précis auquel le jeton a été utilisé. Par exemple, si un jeton a expiré alors qu'un utilisateur essayait de l'utiliser, vous devrez avoir accès à ce jeton spécifique pour effectuer le débogage de l'incident.\n\n**4\\. Confidentialité et sécurité :** les jetons JWT peuvent contenir des informations sensibles et doivent donc être manipulés avec précaution. L'obtention d'un jeton JWT auprès d'un utilisateur peut exposer ses informations personnelles ou ses identifiants de sécurité à la personne effectuant le débogage de l'incident. En outre, si un utilisateur transmet son jeton JWT à un développeur ou un service d'assistance informatique via un canal non sécurisé, le jeton JWT peut être intercepté (Cloudflare a récemment publié une application gratuite d'[assainissement de fichiers HAR](https://blog.cloudflare.com/introducing-har-sanitizer-secure-har-sharing/) afin d'aider à atténuer cette préoccupation).\n\nEn raison de ces facteurs, il est difficile de résoudre les problèmes liés à l'authentification par jeton JWT sans avoir accès au jeton en question.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image3.png)\n\n### **Une meilleure façon de déboguer les incidents liés à l'identité**\n\nNous avons entrepris de développer une meilleure manière de déboguer les incidents liés à l'identité d'un utilisateur dans Cloudflare Zero Trust, qui ne nécessite pas la transmission de jetons JWT ou de fichiers HAR, dans un sens ou dans l'autre. Les administrateurs peuvent désormais visualiser l'identité de registre (« Registry Identity ») d'un utilisateur (utilisée pour les politiques de passerelle), ainsi que toutes les sessions Access actives.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image7.png)\n\nCes informations sur les sessions incluent l'identité complète évaluée par la solution Zero Trust, notamment les affirmations de fournisseurs d'identité, les informations sur la stratégie de sécurité de l'appareil, le contexte réseau et bien davantage. Nous avons pu développer cette fonctionnalité sans ajouter une charge supplémentaire à la logique d'authentification d'Access, en utilisant Cloudflare Workers KV. Lorsqu'un utilisateur s'authentifie auprès d'Access, l'identité qui lui est associée est immédiatement enregistrée dans une paire clé/valeur dans Workers KV. Tout cela se déroule dans le contexte de l'événement d'authentification de l'utilisateur, permettant ainsi de minimiser l'impact en termes de latence ou la dépendance vis-à-vis d'un service externe.\n\nCette fonctionnalité est disponible pour tous les clients de toutes les offres Zero Trust. Si vous souhaitez faire vos premiers pas avec Cloudflare Zero Trust, [inscrivez-vous dès aujourd'hui pour créer un compte gratuit](https://dash.cloudflare.com/sign-up/teams) pour jusqu'à 50 utilisateurs. Vous pouvez également [contacter les experts de Cloudflare](https://www.cloudflare.com/products/zero-trust/plans/enterprise/) pour parler de l'approche SSE ou SASE pour votre entreprise et examiner vos scénarios d'utilisation de la sécurité Zero Trust, étape par étape.\n\nNous protégeons [des réseaux d'entreprise entiers](https://www.cloudflare.com/fr-fr/network-services/), aidons nos clients à développer [efficacement des applications à l'échelle d'Internet](https://workers.cloudflare.com/), accélérons n'importe quel [site web ou application Internet,](https://www.cloudflare.com/fr-fr/performance/accelerate-internet-applications/) repoussons [les attaques DDoS](https://www.cloudflare.com/fr-fr/ddos/), maintenons [les pirates à distance](https://www.cloudflare.com/fr-fr/application-security/) et pouvons vous aider dans [votre parcours vers le Zero Trust](https://www.cloudflare.com/fr-fr/products/zero-trust/).\n\nRendez-vous sur [1.1.1.1](https://1.1.1.1/) depuis n'importe quel appareil pour commencer à utiliser notre application gratuite, qui rend votre navigation Internet plus rapide et plus sûre.\n\nPour en savoir plus sur notre mission visant à bâtir un meilleur Internet, cliquez [ici](https://www.cloudflare.com/fr-fr/learning/what-is-cloudflare/). Si vous cherchez de nouvelles perspectives professionnelles, consultez [nos postes vacants](https://cloudflare.com/fr-fr/careers).\n\n[SASE (FR)](https://blog.cloudflare.com/tag/sase-fr/) [Cloudflare Zero Trust (FR)](https://blog.cloudflare.com/tag/cloudflare-zero-trust-fr/) [Product News (FR)](https://blog.cloudflare.com/tag/product-news-fr/) [Cloudflare One (FR)](https://blog.cloudflare.com/tag/cloudflare-one-fr/) [Cloudflare Workers KV (FR)](https://blog.cloudflare.com/tag/workers-kv-fr/)"
    },
    {
      "url": "https://blog.cloudflare.com/ko-kr/introducing-advanced-session-audit-capabilities-in-cloudflare-one-ko-kr/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/ko-kr/introducing-advanced-session-audit-capabilities-in-cloudflare-one-ko-kr/",
        "loadedTime": "2023-12-05T02:37:04.158Z",
        "referrerUrl": "https://blog.cloudflare.com/introducing-advanced-session-audit-capabilities-in-cloudflare-one/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/ko-kr/introducing-advanced-session-audit-capabilities-in-cloudflare-one-ko-kr/",
        "title": "Cloudflare One의 고급 세션 감사 기능 도입",
        "description": "이제 관리자는 Cloudflare One 정책에서 사용하는 모든 활성 사용자 세션과 관련 데이터를 쉽게 감사할 수 있습니다. 이를 통해 매우 세분화된 제어와 향상된 문제 해결 및 진단 유지의 이점을 모두 누릴 수 있습니다.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/16/2023\n10 min read\nZero Trust의 기본은 애플리케이션, 사용자 및 장치별로 세분화된 제어 및 권한 부여 정책을 정의하는 것입니다. 이를 수행할 수 있는 충분한 수준의 세분화된 시스템을 갖추는 것은 규정 및 보안 요구 사항을 모두 충족하는 데 중요합니다. 그러나 제어 기능이 너무 많으면 잠재적인 단점이 있습니다. 관리자는 사용자 문제를 해결하기 위해 애플리케이션, 사용자 ID 및 장치 정보 전반에 걸쳐 복잡한 변수 조합을 고려해야 하며, 이를 위해 로그를 면밀히 검토해야 할 수도 있습니다.\n당사는 더 나은 방법이 있다고 믿기 때문에 오늘부터 관리자는 Cloudflare One 정책에 사용된 모든 활성 사용자 세션과 관련 데이터를 쉽게 감사할 수 있습니다. 이렇게 하면 간단한 단일 제어판에서 Zero Trust 배포 문제를 해결하고 진단하는 향상된 기능을 유지하는 동시에 매우 세분화된 제어의 이점을 모두 누릴 수 있습니다. 이제 관리자는 최종 사용자에게 불편을 주거나 로그를 파헤칠 필요 없이 사용자의 브라우저에 존재하거나 동적으로 변화하는 정보에 액세스할 수 있습니다.\n애플리케이션 인증 및 권한 부여에 대한 빠른 지침서\n인증 및 권한 부여 는 Zero Trust 정책이 리소스에 대한 사용자 액세스를 허용하기 전에 평가하는 두 가지 구성 요소입니다.\n인증은 사용자, 장치 또는 시스템 ID를 확인하는 프로세스입니다. 일반적인 인증 방법에는 사용자 이름과 비밀번호 입력, 디지털 인증서 제시 또는 지문이나 얼굴 스캔과 같은 생체 인식이 포함됩니다. 다단계 인증(MFA)은 보안을 강화하기 위해 비밀번호와 하드웨어 키를 결합하는 등 두 가지 이상의 개별 인증 방법을 사용해야 합니다.\n권한 부여는 엔터티가 성공적으로 인증된 후 특정 리소스 또는 권한에 대한 액세스 권한을 부여하거나 거부하는 프로세스입니다. 인증된 엔터티가 시스템 내에서 수행할 수 있는 것과 수행할 수 없는 작업을 정의합니다.\n애플리케이션 인증/권한 부여 메커니즘\n이 글에서 집중적으로 살펴볼 웹 애플리케이션은 일반적으로 HTTP 쿠키를 사용하여 인증과 권한 부여를 모두 처리합니다.\n인증\n로그인: 사용자가 웹 애플리케이션에 로그인하기 위해 사용자 이름과 비밀번호를 입력하면 애플리케이션은 데이터베이스 또는 ID 공급자(IdP)에 대해 이러한 자격 증명을 확인합니다. 여러 인증 요소를 달성하기 위해 추가 인증 형태를 적용할 수도 있습니다. 이러한 요소가 일치하는 경우 서버 또는 외부 보안 서비스(예: Cloudflare Access)는 사용자가 인증된 것으로 간주합니다.\n쿠키/토큰 생성: 다음으로 서버는 쿠키 또는 JSON 웹 토큰의 형태로 사용자에 대한 세션을 생성합니다. 쿠키는 사용자가 다시 인증해야 할 때까지 일정 기간 동안 유효합니다.\n쿠키 전송 및 저장: 서버가 사용자의 브라우저에 응답을 전송할 때 쿠키에 세션 ID 및 기타 사용자 식별 정보가 포함됩니다. 그러면 브라우저는 이 쿠키를 저장합니다. 해당 쿠키는 후속 요청에서 사용자를 인식하는 데 사용합니다.\n권한 부여:\n후속 요청: 웹 애플리케이션에 대한 모든 후속 요청의 경우 사용자의 브라우저는 요청에 쿠키(세션 ID 및 기타 식별 정보 포함)를 자동으로 포함합니다.\n서버 측 확인: 서버는 쿠키에서 사용자 데이터를 검색하여 세션이 유효한지 확인합니다. 세션이 유효한 경우 서버는 해당 세션 ID와 관련된 사용자의 세부 정보 및 액세스 권한도 검색합니다.\n권한 부여 결정: 서버는 사용자의 액세스 권한에 따라 사용자가 요청된 작업을 수행하거나 요청된 리소스에 액세스할 수 있는 권한이 있는지 여부를 결정합니다.\n이를 통해 사용자는 로그인 후 세션이 만료되거나 로그아웃할 때까지 모든 후속 요청에 대해 인증된 상태를 유지하고 권한 부여를 확인할 수 있습니다.\n최신 웹 애플리케이션에서 이 세션 상태는 일반적으로 JSON 웹 토큰(JWT) 형식으로 저장됩니다.\nJWT 기반 인증 디버깅\nJWT는 인증 및 권한 부여 목적으로 많은 최신 웹 애플리케이션과 Cloudflare Access와 같은 Zero Trust 네트워크 액세스(ZTNA) 솔루션에서 활용됩니다. 여기에는 사용자 및 기타 데이터에 대한 정보를 인코딩하는 페이로드가 포함되어 있으며 변조를 방지하기 위해 서버에서 서명합니다. JWT는 종종 상태 비저장 방식으로 사용되는데, 이는 서버가 각 JWT의 사본을 보관하지 않고 요청을 수신할 때 이를 확인하고 디코딩한다는 의미입니다. JWT의 상태 비저장 특성으로 인해 사용자 세션 관리를 처리하기 위해 중앙 시스템에 의존할 필요가 없으므로 시스템에 액세스하는 사용자 수가 증가하더라도 확장성을 보장할 수 있습니다.\n그러나 JWT의 이러한 상태 비저장 특성으로 인해 사용자로부터 특정 JWT를 가져오지 않고는 JWT 기반 인증을 디버깅하기가 어려울 수 있습니다. 그 이유는 다음과 같습니다.\n1. 토큰 특이성: 각 JWT는 사용자 및 세션에 따라 다릅니다. 여기에는 사용자, 발급 기관, 토큰 발급 시간, 만료 시간 및 기타 데이터에 대한 정보(클레임)가 포함되어 있습니다. 따라서 문제를 디버깅하려면 문제의 원인이 되는 정확한 JWT가 필요한 경우가 많습니다.\n2. 서버 측 레코드 없음: JWT는 상태 비저장 방식이므로 서버는 기본적으로 세션을 저장하지 않습니다. 과거 토큰 또는 관련 상태를 기록하도록 특별히 설계된 경우가 아니라면 이를 조회할 수 없으며, 일반적으로 개인정보 보호 및 데이터 최소화 고려 사항으로 인해 검색하지 않습니다.\n3. 일시적인 문제: JWT는 토큰이 사용된 특정 시점과 관련된 일시적인 문제가 있을 수 있습니다. 예를 들어, 사용자가 만료된 토큰을 사용하려고 할 때 문제를 디버깅하려면 해당 특정 토큰이 있어야 합니다.\n4. 개인정보 보호 및 보안: JWT는 민감한 정보를 포함할 수 있으므로 신중하게 처리해야 합니다. 사용자로부터 JWT를 받을 때 문제를 디버깅하는 사람은 사용자의 개인 정보나 보안 자격 증명에 노출될 수 있습니다. 또한 사용자가 안전하지 않은 채널을 통해 개발자나 IT 헬프 데스크에 JWT를 보내면 가로챌 수 있습니다(최근 Cloudflare는 이 문제를 완화하는 데 도움이 될 수 있도록 HAR Sanitizer를 무료로 출시했습니다).\n이러한 요인으로 인해 문제와 관련된 특정 토큰이 없으면 JWT 기반 인증 문제를 해결하기가 어렵습니다.\nID 문제를 디버깅하는 더 나은 방법\nCloudflare는 JWT를 공유하거나 HAR 파일을 주고받지 않고도 Cloudflare Zero Trust에서 사용자 ID와 관련된 문제를 디버깅할 수 있는 더 나은 방법을 구축하기 시작했습니다. 이제 관리자는 사용자의 레지스트리 ID(게이트웨이 정책에 사용됨)와 모든 활성 Access 세션을 볼 수 있습니다.\n이 세션 정보에는 IdP 클레임, 장치 상태 정보, 네트워크 컨텍스트 등 Zero Trust 에서 평가된 전체 ID가 포함됩니다. 당사는 Cloudflare Workers KV를 활용하여 Access의 인증 로직에 추가적인 부하를 주지 않고도 이 기능을 구축할 수 있었습니다. 사용자가 Access를 사용하여 인증할 때 연결된 ID는 Workers KV의 키/값 쌍에 즉시 저장됩니다. 이 모든 것은 사용자의 인증 이벤트 컨텍스트 내에서 이루어지므로 대기 시간 영향이나 서비스에 대한 의존성을 최소화할 수 있습니다.\n이 기능은 Zero Trust 요금제를 사용하는 모든 고객에게 제공됩니다. 최대 50명의 사용자를 위한 무료 계정에 가입하고 Cloudflare Zero Trust를 지금 바로 사용해 보세요! 또는 Cloudflare 전문가와 협력하여 조직을 위한 SSE 또는 SASE에 대해 논의하고 Zero Trust 사용 사례를 단계별로 해결해 보세요.\nCloudflare는 전체 기업 네트워크 를 보호하고, 고객이 인터넷 규모의 애플리케이션을 효율적으로 구축하도록 도우며, 웹사이트 또는 인터넷 애플리케이션 을 가속하고, DDoS 공격 을 막으며, 해커의 침입을 방지 하고, Zero Trust로의 여정을 도와드릴 수 있습니다. \n아무 장치로 1.1.1.1 에 방문해서 인터넷을 더 빠르고 안전하게 만드는 Cloudflare의 무료 앱을 시작해 보세요. \n더 나은 인터넷을 만들겠다는 Cloudflare의 사명을 더욱 자세히 알아보려면, 이곳 을 확인해 보세요. 새로운 방향성을 제시하는 직업을 찾고 있다면, 채용 중인 직무 목록을 확인해 보세요. \nSASE (KO) Cloudflare Zero Trust (KO) Product News (KO) Cloudflare One (KO) Cloudflare Workers KV (KO)",
      "markdown": "11/16/2023\n\n*   [![Kenny Johnson](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2020/12/0DA903DE-E315-43D5-B0DE-39870448303D.jpeg)](https://blog.cloudflare.com/author/kenny/)\n\n10 min read\n\n![](https://blog.cloudflare.com/content/images/2023/11/image5.png)\n\nZero Trust의 기본은 애플리케이션, 사용자 및 장치별로 세분화된 제어 및 권한 부여 정책을 정의하는 것입니다. 이를 수행할 수 있는 충분한 수준의 세분화된 시스템을 갖추는 것은 규정 및 보안 요구 사항을 모두 충족하는 데 중요합니다. 그러나 제어 기능이 너무 많으면 잠재적인 단점이 있습니다. 관리자는 사용자 문제를 해결하기 위해 애플리케이션, 사용자 ID 및 장치 정보 전반에 걸쳐 복잡한 변수 조합을 고려해야 하며, 이를 위해 로그를 면밀히 검토해야 할 수도 있습니다.\n\n당사는 더 나은 방법이 있다고 믿기 때문에 오늘부터 관리자는 Cloudflare One 정책에 사용된 모든 활성 사용자 세션과 관련 데이터를 쉽게 감사할 수 있습니다. 이렇게 하면 간단한 단일 제어판에서 [Zero Trust](https://www.cloudflare.com/learning/security/glossary/what-is-zero-trust/) 배포 문제를 해결하고 진단하는 향상된 기능을 유지하는 동시에 매우 세분화된 제어의 이점을 모두 누릴 수 있습니다. 이제 관리자는 최종 사용자에게 불편을 주거나 로그를 파헤칠 필요 없이 사용자의 브라우저에 존재하거나 동적으로 변화하는 정보에 액세스할 수 있습니다.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image4.png)\n\n### **애플리케이션 인증 및 권한 부여에 대한 빠른 지침서**\n\n_인증_ 및 _권한 부여_ 는 Zero Trust 정책이 리소스에 대한 사용자 액세스를 허용하기 전에 평가하는 두 가지 구성 요소입니다.\n\n**인증**은 사용자, 장치 또는 시스템 ID를 확인하는 프로세스입니다. 일반적인 [인증](https://www.cloudflare.com/learning/access-management/what-is-authentication/) 방법에는 사용자 이름과 비밀번호 입력, 디지털 인증서 제시 또는 지문이나 얼굴 스캔과 같은 생체 인식이 포함됩니다. [다단계 인증(MFA)](https://www.cloudflare.com/learning/access-management/what-is-multi-factor-authentication/)은 보안을 강화하기 위해 비밀번호와 하드웨어 키를 결합하는 등 두 가지 이상의 개별 인증 방법을 사용해야 합니다.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image6.png)\n\n권한 부여는 엔터티가 성공적으로 인증된 후 특정 리소스 또는 권한에 대한 액세스 권한을 부여하거나 거부하는 프로세스입니다. 인증된 엔터티가 시스템 내에서 수행할 수 있는 것과 수행할 수 없는 작업을 정의합니다.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image1-3.png)\n\n### **애플리케이션 인증/권한 부여 메커니즘**\n\n이 글에서 집중적으로 살펴볼 웹 애플리케이션은 일반적으로 HTTP 쿠키를 사용하여 인증과 권한 부여를 모두 처리합니다.\n\n**인증**\n\n1.  **로그인:** 사용자가 웹 애플리케이션에 로그인하기 위해 사용자 이름과 비밀번호를 입력하면 애플리케이션은 데이터베이스 또는 [ID 공급자(IdP)](https://www.cloudflare.com/learning/access-management/what-is-an-identity-provider/)에 대해 이러한 자격 증명을 확인합니다. 여러 인증 요소를 달성하기 위해 추가 인증 형태를 적용할 수도 있습니다. 이러한 요소가 일치하는 경우 서버 또는 외부 보안 서비스(예: Cloudflare Access)는 사용자가 인증된 것으로 간주합니다.\n2.  **쿠키/토큰 생성:** 다음으로 서버는 쿠키 또는 JSON 웹 토큰의 형태로 사용자에 대한 세션을 생성합니다. 쿠키는 사용자가 다시 인증해야 할 때까지 일정 기간 동안 유효합니다.\n3.  **쿠키 전송 및 저장:** 서버가 사용자의 브라우저에 응답을 전송할 때 쿠키에 세션 ID 및 기타 사용자 식별 정보가 포함됩니다. 그러면 브라우저는 이 쿠키를 저장합니다. 해당 쿠키는 후속 요청에서 사용자를 인식하는 데 사용합니다.\n\n**권한 부여:**\n\n1.  **후속 요청:** 웹 애플리케이션에 대한 모든 후속 요청의 경우 사용자의 브라우저는 요청에 쿠키(세션 ID 및 기타 식별 정보 포함)를 자동으로 포함합니다.\n2.  **서버 측 확인:** 서버는 쿠키에서 사용자 데이터를 검색하여 세션이 유효한지 확인합니다. 세션이 유효한 경우 서버는 해당 세션 ID와 관련된 사용자의 세부 정보 및 액세스 권한도 검색합니다.\n3.  **권한 부여 결정:** 서버는 사용자의 액세스 권한에 따라 사용자가 요청된 작업을 수행하거나 요청된 리소스에 액세스할 수 있는 권한이 있는지 여부를 결정합니다.\n\n이를 통해 사용자는 로그인 후 세션이 만료되거나 로그아웃할 때까지 모든 후속 요청에 대해 인증된 상태를 유지하고 권한 부여를 확인할 수 있습니다.\n\n최신 웹 애플리케이션에서 이 세션 상태는 일반적으로 JSON 웹 토큰(JWT) 형식으로 저장됩니다.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image8.png)\n\n### **JWT 기반 인증 디버깅**\n\nJWT는 인증 및 권한 부여 목적으로 많은 최신 웹 애플리케이션과 Cloudflare Access와 같은 [Zero Trust 네트워크 액세스(ZTNA)](https://www.cloudflare.com/learning/access-management/what-is-ztna/) 솔루션에서 활용됩니다. 여기에는 사용자 및 기타 데이터에 대한 정보를 인코딩하는 페이로드가 포함되어 있으며 변조를 방지하기 위해 서버에서 서명합니다. JWT는 종종 상태 비저장 방식으로 사용되는데, 이는 서버가 각 JWT의 사본을 보관하지 않고 요청을 수신할 때 이를 확인하고 디코딩한다는 의미입니다. JWT의 상태 비저장 특성으로 인해 사용자 세션 관리를 처리하기 위해 중앙 시스템에 의존할 필요가 없으므로 시스템에 액세스하는 사용자 수가 증가하더라도 확장성을 보장할 수 있습니다.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image8.png)\n\n그러나 JWT의 이러한 상태 비저장 특성으로 인해 사용자로부터 특정 JWT를 가져오지 않고는 JWT 기반 인증을 디버깅하기가 어려울 수 있습니다. 그 이유는 다음과 같습니다.\n\n**1\\. 토큰 특이성:** 각 JWT는 사용자 및 세션에 따라 다릅니다. 여기에는 사용자, 발급 기관, 토큰 발급 시간, 만료 시간 및 기타 데이터에 대한 정보(클레임)가 포함되어 있습니다. 따라서 문제를 디버깅하려면 문제의 원인이 되는 정확한 JWT가 필요한 경우가 많습니다.\n\n**2\\. 서버 측 레코드 없음:** JWT는 상태 비저장 방식이므로 서버는 기본적으로 세션을 저장하지 않습니다. 과거 토큰 또는 관련 상태를 기록하도록 특별히 설계된 경우가 아니라면 이를 조회할 수 없으며, 일반적으로 개인정보 보호 및 데이터 최소화 고려 사항으로 인해 검색하지 않습니다.\n\n**3\\. 일시적인 문제:** JWT는 토큰이 사용된 특정 시점과 관련된 일시적인 문제가 있을 수 있습니다. 예를 들어, 사용자가 만료된 토큰을 사용하려고 할 때 문제를 디버깅하려면 해당 특정 토큰이 있어야 합니다.\n\n**4\\. 개인정보 보호 및 보안:** JWT는 민감한 정보를 포함할 수 있으므로 신중하게 처리해야 합니다. 사용자로부터 JWT를 받을 때 문제를 디버깅하는 사람은 사용자의 개인 정보나 보안 자격 증명에 노출될 수 있습니다. 또한 사용자가 안전하지 않은 채널을 통해 개발자나 IT 헬프 데스크에 JWT를 보내면 가로챌 수 있습니다(최근 Cloudflare는 이 문제를 완화하는 데 도움이 될 수 있도록 [HAR Sanitizer](https://blog.cloudflare.com/introducing-har-sanitizer-secure-har-sharing/)를 무료로 출시했습니다).\n\n이러한 요인으로 인해 문제와 관련된 특정 토큰이 없으면 JWT 기반 인증 문제를 해결하기가 어렵습니다.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image3.png)\n\n### **ID 문제를 디버깅하는 더 나은 방법**\n\nCloudflare는 JWT를 공유하거나 HAR 파일을 주고받지 않고도 Cloudflare Zero Trust에서 사용자 ID와 관련된 문제를 디버깅할 수 있는 더 나은 방법을 구축하기 시작했습니다. 이제 관리자는 사용자의 레지스트리 ID(게이트웨이 정책에 사용됨)와 모든 활성 Access 세션을 볼 수 있습니다.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image7.png)\n\n이 세션 정보에는 IdP 클레임, 장치 상태 정보, 네트워크 컨텍스트 등 Zero Trust 에서 평가된 전체 ID가 포함됩니다. 당사는 Cloudflare Workers KV를 활용하여 Access의 인증 로직에 추가적인 부하를 주지 않고도 이 기능을 구축할 수 있었습니다. 사용자가 Access를 사용하여 인증할 때 연결된 ID는 Workers KV의 키/값 쌍에 즉시 저장됩니다. 이 모든 것은 사용자의 인증 이벤트 컨텍스트 내에서 이루어지므로 대기 시간 영향이나 서비스에 대한 의존성을 최소화할 수 있습니다.\n\n이 기능은 Zero Trust 요금제를 사용하는 모든 고객에게 제공됩니다. 최대 50명의 사용자를 위한 [무료 계정에 가입하고](https://dash.cloudflare.com/sign-up/teams) Cloudflare Zero Trust를 지금 바로 사용해 보세요! 또는 [Cloudflare 전문가와 협력하여](https://www.cloudflare.com/products/zero-trust/plans/enterprise/) 조직을 위한 SSE 또는 SASE에 대해 논의하고 Zero Trust 사용 사례를 단계별로 해결해 보세요.\n\nCloudflare는 [전체 기업 네트워크](https://www.cloudflare.com/ko-kr/network-services/) 를 보호하고, 고객이 [인터넷 규모의 애플리케이션을 효율적으로](https://workers.cloudflare.com/) 구축하도록 도우며, [웹사이트 또는 인터넷 애플리케이션](https://www.cloudflare.com/ko-kr/performance/accelerate-internet-applications/) 을 가속하고, [DDoS 공격](https://www.cloudflare.com/ko-kr/ddos/) 을 막으며, [해커의 침입을 방지](https://www.cloudflare.com/ko-kr/application-security/) 하고, [Zero Trust로의 여정을](https://www.cloudflare.com/ko-kr/products/zero-trust/) 도와드릴 수 있습니다.\n\n아무 장치로 [1.1.1.1](https://1.1.1.1/) 에 방문해서 인터넷을 더 빠르고 안전하게 만드는 Cloudflare의 무료 앱을 시작해 보세요.\n\n더 나은 인터넷을 만들겠다는 Cloudflare의 사명을 더욱 자세히 알아보려면, [이곳](https://www.cloudflare.com/ko-kr/learning/what-is-cloudflare/) 을 확인해 보세요. 새로운 방향성을 제시하는 직업을 찾고 있다면, [채용 중인 직무](https://cloudflare.com/ko-kr/careers) 목록을 확인해 보세요.\n\n[SASE (KO)](https://blog.cloudflare.com/tag/sase-ko/) [Cloudflare Zero Trust (KO)](https://blog.cloudflare.com/tag/cloudflare-zero-trust-ko/) [Product News (KO)](https://blog.cloudflare.com/tag/product-news-ko/) [Cloudflare One (KO)](https://blog.cloudflare.com/tag/cloudflare-one-ko/) [Cloudflare Workers KV (KO)](https://blog.cloudflare.com/tag/cloudflare-workers-kv-ko/)"
    },
    {
      "url": "https://blog.cloudflare.com/es-es/introducing-advanced-session-audit-capabilities-in-cloudflare-one-es-es/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/es-es/introducing-advanced-session-audit-capabilities-in-cloudflare-one-es-es/",
        "loadedTime": "2023-12-05T02:37:14.236Z",
        "referrerUrl": "https://blog.cloudflare.com/introducing-advanced-session-audit-capabilities-in-cloudflare-one/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/es-es/introducing-advanced-session-audit-capabilities-in-cloudflare-one-es-es/",
        "title": "Novedad: funciones avanzadas de auditoría de sesiones en Cloudflare One",
        "description": ": Ahora los administradores pueden auditar todas las sesiones de usuario activas y los datos asociados utilizados por sus políticas de Cloudflare One. Esto permite lo mejor de ambos mundos",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/16/2023\n6 min read\nZero Trust se fundamenta en la definición de controles granulares y políticas de autorización por aplicación, usuario y dispositivo. Para ello, es imprescindible contar con un sistema con un nivel de granularidad suficiente, a fin de cumplir con los requisitos normativos y de seguridad. Sin embargo, un número tan elevado de controles supone un posible inconveniente: para resolver los problemas de los usuarios, un administrador debe tener en cuenta una compleja combinación de variables de las aplicaciones, la identidad de los usuarios y la información de los dispositivos, y todo esto puede requerir un análisis minucioso de los registros.\nCreemos que hay una forma mejor de hacerlo. Por este motivo, a partir de hoy, los administradores pueden auditar fácilmente todas las sesiones de usuario activas y los datos asociados utilizados por sus políticas de Cloudflare One. Esto permite lo mejor de ambos mundos: controles muy granulares al mismo tiempo que se mantiene una capacidad mejorada de resolución y diagnóstico de problemas de las implementaciones Zero Trust en un único y sencillo panel de control. Ahora los administradores pueden acceder a la información que anteriormente se encontraba en el navegador de un usuario o que cambiaba dinámicamente, sin necesidad de molestar a un usuario final o de analizar los registros.\nUna introducción rápida a la autenticación y la autorización de aplicaciones\nLa autenticación y la autorización son los dos componentes que evalúa una política Zero Trust antes de permitir que un usuario acceda a un recurso.\nLa autenticación es el proceso de verificar la identidad de un usuario, un dispositivo o un sistema. Algunos de los métodos más comunes de autenticación son la especificación de nombres de usuario y contraseñas, la presentación de un certificado digital o incluso factores biométricos como una huella dactilar o un reconocimiento facial. La autenticación multifactor (MFA) requiere dos o más métodos independientes de autenticación para una mayor seguridad, por ejemplo, una clave de hardware junto con una contraseña.\nLa autorización es el proceso de otorgar o denegar el acceso a recursos o permisos específicos una vez que se ha autenticado con éxito una entidad. Define qué puede y no puede hacer la entidad autenticada en el sistema.\nMecanismos de autenticación/autorización de aplicaciones\nLas aplicaciones web, en las que nos centraremos, suelen utilizar cookies HTTP para gestionar la autenticación y la autorización.\nAutenticación:\nInicio de sesión: cuando un usuario inicia sesión en una aplicación web especificando su nombre de usuario y contraseña, la aplicación verifica estas credenciales respecto a su base de datos o en un proveedor de identidad (IdP). También es posible aplicar otros métodos de autenticación para un mecanismo de autenticación de varios factores. Si estos coinciden, el servidor o el servicio de seguridad externo (p. ej., Cloudflare Access) considera que el usuario se ha autenticado.\nCreación de cookies/tokens: el servidor crea a continuación una sesión para el usuario como una cookie o un token web JSON. La cookie es válida durante cierto tiempo, hasta que sea necesario que el usuario se vuelva a autenticar.\nEnvío y almacenamiento de cookies: el servidor envía una respuesta al navegador del usuario, y esta incluye en la cookie el ID de sesión y otra información de identificación sobre el usuario. A continuación, el navegador almacena esta cookie, que se utiliza para reconocer el usuario en sus solicitudes posteriores.\nAutorización:\nSolicitudes posteriores: para todas las solicitudes posteriores enviadas a la aplicación web, el navegador del usuario incluye automáticamente la cookie (con el ID de sesión y otra información de identificación) en la solicitud.\nVerificación del lado del servidor: el servidor recibe los datos del usuario de la cookie y comprueba si la sesión es válida. Si es válida, el servidor recupera también los detalles del usuario y sus permisos de acceso asociados con ese ID de sesión.\nDecisión de autorización: en función de los permisos de acceso del usuario, el servidor decide si se autoriza al usuario a realizar la operación solicitada o a acceder al recurso solicitado.\nDe esta forma, el usuario se mantiene autenticado (y su autorización se puede comprobar) para todas las solicitudes posteriores a su inicio de sesión, hasta que esta caduque o hasta que el usuario cierre la sesión.\nEn las aplicaciones web modernas, este estado de la sesión se suele almacenar como un token web JSON (JWT).\nDepuración de la autenticación basada en JWT\nLos JWT se utilizan en muchas aplicaciones web modernas, y en soluciones de acceso a la red Zero Trust (ZTNA) como Cloudflare Access, para la autenticación y la autorización. Un JWT incluye una carga que codifica la información acerca del usuario y posiblemente otros datos, e incluye la firma del servidor para evitar su manipulación. Los JWT se utilizan a menudo sin estado, lo que significa que el servidor no mantiene una copia de cada JWT (simplemente los verifica y descodifica cuando llegan con las solicitudes). Esta condición \"sin estado\" de los JWT significa que no tienes que depender de un sistema central para gestionar las sesiones de los usuarios, lo que evita que surjan problemas de escalabilidad cuando aumente el número de usuarios que accedan a un sistema.\nNo obstante, esta condición \"sin estado\" de los JWT hace que la depuración de la autenticación basada en JWT resulte complicada si no se dispone del JWT específico de un usuario. Estas son las razones:\n1. Especificidad de los tokens: cada JWT es específico de un usuario y de una sesión. Contiene información (afirmaciones) sobre el usuario, la autoridad emisora, la fecha y hora de emisión del token, la fecha de caducidad y posiblemente otros datos. Por lo tanto, para depurar un problema, a menudo necesitas el JWT exacto que causa el problema.\n2. No hay registros del lado del servidor: puesto que los JWT son sin estado, el servidor no almacena las sesiones por defecto. No puede buscar tokens anteriores o su estado asociado, a menos que se haya diseñado específicamente para registrarlos. Esto no suele ser el caso, debido a consideraciones acerca de la privacidad y la minimización de los datos.\n3. Problemas temporales: los problemas vinculados con los JWT pueden ser temporales (relacionados con el momento específico en que se ha utilizado el token). Por ejemplo, si un token había caducado cuando un usuario intentaba utilizarlo, necesitarías ese token específico para depurar el problema.\n4. Privacidad y seguridad: los JWT pueden contener información confidencial, por lo que se deben manejar cuidadosamente. Obtener un JWT de un usuario podría exponer su información personal o sus credenciales de seguridad a quienquiera que esté depurando el problema. Asimismo, si un usuario envía su JWT mediante un canal no seguro a un desarrollador o a un servicio de asistencia informática, este podría ser interceptado (Cloudflare lanzó recientemente una solución gratuita, HAR Sanitizer, para ayudar a mitigar este problema).\nEstos factores dificultan la resolución de problemas vinculados con la autenticación basada en JWT si no se dispone del token específico.\nUna forma mejor de depurar los problemas de identidad\nNos propusimos desarrollar una forma mejor de depurar los problemas relacionados con la identidad de un usuario en Cloudflare Zero Trust sin necesidad de compartir los JWT o los archivos HAR. Ahora los administradores pueden ver la identidad del registro de un usuario (que se utiliza para las políticas de Gateway) y todas las sesiones de Access activas.\nEsta información de la sesión incluye la identidad completa evaluada por Zero Trust, que incluye las afirmaciones del IdP, la información acerca de la postura del dispositivo, el contexto de red y más. Gracias a Cloudflare Workers KV, hemos podido desarrollar esta función sin añadir más carga a la lógica de autenticación de Access. Cuando un usuario se autentica con Access, su identidad asociada se guarda de inmediato en un par clave-valor en Workers KV. Todo esto tiene lugar en el contexto del evento de autenticación del usuario, lo que significa que el impacto en la latencia o la dependencia de un servicio externo son mínimos.\nEsta función está disponible para todos los clientes en todos los planes Zero Trust. Si deseas empezar con Cloudflare Zero Trust, regístrate hoy mismo para conseguir una cuenta gratuita para hasta 50 usuarios. O bien colabora con los expertos de Cloudflare para comentar las opciones SSE o SASE para tu organización y abordar tus casos Zero Trust paso a paso.\nProtegemos redes corporativas completas, ayudamos a los clientes a desarrollar aplicaciones web de forma eficiente, aceleramos cualquier sitio o aplicación web, prevenimos contra los ataques DDoS , mantenemos a raya a los hackers, y podemos ayudarte en tu recorrido hacia la seguridad Zero Trust. \nVisita 1.1.1.1 desde cualquier dispositivo para empezar a utilizar nuestra aplicación gratuita y beneficiarte de una navegación más rápida y segura. \nPara saber más sobre nuestra misión de ayudar a mejorar Internet, empieza aquí. Si estás buscando un nuevo rumbo profesional, consulta nuestras ofertas de empleo. \nSASE (ES) Cloudflare Zero Trust (ES) Product News (ES) Cloudflare One (ES) Cloudflare Workers KV (ES)",
      "markdown": "11/16/2023\n\n*   [![Kenny Johnson](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2020/12/0DA903DE-E315-43D5-B0DE-39870448303D.jpeg)](https://blog.cloudflare.com/author/kenny/)\n\n6 min read\n\n![](https://blog.cloudflare.com/content/images/2023/11/image5.png)\n\nZero Trust se fundamenta en la definición de controles granulares y políticas de autorización por aplicación, usuario y dispositivo. Para ello, es imprescindible contar con un sistema con un nivel de granularidad suficiente, a fin de cumplir con los requisitos normativos y de seguridad. Sin embargo, un número tan elevado de controles supone un posible inconveniente: para resolver los problemas de los usuarios, un administrador debe tener en cuenta una compleja combinación de variables de las aplicaciones, la identidad de los usuarios y la información de los dispositivos, y todo esto puede requerir un análisis minucioso de los registros.\n\nCreemos que hay una forma mejor de hacerlo. Por este motivo, a partir de hoy, los administradores pueden auditar fácilmente todas las sesiones de usuario activas y los datos asociados utilizados por sus políticas de Cloudflare One. Esto permite lo mejor de ambos mundos: controles muy granulares al mismo tiempo que se mantiene una capacidad mejorada de resolución y diagnóstico de problemas de las implementaciones [Zero Trust](https://www.cloudflare.com/learning/security/glossary/what-is-zero-trust/) en un único y sencillo panel de control. Ahora los administradores pueden acceder a la información que anteriormente se encontraba en el navegador de un usuario o que cambiaba dinámicamente, sin necesidad de molestar a un usuario final o de analizar los registros.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image4.png)\n\n### **Una introducción rápida a la autenticación y la autorización de aplicaciones**\n\nLa _autenticación_ y la _autorización_ son los dos componentes que evalúa una política Zero Trust antes de permitir que un usuario acceda a un recurso.\n\nLa **autenticación** es el proceso de verificar la identidad de un usuario, un dispositivo o un sistema. Algunos de los métodos más comunes de [autenticación](https://www.cloudflare.com/learning/access-management/what-is-authentication/) son la especificación de nombres de usuario y contraseñas, la presentación de un certificado digital o incluso factores biométricos como una huella dactilar o un reconocimiento facial. La [autenticación multifactor (MFA)](https://www.cloudflare.com/learning/access-management/what-is-multi-factor-authentication/) requiere dos o más métodos independientes de autenticación para una mayor seguridad, por ejemplo, una clave de hardware junto con una contraseña.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image6.png)\n\nLa **autorización** es el proceso de otorgar o denegar el acceso a recursos o permisos específicos una vez que se ha autenticado con éxito una entidad. Define qué puede y no puede hacer la entidad autenticada en el sistema.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image1-3.png)\n\n### **Mecanismos de autenticación/autorización de aplicaciones**\n\nLas aplicaciones web, en las que nos centraremos, suelen utilizar cookies HTTP para gestionar la autenticación y la autorización.\n\n**Autenticación**:\n\n1.  **Inicio de sesión:** cuando un usuario inicia sesión en una aplicación web especificando su nombre de usuario y contraseña, la aplicación verifica estas credenciales respecto a su base de datos o en un [proveedor de identidad (IdP)](https://www.cloudflare.com/learning/access-management/what-is-an-identity-provider/). También es posible aplicar otros métodos de autenticación para un mecanismo de autenticación de varios factores. Si estos coinciden, el servidor o el servicio de seguridad externo (p. ej., Cloudflare Access) considera que el usuario se ha autenticado.\n2.  **Creación de cookies/tokens:** el servidor crea a continuación una sesión para el usuario como una cookie o un token web JSON. La cookie es válida durante cierto tiempo, hasta que sea necesario que el usuario se vuelva a autenticar.\n3.  **Envío y almacenamiento de cookies:** el servidor envía una respuesta al navegador del usuario, y esta incluye en la cookie el ID de sesión y otra información de identificación sobre el usuario. A continuación, el navegador almacena esta cookie, que se utiliza para reconocer el usuario en sus solicitudes posteriores.\n\n**Autorización:**\n\n1.  **Solicitudes posteriores:** para todas las solicitudes posteriores enviadas a la aplicación web, el navegador del usuario incluye automáticamente la cookie (con el ID de sesión y otra información de identificación) en la solicitud.\n2.  **Verificación del lado del servidor:** el servidor recibe los datos del usuario de la cookie y comprueba si la sesión es válida. Si es válida, el servidor recupera también los detalles del usuario y sus permisos de acceso asociados con ese ID de sesión.\n3.  **Decisión de autorización:** en función de los permisos de acceso del usuario, el servidor decide si se autoriza al usuario a realizar la operación solicitada o a acceder al recurso solicitado.\n\nDe esta forma, el usuario se mantiene autenticado (y su autorización se puede comprobar) para todas las solicitudes posteriores a su inicio de sesión, hasta que esta caduque o hasta que el usuario cierre la sesión.\n\nEn las aplicaciones web modernas, este estado de la sesión se suele almacenar como un token web JSON (JWT).\n\n![](https://blog.cloudflare.com/content/images/2023/11/image8.png)\n\n### **Depuración de la autenticación basada en JWT**\n\nLos JWT se utilizan en muchas aplicaciones web modernas, y en soluciones de [acceso a la red Zero Trust (ZTNA)](https://www.cloudflare.com/learning/access-management/what-is-ztna/) como Cloudflare Access, para la autenticación y la autorización. Un JWT incluye una carga que codifica la información acerca del usuario y posiblemente otros datos, e incluye la firma del servidor para evitar su manipulación. Los JWT se utilizan a menudo sin estado, lo que significa que el servidor no mantiene una copia de cada JWT (simplemente los verifica y descodifica cuando llegan con las solicitudes). Esta condición \"sin estado\" de los JWT significa que no tienes que depender de un sistema central para gestionar las sesiones de los usuarios, lo que evita que surjan problemas de escalabilidad cuando aumente el número de usuarios que accedan a un sistema.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image2-2.png)\n\nNo obstante, esta condición \"sin estado\" de los JWT hace que la depuración de la autenticación basada en JWT resulte complicada si no se dispone del JWT específico de un usuario. Estas son las razones:\n\n**1\\. Especificidad de los tokens:** cada JWT es específico de un usuario y de una sesión. Contiene información (afirmaciones) sobre el usuario, la autoridad emisora, la fecha y hora de emisión del token, la fecha de caducidad y posiblemente otros datos. Por lo tanto, para depurar un problema, a menudo necesitas el JWT exacto que causa el problema.\n\n**2\\. No hay registros del lado del servidor:** puesto que los JWT son sin estado, el servidor no almacena las sesiones por defecto. No puede buscar tokens anteriores o su estado asociado, a menos que se haya diseñado específicamente para registrarlos. Esto no suele ser el caso, debido a consideraciones acerca de la privacidad y la minimización de los datos.\n\n**3\\. Problemas temporales:** los problemas vinculados con los JWT pueden ser temporales (relacionados con el momento específico en que se ha utilizado el token). Por ejemplo, si un token había caducado cuando un usuario intentaba utilizarlo, necesitarías ese token específico para depurar el problema.\n\n**4\\. Privacidad y seguridad:** los JWT pueden contener información confidencial, por lo que se deben manejar cuidadosamente. Obtener un JWT de un usuario podría exponer su información personal o sus credenciales de seguridad a quienquiera que esté depurando el problema. Asimismo, si un usuario envía su JWT mediante un canal no seguro a un desarrollador o a un servicio de asistencia informática, este podría ser interceptado (Cloudflare lanzó recientemente una solución gratuita, [HAR Sanitizer](https://blog.cloudflare.com/introducing-har-sanitizer-secure-har-sharing/), para ayudar a mitigar este problema).\n\nEstos factores dificultan la resolución de problemas vinculados con la autenticación basada en JWT si no se dispone del token específico.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image3.png)\n\n### **Una forma mejor de depurar los problemas de identidad**\n\nNos propusimos desarrollar una forma mejor de depurar los problemas relacionados con la identidad de un usuario en Cloudflare Zero Trust sin necesidad de compartir los JWT o los archivos HAR. Ahora los administradores pueden ver la identidad del registro de un usuario (que se utiliza para las políticas de Gateway) y todas las sesiones de Access activas.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image7.png)\n\nEsta información de la sesión incluye la identidad completa evaluada por Zero Trust, que incluye las afirmaciones del IdP, la información acerca de la postura del dispositivo, el contexto de red y más. Gracias a Cloudflare Workers KV, hemos podido desarrollar esta función sin añadir más carga a la lógica de autenticación de Access. Cuando un usuario se autentica con Access, su identidad asociada se guarda de inmediato en un par clave-valor en Workers KV. Todo esto tiene lugar en el contexto del evento de autenticación del usuario, lo que significa que el impacto en la latencia o la dependencia de un servicio externo son mínimos.\n\nEsta función está disponible para todos los clientes en todos los planes Zero Trust. Si deseas empezar con Cloudflare Zero Trust, [regístrate hoy mismo para conseguir una cuenta gratuita](https://dash.cloudflare.com/sign-up/teams) para hasta 50 usuarios. O bien [colabora con los expertos de Cloudflare](https://www.cloudflare.com/products/zero-trust/plans/enterprise/) para comentar las opciones SSE o SASE para tu organización y abordar tus casos Zero Trust paso a paso.\n\nProtegemos [redes corporativas completas](https://www.cloudflare.com/es-es/network-services/), ayudamos a los clientes a desarrollar [aplicaciones web de forma eficiente](https://workers.cloudflare.com/), aceleramos cualquier [sitio o aplicación web, prevenimos contra los ataques DDoS](https://www.cloudflare.com/es-es/ddos/) , mantenemos a raya [a los hackers](https://www.cloudflare.com/es-es/application-security/), y podemos ayudarte en [tu recorrido hacia la seguridad Zero Trust](https://www.cloudflare.com/es-es/products/zero-trust/).\n\nVisita [1.1.1.1](https://1.1.1.1/) desde cualquier dispositivo para empezar a utilizar nuestra aplicación gratuita y beneficiarte de una navegación más rápida y segura.\n\nPara saber más sobre nuestra misión de ayudar a mejorar Internet, empieza [aquí](https://www.cloudflare.com/es-es/learning/what-is-cloudflare/). Si estás buscando un nuevo rumbo profesional, consulta [nuestras ofertas de empleo](https://cloudflare.com/es-es/careers).\n\n[SASE (ES)](https://blog.cloudflare.com/tag/sase-es/) [Cloudflare Zero Trust (ES)](https://blog.cloudflare.com/tag/cloudflare-zero-trust-es/) [Product News (ES)](https://blog.cloudflare.com/tag/product-news-es/) [Cloudflare One (ES)](https://blog.cloudflare.com/tag/cloudflare-one-es/) [Cloudflare Workers KV (ES)](https://blog.cloudflare.com/tag/cloudflare-workers-kv-es/)"
    },
    {
      "url": "https://blog.cloudflare.com/de-de/introducing-advanced-session-audit-capabilities-in-cloudflare-one-de-de/",
      "crawl": {
        "loadedUrl": "https://blog.cloudflare.com/de-de/introducing-advanced-session-audit-capabilities-in-cloudflare-one-de-de/",
        "loadedTime": "2023-12-05T02:37:14.635Z",
        "referrerUrl": "https://blog.cloudflare.com/introducing-advanced-session-audit-capabilities-in-cloudflare-one/",
        "depth": 2,
        "httpStatusCode": 200
      },
      "metadata": {
        "canonicalUrl": "https://blog.cloudflare.com/de-de/introducing-advanced-session-audit-capabilities-in-cloudflare-one-de-de/",
        "title": "Einführung der erweiterten Sitzungsprüfungsfunktionen bei Cloudflare One",
        "description": "Administratoren können jetzt alle aktiven Nutzersitzungen und die damit verbundenen Daten, die von ihren Cloudflare One-Richtlinien verwendet werden, einfach überprüfen. Sie erhalten somit das Beste aus beiden Welten: extrem fein justierte Kontrollen und gleichzeitig verbesserte Fehlerbehebungs- und Diagnosemöglichkeiten.",
        "author": null,
        "keywords": null,
        "languageCode": "en"
      },
      "screenshotUrl": null,
      "text": "11/16/2023\n6 min read\nDurch das Definieren von genau auf die jeweilige Anwendung, den jeweiligen Nutzer und das jeweilige Geräte abgestimmten Kontrollen und Autorisierungsrichtlinien wird das Fundament für Zero Trust geschaffen. Damit die Vorschriften und Sicherheitsvorgaben eingehalten werden können, muss ein System über ausreichend umfangreiche Möglichkeiten zur Feinabstimmung verfügen. Eine große Zahl von Kontrollen hat unter Umständen aber einen Haken: Um Nutzerprobleme zu beheben, muss ein Administrator eine komplexe Kombination von Variablen in Bezug auf Anwendungen, Nutzeridentität und Geräteinformationen berücksichtigen. Das kann bedeuten, dass Protokolle sorgfältig überprüft werden müssen.\nUnserer Meinung nach gibt es dafür aber einen besseren Weg. Ab heute können Administratoren alle aktiven Nutzersitzungen und die damit verbundenen Daten, die von ihren Cloudflare One-Richtlinien verwendet werden, einfach kontrollieren. Dadurch erhält man das Beste aus beiden Welten: extrem fein justierte Kontrollen und gleichzeitig verbesserte Fehlerbehebungs- und Diagnosemöglichkeiten für Zero Trust-Implementierungen mit einer einzigen, einfachen Steuerungsebene. Damit können Administratoren nun auf Informationen zugreifen, die zuvor im Browser des Nutzers gespeichert wurden oder sich dynamisch geändert haben, ohne einen Endnutzer zu belästigen oder Protokolle durchforsten zu müssen.\nEine kurze Einführung in die Authentifizierung und Autorisierung bei Anwendungen\nAuthentifizierung und Autorisierung sind die beiden Komponenten, die von einer Zero Trust-Richtlinie ausgewertet werden, bevor diese einem Nutzer den Zugriff auf eine Ressource erlaubt.\nBei der Authentifizierung wird die Identität eines Nutzers, eines Geräts oder eines Systems überprüft. Zu den üblichen Authentifizierungsmethoden gehören die Eingabe von Benutzernamen und Kennwörtern, die Vorlage eines digitalen Zertifikats oder sogar biometrischer Daten wie Fingerabdrücke oder Gesichtsscans. Bei der Multi-Faktor-Authentifizierung (MFA) werden zwei oder mehr separate Authentifizierungsmethoden vorgeschrieben, um die Sicherheit zu erhöhen, z. B. ein Security-Token in Kombination mit einem Passwort.\nUnter Autorisierung versteht man das Gewähren oder Verweigern des Zugriffs auf bestimmte Ressourcen oder der erforderlichen Berechtigungen nach erfolgreicher Authentifizierung. Dabei wird festgelegt, was die authentifizierte Einheit innerhalb des Systems tun darf und was nicht.\nMechanismen der Authentifizierung/Autorisierung bei Anwendungen\nIm Fall von Webanwendungen, auf die hier der Fokus gelegt werden soll, werden im Allgemeinen HTTP-Cookies zur Durchführung sowohl der Authentifizierung als auch der Autorisierung verwendet.\nAuthentifizierung:\nLogin: Meldet sich ein User bei einer Webanwendung durch Eingabe seines Benutzernamens und seines Kennworts an, gleicht die Applikation diese Anmeldedaten mit ihrer Datenbank oder einem Identitätsanbieter (IdP) ab. Bei der Authentifizierung können auch zusätzliche Faktoren einbezogen werden. Wird eine Übereinstimmung festgestellt, erachtet der Server oder der externe Sicherheitsdienst (z. B. Cloudflare Access) den Nutzer als authentifiziert.\nCookie/Token-Erstellung: Der Server erstellt dann eine Sitzung für den Nutzer in Form eines Cookies oder JSON-Web-Token. Das Cookie ist für einen bestimmten Zeitraum gültig, nach dessen Ablauf sich der Nutzer erneut authentifizieren muss.\nSenden und Speichern von Cookies: Der Server sendet eine Antwort an den Browser des Nutzers zurück, die die Sitzungs-ID und andere identifizierende Informationen zu dem Nutzer in dem Cookie enthält. Der Browser speichert dann dieses Cookie. Damit kann der Nutzer bei weiteren Anfragen wiedererkannt werden.\nAutorisierung:\nFolgende Anfragen: Bei allen folgenden Anfragen an die Webanwendung fügt der Browser des Nutzers automatisch das Cookie (mit der Sitzungs-ID und anderen identifizierenden Informationen) in die Anfrage ein.\nServerseitige Verifizierung: Der Server erhält die Nutzerdaten aus dem Cookie und prüft, ob die Sitzung gültig ist. Ist das der Fall, ruft er auch die Nutzerdaten und die mit der Sitzungs-ID verbundenen Zugriffsberechtigungen ab.\nBerechtigungsentscheidung: Auf Grundlage der Zugriffsberechtigungen des Nutzers entscheidet der Server, ob der Nutzer berechtigt ist, die angeforderte Operation durchzuführen oder auf die angeforderte Ressource zuzugreifen.\nSo bleibt die Authentifizierung des Nutzers bei allen folgenden Anfragen bestehen (und seine Berechtigung kann überprüft werden), bis die Sitzung abläuft oder er sich ausloggt.\nIn modernen Webanwendungen wird dieser Sitzungsstatus in der Regel in Form eines JSON-Web-Token (JWT) gespeichert.\nFehlersuche bei JWT-basierter Authentifizierung\nJWT werden in vielen modernen Webanwendungen und Zero Trust Network Access (ZTNA)-Lösungen wie Cloudflare Access zur Authentifizierung und Autorisierung verwendet. Ein JWT enthält eine Payload, also Informationen zu dem Nutzer und möglicherweise andere Daten in verschlüsselter Form, und wird vom Server signiert, um Manipulationen zu verhindern. JWT werden häufig zustandslos verwendet, d. h. der Server speichert keine Kopie sämtlicher JWT, sondern verifiziert und entschlüsselt einfach jedes JWT, das mit Anfragen eintrifft. Aufgrund ihrer Zustandslosigkeit sind JWT für die Verwaltung von Nutzersitzungen nicht auf ein zentrales System angewiesen, wodurch Skalierungsprobleme vermieden werden, wenn die Zahl der auf ein System zugreifenden Nutzer steigt.\nAllerdings erschwert die gleiche Eigenschaft die Fehlersuche bei der JWT-basierten Authentifizierung, wenn man nicht das spezifische JWT eines Nutzers erhält. Das hat folgende Gründe:\n1. Spezifität des Token: Jedes JWT ist spezifisch für einen Nutzer und eine Sitzung. Es enthält Informationen (Behauptungen) bezüglich des Nutzers, der ausstellenden Instanz, der Ausstellungszeit des Token, der Ablaufzeit und möglicherweise weitere Daten. Um ein Problem beheben zu können, wird deshalb oft genau das JWT benötigt, das es verursacht.\n2. Keine serverseitigen Aufzeichnungen: Da JWT zustandslos sind, werden Sitzungen vom Server standardmäßig nicht gespeichert. Er kann keine früheren Token oder deren zugehörigen Status zum Abgleich abrufen – es sei denn, er wurde speziell dafür entwickelt, sie zu protokollieren, was aus Gründen des Datenschutzes und der Datenminimierung normalerweise nicht der Fall ist.\n3. Vorübergehende Probleme: JWT-Probleme treten unter Umständen nur vorübergehend auf und hängen daher möglicherweise mit dem spezifischen Zeitpunkt zusammen, zu dem das Token verwendet wurde. Wenn beispielsweise ein Token abgelaufen war, als ein Nutzer versucht hat, es zu verwenden, benötigt man genau dieses Token zur Behebung des Problems.\n4. Datenschutz und Sicherheit: JWT können sensible Informationen enthalten, deshalb sollte man damit sorgfältig umgehen. Erhält man ein JWT von einem Nutzer, werden dabei eventuell dessen personenbezogene Informationen oder Sicherheitsdaten demjenigen offengelegt, der das Problem behebt. Wenn ein Nutzer sein JWT auf unsicherem Weg an einen Entwickler oder ein IT-Helpdesk sendet, könnte es außerdem abgefangen werden. (Cloudflare hat kürzlich das kostenlose Tool HAR Sanitizer auf den Markt gebracht, um das zu verhindern.)\nDiese Faktoren erschweren die Fehlersuche bei Problemen mit JWT-basierter Authentifizierung, wenn man keinen Zugriff auf das fragliche Token hat.\nEine bessere Methode zur Fehlerbehebung bei Problemen mit der Identitätsüberprüfung\nWir haben uns vorgenommen, eine bessere Lösung für Probleme im Zusammenhang mit der Nutzeridentität bei Cloudflare Zero Trust anzubieten, für die keine JWT oder HAR-Dateien hin und her geschickt werden müssen. Administratoren können sich jetzt die Registry-Identität eines Nutzers (die für Gateway-Richtlinien verwendet wird) und alle aktiven Access-Sitzungen anzeigen lassen.\nDiese Sitzungsinformationen umfassen die vollständige Identität, die von unserer Zero Trust-Lösung ausgewertet wird, einschließlich Identitätsanbieter-Anforderungen, Informationen zum Gerätestatus, Netzwerkkontext und mehr. Durch die Verwendung von Cloudflare Workers KV ist es uns gelungen, diese Funktion ohne zusätzliche Belastung der Authentifizierungslogik von Access zu entwickeln. Wenn sich ein Nutzer bei Access authentifiziert, wird die zugehörige Identität sofort in einem Schlüssel-Wert-Paar in Workers KV gespeichert. Dies geschieht alles im Kontext des Ereignisses der Nutzerauthentifizierung, sodass die Auswirkungen auf die Latenz oder die Abhängigkeit von einem externen Dienst minimal sind.\nDiese Funktion ist für alle Kunden und sämtliche Zero Trust-Tarife verfügbar. Wenn Sie mit Cloudflare Zero Trust loslegen möchten, melden Sie sich noch heute für ein kostenloses Konto an, das von bis zu 50 Nutzern verwendet werden kann. Oder arbeiten Sie mit Cloudflare-Experten zusammen, um über eine SSE- oder SASE-Lösung für Ihr Unternehmen zu sprechen und Ihre Zero Trust-Anwendungsfälle Schritt für Schritt anzugehen.\nWir schützen ganze Firmennetzwerke, helfen Kunden dabei, Internet-Anwendungen effizient zu entwickeln, jede Website oder Internetanwendung zu beschleunigen, DDoS-Angriffe abzuwehren, Hacker in Schach zu halten und unterstützen Sie bei Ihrer Umstellung auf Zero-Trust. \nBesuchen Sie 1.1.1.1 von einem beliebigen Gerät aus und nutzen Sie unsere kostenlose App, die Ihr Internet schneller und sicherer macht. \nWeitere Informationen über unsere Mission, ein besseres Internet zu schaffen, finden Sie hier. Sie möchten sich beruflich neu orientieren? Dann werfen Sie doch einen Blick auf unsere offenen Stellen. \nSASE (DE) Cloudflare Zero Trust (DE) Product News (DE) Cloudflare One (DE) Cloudflare Workers KV (DE)",
      "markdown": "11/16/2023\n\n*   [![Kenny Johnson](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2020/12/0DA903DE-E315-43D5-B0DE-39870448303D.jpeg)](https://blog.cloudflare.com/author/kenny/)\n\n6 min read\n\n![](https://blog.cloudflare.com/content/images/2023/11/image5.png)\n\nDurch das Definieren von genau auf die jeweilige Anwendung, den jeweiligen Nutzer und das jeweilige Geräte abgestimmten Kontrollen und Autorisierungsrichtlinien wird das Fundament für Zero Trust geschaffen. Damit die Vorschriften und Sicherheitsvorgaben eingehalten werden können, muss ein System über ausreichend umfangreiche Möglichkeiten zur Feinabstimmung verfügen. Eine große Zahl von Kontrollen hat unter Umständen aber einen Haken: Um Nutzerprobleme zu beheben, muss ein Administrator eine komplexe Kombination von Variablen in Bezug auf Anwendungen, Nutzeridentität und Geräteinformationen berücksichtigen. Das kann bedeuten, dass Protokolle sorgfältig überprüft werden müssen.\n\nUnserer Meinung nach gibt es dafür aber einen besseren Weg. Ab heute können Administratoren alle aktiven Nutzersitzungen und die damit verbundenen Daten, die von ihren Cloudflare One-Richtlinien verwendet werden, einfach kontrollieren. Dadurch erhält man das Beste aus beiden Welten: extrem fein justierte Kontrollen und gleichzeitig verbesserte Fehlerbehebungs- und Diagnosemöglichkeiten für [Zero Trust](https://www.cloudflare.com/learning/security/glossary/what-is-zero-trust/)\\-Implementierungen mit einer einzigen, einfachen Steuerungsebene. Damit können Administratoren nun auf Informationen zugreifen, die zuvor im Browser des Nutzers gespeichert wurden oder sich dynamisch geändert haben, ohne einen Endnutzer zu belästigen oder Protokolle durchforsten zu müssen.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image4.png)\n\n### **Eine kurze Einführung in die Authentifizierung und Autorisierung bei Anwendungen**\n\n_Authentifizierung_ und _Autorisierung_ sind die beiden Komponenten, die von einer Zero Trust-Richtlinie ausgewertet werden, bevor diese einem Nutzer den Zugriff auf eine Ressource erlaubt.\n\nBei der **Authentifizierung** wird die Identität eines Nutzers, eines Geräts oder eines Systems überprüft. Zu den üblichen [Authentifizierungsmethoden](https://www.cloudflare.com/learning/access-management/what-is-authentication/) gehören die Eingabe von Benutzernamen und Kennwörtern, die Vorlage eines digitalen Zertifikats oder sogar biometrischer Daten wie Fingerabdrücke oder Gesichtsscans. Bei der [Multi-Faktor-Authentifizierung (MFA)](https://www.cloudflare.com/learning/access-management/what-is-multi-factor-authentication/) werden zwei oder mehr separate Authentifizierungsmethoden vorgeschrieben, um die Sicherheit zu erhöhen, z. B. ein Security-Token in Kombination mit einem Passwort.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image6.png)\n\nUnter **Autorisierung** versteht man das Gewähren oder Verweigern des Zugriffs auf bestimmte Ressourcen oder der erforderlichen Berechtigungen nach erfolgreicher Authentifizierung. Dabei wird festgelegt, was die authentifizierte Einheit innerhalb des Systems tun darf und was nicht.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image1-3.png)\n\n### **Mechanismen der Authentifizierung/Autorisierung bei Anwendungen**\n\nIm Fall von Webanwendungen, auf die hier der Fokus gelegt werden soll, werden im Allgemeinen HTTP-Cookies zur Durchführung sowohl der Authentifizierung als auch der Autorisierung verwendet.\n\n**Authentifizierung:**\n\n1.  **Login:** Meldet sich ein User bei einer Webanwendung durch Eingabe seines Benutzernamens und seines Kennworts an, gleicht die Applikation diese Anmeldedaten mit ihrer Datenbank oder einem [Identitätsanbieter (IdP)](https://www.cloudflare.com/learning/access-management/what-is-an-identity-provider/) ab. Bei der Authentifizierung können auch zusätzliche Faktoren einbezogen werden. Wird eine Übereinstimmung festgestellt, erachtet der Server oder der externe Sicherheitsdienst (z. B. Cloudflare Access) den Nutzer als authentifiziert.\n2.  **Cookie/Token-Erstellung:** Der Server erstellt dann eine Sitzung für den Nutzer in Form eines Cookies oder JSON-Web-Token. Das Cookie ist für einen bestimmten Zeitraum gültig, nach dessen Ablauf sich der Nutzer erneut authentifizieren muss.\n3.  **Senden und Speichern von Cookies**: Der Server sendet eine Antwort an den Browser des Nutzers zurück, die die Sitzungs-ID und andere identifizierende Informationen zu dem Nutzer in dem Cookie enthält. Der Browser speichert dann dieses Cookie. Damit kann der Nutzer bei weiteren Anfragen wiedererkannt werden.\n\n**Autorisierung:**\n\n1.  **Folgende Anfragen:** Bei allen folgenden Anfragen an die Webanwendung fügt der Browser des Nutzers automatisch das Cookie (mit der Sitzungs-ID und anderen identifizierenden Informationen) in die Anfrage ein.\n2.  **Serverseitige Verifizierung:** Der Server erhält die Nutzerdaten aus dem Cookie und prüft, ob die Sitzung gültig ist. Ist das der Fall, ruft er auch die Nutzerdaten und die mit der Sitzungs-ID verbundenen Zugriffsberechtigungen ab.\n3.  **Berechtigungsentscheidung:** Auf Grundlage der Zugriffsberechtigungen des Nutzers entscheidet der Server, ob der Nutzer berechtigt ist, die angeforderte Operation durchzuführen oder auf die angeforderte Ressource zuzugreifen.\n\nSo bleibt die Authentifizierung des Nutzers bei allen folgenden Anfragen bestehen (und seine Berechtigung kann überprüft werden), bis die Sitzung abläuft oder er sich ausloggt.\n\nIn modernen Webanwendungen wird dieser Sitzungsstatus in der Regel in Form eines JSON-Web-Token (JWT) gespeichert.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image8.png)\n\n### **Fehlersuche bei JWT-basierter Authentifizierung**\n\nJWT werden in vielen modernen Webanwendungen und [Zero Trust Network Access (ZTNA)](https://www.cloudflare.com/learning/access-management/what-is-ztna/)\\-Lösungen wie Cloudflare Access zur Authentifizierung und Autorisierung verwendet. Ein JWT enthält eine Payload, also Informationen zu dem Nutzer und möglicherweise andere Daten in verschlüsselter Form, und wird vom Server signiert, um Manipulationen zu verhindern. JWT werden häufig zustandslos verwendet, d. h. der Server speichert keine Kopie sämtlicher JWT, sondern verifiziert und entschlüsselt einfach jedes JWT, das mit Anfragen eintrifft. Aufgrund ihrer Zustandslosigkeit sind JWT für die Verwaltung von Nutzersitzungen nicht auf ein zentrales System angewiesen, wodurch Skalierungsprobleme vermieden werden, wenn die Zahl der auf ein System zugreifenden Nutzer steigt.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image2-2.png)\n\nAllerdings erschwert die gleiche Eigenschaft die Fehlersuche bei der JWT-basierten Authentifizierung, wenn man nicht das spezifische JWT eines Nutzers erhält. Das hat folgende Gründe:\n\n**1\\. Spezifität des Token:** Jedes JWT ist spezifisch für einen Nutzer und eine Sitzung. Es enthält Informationen (Behauptungen) bezüglich des Nutzers, der ausstellenden Instanz, der Ausstellungszeit des Token, der Ablaufzeit und möglicherweise weitere Daten. Um ein Problem beheben zu können, wird deshalb oft genau das JWT benötigt, das es verursacht.\n\n**2\\. Keine serverseitigen Aufzeichnungen:** Da JWT zustandslos sind, werden Sitzungen vom Server standardmäßig nicht gespeichert. Er kann keine früheren Token oder deren zugehörigen Status zum Abgleich abrufen – es sei denn, er wurde speziell dafür entwickelt, sie zu protokollieren, was aus Gründen des Datenschutzes und der Datenminimierung normalerweise nicht der Fall ist.\n\n**3\\. Vorübergehende Probleme:** JWT-Probleme treten unter Umständen nur vorübergehend auf und hängen daher möglicherweise mit dem spezifischen Zeitpunkt zusammen, zu dem das Token verwendet wurde. Wenn beispielsweise ein Token abgelaufen war, als ein Nutzer versucht hat, es zu verwenden, benötigt man genau dieses Token zur Behebung des Problems.\n\n**4\\. Datenschutz und Sicherheit:** JWT können sensible Informationen enthalten, deshalb sollte man damit sorgfältig umgehen. Erhält man ein JWT von einem Nutzer, werden dabei eventuell dessen personenbezogene Informationen oder Sicherheitsdaten demjenigen offengelegt, der das Problem behebt. Wenn ein Nutzer sein JWT auf unsicherem Weg an einen Entwickler oder ein IT-Helpdesk sendet, könnte es außerdem abgefangen werden. (Cloudflare hat kürzlich das kostenlose Tool [HAR Sanitizer](https://blog.cloudflare.com/introducing-har-sanitizer-secure-har-sharing/) auf den Markt gebracht, um das zu verhindern.)\n\nDiese Faktoren erschweren die Fehlersuche bei Problemen mit JWT-basierter Authentifizierung, wenn man keinen Zugriff auf das fragliche Token hat.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image3.png)\n\n### **Eine bessere Methode zur Fehlerbehebung bei Problemen mit der Identitätsüberprüfung**\n\nWir haben uns vorgenommen, eine bessere Lösung für Probleme im Zusammenhang mit der Nutzeridentität bei Cloudflare Zero Trust anzubieten, für die keine JWT oder HAR-Dateien hin und her geschickt werden müssen. Administratoren können sich jetzt die Registry-Identität eines Nutzers (die für Gateway-Richtlinien verwendet wird) und alle aktiven Access-Sitzungen anzeigen lassen.\n\n![](https://blog.cloudflare.com/content/images/2023/11/image7.png)\n\nDiese Sitzungsinformationen umfassen die vollständige Identität, die von unserer Zero Trust-Lösung ausgewertet wird, einschließlich Identitätsanbieter-Anforderungen, Informationen zum Gerätestatus, Netzwerkkontext und mehr. Durch die Verwendung von Cloudflare Workers KV ist es uns gelungen, diese Funktion ohne zusätzliche Belastung der Authentifizierungslogik von Access zu entwickeln. Wenn sich ein Nutzer bei Access authentifiziert, wird die zugehörige Identität sofort in einem Schlüssel-Wert-Paar in Workers KV gespeichert. Dies geschieht alles im Kontext des Ereignisses der Nutzerauthentifizierung, sodass die Auswirkungen auf die Latenz oder die Abhängigkeit von einem externen Dienst minimal sind.\n\nDiese Funktion ist für alle Kunden und sämtliche Zero Trust-Tarife verfügbar. Wenn Sie mit Cloudflare Zero Trust loslegen möchten, melden Sie sich noch heute für ein [kostenloses Konto](https://dash.cloudflare.com/sign-up/teams) an, das von bis zu 50 Nutzern verwendet werden kann. Oder [arbeiten Sie mit Cloudflare-Experten zusammen](https://www.cloudflare.com/products/zero-trust/plans/enterprise/), um über eine SSE- oder SASE-Lösung für Ihr Unternehmen zu sprechen und Ihre Zero Trust-Anwendungsfälle Schritt für Schritt anzugehen.\n\nWir schützen [ganze Firmennetzwerke](https://www.cloudflare.com/de-de/network-services/), helfen Kunden dabei, [Internet-Anwendungen effizient zu entwickeln](https://workers.cloudflare.com/), jede [Website oder Internetanwendung zu beschleunigen,](https://www.cloudflare.com/de-de/performance/accelerate-internet-applications/) [DDoS-Angriffe abzuwehren](https://www.cloudflare.com/ddos/), [Hacker in Schach zu halten](https://www.cloudflare.com/de-de/application-security/) und unterstützen Sie bei [Ihrer Umstellung auf Zero-Trust](https://www.cloudflare.com/de-de/products/zero-trust/).\n\nBesuchen Sie [1.1.1.1](https://1.1.1.1/) von einem beliebigen Gerät aus und nutzen Sie unsere kostenlose App, die Ihr Internet schneller und sicherer macht.\n\nWeitere Informationen über unsere Mission, ein besseres Internet zu schaffen, finden Sie [hier](https://www.cloudflare.com/de-de/learning/what-is-cloudflare/). Sie möchten sich beruflich neu orientieren? Dann werfen Sie doch einen Blick auf [unsere offenen Stellen](https://cloudflare.com/de-de/careers).\n\n[SASE (DE)](https://blog.cloudflare.com/tag/sase-de/) [Cloudflare Zero Trust (DE)](https://blog.cloudflare.com/tag/cloudflare-zero-trust-de/) [Product News (DE)](https://blog.cloudflare.com/tag/product-news-de/) [Cloudflare One (DE)](https://blog.cloudflare.com/tag/cloudflare-one-de/) [Cloudflare Workers KV (DE)](https://blog.cloudflare.com/tag/workers-kv-de/)"
    }
  ]
}